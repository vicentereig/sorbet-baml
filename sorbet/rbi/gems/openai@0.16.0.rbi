# typed: true

# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `openai` gem.
# Please instead update this file by running `bin/tapioca gem openai`.


# typed: strong

# Pass through other raw events
# frozen_string_literal: true

module OpenAI
  AllModels = OpenAI::Models::AllModels
  ArrayOf = OpenAI::Helpers::StructuredOutput::ArrayOf
  Audio = OpenAI::Models::Audio
  AudioModel = OpenAI::Models::AudioModel
  AudioResponseFormat = OpenAI::Models::AudioResponseFormat
  AutoFileChunkingStrategyParam = OpenAI::Models::AutoFileChunkingStrategyParam
  BaseModel = OpenAI::Helpers::StructuredOutput::BaseModel
  Batch = OpenAI::Models::Batch
  BatchCancelParams = OpenAI::Models::BatchCancelParams
  BatchCreateParams = OpenAI::Models::BatchCreateParams
  BatchError = OpenAI::Models::BatchError
  BatchListParams = OpenAI::Models::BatchListParams
  BatchRequestCounts = OpenAI::Models::BatchRequestCounts
  BatchRetrieveParams = OpenAI::Models::BatchRetrieveParams
  Beta = OpenAI::Models::Beta
  Boolean = OpenAI::Helpers::StructuredOutput::Boolean
  Chat = OpenAI::Models::Chat
  ChatModel = OpenAI::Models::ChatModel

  class Client < OpenAI::Internal::Transport::BaseClient
    sig { returns(String) }
    attr_reader :api_key

    sig { returns(OpenAI::Resources::Audio) }
    attr_reader :audio

    sig { returns(OpenAI::Resources::Batches) }
    attr_reader :batches

    sig { returns(OpenAI::Resources::Beta) }
    attr_reader :beta

    sig { returns(OpenAI::Resources::Chat) }
    attr_reader :chat

    sig { returns(OpenAI::Resources::Completions) }
    attr_reader :completions

    sig { returns(OpenAI::Resources::Containers) }
    attr_reader :containers

    sig { returns(OpenAI::Resources::Embeddings) }
    attr_reader :embeddings

    sig { returns(OpenAI::Resources::Evals) }
    attr_reader :evals

    sig { returns(OpenAI::Resources::Files) }
    attr_reader :files

    sig { returns(OpenAI::Resources::FineTuning) }
    attr_reader :fine_tuning

    sig { returns(OpenAI::Resources::Graders) }
    attr_reader :graders

    sig { returns(OpenAI::Resources::Images) }
    attr_reader :images

    sig { returns(OpenAI::Resources::Models) }
    attr_reader :models

    sig { returns(OpenAI::Resources::Moderations) }
    attr_reader :moderations

    sig { returns(T.nilable(String)) }
    attr_reader :organization

    sig { returns(T.nilable(String)) }
    attr_reader :project

    sig { returns(OpenAI::Resources::Responses) }
    attr_reader :responses

    sig { returns(OpenAI::Resources::Uploads) }
    attr_reader :uploads

    sig { returns(OpenAI::Resources::VectorStores) }
    attr_reader :vector_stores

    sig { returns(OpenAI::Resources::Webhooks) }
    attr_reader :webhooks

    private

    # @api private
    sig { override.returns(T::Hash[String, String]) }
    def auth_headers; end

    class << self
      # Creates and returns a new client for interacting with the API.
      sig do
        params(
          api_key: T.nilable(String),
          organization: T.nilable(String),
          project: T.nilable(String),
          base_url: T.nilable(String),
          max_retries: Integer,
          timeout: Float,
          initial_retry_delay: Float,
          max_retry_delay: Float
        ).returns(T.attached_class)
      end
      def new(
        api_key: ENV["OPENAI_API_KEY"], # Defaults to `ENV["OPENAI_API_KEY"]`
        organization: ENV["OPENAI_ORG_ID"], # Defaults to `ENV["OPENAI_ORG_ID"]`
        project: ENV["OPENAI_PROJECT_ID"], # Defaults to `ENV["OPENAI_PROJECT_ID"]`
        base_url: ENV["OPENAI_BASE_URL"], # Override the default base URL for the API, e.g.,
                                          # `"https://api.example.com/v2/"`. Defaults to `ENV["OPENAI_BASE_URL"]`
        max_retries: OpenAI::Client::DEFAULT_MAX_RETRIES, # Max number of retries to attempt after a failed retryable request.
        timeout: OpenAI::Client::DEFAULT_TIMEOUT_IN_SECONDS,
        initial_retry_delay: OpenAI::Client::DEFAULT_INITIAL_RETRY_DELAY,
        max_retry_delay: OpenAI::Client::DEFAULT_MAX_RETRY_DELAY
); end
    end

    DEFAULT_INITIAL_RETRY_DELAY = T.let(0.5, Float)
    DEFAULT_MAX_RETRIES = 2
    DEFAULT_MAX_RETRY_DELAY = T.let(8.0, Float)
    DEFAULT_TIMEOUT_IN_SECONDS = T.let(600.0, Float)
  end

  ComparisonFilter = OpenAI::Models::ComparisonFilter
  Completion = OpenAI::Models::Completion
  CompletionChoice = OpenAI::Models::CompletionChoice
  CompletionCreateParams = OpenAI::Models::CompletionCreateParams
  CompletionUsage = OpenAI::Models::CompletionUsage
  CompoundFilter = OpenAI::Models::CompoundFilter
  ContainerCreateParams = OpenAI::Models::ContainerCreateParams
  ContainerDeleteParams = OpenAI::Models::ContainerDeleteParams
  ContainerListParams = OpenAI::Models::ContainerListParams
  ContainerRetrieveParams = OpenAI::Models::ContainerRetrieveParams
  Containers = OpenAI::Models::Containers
  CreateEmbeddingResponse = OpenAI::Models::CreateEmbeddingResponse
  Embedding = OpenAI::Models::Embedding
  EmbeddingCreateParams = OpenAI::Models::EmbeddingCreateParams
  EmbeddingModel = OpenAI::Models::EmbeddingModel
  EnumOf = OpenAI::Helpers::StructuredOutput::EnumOf
  ErrorObject = OpenAI::Models::ErrorObject

  module Errors
    class APIConnectionError < OpenAI::Errors::APIError
      sig { void }
      attr_accessor :body

      sig { void }
      attr_accessor :code

      sig { void }
      attr_accessor :param

      sig { void }
      attr_accessor :status

      sig { void }
      attr_accessor :type

      class << self
        # @api private
        sig do
          params(
            url: URI::Generic,
            status: NilClass,
            body: NilClass,
            request: NilClass,
            response: NilClass,
            message: T.nilable(String)
          ).returns(T.attached_class)
        end
        def new(url:, status: nil, body: nil, request: nil, response: nil, message: "Connection error."); end
      end
    end

    class APIError < OpenAI::Errors::Error
      sig { returns(T.nilable(T.anything)) }
      attr_accessor :body

      sig { returns(T.nilable(String)) }
      attr_accessor :code

      sig { returns(T.nilable(String)) }
      attr_accessor :param

      sig { returns(T.nilable(Integer)) }
      attr_accessor :status

      sig { returns(T.nilable(String)) }
      attr_accessor :type

      sig { returns(URI::Generic) }
      attr_accessor :url

      class << self
        # @api private
        sig do
          params(
            url: URI::Generic,
            status: T.nilable(Integer),
            body: T.nilable(Object),
            request: NilClass,
            response: NilClass,
            message: T.nilable(String)
          ).returns(T.attached_class)
        end
        def new(url:, status: nil, body: nil, request: nil, response: nil, message: nil); end
      end
    end

    class APIStatusError < OpenAI::Errors::APIError
      sig { returns(T.nilable(String)) }
      attr_accessor :code

      sig { returns(T.nilable(String)) }
      attr_accessor :param

      sig { returns(Integer) }
      attr_accessor :status

      sig { returns(T.nilable(String)) }
      attr_accessor :type

      class << self
        # @api private
        sig do
          params(
            url: URI::Generic,
            status: Integer,
            body: T.nilable(Object),
            request: NilClass,
            response: NilClass,
            message: T.nilable(String)
          ).returns(T.attached_class)
        end
        def for(url:, status:, body:, request:, response:, message: nil); end

        # @api private
        sig do
          params(
            url: URI::Generic,
            status: Integer,
            body: T.nilable(Object),
            request: NilClass,
            response: NilClass,
            message: T.nilable(String)
          ).returns(T.attached_class)
        end
        def new(url:, status:, body:, request:, response:, message: nil); end
      end
    end

    class APITimeoutError < OpenAI::Errors::APIConnectionError
      class << self
        # @api private
        sig do
          params(
            url: URI::Generic,
            status: NilClass,
            body: NilClass,
            request: NilClass,
            response: NilClass,
            message: T.nilable(String)
          ).returns(T.attached_class)
        end
        def new(url:, status: nil, body: nil, request: nil, response: nil, message: "Request timed out."); end
      end
    end

    class AuthenticationError < OpenAI::Errors::APIStatusError
      HTTP_STATUS = 401
    end

    class BadRequestError < OpenAI::Errors::APIStatusError
      HTTP_STATUS = 400
    end

    class ConflictError < OpenAI::Errors::APIStatusError
      HTTP_STATUS = 409
    end

    class ConversionError < OpenAI::Errors::Error
      sig { returns(T.nilable(StandardError)) }
      def cause; end

      class << self
        # @api private
        sig do
          params(
            on: T::Class[StandardError],
            method: Symbol,
            target: T.anything,
            value: T.anything,
            cause: T.nilable(StandardError)
          ).returns(T.attached_class)
        end
        def new(on:, method:, target:, value:, cause: nil); end
      end
    end

    class Error < StandardError
      sig { returns(T.nilable(StandardError)) }
      attr_accessor :cause
    end

    class InternalServerError < OpenAI::Errors::APIStatusError
      HTTP_STATUS = T.let((500..), T::Range[Integer])
    end

    class NotFoundError < OpenAI::Errors::APIStatusError
      HTTP_STATUS = 404
    end

    class PermissionDeniedError < OpenAI::Errors::APIStatusError
      HTTP_STATUS = 403
    end

    class RateLimitError < OpenAI::Errors::APIStatusError
      HTTP_STATUS = 429
    end

    class UnprocessableEntityError < OpenAI::Errors::APIStatusError
      HTTP_STATUS = 422
    end
  end

  EvalCreateParams = OpenAI::Models::EvalCreateParams
  EvalCustomDataSourceConfig = OpenAI::Models::EvalCustomDataSourceConfig
  EvalDeleteParams = OpenAI::Models::EvalDeleteParams
  EvalListParams = OpenAI::Models::EvalListParams
  EvalRetrieveParams = OpenAI::Models::EvalRetrieveParams

  EvalStoredCompletionsDataSourceConfig = OpenAI::Models::EvalStoredCompletionsDataSourceConfig

  EvalUpdateParams = OpenAI::Models::EvalUpdateParams
  Evals = OpenAI::Models::Evals
  FileChunkingStrategy = OpenAI::Models::FileChunkingStrategy
  FileChunkingStrategyParam = OpenAI::Models::FileChunkingStrategyParam
  FileContent = OpenAI::Models::FileContent
  FileContentParams = OpenAI::Models::FileContentParams
  FileCreateParams = OpenAI::Models::FileCreateParams
  FileDeleteParams = OpenAI::Models::FileDeleteParams
  FileDeleted = OpenAI::Models::FileDeleted
  FileListParams = OpenAI::Models::FileListParams
  FileObject = OpenAI::Models::FileObject

  class FilePart
    sig { returns(T.any(Pathname, StringIO, IO, String)) }
    attr_reader :content

    sig { returns(T.nilable(String)) }
    attr_reader :content_type

    sig { returns(T.nilable(String)) }
    attr_reader :filename

    sig { params(a: T.anything).returns(String) }
    def to_json(*a); end

    sig { params(a: T.anything).returns(String) }
    def to_yaml(*a); end

    private

    # @api private
    sig { returns(String) }
    def read; end

    class << self
      sig do
        params(
          content: T.any(Pathname, StringIO, IO, String),
          filename: T.nilable(String),
          content_type: T.nilable(String)
        ).returns(T.attached_class)
      end
      def new(content, filename: nil, content_type: nil); end
    end
  end

  FilePurpose = OpenAI::Models::FilePurpose
  FileRetrieveParams = OpenAI::Models::FileRetrieveParams
  FineTuning = OpenAI::Models::FineTuning
  FunctionDefinition = OpenAI::Models::FunctionDefinition

  FunctionParameters = T.let(OpenAI::Models::FunctionParameters, OpenAI::Internal::Type::Converter)

  Graders = OpenAI::Models::Graders

  module Helpers
    module Streaming
      class ResponseCompletedEvent < OpenAI::Models::Responses::ResponseCompletedEvent
        sig { returns(OpenAI::Models::Responses::Response) }
        def response; end
      end

      class ResponseFunctionCallArgumentsDeltaEvent < OpenAI::Models::Responses::ResponseFunctionCallArgumentsDeltaEvent
        sig { returns(String) }
        def snapshot; end
      end

      class ResponseStream
        include OpenAI::Internal::Type::BaseStream

        Message = type_member { { fixed: ResponseStreamEvent } }
        Elem = type_member { { fixed: ResponseStreamEvent } }

        sig { params(raw_stream: T.untyped, text_format: T.untyped, starting_after: T.nilable(Integer)).void }
        def initialize(raw_stream:, text_format:, starting_after:); end

        sig { void }
        def close; end

        # Override the each method to properly type the yielded events
        sig do
          params(
            block: T.nilable(T.proc.params(event: ResponseStreamEvent).void)
          ).returns(T.any(T.self_type, T::Enumerator[ResponseStreamEvent]))
        end
        def each(&block); end

        sig { returns(OpenAI::Models::Responses::Response) }
        def get_final_response; end

        sig { returns(String) }
        def get_output_text; end

        sig { returns(T.untyped) }
        def iterator; end

        sig { returns(T::Enumerator::Lazy[String]) }
        def text; end

        sig { returns(T.self_type) }
        def until_done; end

        # Define the type union for streaming events that can be yielded
        ResponseStreamEvent = T.type_alias do
            T.any(
              OpenAI::Streaming::ResponseTextDeltaEvent,
              OpenAI::Streaming::ResponseTextDoneEvent,
              OpenAI::Streaming::ResponseCompletedEvent,
              OpenAI::Streaming::ResponseFunctionCallArgumentsDeltaEvent,
              # Pass through other raw events
              OpenAI::Models::Responses::ResponseStreamEvent::Variants
            )
          end
      end

      class ResponseStreamState
        sig { returns(T.nilable(OpenAI::Models::Responses::Response)) }
        attr_reader :completed_response

        sig { params(text_format: T.untyped).void }
        def initialize(text_format:); end

        sig do
          params(
            event: T.untyped,
            current_snapshot: T.nilable(OpenAI::Models::Responses::Response)
          ).returns(OpenAI::Models::Responses::Response)
        end
        def accumulate_event(event:, current_snapshot:); end

        sig { params(object: T.untyped, expected_type: Symbol).void }
        def assert_type(object, expected_type); end

        sig { params(event: T.untyped).returns(T::Array[T.untyped]) }
        def handle_event(event); end

        sig { params(text: T.nilable(String)).returns(T.untyped) }
        def parse_structured_text(text); end
      end

      class ResponseTextDeltaEvent < OpenAI::Models::Responses::ResponseTextDeltaEvent
        sig { returns(String) }
        def snapshot; end
      end

      class ResponseTextDoneEvent < OpenAI::Models::Responses::ResponseTextDoneEvent
        sig { returns(T.untyped) }
        def parsed; end
      end
    end

    # Helpers for the structured output API.
    #
    # see https://platform.openai.com/docs/guides/structured-outputs
    # see https://json-schema.org
    #
    # Based on the DSL in {OpenAI::Internal::Type}, but currently only support the limited subset of JSON schema types used in structured output APIs.
    #
    # Supported types: {NilClass} {String} {Symbol} {Integer} {Float} {OpenAI::Boolean}, {OpenAI::EnumOf}, {OpenAI::UnionOf}, {OpenAI::ArrayOf}, {OpenAI::BaseModel}
    module StructuredOutput
      class ArrayOf < OpenAI::Internal::Type::ArrayOf
        include OpenAI::Helpers::StructuredOutput::JsonSchemaConverter

        Elem = type_member(:out)

        sig { returns(String) }
        attr_reader :description
      end

      # Represents a response from OpenAI's API where the model's output has been structured according to a schema predefined by the user.
      #
      # This class is specifically used when making requests with the `response_format` parameter set to use structured output (e.g., JSON).
      #
      # See {examples/structured_outputs_chat_completions.rb} for a complete example of use
      class BaseModel < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Helpers::StructuredOutput::JsonSchemaConverter

        class << self
          sig { returns(T.noreturn) }
          def optional; end
        end
      end

      class Boolean < OpenAI::Internal::Type::Boolean
        extend OpenAI::Helpers::StructuredOutput::JsonSchemaConverter
      end

      # @example
      #   example = OpenAI::EnumOf[:foo, :bar, :zoo]
      #
      # @example
      #   example = OpenAI::EnumOf[1, 2, 3]
      class EnumOf
        include OpenAI::Internal::Type::Enum
        include OpenAI::Helpers::StructuredOutput::JsonSchemaConverter

        sig { returns(T::Array[T.any(NilClass, T::Boolean, Integer, Float, Symbol)]) }
        attr_reader :values

        class << self
          sig { params(values: T.any(NilClass, T::Boolean, Integer, Float, Symbol)).returns(T.attached_class) }
          def [](*values); end
        end
      end

      JsonSchema = T.type_alias { OpenAI::Internal::AnyHash }

      # To customize the JSON schema conversion for a type, implement the `JsonSchemaConverter` interface.
      module JsonSchemaConverter
        # The exact JSON schema produced is subject to improvement between minor release versions.
        sig do
          params(
            state: OpenAI::Helpers::StructuredOutput::JsonSchemaConverter::State
          ).returns(OpenAI::Helpers::StructuredOutput::JsonSchema)
        end
        def to_json_schema_inner(state:); end

        # Internal helpers methods.
        class << self
          # @api private
          sig do
            params(
              state: OpenAI::Helpers::StructuredOutput::JsonSchemaConverter::State,
              type: OpenAI::Helpers::StructuredOutput::JsonSchemaConverter::Input,
              blk: T.proc.returns(OpenAI::Helpers::StructuredOutput::JsonSchema)
            ).void
          end
          def cache_def!(state, type:, &blk); end

          # @api private
          sig do
            params(
              type: OpenAI::Helpers::StructuredOutput::JsonSchemaConverter::Input
            ).returns(OpenAI::Helpers::StructuredOutput::JsonSchema)
          end
          def to_json_schema(type); end

          # @api private
          sig do
            params(
              type: OpenAI::Helpers::StructuredOutput::JsonSchemaConverter::Input,
              state: OpenAI::Helpers::StructuredOutput::JsonSchemaConverter::State
            ).returns(OpenAI::Helpers::StructuredOutput::JsonSchema)
          end
          def to_json_schema_inner(type, state:); end

          # @api private
          sig do
            params(
              schema: OpenAI::Helpers::StructuredOutput::JsonSchema
            ).returns(OpenAI::Helpers::StructuredOutput::JsonSchema)
          end
          def to_nilable(schema); end
        end

        # @api private
        COUNTER = T.let(Object.new.freeze, T.anything)

        Input = T.type_alias do
            T.any(
              OpenAI::Helpers::StructuredOutput::JsonSchemaConverter,
              T::Class[T.anything]
            )
          end

        # @api private
        NO_REF = T.let(Object.new.freeze, T.anything)

        # @api private
        POINTER = T.let(Object.new.freeze, T.anything)

        State = T.type_alias do
            { defs: T::Hash[Object, String], path: T::Array[String] }
          end
      end

      # @example
      #   example = OpenAI::UnionOf[Float, OpenAI::ArrayOf[Integer]]
      class UnionOf
        include OpenAI::Internal::Type::Union
        include OpenAI::Helpers::StructuredOutput::JsonSchemaConverter

        class << self
          sig do
            params(
              variants: OpenAI::Helpers::StructuredOutput::JsonSchemaConverter::Input
            ).returns(T.attached_class)
          end
          def [](*variants); end
        end
      end
    end
  end

  Image = OpenAI::Models::Image
  ImageCreateVariationParams = OpenAI::Models::ImageCreateVariationParams
  ImageEditCompletedEvent = OpenAI::Models::ImageEditCompletedEvent
  ImageEditParams = OpenAI::Models::ImageEditParams
  ImageEditPartialImageEvent = OpenAI::Models::ImageEditPartialImageEvent
  ImageEditStreamEvent = OpenAI::Models::ImageEditStreamEvent
  ImageGenCompletedEvent = OpenAI::Models::ImageGenCompletedEvent
  ImageGenPartialImageEvent = OpenAI::Models::ImageGenPartialImageEvent
  ImageGenStreamEvent = OpenAI::Models::ImageGenStreamEvent
  ImageGenerateParams = OpenAI::Models::ImageGenerateParams
  ImageModel = OpenAI::Models::ImageModel
  ImagesResponse = OpenAI::Models::ImagesResponse

  module Internal
    extend OpenAI::Internal::Util::SorbetRuntimeSupport

    # Due to the current WIP status of Shapes support in Sorbet, types referencing
    # this alias might be refined in the future.
    AnyHash = T.type_alias { T::Hash[Symbol, T.anything] }

    class CursorPage
      include OpenAI::Internal::Type::BasePage

      Elem = type_member

      sig { returns(T.nilable(T::Array[Elem])) }
      attr_accessor :data

      sig { returns(T::Boolean) }
      attr_accessor :has_more

      # @api private
      sig { returns(String) }
      def inspect; end
    end

    FileInput = T.type_alias { T.any(Pathname, StringIO, IO, String, OpenAI::FilePart) }

    OMIT = T.let(Object.new.freeze, T.anything)

    class Page
      include OpenAI::Internal::Type::BasePage

      Elem = type_member

      sig { returns(T.nilable(T::Array[Elem])) }
      attr_accessor :data

      sig { returns(String) }
      attr_accessor :object

      # @api private
      sig { returns(String) }
      def inspect; end
    end

    class Stream
      include OpenAI::Internal::Type::BaseStream

      Message = type_member(:in) { { fixed: OpenAI::Internal::Util::ServerSentEvent } }
      Elem = type_member(:out)

      private

      # @api private
      sig { override.returns(T::Enumerable[Elem]) }
      def iterator; end
    end

    module Transport
      # @api private
      class BaseClient
        extend OpenAI::Internal::Util::SorbetRuntimeSupport

        abstract!

        sig { returns(URI::Generic) }
        attr_reader :base_url

        sig { returns(T::Hash[String, String]) }
        attr_reader :headers

        sig { returns(T.nilable(String)) }
        attr_reader :idempotency_header

        sig { returns(Float) }
        attr_reader :initial_retry_delay

        sig { returns(Integer) }
        attr_reader :max_retries

        sig { returns(Float) }
        attr_reader :max_retry_delay

        # @api private
        sig { returns(OpenAI::Internal::Transport::PooledNetRequester) }
        attr_reader :requester

        sig { returns(Float) }
        attr_reader :timeout

        # @api private
        sig { returns(String) }
        def inspect; end

        # Execute the request specified by `req`. This is the method that all resource
        # methods call into.
        #
        # @overload request(method, path, query: {}, headers: {}, body: nil, unwrap: nil, page: nil, stream: nil, model: OpenAI::Internal::Type::Unknown, options: {})
        sig do
          params(
            method: Symbol,
            path: T.any(String, T::Array[String]),
            query: T.nilable(
                T::Hash[String, T.nilable(T.any(T::Array[String], String))]
              ),
            headers: T.nilable(
                T::Hash[
                  String,
                  T.nilable(
                    T.any(
                      String,
                      Integer,
                      T::Array[T.nilable(T.any(String, Integer))]
                    )
                  )
                ]
              ),
            body: T.nilable(T.anything),
            unwrap: T.nilable(
                T.any(
                  Symbol,
                  Integer,
                  T::Array[T.any(Symbol, Integer)],
                  T.proc.params(arg0: T.anything).returns(T.anything)
                )
              ),
            page: T.nilable(
                T::Class[
                  OpenAI::Internal::Type::BasePage[
                    OpenAI::Internal::Type::BaseModel
                  ]
                ]
              ),
            stream: T.nilable(
                T::Class[
                  OpenAI::Internal::Type::BaseStream[
                    T.anything,
                    OpenAI::Internal::Type::BaseModel
                  ]
                ]
              ),
            model: T.nilable(OpenAI::Internal::Type::Converter::Input),
            options: T.nilable(OpenAI::RequestOptions::OrHash)
          ).returns(T.anything)
        end
        def request(method, path, query: {}, headers: {}, body: nil, unwrap: nil, page: nil, stream: nil, model: OpenAI::Internal::Type::Unknown, options: {}); end

        private

        # @api private
        sig { overridable.returns(T::Hash[String, String]) }
        def auth_headers; end

        # @api private
        sig do
          overridable
            .params(
              req: OpenAI::Internal::Transport::BaseClient::RequestComponents,
              opts: OpenAI::Internal::AnyHash
            ).returns(OpenAI::Internal::Transport::BaseClient::RequestInput)
        end
        def build_request(req, opts); end

        # @api private
        sig { returns(String) }
        def generate_idempotency_key; end

        # @api private
        sig { params(headers: T::Hash[String, String], retry_count: Integer).returns(Float) }
        def retry_delay(headers, retry_count:); end

        # @api private
        sig do
          params(
            request: OpenAI::Internal::Transport::BaseClient::RequestInput,
            redirect_count: Integer,
            retry_count: Integer,
            send_retry_header: T::Boolean
          ).returns([Integer, Net::HTTPResponse, T::Enumerable[String]])
        end
        def send_request(request, redirect_count:, retry_count:, send_retry_header:); end

        class << self
          # @api private
          sig do
            params(
              request: OpenAI::Internal::Transport::BaseClient::RequestInput,
              status: Integer,
              response_headers: T.any(T::Hash[String, String], Net::HTTPHeader)
            ).returns(OpenAI::Internal::Transport::BaseClient::RequestInput)
          end
          def follow_redirect(request, status:, response_headers:); end

          # @api private
          sig do
            params(
              status: T.any(Integer, OpenAI::Errors::APIConnectionError),
              stream: T.nilable(T::Enumerable[String])
            ).void
          end
          def reap_connection!(status, stream:); end

          # @api private
          sig { params(status: Integer, headers: T.any(T::Hash[String, String], Net::HTTPHeader)).returns(T::Boolean) }
          def should_retry?(status, headers:); end

          # @api private
          sig { params(req: OpenAI::Internal::Transport::BaseClient::RequestComponents).void }
          def validate!(req); end
        end

        class << self
          # @api private
          sig do
            params(
              base_url: String,
              timeout: Float,
              max_retries: Integer,
              initial_retry_delay: Float,
              max_retry_delay: Float,
              headers: T::Hash[
                String,
                T.nilable(
                  T.any(
                    String,
                    Integer,
                    T::Array[T.nilable(T.any(String, Integer))]
                  )
                )
              ],
              idempotency_header: T.nilable(String)
            ).returns(T.attached_class)
          end
          def new(base_url:, timeout: 0.0, max_retries: 0, initial_retry_delay: 0.0, max_retry_delay: 0.0, headers: {}, idempotency_header: nil); end
        end

        # from whatwg fetch spec
        MAX_REDIRECTS = 20

        PLATFORM_HEADERS = T::Hash[String, String]

        RequestComponents = T.type_alias do
            {
              method: Symbol,
              path: T.any(String, T::Array[String]),
              query:
                T.nilable(
                  T::Hash[String, T.nilable(T.any(T::Array[String], String))]
                ),
              headers:
                T.nilable(
                  T::Hash[
                    String,
                    T.nilable(
                      T.any(
                        String,
                        Integer,
                        T::Array[T.nilable(T.any(String, Integer))]
                      )
                    )
                  ]
                ),
              body: T.nilable(T.anything),
              unwrap:
                T.nilable(
                  T.any(
                    Symbol,
                    Integer,
                    T::Array[T.any(Symbol, Integer)],
                    T.proc.params(arg0: T.anything).returns(T.anything)
                  )
                ),
              page:
                T.nilable(
                  T::Class[
                    OpenAI::Internal::Type::BasePage[
                      OpenAI::Internal::Type::BaseModel
                    ]
                  ]
                ),
              stream:
                T.nilable(
                  T::Class[
                    OpenAI::Internal::Type::BaseStream[
                      T.anything,
                      OpenAI::Internal::Type::BaseModel
                    ]
                  ]
                ),
              model: T.nilable(OpenAI::Internal::Type::Converter::Input),
              options: T.nilable(OpenAI::RequestOptions::OrHash)
            }
          end

        RequestInput = T.type_alias do
            {
              method: Symbol,
              url: URI::Generic,
              headers: T::Hash[String, String],
              body: T.anything,
              max_retries: Integer,
              timeout: Float
            }
          end
      end

      # @api private
      class PooledNetRequester
        extend OpenAI::Internal::Util::SorbetRuntimeSupport

        # @api private
        sig do
          params(
            request: OpenAI::Internal::Transport::PooledNetRequester::Request
          ).returns([Integer, Net::HTTPResponse, T::Enumerable[String]])
        end
        def execute(request); end

        private

        # @api private
        sig { params(url: URI::Generic, deadline: Float, blk: T.proc.params(arg0: Net::HTTP).void).void }
        def with_pool(url, deadline:, &blk); end

        class << self
          # @api private
          sig do
            params(
              request: OpenAI::Internal::Transport::PooledNetRequester::Request,
              blk: T.proc.params(arg0: String).void
            ).returns([Net::HTTPGenericRequest, T.proc.void])
          end
          def build_request(request, &blk); end

          # @api private
          sig { params(conn: Net::HTTP, deadline: Float).void }
          def calibrate_socket_timeout(conn, deadline); end

          # @api private
          sig { params(url: URI::Generic).returns(Net::HTTP) }
          def connect(url); end
        end

        class << self
          # @api private
          sig { params(size: Integer).returns(T.attached_class) }
          def new(size: OpenAI::Internal::Transport::PooledNetRequester::DEFAULT_MAX_CONNECTIONS); end
        end

        DEFAULT_MAX_CONNECTIONS = T.let(T.unsafe(nil), Integer)

        # from the golang stdlib
        # https://github.com/golang/go/blob/c8eced8580028328fde7c03cbfcb720ce15b2358/src/net/http/transport.go#L49
        KEEP_ALIVE_TIMEOUT = 30

        Request = T.type_alias do
            {
              method: Symbol,
              url: URI::Generic,
              headers: T::Hash[String, String],
              body: T.anything,
              deadline: Float
            }
          end
      end
    end

    module Type
      # @api private
      #
      # Array of items of a given type.
      class ArrayOf
        include OpenAI::Internal::Type::Converter
        include OpenAI::Internal::Util::SorbetRuntimeSupport

        abstract!

        Elem = type_member(:out)

        # @api private
        sig do
          params(
            type_info: T.any(
                OpenAI::Internal::AnyHash,
                T.proc.returns(OpenAI::Internal::Type::Converter::Input),
                OpenAI::Internal::Type::Converter::Input
              ),
            spec: OpenAI::Internal::AnyHash
          ).void
        end
        def initialize(type_info, spec = {}); end

        sig { params(other: T.anything).returns(T::Boolean) }
        def ==(other); end

        sig { params(other: T.anything).returns(T::Boolean) }
        def ===(other); end

        # @api private
        sig do
          override
            .params(
              value: T.any(T::Array[T.anything], T.anything),
              state: OpenAI::Internal::Type::Converter::CoerceState
            ).returns(T.any(T::Array[T.anything], T.anything))
        end
        def coerce(value, state:); end

        # @api private
        sig do
          override
            .params(
              value: T.any(T::Array[T.anything], T.anything),
              state: OpenAI::Internal::Type::Converter::DumpState
            ).returns(T.any(T::Array[T.anything], T.anything))
        end
        def dump(value, state:); end

        sig { returns(Integer) }
        def hash; end

        # @api private
        sig { params(depth: Integer).returns(String) }
        def inspect(depth: 0); end

        # @api private
        sig { returns(T.anything) }
        def to_sorbet_type; end

        protected

        # @api private
        sig { returns(Elem) }
        def item_type; end

        # @api private
        sig { returns(T::Boolean) }
        def nilable?; end

        class << self
          sig do
            params(
              type_info: T.any(
                OpenAI::Internal::AnyHash,
                T.proc.returns(OpenAI::Internal::Type::Converter::Input),
                OpenAI::Internal::Type::Converter::Input
              ),
              spec: OpenAI::Internal::AnyHash
            ).returns(T.attached_class)
          end
          def [](type_info, spec = {}); end
        end
      end

      class BaseModel
        extend OpenAI::Internal::Type::Converter
        extend OpenAI::Internal::Util::SorbetRuntimeSupport

        abstract!

        sig { params(other: T.anything).returns(T::Boolean) }
        def ==(other); end

        # Returns the raw value associated with the given key, if found. Otherwise, nil is
        # returned.
        #
        # It is valid to lookup keys that are not in the API spec, for example to access
        # undocumented features. This method does not parse response data into
        # higher-level types. Lookup by anything other than a Symbol is an ArgumentError.
        sig { params(key: Symbol).returns(T.nilable(T.anything)) }
        def [](key); end

        sig { params(keys: T.nilable(T::Array[Symbol])).returns(OpenAI::Internal::AnyHash) }
        def deconstruct_keys(keys); end

        # In addition to the behaviour of `#to_h`, this method will recursively call
        # `#to_h` on nested models.
        sig { overridable.returns(OpenAI::Internal::AnyHash) }
        def deep_to_h; end

        sig { returns(Integer) }
        def hash; end

        # @api private
        sig { returns(String) }
        def inspect; end

        # Returns a Hash of the data underlying this object. O(1)
        #
        # Keys are Symbols and values are the raw values from the response. The return
        # value indicates which values were ever set on the object. i.e. there will be a
        # key in this hash if they ever were, even if the set value was nil.
        #
        # This method is not recursive. The returned value is shared by the object, so it
        # should not be mutated.
        sig { overridable.returns(OpenAI::Internal::AnyHash) }
        def to_h; end

        # Returns a Hash of the data underlying this object. O(1)
        #
        # Keys are Symbols and values are the raw values from the response. The return
        # value indicates which values were ever set on the object. i.e. there will be a
        # key in this hash if they ever were, even if the set value was nil.
        #
        # This method is not recursive. The returned value is shared by the object, so it
        # should not be mutated.
        sig { overridable.returns(OpenAI::Internal::AnyHash) }
        def to_hash; end

        sig { params(a: T.anything).returns(String) }
        def to_json(*a); end

        sig { returns(String) }
        def to_s; end

        sig { params(a: T.anything).returns(String) }
        def to_yaml(*a); end

        class << self
          sig { params(other: T.anything).returns(T::Boolean) }
          def ==(other); end

          # @api private
          sig do
            returns(T::Hash[
                Symbol,
                T.all(
                  OpenAI::Internal::Type::BaseModel::KnownField,
                  { type: OpenAI::Internal::Type::Converter::Input }
                )
              ])
          end
          def fields; end

          sig { returns(Integer) }
          def hash; end

          # @api private
          #
          # Assumes superclass fields are totally defined before fields are accessed /
          # defined on subclasses.
          sig { params(child: T.self_type).void }
          def inherited(child); end

          # @api private
          sig do
            returns(T::Hash[
                Symbol,
                T.all(
                  OpenAI::Internal::Type::BaseModel::KnownField,
                  {
                    type_fn:
                      T.proc.returns(OpenAI::Internal::Type::Converter::Input)
                  }
                )
              ])
          end
          def known_fields; end

          # @api private
          sig do
            params(
              name_sym: Symbol,
              type_info: T.any(
                  OpenAI::Internal::AnyHash,
                  T.proc.returns(OpenAI::Internal::Type::Converter::Input),
                  OpenAI::Internal::Type::Converter::Input
                ),
              spec: OpenAI::Internal::AnyHash
            ).void
          end
          def optional(name_sym, type_info, spec = {}); end

          # @api private
          sig do
            params(
              name_sym: Symbol,
              type_info: T.any(
                  OpenAI::Internal::AnyHash,
                  T.proc.returns(OpenAI::Internal::Type::Converter::Input),
                  OpenAI::Internal::Type::Converter::Input
                ),
              spec: OpenAI::Internal::AnyHash
            ).void
          end
          def required(name_sym, type_info, spec = {}); end

          private

          # @api private
          sig do
            params(
              name_sym: Symbol,
              required: T::Boolean,
              type_info: T.any(
                  {
                    const:
                      T.nilable(
                        T.any(NilClass, T::Boolean, Integer, Float, Symbol)
                      ),
                    enum:
                      T.nilable(
                        T.proc.returns(OpenAI::Internal::Type::Converter::Input)
                      ),
                    union:
                      T.nilable(
                        T.proc.returns(OpenAI::Internal::Type::Converter::Input)
                      ),
                    api_name: Symbol,
                    nil?: T::Boolean
                  },
                  T.proc.returns(OpenAI::Internal::Type::Converter::Input),
                  OpenAI::Internal::Type::Converter::Input
                ),
              spec: OpenAI::Internal::AnyHash
            ).void
          end
          def add_field(name_sym, required:, type_info:, spec:); end

          # @api private
          #
          # `request_only` attributes not excluded from `.#coerce` when receiving responses
          # even if well behaved servers should not send them
          sig { params(blk: T.proc.void).void }
          def request_only(&blk); end

          # @api private
          #
          # `response_only` attributes are omitted from `.#dump` when making requests
          sig { params(blk: T.proc.void).void }
          def response_only(&blk); end
        end

        class << self
          # @api private
          sig do
            override
              .params(
                value: T.any(
                    OpenAI::Internal::Type::BaseModel,
                    T::Hash[T.anything, T.anything],
                    T.anything
                  ),
                state: OpenAI::Internal::Type::Converter::CoerceState
              ).returns(T.any(T.attached_class, T.anything))
          end
          def coerce(value, state:); end

          # @api private
          sig do
            override
              .params(
                value: T.any(T.attached_class, T.anything),
                state: OpenAI::Internal::Type::Converter::DumpState
              ).returns(T.any(T::Hash[T.anything, T.anything], T.anything))
          end
          def dump(value, state:); end

          # @api private
          sig { returns(T.anything) }
          def to_sorbet_type; end
        end

        class << self
          # @api private
          sig do
            params(
              model: OpenAI::Internal::Type::BaseModel,
              convert: T::Boolean
            ).returns(OpenAI::Internal::AnyHash)
          end
          def recursively_to_h(model, convert:); end
        end

        class << self
          # @api private
          sig { params(depth: Integer).returns(String) }
          def inspect(depth: 0); end
        end

        class << self
          # Create a new instance of a model.
          sig { params(data: T.any(T::Hash[Symbol, T.anything], T.self_type)).returns(T.attached_class) }
          def new(data = {}); end
        end

        KnownField = T.type_alias do
            {
              mode: T.nilable(Symbol),
              required: T::Boolean,
              nilable: T::Boolean
            }
          end

        OrHash = T.type_alias do
            T.any(OpenAI::Internal::Type::BaseModel, OpenAI::Internal::AnyHash)
          end
      end

      # @api private
      #
      # This module provides a base implementation for paginated responses in the SDK.
      module BasePage
        Elem = type_member(:out)

        # @api private
        sig do
          params(
            client: OpenAI::Internal::Transport::BaseClient,
            req: OpenAI::Internal::Transport::BaseClient::RequestComponents,
            headers: T.any(T::Hash[String, String], Net::HTTPHeader),
            page_data: T.anything
          ).void
        end
        def initialize(client:, req:, headers:, page_data:); end

        sig { overridable.params(blk: T.proc.params(arg0: Elem).void).void }
        def auto_paging_each(&blk); end

        sig { overridable.returns(T.self_type) }
        def next_page; end

        sig { overridable.returns(T::Boolean) }
        def next_page?; end

        sig { returns(T::Enumerable[Elem]) }
        def to_enum; end
      end

      # @api private
      #
      # This module provides a base implementation for streaming responses in the SDK.
      module BaseStream
        include Enumerable

        Message = type_member(:in)
        Elem = type_member(:out)

        # @api private
        sig do
          params(
            model: T.any(T::Class[T.anything], OpenAI::Internal::Type::Converter),
            url: URI::Generic,
            status: Integer,
            response: Net::HTTPResponse,
            unwrap: T.any(
                Symbol,
                Integer,
                T::Array[T.any(Symbol, Integer)],
                T.proc.params(arg0: T.anything).returns(T.anything)
              ),
            stream: T::Enumerable[Message]
          ).void
        end
        def initialize(model:, url:, status:, response:, unwrap:, stream:); end

        sig { void }
        def close; end

        sig { params(blk: T.proc.params(arg0: Elem).void).void }
        def each(&blk); end

        # @api private
        sig { returns(String) }
        def inspect; end

        sig { returns(T::Enumerator[Elem]) }
        def to_enum; end

        private

        # @api private
        sig { overridable.returns(T::Enumerable[Elem]) }
        def iterator; end

        class << self
          # Attempt to close the underlying transport when the stream itself is garbage
          # collected.
          #
          # This should not be relied upon for resource clean up, as the garbage collector
          # is not guaranteed to run.
          sig { params(stream: T::Enumerable[T.anything]).returns(T.proc.params(arg0: Integer).void) }
          def defer_closing(stream); end
        end
      end

      # @api private
      #
      # Ruby has no Boolean class; this is something for models to refer to.
      class Boolean
        extend OpenAI::Internal::Type::Converter
        extend OpenAI::Internal::Util::SorbetRuntimeSupport

        abstract!

        class << self
          # @api private
          #
          # Coerce value to Boolean if possible, otherwise return the original value.
          sig do
            override
              .params(
                value: T.any(T::Boolean, T.anything),
                state: OpenAI::Internal::Type::Converter::CoerceState
              ).returns(T.any(T::Boolean, T.anything))
          end
          def coerce(value, state:); end

          # @api private
          sig do
            override
              .params(
                value: T.any(T::Boolean, T.anything),
                state: OpenAI::Internal::Type::Converter::DumpState
              ).returns(T.any(T::Boolean, T.anything))
          end
          def dump(value, state:); end

          # @api private
          sig { returns(T.anything) }
          def to_sorbet_type; end
        end

        class << self
          sig { params(other: T.anything).returns(T::Boolean) }
          def ==(other); end

          sig { params(other: T.anything).returns(T::Boolean) }
          def ===(other); end
        end
      end

      # @api private
      module Converter
        extend OpenAI::Internal::Util::SorbetRuntimeSupport

        # @api private
        sig do
          overridable
            .params(
              value: T.anything,
              state: OpenAI::Internal::Type::Converter::CoerceState
            ).returns(T.anything)
        end
        def coerce(value, state:); end

        # @api private
        sig do
          overridable
            .params(
              value: T.anything,
              state: OpenAI::Internal::Type::Converter::DumpState
            ).returns(T.anything)
        end
        def dump(value, state:); end

        # @api private
        sig { params(depth: Integer).returns(String) }
        def inspect(depth: 0); end

        class << self
          class << self
            # @api private
            #
            # Based on `target`, transform `value` into `target`, to the extent possible:
            #
            # 1. if the given `value` conforms to `target` already, return the given `value`
            # 2. if it's possible and safe to convert the given `value` to `target`, then the
            #    converted value
            # 3. otherwise, the given `value` unaltered
            #
            # The coercion process is subject to improvement between minor release versions.
            # See https://docs.pydantic.dev/latest/concepts/unions/#smart-mode
            sig do
              params(
                target: OpenAI::Internal::Type::Converter::Input,
                value: T.anything,
                state: OpenAI::Internal::Type::Converter::CoerceState
              ).returns(T.anything)
            end
            def coerce(
              target,
              value,
              state: OpenAI::Internal::Type::Converter.new_coerce_state # The `strictness` is one of `true`, `false`. This informs the coercion strategy
                                                                        # when we have to decide between multiple possible conversion targets:
                                                                        # - `true`: the conversion must be exact, with minimum coercion.
                                                                        # - `false`: the conversion can be approximate, with some coercion.
                                                                        # The `exactness` is `Hash` with keys being one of `yes`, `no`, or `maybe`. For
                                                                        # any given conversion attempt, the exactness will be updated based on how closely
                                                                        # the value recursively matches the target type:
                                                                        # - `yes`: the value can be converted to the target type with minimum coercion.
                                                                        # - `maybe`: the value can be converted to the target type with some reasonable
                                                                        #   coercion.
                                                                        # - `no`: the value cannot be converted to the target type.
                                                                        # See implementation below for more details.
); end

            # @api private
            sig do
              params(
                target: OpenAI::Internal::Type::Converter::Input,
                value: T.anything,
                state: OpenAI::Internal::Type::Converter::DumpState
              ).returns(T.anything)
            end
            def dump(target, value, state: { can_retry: true }); end

            # @api private
            sig { params(target: T.anything, depth: Integer).returns(String) }
            def inspect(target, depth:); end

            # @api private
            sig { params(translate_names: T::Boolean).returns(OpenAI::Internal::Type::Converter::CoerceState) }
            def new_coerce_state(translate_names: true); end

            # @api private
            sig do
              params(
                spec: T.any(
                  {
                    const:
                      T.nilable(
                        T.any(NilClass, T::Boolean, Integer, Float, Symbol)
                      ),
                    enum:
                      T.nilable(
                        T.proc.returns(OpenAI::Internal::Type::Converter::Input)
                      ),
                    union:
                      T.nilable(
                        T.proc.returns(OpenAI::Internal::Type::Converter::Input)
                      )
                  },
                  T.proc.returns(OpenAI::Internal::Type::Converter::Input),
                  OpenAI::Internal::Type::Converter::Input
                )
              ).returns(T.proc.returns(T.anything))
            end
            def type_info(spec); end
          end
        end

        CoerceState = T.type_alias do
            {
              translate_names: T::Boolean,
              strictness: T::Boolean,
              exactness: {
                yes: Integer,
                no: Integer,
                maybe: Integer
              },
              error: T::Class[StandardError],
              branched: Integer
            }
          end

        DumpState = T.type_alias { { can_retry: T::Boolean } }

        Input = T.type_alias do
            T.any(OpenAI::Internal::Type::Converter, T::Class[T.anything])
          end
      end

      # @api private
      #
      # A value from among a specified list of options. OpenAPI enum values map to Ruby
      # values in the SDK as follows:
      #
      # 1. boolean => true | false
      # 2. integer => Integer
      # 3. float => Float
      # 4. string => Symbol
      #
      # We can therefore convert string values to Symbols, but can't convert other
      # values safely.
      module Enum
        include OpenAI::Internal::Type::Converter
        include OpenAI::Internal::Util::SorbetRuntimeSupport

        sig { params(other: T.anything).returns(T::Boolean) }
        def ==(other); end

        sig { params(other: T.anything).returns(T::Boolean) }
        def ===(other); end

        # @api private
        #
        # Unlike with primitives, `Enum` additionally validates that the value is a member
        # of the enum.
        sig do
          override
            .params(
              value: T.any(String, Symbol, T.anything),
              state: OpenAI::Internal::Type::Converter::CoerceState
            ).returns(T.any(Symbol, T.anything))
        end
        def coerce(value, state:); end

        # @api private
        sig do
          override
            .params(
              value: T.any(Symbol, T.anything),
              state: OpenAI::Internal::Type::Converter::DumpState
            ).returns(T.any(Symbol, T.anything))
        end
        def dump(value, state:); end

        sig { returns(Integer) }
        def hash; end

        # @api private
        sig { params(depth: Integer).returns(String) }
        def inspect(depth: 0); end

        # @api private
        sig { returns(T.anything) }
        def to_sorbet_type; end

        # All of the valid Symbol values for this enum.
        sig { overridable.returns(T::Array[T.any(NilClass, T::Boolean, Integer, Float, Symbol)]) }
        def values; end
      end

      # @api private
      #
      # Either `Pathname` or `StringIO`, or `IO`, or
      # `OpenAI::Internal::Type::FileInput`.
      #
      # Note: when `IO` is used, all retries are disabled, since many IO` streams are
      # not rewindable.
      class FileInput
        extend OpenAI::Internal::Type::Converter

        abstract!

        class << self
          # @api private
          sig do
            override
              .params(
                value: T.any(StringIO, String, T.anything),
                state: OpenAI::Internal::Type::Converter::CoerceState
              ).returns(T.any(StringIO, T.anything))
          end
          def coerce(value, state:); end

          # @api private
          sig do
            override
              .params(
                value: T.any(Pathname, StringIO, IO, String, T.anything),
                state: OpenAI::Internal::Type::Converter::DumpState
              ).returns(T.any(Pathname, StringIO, IO, String, T.anything))
          end
          def dump(value, state:); end

          # @api private
          sig { returns(T.anything) }
          def to_sorbet_type; end
        end

        class << self
          sig { params(other: T.anything).returns(T::Boolean) }
          def ==(other); end

          sig { params(other: T.anything).returns(T::Boolean) }
          def ===(other); end
        end
      end

      # @api private
      #
      # Hash of items of a given type.
      class HashOf
        include OpenAI::Internal::Type::Converter
        include OpenAI::Internal::Util::SorbetRuntimeSupport

        abstract!

        Elem = type_member(:out)

        # @api private
        sig do
          params(
            type_info: T.any(
                OpenAI::Internal::AnyHash,
                T.proc.returns(OpenAI::Internal::Type::Converter::Input),
                OpenAI::Internal::Type::Converter::Input
              ),
            spec: OpenAI::Internal::AnyHash
          ).void
        end
        def initialize(type_info, spec = {}); end

        sig { params(other: T.anything).returns(T::Boolean) }
        def ==(other); end

        sig { params(other: T.anything).returns(T::Boolean) }
        def ===(other); end

        # @api private
        sig do
          override
            .params(
              value: T.any(T::Hash[T.anything, T.anything], T.anything),
              state: OpenAI::Internal::Type::Converter::CoerceState
            ).returns(T.any(OpenAI::Internal::AnyHash, T.anything))
        end
        def coerce(value, state:); end

        # @api private
        sig do
          override
            .params(
              value: T.any(T::Hash[T.anything, T.anything], T.anything),
              state: OpenAI::Internal::Type::Converter::DumpState
            ).returns(T.any(OpenAI::Internal::AnyHash, T.anything))
        end
        def dump(value, state:); end

        sig { returns(Integer) }
        def hash; end

        # @api private
        sig { params(depth: Integer).returns(String) }
        def inspect(depth: 0); end

        # @api private
        sig { returns(T.anything) }
        def to_sorbet_type; end

        protected

        # @api private
        sig { returns(Elem) }
        def item_type; end

        # @api private
        sig { returns(T::Boolean) }
        def nilable?; end

        class << self
          sig do
            params(
              type_info: T.any(
                OpenAI::Internal::AnyHash,
                T.proc.returns(OpenAI::Internal::Type::Converter::Input),
                OpenAI::Internal::Type::Converter::Input
              ),
              spec: OpenAI::Internal::AnyHash
            ).returns(T.attached_class)
          end
          def [](type_info, spec = {}); end
        end
      end

      # @api private
      module RequestParameters
        # Options to specify HTTP behaviour for this request.
        sig { returns(OpenAI::RequestOptions) }
        attr_reader :request_options

        sig { params(request_options: OpenAI::RequestOptions::OrHash).void }
        attr_writer :request_options

        # @api private
        module Converter
          # @api private
          sig { params(params: T.anything).returns([T.anything, OpenAI::Internal::AnyHash]) }
          def dump_request(params); end
        end
      end

      # @api private
      module Union
        include OpenAI::Internal::Type::Converter
        include OpenAI::Internal::Util::SorbetRuntimeSupport

        sig { params(other: T.anything).returns(T::Boolean) }
        def ==(other); end

        sig { params(other: T.anything).returns(T::Boolean) }
        def ===(other); end

        # @api private
        #
        # Tries to efficiently coerce the given value to one of the known variants.
        #
        # If the value cannot match any of the known variants, the coercion is considered
        # non-viable and returns the original value.
        sig do
          override
            .params(
              value: T.anything,
              state: OpenAI::Internal::Type::Converter::CoerceState
            ).returns(T.anything)
        end
        def coerce(value, state:); end

        # @api private
        sig do
          override
            .params(
              value: T.anything,
              state: OpenAI::Internal::Type::Converter::DumpState
            ).returns(T.anything)
        end
        def dump(value, state:); end

        sig { returns(Integer) }
        def hash; end

        # @api private
        sig { params(depth: Integer).returns(String) }
        def inspect(depth: 0); end

        # @api private
        sig { returns(T.anything) }
        def to_sorbet_type; end

        # All of the specified variants for this union.
        sig { overridable.returns(T::Array[T.anything]) }
        def variants; end

        protected

        # @api private
        sig { returns(T::Array[[T.nilable(Symbol), T.anything]]) }
        def derefed_variants; end

        private

        # @api private
        sig { params(property: Symbol).void }
        def discriminator(property); end

        # @api private
        #
        # All of the specified variant info for this union.
        sig do
          returns(T::Array[
              [
                T.nilable(Symbol),
                T.proc.returns(OpenAI::Internal::Type::Converter::Input)
              ]
            ])
        end
        def known_variants; end

        # @api private
        sig { params(value: T.anything).returns(T.nilable(T.anything)) }
        def resolve_variant(value); end

        # @api private
        sig do
          params(
            key: T.any(
                Symbol,
                OpenAI::Internal::AnyHash,
                T.proc.returns(T.anything),
                T.anything
              ),
            spec: T.any(
                OpenAI::Internal::AnyHash,
                T.proc.returns(T.anything),
                T.anything
              )
          ).void
        end
        def variant(key, spec = nil); end
      end

      # @api private
      #
      # When we don't know what to expect for the value.
      class Unknown
        extend OpenAI::Internal::Type::Converter
        extend OpenAI::Internal::Util::SorbetRuntimeSupport

        abstract!

        class << self
          # @api private
          #
          # No coercion needed for Unknown type.
          sig do
            override
              .params(
                value: T.anything,
                state: OpenAI::Internal::Type::Converter::CoerceState
              ).returns(T.anything)
          end
          def coerce(value, state:); end

          # @api private
          sig do
            override
              .params(
                value: T.anything,
                state: OpenAI::Internal::Type::Converter::DumpState
              ).returns(T.anything)
          end
          def dump(value, state:); end

          # @api private
          sig { returns(T.anything) }
          def to_sorbet_type; end
        end

        class << self
          sig { params(other: T.anything).returns(T::Boolean) }
          def ==(other); end

          sig { params(other: T.anything).returns(T::Boolean) }
          def ===(other); end
        end
      end
    end

    # @api private
    module Util
      extend OpenAI::Internal::Util::SorbetRuntimeSupport

      class << self
        # @api private
        sig { returns(String) }
        def arch; end

        # @api private
        sig { returns(String) }
        def os; end
      end

      class << self
        # @api private
        sig { params(input: T.any(String, T::Boolean)).returns(T.any(T::Boolean, T.anything)) }
        def coerce_boolean(input); end

        # @api private
        sig { params(input: T.any(String, T::Boolean)).returns(T.nilable(T::Boolean)) }
        def coerce_boolean!(input); end

        # @api private
        sig { params(input: T.any(String, Integer, Float)).returns(T.any(Float, T.anything)) }
        def coerce_float(input); end

        # @api private
        sig { params(input: T.anything).returns(T.any(T::Hash[T.anything, T.anything], T.anything)) }
        def coerce_hash(input); end

        # @api private
        sig { params(input: T.anything).returns(T.nilable(T::Hash[T.anything, T.anything])) }
        def coerce_hash!(input); end

        # @api private
        sig { params(input: T.any(String, Integer)).returns(T.any(Integer, T.anything)) }
        def coerce_integer(input); end

        # @api private
        sig { params(input: T.anything).returns(T::Boolean) }
        def primitive?(input); end
      end

      class << self
        # @api private
        #
        # Recursively merge one hash with another. If the values at a given key are not
        # both hashes, just take the new value.
        sig do
          params(
            values: T::Array[T.anything],
            sentinel: T.nilable(T.anything),
            concat: T::Boolean
          ).returns(T.anything)
        end
        def deep_merge(
          *values,
          sentinel: nil, # the value to return if no values are provided.
          concat: false # whether to merge sequences by concatenation.
); end

        # @api private
        sig do
          params(
            data: T.any(
                OpenAI::Internal::AnyHash,
                T::Array[T.anything],
                T.anything
              ),
            pick: T.nilable(
                T.any(
                  Symbol,
                  Integer,
                  T::Array[T.any(Symbol, Integer)],
                  T.proc.params(arg0: T.anything).returns(T.anything)
                )
              ),
            blk: T.nilable(T.proc.returns(T.anything))
          ).returns(T.nilable(T.anything))
        end
        def dig(data, pick, &blk); end

        private

        # @api private
        sig { params(lhs: T.anything, rhs: T.anything, concat: T::Boolean).returns(T.anything) }
        def deep_merge_lr(lhs, rhs, concat: false); end
      end

      class << self
        # @api private
        sig { params(path: T.any(String, T::Array[String])).returns(String) }
        def interpolate_path(path); end

        # @api private
        sig { params(uri: URI::Generic).returns(String) }
        def uri_origin(uri); end
      end

      class << self
        # @api private
        sig { params(query: T.nilable(String)).returns(T::Hash[String, T::Array[String]]) }
        def decode_query(query); end

        # @api private
        sig do
          params(
            query: T.nilable(
                T::Hash[String, T.nilable(T.any(T::Array[String], String))]
              )
          ).returns(T.nilable(String))
        end
        def encode_query(query); end
      end

      class << self
        # @api private
        sig do
          params(
            lhs: OpenAI::Internal::Util::ParsedUri,
            rhs: OpenAI::Internal::Util::ParsedUri
          ).returns(URI::Generic)
        end
        def join_parsed_uri(lhs, rhs); end

        # @api private
        sig { params(url: T.any(URI::Generic, String)).returns(OpenAI::Internal::Util::ParsedUri) }
        def parse_uri(url); end

        # @api private
        sig { params(parsed: OpenAI::Internal::Util::ParsedUri).returns(URI::Generic) }
        def unparse_uri(parsed); end
      end

      class << self
        # @api private
        sig do
          params(
            headers: T::Hash[
                String,
                T.nilable(
                  T.any(
                    String,
                    Integer,
                    T::Array[T.nilable(T.any(String, Integer))]
                  )
                )
              ]
          ).returns(T::Hash[String, String])
        end
        def normalized_headers(*headers); end
      end

      class << self
        sig { params(blk: T.proc.params(y: Enumerator::Yielder).void).returns(T::Enumerable[String]) }
        def writable_enum(&blk); end
      end

      class << self
        # @api private
        #
        # Assumes each chunk in stream has `Encoding::BINARY`.
        sig do
          params(
            headers: T.any(T::Hash[String, String], Net::HTTPHeader),
            stream: T::Enumerable[String],
            suppress_error: T::Boolean
          ).returns(T.anything)
        end
        def decode_content(headers, stream:, suppress_error: false); end

        # @api private
        sig { params(headers: T::Hash[String, String], body: T.anything).returns(T.anything) }
        def encode_content(headers, body); end

        # @api private
        #
        # https://www.iana.org/assignments/character-sets/character-sets.xhtml
        sig { params(content_type: String, text: String).void }
        def force_charset!(content_type, text:); end

        private

        # @api private
        #
        # https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.1.1.md#special-considerations-for-multipart-content
        sig { params(body: T.anything).returns([String, T::Enumerable[String]]) }
        def encode_multipart_streaming(body); end

        # @api private
        sig do
          params(
            y: Enumerator::Yielder,
            boundary: String,
            key: T.any(Symbol, String),
            val: T.anything,
            closing: T::Array[T.proc.void]
          ).void
        end
        def write_multipart_chunk(y, boundary:, key:, val:, closing:); end

        # @api private
        sig do
          params(
            y: Enumerator::Yielder,
            val: T.anything,
            closing: T::Array[T.proc.void],
            content_type: T.nilable(String)
          ).void
        end
        def write_multipart_content(y, val:, closing:, content_type: nil); end
      end

      class << self
        # @api private
        sig do
          params(
            enum: T.nilable(T::Enumerable[T.anything]),
            blk: T.proc.params(arg0: Enumerator::Yielder).void
          ).returns(T::Enumerable[T.anything])
        end
        def chain_fused(enum, &blk); end

        # @api private
        sig { params(enum: T.nilable(T::Enumerable[T.anything])).void }
        def close_fused!(enum); end

        # @api private
        #
        # https://doc.rust-lang.org/std/iter/trait.FusedIterator.html
        sig do
          params(
            enum: T::Enumerable[T.anything],
            external: T::Boolean,
            close: T.proc.void
          ).returns(T::Enumerable[T.anything])
        end
        def fused_enum(enum, external: false, &close); end
      end

      class << self
        # @api private
        #
        # Assumes Strings have been forced into having `Encoding::BINARY`.
        #
        # This decoder is responsible for reassembling lines split across multiple
        # fragments.
        sig { params(enum: T::Enumerable[String]).returns(T::Enumerable[String]) }
        def decode_lines(enum); end

        # @api private
        #
        # https://html.spec.whatwg.org/multipage/server-sent-events.html#parsing-an-event-stream
        #
        # Assumes that `lines` has been decoded with `#decode_lines`.
        sig { params(lines: T::Enumerable[String]).returns(T::Enumerable[OpenAI::Internal::Util::ServerSentEvent]) }
        def decode_sse(lines); end
      end

      class << self
        # @api private
        sig { returns(Float) }
        def monotonic_secs; end

        # @api private
        sig do
          params(
            ns: T.any(Module, T::Class[T.anything])
          ).returns(T::Enumerable[T.any(Module, T::Class[T.anything])])
        end
        def walk_namespaces(ns); end
      end

      JSONL_CONTENT = T.let(%r{^application/(:?x-(?:n|l)djson)|(:?(?:x-)?jsonl)}, Regexp)

      JSON_CONTENT = T.let(%r{^application/(?:vnd(?:\.[^.]+)*\+)?json(?!l)}, Regexp)

      ParsedUri = T.type_alias do
          {
            scheme: T.nilable(String),
            host: T.nilable(String),
            port: T.nilable(Integer),
            path: T.nilable(String),
            query: T::Hash[String, T::Array[String]]
          }
        end

      # @api private
      #
      # An adapter that satisfies the IO interface required by `::IO.copy_stream`
      class ReadIOAdapter
        # @api private
        sig { void }
        def close; end

        # @api private
        sig { returns(T.nilable(T::Boolean)) }
        def close?; end

        # @api private
        sig { params(max_len: T.nilable(Integer), out_string: T.nilable(String)).returns(T.nilable(String)) }
        def read(max_len = nil, out_string = nil); end

        private

        # @api private
        sig { params(max_len: T.nilable(Integer)).returns(String) }
        def read_enum(max_len); end

        class << self
          # @api private
          sig do
            params(
              src: T.any(String, Pathname, StringIO, T::Enumerable[String]),
              blk: T.proc.params(arg0: String).void
            ).returns(T.attached_class)
          end
          def new(src, &blk); end
        end
      end

      ServerSentEvent = T.type_alias do
          {
            event: T.nilable(String),
            data: T.nilable(String),
            id: T.nilable(String),
            retry: T.nilable(Integer)
          }
        end

      # @api private
      module SorbetRuntimeSupport
        # @api private
        sig { params(name: Symbol).void }
        def const_missing(name); end

        # @api private
        sig { params(name: Symbol, blk: T.proc.returns(T.anything)).void }
        def define_sorbet_constant!(name, &blk); end

        # @api private
        sig { params(name: Symbol).returns(T::Boolean) }
        def sorbet_constant_defined?(name); end

        # @api private
        sig { returns(T.anything) }
        def to_sorbet_type; end

        private

        # @api private
        sig { returns(T::Hash[Symbol, T.anything]) }
        def sorbet_runtime_constants; end

        class << self
          # @api private
          sig { params(type: T.any(OpenAI::Internal::Util::SorbetRuntimeSupport, T.anything)).returns(T.anything) }
          def to_sorbet_type(type); end
        end

        class MissingSorbetRuntimeError < ::RuntimeError; end
      end
    end
  end

  Metadata = T.let(OpenAI::Models::Metadata, OpenAI::Internal::Type::Converter)
  Model = OpenAI::Models::Model
  ModelDeleteParams = OpenAI::Models::ModelDeleteParams
  ModelDeleted = OpenAI::Models::ModelDeleted
  ModelListParams = OpenAI::Models::ModelListParams
  ModelRetrieveParams = OpenAI::Models::ModelRetrieveParams

  module Models
    module AllModels
      extend OpenAI::Internal::Type::Union

      class << self
        sig { override.returns(T::Array[OpenAI::AllModels::Variants]) }
        def variants; end
      end

      module ResponsesOnlyModel
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol]) }
          def values; end
        end

        COMPUTER_USE_PREVIEW = T.let(
            :"computer-use-preview",
            OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol
          )

        COMPUTER_USE_PREVIEW_2025_03_11 = T.let(
            :"computer-use-preview-2025-03-11",
            OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol
          )

        O1_PRO = T.let(:"o1-pro", OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol)

        O1_PRO_2025_03_19 = T.let(
            :"o1-pro-2025-03-19",
            OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol
          )

        O3_DEEP_RESEARCH = T.let(
            :"o3-deep-research",
            OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol
          )

        O3_DEEP_RESEARCH_2025_06_26 = T.let(
            :"o3-deep-research-2025-06-26",
            OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol
          )

        O3_PRO = T.let(:"o3-pro", OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol)

        O3_PRO_2025_06_10 = T.let(
            :"o3-pro-2025-06-10",
            OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol
          )

        O4_MINI_DEEP_RESEARCH = T.let(
            :"o4-mini-deep-research",
            OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol
          )

        O4_MINI_DEEP_RESEARCH_2025_06_26 = T.let(
            :"o4-mini-deep-research-2025-06-26",
            OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::AllModels::ResponsesOnlyModel) }
      end

      Variants = T.type_alias do
          T.any(
            String,
            OpenAI::ChatModel::TaggedSymbol,
            OpenAI::AllModels::ResponsesOnlyModel::TaggedSymbol
          )
        end
    end

    module Audio
      class SpeechCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # The text to generate audio for. The maximum length is 4096 characters.
        sig { returns(String) }
        attr_accessor :input

        # Control the voice of your generated audio with additional instructions. Does not
        # work with `tts-1` or `tts-1-hd`.
        sig { returns(T.nilable(String)) }
        attr_reader :instructions

        sig { params(instructions: String).void }
        attr_writer :instructions

        # One of the available [TTS models](https://platform.openai.com/docs/models#tts):
        # `tts-1`, `tts-1-hd` or `gpt-4o-mini-tts`.
        sig { returns(T.any(String, OpenAI::Audio::SpeechModel::OrSymbol)) }
        attr_accessor :model

        # The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`,
        # `wav`, and `pcm`.
        sig do
          returns(T.nilable(
              OpenAI::Audio::SpeechCreateParams::ResponseFormat::OrSymbol
            ))
        end
        attr_reader :response_format

        sig { params(response_format: OpenAI::Audio::SpeechCreateParams::ResponseFormat::OrSymbol).void }
        attr_writer :response_format

        # The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is
        # the default.
        sig { returns(T.nilable(Float)) }
        attr_reader :speed

        sig { params(speed: Float).void }
        attr_writer :speed

        # The format to stream the audio in. Supported formats are `sse` and `audio`.
        # `sse` is not supported for `tts-1` or `tts-1-hd`.
        sig { returns(T.nilable(OpenAI::Audio::SpeechCreateParams::StreamFormat::OrSymbol)) }
        attr_reader :stream_format

        sig { params(stream_format: OpenAI::Audio::SpeechCreateParams::StreamFormat::OrSymbol).void }
        attr_writer :stream_format

        # The voice to use when generating the audio. Supported voices are `alloy`, `ash`,
        # `ballad`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`, `shimmer`, and
        # `verse`. Previews of the voices are available in the
        # [Text to speech guide](https://platform.openai.com/docs/guides/text-to-speech#voice-options).
        sig { returns(T.any(String, OpenAI::Audio::SpeechCreateParams::Voice::OrSymbol)) }
        attr_accessor :voice

        sig do
          override
            .returns({
              input: String,
              model: T.any(String, OpenAI::Audio::SpeechModel::OrSymbol),
              voice:
                T.any(
                  String,
                  OpenAI::Audio::SpeechCreateParams::Voice::OrSymbol
                ),
              instructions: String,
              response_format:
                OpenAI::Audio::SpeechCreateParams::ResponseFormat::OrSymbol,
              speed: Float,
              stream_format:
                OpenAI::Audio::SpeechCreateParams::StreamFormat::OrSymbol,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              input: String,
              model: T.any(String, OpenAI::Audio::SpeechModel::OrSymbol),
              voice: T.any(String, OpenAI::Audio::SpeechCreateParams::Voice::OrSymbol),
              instructions: String,
              response_format: OpenAI::Audio::SpeechCreateParams::ResponseFormat::OrSymbol,
              speed: Float,
              stream_format: OpenAI::Audio::SpeechCreateParams::StreamFormat::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            input:, # The text to generate audio for. The maximum length is 4096 characters.
            model:, # One of the available [TTS models](https://platform.openai.com/docs/models#tts):
                    # `tts-1`, `tts-1-hd` or `gpt-4o-mini-tts`.
            voice:, # The voice to use when generating the audio. Supported voices are `alloy`, `ash`,
                    # `ballad`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`, `shimmer`, and
                    # `verse`. Previews of the voices are available in the
                    # [Text to speech guide](https://platform.openai.com/docs/guides/text-to-speech#voice-options).
            instructions: nil, # Control the voice of your generated audio with additional instructions. Does not
                               # work with `tts-1` or `tts-1-hd`.
            response_format: nil, # The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`,
                                  # `wav`, and `pcm`.
            speed: nil, # The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is
                        # the default.
            stream_format: nil, # The format to stream the audio in. Supported formats are `sse` and `audio`.
                                # `sse` is not supported for `tts-1` or `tts-1-hd`.
            request_options: {}
); end
        end

        # One of the available [TTS models](https://platform.openai.com/docs/models#tts):
        # `tts-1`, `tts-1-hd` or `gpt-4o-mini-tts`.
        module Model
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Audio::SpeechCreateParams::Model::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(String, OpenAI::Audio::SpeechModel::TaggedSymbol)
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Audio::SpeechCreateParams, OpenAI::Internal::AnyHash)
          end

        # The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`,
        # `wav`, and `pcm`.
        module ResponseFormat
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Audio::SpeechCreateParams::ResponseFormat::TaggedSymbol
              ])
            end
            def values; end
          end

          AAC = T.let(
              :aac,
              OpenAI::Audio::SpeechCreateParams::ResponseFormat::TaggedSymbol
            )

          FLAC = T.let(
              :flac,
              OpenAI::Audio::SpeechCreateParams::ResponseFormat::TaggedSymbol
            )

          MP3 = T.let(
              :mp3,
              OpenAI::Audio::SpeechCreateParams::ResponseFormat::TaggedSymbol
            )

          OPUS = T.let(
              :opus,
              OpenAI::Audio::SpeechCreateParams::ResponseFormat::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          PCM = T.let(
              :pcm,
              OpenAI::Audio::SpeechCreateParams::ResponseFormat::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Audio::SpeechCreateParams::ResponseFormat)
            end

          WAV = T.let(
              :wav,
              OpenAI::Audio::SpeechCreateParams::ResponseFormat::TaggedSymbol
            )
        end

        # The format to stream the audio in. Supported formats are `sse` and `audio`.
        # `sse` is not supported for `tts-1` or `tts-1-hd`.
        module StreamFormat
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Audio::SpeechCreateParams::StreamFormat::TaggedSymbol
              ])
            end
            def values; end
          end

          AUDIO = T.let(
              :audio,
              OpenAI::Audio::SpeechCreateParams::StreamFormat::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          SSE = T.let(
              :sse,
              OpenAI::Audio::SpeechCreateParams::StreamFormat::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Audio::SpeechCreateParams::StreamFormat)
            end
        end

        # The voice to use when generating the audio. Supported voices are `alloy`, `ash`,
        # `ballad`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`, `shimmer`, and
        # `verse`. Previews of the voices are available in the
        # [Text to speech guide](https://platform.openai.com/docs/guides/text-to-speech#voice-options).
        module Voice
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Audio::SpeechCreateParams::Voice::Variants]) }
            def variants; end
          end

          ALLOY = T.let(
              :alloy,
              OpenAI::Audio::SpeechCreateParams::Voice::TaggedSymbol
            )

          ASH = T.let(:ash, OpenAI::Audio::SpeechCreateParams::Voice::TaggedSymbol)

          BALLAD = T.let(
              :ballad,
              OpenAI::Audio::SpeechCreateParams::Voice::TaggedSymbol
            )

          CORAL = T.let(
              :coral,
              OpenAI::Audio::SpeechCreateParams::Voice::TaggedSymbol
            )

          ECHO = T.let(:echo, OpenAI::Audio::SpeechCreateParams::Voice::TaggedSymbol)

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          SAGE = T.let(:sage, OpenAI::Audio::SpeechCreateParams::Voice::TaggedSymbol)

          SHIMMER = T.let(
              :shimmer,
              OpenAI::Audio::SpeechCreateParams::Voice::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Audio::SpeechCreateParams::Voice)
            end

          VERSE = T.let(
              :verse,
              OpenAI::Audio::SpeechCreateParams::Voice::TaggedSymbol
            )

          Variants = T.type_alias do
              T.any(
                String,
                OpenAI::Audio::SpeechCreateParams::Voice::TaggedSymbol
              )
            end
        end
      end

      module SpeechModel
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Audio::SpeechModel::TaggedSymbol]) }
          def values; end
        end

        GPT_4O_MINI_TTS = T.let(:"gpt-4o-mini-tts", OpenAI::Audio::SpeechModel::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }
        TTS_1 = T.let(:"tts-1", OpenAI::Audio::SpeechModel::TaggedSymbol)
        TTS_1_HD = T.let(:"tts-1-hd", OpenAI::Audio::SpeechModel::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Audio::SpeechModel) }
      end

      class Transcription < OpenAI::Internal::Type::BaseModel
        # The log probabilities of the tokens in the transcription. Only returned with the
        # models `gpt-4o-transcribe` and `gpt-4o-mini-transcribe` if `logprobs` is added
        # to the `include` array.
        sig { returns(T.nilable(T::Array[OpenAI::Audio::Transcription::Logprob])) }
        attr_reader :logprobs

        sig { params(logprobs: T::Array[OpenAI::Audio::Transcription::Logprob::OrHash]).void }
        attr_writer :logprobs

        # The transcribed text.
        sig { returns(String) }
        attr_accessor :text

        # Token usage statistics for the request.
        sig { returns(T.nilable(OpenAI::Audio::Transcription::Usage::Variants)) }
        attr_reader :usage

        sig do
          params(
            usage: T.any(
                OpenAI::Audio::Transcription::Usage::Tokens::OrHash,
                OpenAI::Audio::Transcription::Usage::Duration::OrHash
              )
          ).void
        end
        attr_writer :usage

        sig do
          override
            .returns({
              text: String,
              logprobs: T::Array[OpenAI::Audio::Transcription::Logprob],
              usage: OpenAI::Audio::Transcription::Usage::Variants
            })
        end
        def to_hash; end

        class << self
          # Represents a transcription response returned by model, based on the provided
          # input.
          sig do
            params(
              text: String,
              logprobs: T::Array[OpenAI::Audio::Transcription::Logprob::OrHash],
              usage: T.any(
                OpenAI::Audio::Transcription::Usage::Tokens::OrHash,
                OpenAI::Audio::Transcription::Usage::Duration::OrHash
              )
            ).returns(T.attached_class)
          end
          def new(
            text:, # The transcribed text.
            logprobs: nil, # The log probabilities of the tokens in the transcription. Only returned with the
                           # models `gpt-4o-transcribe` and `gpt-4o-mini-transcribe` if `logprobs` is added
                           # to the `include` array.
            usage: nil # Token usage statistics for the request.
); end
        end

        class Logprob < OpenAI::Internal::Type::BaseModel
          # The bytes of the token.
          sig { returns(T.nilable(T::Array[Float])) }
          attr_reader :bytes

          sig { params(bytes: T::Array[Float]).void }
          attr_writer :bytes

          # The log probability of the token.
          sig { returns(T.nilable(Float)) }
          attr_reader :logprob

          sig { params(logprob: Float).void }
          attr_writer :logprob

          # The token in the transcription.
          sig { returns(T.nilable(String)) }
          attr_reader :token

          sig { params(token: String).void }
          attr_writer :token

          sig { override.returns({ token: String, bytes: T::Array[Float], logprob: Float }) }
          def to_hash; end

          class << self
            sig { params(token: String, bytes: T::Array[Float], logprob: Float).returns(T.attached_class) }
            def new(
              token: nil, # The token in the transcription.
              bytes: nil, # The bytes of the token.
              logprob: nil # The log probability of the token.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Audio::Transcription::Logprob,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Audio::Transcription, OpenAI::Internal::AnyHash)
          end

        # Token usage statistics for the request.
        module Usage
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Audio::Transcription::Usage::Variants]) }
            def variants; end
          end

          class Duration < OpenAI::Internal::Type::BaseModel
            # Duration of the input audio in seconds.
            sig { returns(Float) }
            attr_accessor :seconds

            # The type of the usage object. Always `duration` for this variant.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ seconds: Float, type: Symbol }) }
            def to_hash; end

            class << self
              # Usage statistics for models billed by audio input duration.
              sig { params(seconds: Float, type: Symbol).returns(T.attached_class) }
              def new(
                seconds:, # Duration of the input audio in seconds.
                type: :duration # The type of the usage object. Always `duration` for this variant.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Audio::Transcription::Usage::Duration,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class Tokens < OpenAI::Internal::Type::BaseModel
            # Details about the input tokens billed for this request.
            sig do
              returns(T.nilable(
                  OpenAI::Audio::Transcription::Usage::Tokens::InputTokenDetails
                ))
            end
            attr_reader :input_token_details

            sig do
              params(
                input_token_details: OpenAI::Audio::Transcription::Usage::Tokens::InputTokenDetails::OrHash
              ).void
            end
            attr_writer :input_token_details

            # Number of input tokens billed for this request.
            sig { returns(Integer) }
            attr_accessor :input_tokens

            # Number of output tokens generated.
            sig { returns(Integer) }
            attr_accessor :output_tokens

            # Total number of tokens used (input + output).
            sig { returns(Integer) }
            attr_accessor :total_tokens

            # The type of the usage object. Always `tokens` for this variant.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  input_tokens: Integer,
                  output_tokens: Integer,
                  total_tokens: Integer,
                  type: Symbol,
                  input_token_details:
                    OpenAI::Audio::Transcription::Usage::Tokens::InputTokenDetails
                })
            end
            def to_hash; end

            class << self
              # Usage statistics for models billed by token usage.
              sig do
                params(
                  input_tokens: Integer,
                  output_tokens: Integer,
                  total_tokens: Integer,
                  input_token_details: OpenAI::Audio::Transcription::Usage::Tokens::InputTokenDetails::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                input_tokens:, # Number of input tokens billed for this request.
                output_tokens:, # Number of output tokens generated.
                total_tokens:, # Total number of tokens used (input + output).
                input_token_details: nil, # Details about the input tokens billed for this request.
                type: :tokens # The type of the usage object. Always `tokens` for this variant.
); end
            end

            class InputTokenDetails < OpenAI::Internal::Type::BaseModel
              # Number of audio tokens billed for this request.
              sig { returns(T.nilable(Integer)) }
              attr_reader :audio_tokens

              sig { params(audio_tokens: Integer).void }
              attr_writer :audio_tokens

              # Number of text tokens billed for this request.
              sig { returns(T.nilable(Integer)) }
              attr_reader :text_tokens

              sig { params(text_tokens: Integer).void }
              attr_writer :text_tokens

              sig { override.returns({ audio_tokens: Integer, text_tokens: Integer }) }
              def to_hash; end

              class << self
                # Details about the input tokens billed for this request.
                sig { params(audio_tokens: Integer, text_tokens: Integer).returns(T.attached_class) }
                def new(
                  audio_tokens: nil, # Number of audio tokens billed for this request.
                  text_tokens: nil # Number of text tokens billed for this request.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Audio::Transcription::Usage::Tokens::InputTokenDetails,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Audio::Transcription::Usage::Tokens,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Audio::Transcription::Usage::Tokens,
                OpenAI::Audio::Transcription::Usage::Duration
              )
            end
        end
      end

      class TranscriptionCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Controls how the audio is cut into chunks. When set to `"auto"`, the server
        # first normalizes loudness and then uses voice activity detection (VAD) to choose
        # boundaries. `server_vad` object can be provided to tweak VAD detection
        # parameters manually. If unset, the audio is transcribed as a single block.
        sig do
          returns(T.nilable(
              T.any(
                Symbol,
                OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig
              )
            ))
        end
        attr_accessor :chunking_strategy

        # The audio file object (not file name) to transcribe, in one of these formats:
        # flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
        sig { returns(OpenAI::Internal::FileInput) }
        attr_accessor :file

        # Additional information to include in the transcription response. `logprobs` will
        # return the log probabilities of the tokens in the response to understand the
        # model's confidence in the transcription. `logprobs` only works with
        # response_format set to `json` and only with the models `gpt-4o-transcribe` and
        # `gpt-4o-mini-transcribe`.
        sig { returns(T.nilable(T::Array[OpenAI::Audio::TranscriptionInclude::OrSymbol])) }
        attr_reader :include

        sig { params(include: T::Array[OpenAI::Audio::TranscriptionInclude::OrSymbol]).void }
        attr_writer :include

        # The language of the input audio. Supplying the input language in
        # [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)
        # format will improve accuracy and latency.
        sig { returns(T.nilable(String)) }
        attr_reader :language

        sig { params(language: String).void }
        attr_writer :language

        # ID of the model to use. The options are `gpt-4o-transcribe`,
        # `gpt-4o-mini-transcribe`, and `whisper-1` (which is powered by our open source
        # Whisper V2 model).
        sig { returns(T.any(String, OpenAI::AudioModel::OrSymbol)) }
        attr_accessor :model

        # An optional text to guide the model's style or continue a previous audio
        # segment. The
        # [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)
        # should match the audio language.
        sig { returns(T.nilable(String)) }
        attr_reader :prompt

        sig { params(prompt: String).void }
        attr_writer :prompt

        # The format of the output, in one of these options: `json`, `text`, `srt`,
        # `verbose_json`, or `vtt`. For `gpt-4o-transcribe` and `gpt-4o-mini-transcribe`,
        # the only supported format is `json`.
        sig { returns(T.nilable(OpenAI::AudioResponseFormat::OrSymbol)) }
        attr_reader :response_format

        sig { params(response_format: OpenAI::AudioResponseFormat::OrSymbol).void }
        attr_writer :response_format

        # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the
        # output more random, while lower values like 0.2 will make it more focused and
        # deterministic. If set to 0, the model will use
        # [log probability](https://en.wikipedia.org/wiki/Log_probability) to
        # automatically increase the temperature until certain thresholds are hit.
        sig { returns(T.nilable(Float)) }
        attr_reader :temperature

        sig { params(temperature: Float).void }
        attr_writer :temperature

        # The timestamp granularities to populate for this transcription.
        # `response_format` must be set `verbose_json` to use timestamp granularities.
        # Either or both of these options are supported: `word`, or `segment`. Note: There
        # is no additional latency for segment timestamps, but generating word timestamps
        # incurs additional latency.
        sig do
          returns(T.nilable(
              T::Array[
                OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity::OrSymbol
              ]
            ))
        end
        attr_reader :timestamp_granularities

        sig do
          params(
            timestamp_granularities: T::Array[
                OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity::OrSymbol
              ]
          ).void
        end
        attr_writer :timestamp_granularities

        sig do
          override
            .returns({
              file: OpenAI::Internal::FileInput,
              model: T.any(String, OpenAI::AudioModel::OrSymbol),
              chunking_strategy:
                T.nilable(
                  T.any(
                    Symbol,
                    OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig
                  )
                ),
              include: T::Array[OpenAI::Audio::TranscriptionInclude::OrSymbol],
              language: String,
              prompt: String,
              response_format: OpenAI::AudioResponseFormat::OrSymbol,
              temperature: Float,
              timestamp_granularities:
                T::Array[
                  OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity::OrSymbol
                ],
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              file: OpenAI::Internal::FileInput,
              model: T.any(String, OpenAI::AudioModel::OrSymbol),
              chunking_strategy: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig::OrHash
                )
              ),
              include: T::Array[OpenAI::Audio::TranscriptionInclude::OrSymbol],
              language: String,
              prompt: String,
              response_format: OpenAI::AudioResponseFormat::OrSymbol,
              temperature: Float,
              timestamp_granularities: T::Array[
                OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity::OrSymbol
              ],
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            file:, # The audio file object (not file name) to transcribe, in one of these formats:
                   # flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
            model:, # ID of the model to use. The options are `gpt-4o-transcribe`,
                    # `gpt-4o-mini-transcribe`, and `whisper-1` (which is powered by our open source
                    # Whisper V2 model).
            chunking_strategy: nil, # Controls how the audio is cut into chunks. When set to `"auto"`, the server
                                    # first normalizes loudness and then uses voice activity detection (VAD) to choose
                                    # boundaries. `server_vad` object can be provided to tweak VAD detection
                                    # parameters manually. If unset, the audio is transcribed as a single block.
            include: nil, # Additional information to include in the transcription response. `logprobs` will
                          # return the log probabilities of the tokens in the response to understand the
                          # model's confidence in the transcription. `logprobs` only works with
                          # response_format set to `json` and only with the models `gpt-4o-transcribe` and
                          # `gpt-4o-mini-transcribe`.
            language: nil, # The language of the input audio. Supplying the input language in
                           # [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)
                           # format will improve accuracy and latency.
            prompt: nil, # An optional text to guide the model's style or continue a previous audio
                         # segment. The
                         # [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)
                         # should match the audio language.
            response_format: nil, # The format of the output, in one of these options: `json`, `text`, `srt`,
                                  # `verbose_json`, or `vtt`. For `gpt-4o-transcribe` and `gpt-4o-mini-transcribe`,
                                  # the only supported format is `json`.
            temperature: nil, # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the
                              # output more random, while lower values like 0.2 will make it more focused and
                              # deterministic. If set to 0, the model will use
                              # [log probability](https://en.wikipedia.org/wiki/Log_probability) to
                              # automatically increase the temperature until certain thresholds are hit.
            timestamp_granularities: nil, # The timestamp granularities to populate for this transcription.
                                          # `response_format` must be set `verbose_json` to use timestamp granularities.
                                          # Either or both of these options are supported: `word`, or `segment`. Note: There
                                          # is no additional latency for segment timestamps, but generating word timestamps
                                          # incurs additional latency.
            request_options: {}
); end
        end

        # Controls how the audio is cut into chunks. When set to `"auto"`, the server
        # first normalizes loudness and then uses voice activity detection (VAD) to choose
        # boundaries. `server_vad` object can be provided to tweak VAD detection
        # parameters manually. If unset, the audio is transcribed as a single block.
        module ChunkingStrategy
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::Variants
              ])
            end
            def variants; end
          end

          class VadConfig < OpenAI::Internal::Type::BaseModel
            # Amount of audio to include before the VAD detected speech (in milliseconds).
            sig { returns(T.nilable(Integer)) }
            attr_reader :prefix_padding_ms

            sig { params(prefix_padding_ms: Integer).void }
            attr_writer :prefix_padding_ms

            # Duration of silence to detect speech stop (in milliseconds). With shorter values
            # the model will respond more quickly, but may jump in on short pauses from the
            # user.
            sig { returns(T.nilable(Integer)) }
            attr_reader :silence_duration_ms

            sig { params(silence_duration_ms: Integer).void }
            attr_writer :silence_duration_ms

            # Sensitivity threshold (0.0 to 1.0) for voice activity detection. A higher
            # threshold will require louder audio to activate the model, and thus might
            # perform better in noisy environments.
            sig { returns(T.nilable(Float)) }
            attr_reader :threshold

            sig { params(threshold: Float).void }
            attr_writer :threshold

            # Must be set to `server_vad` to enable manual chunking using server side VAD.
            sig { returns(OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig::Type::OrSymbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  type:
                    OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig::Type::OrSymbol,
                  prefix_padding_ms: Integer,
                  silence_duration_ms: Integer,
                  threshold: Float
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  type: OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig::Type::OrSymbol,
                  prefix_padding_ms: Integer,
                  silence_duration_ms: Integer,
                  threshold: Float
                ).returns(T.attached_class)
              end
              def new(
                type:, # Must be set to `server_vad` to enable manual chunking using server side VAD.
                prefix_padding_ms: nil, # Amount of audio to include before the VAD detected speech (in milliseconds).
                silence_duration_ms: nil, # Duration of silence to detect speech stop (in milliseconds). With shorter values
                                          # the model will respond more quickly, but may jump in on short pauses from the
                                          # user.
                threshold: nil # Sensitivity threshold (0.0 to 1.0) for voice activity detection. A higher
                               # threshold will require louder audio to activate the model, and thus might
                               # perform better in noisy environments.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig,
                  OpenAI::Internal::AnyHash
                )
              end

            # Must be set to `server_vad` to enable manual chunking using server side VAD.
            module Type
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig::Type::TaggedSymbol
                  ])
                end
                def values; end
              end

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              SERVER_VAD = T.let(
                  :server_vad,
                  OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig::Type::TaggedSymbol
                )

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig::Type
                  )
                end
            end
          end

          Variants = T.type_alias do
              T.any(
                Symbol,
                OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig
              )
            end
        end

        # ID of the model to use. The options are `gpt-4o-transcribe`,
        # `gpt-4o-mini-transcribe`, and `whisper-1` (which is powered by our open source
        # Whisper V2 model).
        module Model
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Audio::TranscriptionCreateParams::Model::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(String, OpenAI::AudioModel::TaggedSymbol) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Audio::TranscriptionCreateParams,
              OpenAI::Internal::AnyHash
            )
          end

        module TimestampGranularity
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity::TaggedSymbol
              ])
            end
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          SEGMENT = T.let(
              :segment,
              OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity
              )
            end

          WORD = T.let(
              :word,
              OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity::TaggedSymbol
            )
        end
      end

      # Represents a transcription response returned by model, based on the provided
      # input.
      module TranscriptionCreateResponse
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::Audio::TranscriptionCreateResponse::Variants
            ])
          end
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Audio::Transcription,
              OpenAI::Audio::TranscriptionVerbose
            )
          end
      end

      module TranscriptionInclude
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Audio::TranscriptionInclude::TaggedSymbol]) }
          def values; end
        end

        LOGPROBS = T.let(:logprobs, OpenAI::Audio::TranscriptionInclude::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Audio::TranscriptionInclude) }
      end

      class TranscriptionSegment < OpenAI::Internal::Type::BaseModel
        # Average logprob of the segment. If the value is lower than -1, consider the
        # logprobs failed.
        sig { returns(Float) }
        attr_accessor :avg_logprob

        # Compression ratio of the segment. If the value is greater than 2.4, consider the
        # compression failed.
        sig { returns(Float) }
        attr_accessor :compression_ratio

        # End time of the segment in seconds.
        sig { returns(Float) }
        attr_accessor :end_

        # Unique identifier of the segment.
        sig { returns(Integer) }
        attr_accessor :id

        # Probability of no speech in the segment. If the value is higher than 1.0 and the
        # `avg_logprob` is below -1, consider this segment silent.
        sig { returns(Float) }
        attr_accessor :no_speech_prob

        # Seek offset of the segment.
        sig { returns(Integer) }
        attr_accessor :seek

        # Start time of the segment in seconds.
        sig { returns(Float) }
        attr_accessor :start

        # Temperature parameter used for generating the segment.
        sig { returns(Float) }
        attr_accessor :temperature

        # Text content of the segment.
        sig { returns(String) }
        attr_accessor :text

        # Array of token IDs for the text content.
        sig { returns(T::Array[Integer]) }
        attr_accessor :tokens

        sig do
          override
            .returns({
              id: Integer,
              avg_logprob: Float,
              compression_ratio: Float,
              end_: Float,
              no_speech_prob: Float,
              seek: Integer,
              start: Float,
              temperature: Float,
              text: String,
              tokens: T::Array[Integer]
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              id: Integer,
              avg_logprob: Float,
              compression_ratio: Float,
              end_: Float,
              no_speech_prob: Float,
              seek: Integer,
              start: Float,
              temperature: Float,
              text: String,
              tokens: T::Array[Integer]
            ).returns(T.attached_class)
          end
          def new(
            id:, # Unique identifier of the segment.
            avg_logprob:, # Average logprob of the segment. If the value is lower than -1, consider the
                          # logprobs failed.
            compression_ratio:, # Compression ratio of the segment. If the value is greater than 2.4, consider the
                                # compression failed.
            end_:, # End time of the segment in seconds.
            no_speech_prob:, # Probability of no speech in the segment. If the value is higher than 1.0 and the
                             # `avg_logprob` is below -1, consider this segment silent.
            seek:, # Seek offset of the segment.
            start:, # Start time of the segment in seconds.
            temperature:, # Temperature parameter used for generating the segment.
            text:, # Text content of the segment.
            tokens: # Array of token IDs for the text content.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Audio::TranscriptionSegment,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # Emitted when there is an additional text delta. This is also the first event
      # emitted when the transcription starts. Only emitted when you
      # [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)
      # with the `Stream` parameter set to `true`.
      module TranscriptionStreamEvent
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Audio::TranscriptionStreamEvent::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Audio::TranscriptionTextDeltaEvent,
              OpenAI::Audio::TranscriptionTextDoneEvent
            )
          end
      end

      class TranscriptionTextDeltaEvent < OpenAI::Internal::Type::BaseModel
        # The text delta that was additionally transcribed.
        sig { returns(String) }
        attr_accessor :delta

        # The log probabilities of the delta. Only included if you
        # [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)
        # with the `include[]` parameter set to `logprobs`.
        sig do
          returns(T.nilable(
              T::Array[OpenAI::Audio::TranscriptionTextDeltaEvent::Logprob]
            ))
        end
        attr_reader :logprobs

        sig do
          params(
            logprobs: T::Array[
                OpenAI::Audio::TranscriptionTextDeltaEvent::Logprob::OrHash
              ]
          ).void
        end
        attr_writer :logprobs

        # The type of the event. Always `transcript.text.delta`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              delta: String,
              type: Symbol,
              logprobs:
                T::Array[OpenAI::Audio::TranscriptionTextDeltaEvent::Logprob]
            })
        end
        def to_hash; end

        class << self
          # Emitted when there is an additional text delta. This is also the first event
          # emitted when the transcription starts. Only emitted when you
          # [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)
          # with the `Stream` parameter set to `true`.
          sig do
            params(
              delta: String,
              logprobs: T::Array[
                OpenAI::Audio::TranscriptionTextDeltaEvent::Logprob::OrHash
              ],
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            delta:, # The text delta that was additionally transcribed.
            logprobs: nil, # The log probabilities of the delta. Only included if you
                           # [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)
                           # with the `include[]` parameter set to `logprobs`.
            type: :"transcript.text.delta" # The type of the event. Always `transcript.text.delta`.
); end
        end

        class Logprob < OpenAI::Internal::Type::BaseModel
          # The bytes that were used to generate the log probability.
          sig { returns(T.nilable(T::Array[Integer])) }
          attr_reader :bytes

          sig { params(bytes: T::Array[Integer]).void }
          attr_writer :bytes

          # The log probability of the token.
          sig { returns(T.nilable(Float)) }
          attr_reader :logprob

          sig { params(logprob: Float).void }
          attr_writer :logprob

          # The token that was used to generate the log probability.
          sig { returns(T.nilable(String)) }
          attr_reader :token

          sig { params(token: String).void }
          attr_writer :token

          sig { override.returns({ token: String, bytes: T::Array[Integer], logprob: Float }) }
          def to_hash; end

          class << self
            sig { params(token: String, bytes: T::Array[Integer], logprob: Float).returns(T.attached_class) }
            def new(
              token: nil, # The token that was used to generate the log probability.
              bytes: nil, # The bytes that were used to generate the log probability.
              logprob: nil # The log probability of the token.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Audio::TranscriptionTextDeltaEvent::Logprob,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Audio::TranscriptionTextDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class TranscriptionTextDoneEvent < OpenAI::Internal::Type::BaseModel
        # The log probabilities of the individual tokens in the transcription. Only
        # included if you
        # [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)
        # with the `include[]` parameter set to `logprobs`.
        sig do
          returns(T.nilable(
              T::Array[OpenAI::Audio::TranscriptionTextDoneEvent::Logprob]
            ))
        end
        attr_reader :logprobs

        sig do
          params(
            logprobs: T::Array[
                OpenAI::Audio::TranscriptionTextDoneEvent::Logprob::OrHash
              ]
          ).void
        end
        attr_writer :logprobs

        # The text that was transcribed.
        sig { returns(String) }
        attr_accessor :text

        # The type of the event. Always `transcript.text.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        # Usage statistics for models billed by token usage.
        sig { returns(T.nilable(OpenAI::Audio::TranscriptionTextDoneEvent::Usage)) }
        attr_reader :usage

        sig { params(usage: OpenAI::Audio::TranscriptionTextDoneEvent::Usage::OrHash).void }
        attr_writer :usage

        sig do
          override
            .returns({
              text: String,
              type: Symbol,
              logprobs:
                T::Array[OpenAI::Audio::TranscriptionTextDoneEvent::Logprob],
              usage: OpenAI::Audio::TranscriptionTextDoneEvent::Usage
            })
        end
        def to_hash; end

        class << self
          # Emitted when the transcription is complete. Contains the complete transcription
          # text. Only emitted when you
          # [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)
          # with the `Stream` parameter set to `true`.
          sig do
            params(
              text: String,
              logprobs: T::Array[
                OpenAI::Audio::TranscriptionTextDoneEvent::Logprob::OrHash
              ],
              usage: OpenAI::Audio::TranscriptionTextDoneEvent::Usage::OrHash,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            text:, # The text that was transcribed.
            logprobs: nil, # The log probabilities of the individual tokens in the transcription. Only
                           # included if you
                           # [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription)
                           # with the `include[]` parameter set to `logprobs`.
            usage: nil, # Usage statistics for models billed by token usage.
            type: :"transcript.text.done" # The type of the event. Always `transcript.text.done`.
); end
        end

        class Logprob < OpenAI::Internal::Type::BaseModel
          # The bytes that were used to generate the log probability.
          sig { returns(T.nilable(T::Array[Integer])) }
          attr_reader :bytes

          sig { params(bytes: T::Array[Integer]).void }
          attr_writer :bytes

          # The log probability of the token.
          sig { returns(T.nilable(Float)) }
          attr_reader :logprob

          sig { params(logprob: Float).void }
          attr_writer :logprob

          # The token that was used to generate the log probability.
          sig { returns(T.nilable(String)) }
          attr_reader :token

          sig { params(token: String).void }
          attr_writer :token

          sig { override.returns({ token: String, bytes: T::Array[Integer], logprob: Float }) }
          def to_hash; end

          class << self
            sig { params(token: String, bytes: T::Array[Integer], logprob: Float).returns(T.attached_class) }
            def new(
              token: nil, # The token that was used to generate the log probability.
              bytes: nil, # The bytes that were used to generate the log probability.
              logprob: nil # The log probability of the token.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Audio::TranscriptionTextDoneEvent::Logprob,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Audio::TranscriptionTextDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end

        class Usage < OpenAI::Internal::Type::BaseModel
          # Details about the input tokens billed for this request.
          sig do
            returns(T.nilable(
                OpenAI::Audio::TranscriptionTextDoneEvent::Usage::InputTokenDetails
              ))
          end
          attr_reader :input_token_details

          sig do
            params(
              input_token_details: OpenAI::Audio::TranscriptionTextDoneEvent::Usage::InputTokenDetails::OrHash
            ).void
          end
          attr_writer :input_token_details

          # Number of input tokens billed for this request.
          sig { returns(Integer) }
          attr_accessor :input_tokens

          # Number of output tokens generated.
          sig { returns(Integer) }
          attr_accessor :output_tokens

          # Total number of tokens used (input + output).
          sig { returns(Integer) }
          attr_accessor :total_tokens

          # The type of the usage object. Always `tokens` for this variant.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                input_tokens: Integer,
                output_tokens: Integer,
                total_tokens: Integer,
                type: Symbol,
                input_token_details:
                  OpenAI::Audio::TranscriptionTextDoneEvent::Usage::InputTokenDetails
              })
          end
          def to_hash; end

          class << self
            # Usage statistics for models billed by token usage.
            sig do
              params(
                input_tokens: Integer,
                output_tokens: Integer,
                total_tokens: Integer,
                input_token_details: OpenAI::Audio::TranscriptionTextDoneEvent::Usage::InputTokenDetails::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              input_tokens:, # Number of input tokens billed for this request.
              output_tokens:, # Number of output tokens generated.
              total_tokens:, # Total number of tokens used (input + output).
              input_token_details: nil, # Details about the input tokens billed for this request.
              type: :tokens # The type of the usage object. Always `tokens` for this variant.
); end
          end

          class InputTokenDetails < OpenAI::Internal::Type::BaseModel
            # Number of audio tokens billed for this request.
            sig { returns(T.nilable(Integer)) }
            attr_reader :audio_tokens

            sig { params(audio_tokens: Integer).void }
            attr_writer :audio_tokens

            # Number of text tokens billed for this request.
            sig { returns(T.nilable(Integer)) }
            attr_reader :text_tokens

            sig { params(text_tokens: Integer).void }
            attr_writer :text_tokens

            sig { override.returns({ audio_tokens: Integer, text_tokens: Integer }) }
            def to_hash; end

            class << self
              # Details about the input tokens billed for this request.
              sig { params(audio_tokens: Integer, text_tokens: Integer).returns(T.attached_class) }
              def new(
                audio_tokens: nil, # Number of audio tokens billed for this request.
                text_tokens: nil # Number of text tokens billed for this request.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Audio::TranscriptionTextDoneEvent::Usage::InputTokenDetails,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Audio::TranscriptionTextDoneEvent::Usage,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class TranscriptionVerbose < OpenAI::Internal::Type::BaseModel
        # The duration of the input audio.
        sig { returns(Float) }
        attr_accessor :duration

        # The language of the input audio.
        sig { returns(String) }
        attr_accessor :language

        # Segments of the transcribed text and their corresponding details.
        sig { returns(T.nilable(T::Array[OpenAI::Audio::TranscriptionSegment])) }
        attr_reader :segments

        sig { params(segments: T::Array[OpenAI::Audio::TranscriptionSegment::OrHash]).void }
        attr_writer :segments

        # The transcribed text.
        sig { returns(String) }
        attr_accessor :text

        # Usage statistics for models billed by audio input duration.
        sig { returns(T.nilable(OpenAI::Audio::TranscriptionVerbose::Usage)) }
        attr_reader :usage

        sig { params(usage: OpenAI::Audio::TranscriptionVerbose::Usage::OrHash).void }
        attr_writer :usage

        # Extracted words and their corresponding timestamps.
        sig { returns(T.nilable(T::Array[OpenAI::Audio::TranscriptionWord])) }
        attr_reader :words

        sig { params(words: T::Array[OpenAI::Audio::TranscriptionWord::OrHash]).void }
        attr_writer :words

        sig do
          override
            .returns({
              duration: Float,
              language: String,
              text: String,
              segments: T::Array[OpenAI::Audio::TranscriptionSegment],
              usage: OpenAI::Audio::TranscriptionVerbose::Usage,
              words: T::Array[OpenAI::Audio::TranscriptionWord]
            })
        end
        def to_hash; end

        class << self
          # Represents a verbose json transcription response returned by model, based on the
          # provided input.
          sig do
            params(
              duration: Float,
              language: String,
              text: String,
              segments: T::Array[OpenAI::Audio::TranscriptionSegment::OrHash],
              usage: OpenAI::Audio::TranscriptionVerbose::Usage::OrHash,
              words: T::Array[OpenAI::Audio::TranscriptionWord::OrHash]
            ).returns(T.attached_class)
          end
          def new(
            duration:, # The duration of the input audio.
            language:, # The language of the input audio.
            text:, # The transcribed text.
            segments: nil, # Segments of the transcribed text and their corresponding details.
            usage: nil, # Usage statistics for models billed by audio input duration.
            words: nil # Extracted words and their corresponding timestamps.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Audio::TranscriptionVerbose,
              OpenAI::Internal::AnyHash
            )
          end

        class Usage < OpenAI::Internal::Type::BaseModel
          # Duration of the input audio in seconds.
          sig { returns(Float) }
          attr_accessor :seconds

          # The type of the usage object. Always `duration` for this variant.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ seconds: Float, type: Symbol }) }
          def to_hash; end

          class << self
            # Usage statistics for models billed by audio input duration.
            sig { params(seconds: Float, type: Symbol).returns(T.attached_class) }
            def new(
              seconds:, # Duration of the input audio in seconds.
              type: :duration # The type of the usage object. Always `duration` for this variant.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Audio::TranscriptionVerbose::Usage,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class TranscriptionWord < OpenAI::Internal::Type::BaseModel
        # End time of the word in seconds.
        sig { returns(Float) }
        attr_accessor :end_

        # Start time of the word in seconds.
        sig { returns(Float) }
        attr_accessor :start

        # The text content of the word.
        sig { returns(String) }
        attr_accessor :word

        sig { override.returns({ end_: Float, start: Float, word: String }) }
        def to_hash; end

        class << self
          sig { params(end_: Float, start: Float, word: String).returns(T.attached_class) }
          def new(
            end_:, # End time of the word in seconds.
            start:, # Start time of the word in seconds.
            word: # The text content of the word.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Audio::TranscriptionWord, OpenAI::Internal::AnyHash)
          end
      end

      class Translation < OpenAI::Internal::Type::BaseModel
        sig { returns(String) }
        attr_accessor :text

        sig { override.returns({ text: String }) }
        def to_hash; end

        class << self
          sig { params(text: String).returns(T.attached_class) }
          def new(text:); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Audio::Translation, OpenAI::Internal::AnyHash)
          end
      end

      class TranslationCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # The audio file object (not file name) translate, in one of these formats: flac,
        # mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
        sig { returns(OpenAI::Internal::FileInput) }
        attr_accessor :file

        # ID of the model to use. Only `whisper-1` (which is powered by our open source
        # Whisper V2 model) is currently available.
        sig { returns(T.any(String, OpenAI::AudioModel::OrSymbol)) }
        attr_accessor :model

        # An optional text to guide the model's style or continue a previous audio
        # segment. The
        # [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)
        # should be in English.
        sig { returns(T.nilable(String)) }
        attr_reader :prompt

        sig { params(prompt: String).void }
        attr_writer :prompt

        # The format of the output, in one of these options: `json`, `text`, `srt`,
        # `verbose_json`, or `vtt`.
        sig do
          returns(T.nilable(
              OpenAI::Audio::TranslationCreateParams::ResponseFormat::OrSymbol
            ))
        end
        attr_reader :response_format

        sig { params(response_format: OpenAI::Audio::TranslationCreateParams::ResponseFormat::OrSymbol).void }
        attr_writer :response_format

        # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the
        # output more random, while lower values like 0.2 will make it more focused and
        # deterministic. If set to 0, the model will use
        # [log probability](https://en.wikipedia.org/wiki/Log_probability) to
        # automatically increase the temperature until certain thresholds are hit.
        sig { returns(T.nilable(Float)) }
        attr_reader :temperature

        sig { params(temperature: Float).void }
        attr_writer :temperature

        sig do
          override
            .returns({
              file: OpenAI::Internal::FileInput,
              model: T.any(String, OpenAI::AudioModel::OrSymbol),
              prompt: String,
              response_format:
                OpenAI::Audio::TranslationCreateParams::ResponseFormat::OrSymbol,
              temperature: Float,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              file: OpenAI::Internal::FileInput,
              model: T.any(String, OpenAI::AudioModel::OrSymbol),
              prompt: String,
              response_format: OpenAI::Audio::TranslationCreateParams::ResponseFormat::OrSymbol,
              temperature: Float,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            file:, # The audio file object (not file name) translate, in one of these formats: flac,
                   # mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
            model:, # ID of the model to use. Only `whisper-1` (which is powered by our open source
                    # Whisper V2 model) is currently available.
            prompt: nil, # An optional text to guide the model's style or continue a previous audio
                         # segment. The
                         # [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)
                         # should be in English.
            response_format: nil, # The format of the output, in one of these options: `json`, `text`, `srt`,
                                  # `verbose_json`, or `vtt`.
            temperature: nil, # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the
                              # output more random, while lower values like 0.2 will make it more focused and
                              # deterministic. If set to 0, the model will use
                              # [log probability](https://en.wikipedia.org/wiki/Log_probability) to
                              # automatically increase the temperature until certain thresholds are hit.
            request_options: {}
); end
        end

        # ID of the model to use. Only `whisper-1` (which is powered by our open source
        # Whisper V2 model) is currently available.
        module Model
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Audio::TranslationCreateParams::Model::Variants]) }
            def variants; end
          end

          Variants = T.type_alias { T.any(String, OpenAI::AudioModel::TaggedSymbol) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Audio::TranslationCreateParams,
              OpenAI::Internal::AnyHash
            )
          end

        # The format of the output, in one of these options: `json`, `text`, `srt`,
        # `verbose_json`, or `vtt`.
        module ResponseFormat
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Audio::TranslationCreateParams::ResponseFormat::TaggedSymbol
              ])
            end
            def values; end
          end

          JSON = T.let(
              :json,
              OpenAI::Audio::TranslationCreateParams::ResponseFormat::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          SRT = T.let(
              :srt,
              OpenAI::Audio::TranslationCreateParams::ResponseFormat::TaggedSymbol
            )

          TEXT = T.let(
              :text,
              OpenAI::Audio::TranslationCreateParams::ResponseFormat::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Audio::TranslationCreateParams::ResponseFormat
              )
            end

          VERBOSE_JSON = T.let(
              :verbose_json,
              OpenAI::Audio::TranslationCreateParams::ResponseFormat::TaggedSymbol
            )

          VTT = T.let(
              :vtt,
              OpenAI::Audio::TranslationCreateParams::ResponseFormat::TaggedSymbol
            )
        end
      end

      module TranslationCreateResponse
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Models::Audio::TranslationCreateResponse::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(OpenAI::Audio::Translation, OpenAI::Audio::TranslationVerbose)
          end
      end

      class TranslationVerbose < OpenAI::Internal::Type::BaseModel
        # The duration of the input audio.
        sig { returns(Float) }
        attr_accessor :duration

        # The language of the output translation (always `english`).
        sig { returns(String) }
        attr_accessor :language

        # Segments of the translated text and their corresponding details.
        sig { returns(T.nilable(T::Array[OpenAI::Audio::TranscriptionSegment])) }
        attr_reader :segments

        sig { params(segments: T::Array[OpenAI::Audio::TranscriptionSegment::OrHash]).void }
        attr_writer :segments

        # The translated text.
        sig { returns(String) }
        attr_accessor :text

        sig do
          override
            .returns({
              duration: Float,
              language: String,
              text: String,
              segments: T::Array[OpenAI::Audio::TranscriptionSegment]
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              duration: Float,
              language: String,
              text: String,
              segments: T::Array[OpenAI::Audio::TranscriptionSegment::OrHash]
            ).returns(T.attached_class)
          end
          def new(
            duration:, # The duration of the input audio.
            language:, # The language of the output translation (always `english`).
            text:, # The translated text.
            segments: nil # Segments of the translated text and their corresponding details.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Audio::TranslationVerbose, OpenAI::Internal::AnyHash)
          end
      end
    end

    module AudioModel
      extend OpenAI::Internal::Type::Enum

      class << self
        sig { override.returns(T::Array[OpenAI::AudioModel::TaggedSymbol]) }
        def values; end
      end

      GPT_4O_MINI_TRANSCRIBE = T.let(:"gpt-4o-mini-transcribe", OpenAI::AudioModel::TaggedSymbol)

      GPT_4O_TRANSCRIBE = T.let(:"gpt-4o-transcribe", OpenAI::AudioModel::TaggedSymbol)

      OrSymbol = T.type_alias { T.any(Symbol, String) }
      TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::AudioModel) }
      WHISPER_1 = T.let(:"whisper-1", OpenAI::AudioModel::TaggedSymbol)
    end

    # The format of the output, in one of these options: `json`, `text`, `srt`,
    # `verbose_json`, or `vtt`. For `gpt-4o-transcribe` and `gpt-4o-mini-transcribe`,
    # the only supported format is `json`.
    module AudioResponseFormat
      extend OpenAI::Internal::Type::Enum

      class << self
        sig { override.returns(T::Array[OpenAI::AudioResponseFormat::TaggedSymbol]) }
        def values; end
      end

      JSON = T.let(:json, OpenAI::AudioResponseFormat::TaggedSymbol)
      OrSymbol = T.type_alias { T.any(Symbol, String) }
      SRT = T.let(:srt, OpenAI::AudioResponseFormat::TaggedSymbol)
      TEXT = T.let(:text, OpenAI::AudioResponseFormat::TaggedSymbol)
      TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::AudioResponseFormat) }

      VERBOSE_JSON = T.let(:verbose_json, OpenAI::AudioResponseFormat::TaggedSymbol)

      VTT = T.let(:vtt, OpenAI::AudioResponseFormat::TaggedSymbol)
    end

    class AutoFileChunkingStrategyParam < OpenAI::Internal::Type::BaseModel
      # Always `auto`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig { override.returns({ type: Symbol }) }
      def to_hash; end

      class << self
        # The default strategy. This strategy currently uses a `max_chunk_size_tokens` of
        # `800` and `chunk_overlap_tokens` of `400`.
        sig { params(type: Symbol).returns(T.attached_class) }
        def new(
          type: :auto # Always `auto`.
); end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::AutoFileChunkingStrategyParam,
            OpenAI::Internal::AnyHash
          )
        end
    end

    class Batch < OpenAI::Internal::Type::BaseModel
      # The Unix timestamp (in seconds) for when the batch was cancelled.
      sig { returns(T.nilable(Integer)) }
      attr_reader :cancelled_at

      sig { params(cancelled_at: Integer).void }
      attr_writer :cancelled_at

      # The Unix timestamp (in seconds) for when the batch started cancelling.
      sig { returns(T.nilable(Integer)) }
      attr_reader :cancelling_at

      sig { params(cancelling_at: Integer).void }
      attr_writer :cancelling_at

      # The Unix timestamp (in seconds) for when the batch was completed.
      sig { returns(T.nilable(Integer)) }
      attr_reader :completed_at

      sig { params(completed_at: Integer).void }
      attr_writer :completed_at

      # The time frame within which the batch should be processed.
      sig { returns(String) }
      attr_accessor :completion_window

      # The Unix timestamp (in seconds) for when the batch was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The OpenAI API endpoint used by the batch.
      sig { returns(String) }
      attr_accessor :endpoint

      # The ID of the file containing the outputs of requests with errors.
      sig { returns(T.nilable(String)) }
      attr_reader :error_file_id

      sig { params(error_file_id: String).void }
      attr_writer :error_file_id

      sig { returns(T.nilable(OpenAI::Batch::Errors)) }
      attr_reader :errors

      sig { params(errors: OpenAI::Batch::Errors::OrHash).void }
      attr_writer :errors

      # The Unix timestamp (in seconds) for when the batch expired.
      sig { returns(T.nilable(Integer)) }
      attr_reader :expired_at

      sig { params(expired_at: Integer).void }
      attr_writer :expired_at

      # The Unix timestamp (in seconds) for when the batch will expire.
      sig { returns(T.nilable(Integer)) }
      attr_reader :expires_at

      sig { params(expires_at: Integer).void }
      attr_writer :expires_at

      # The Unix timestamp (in seconds) for when the batch failed.
      sig { returns(T.nilable(Integer)) }
      attr_reader :failed_at

      sig { params(failed_at: Integer).void }
      attr_writer :failed_at

      # The Unix timestamp (in seconds) for when the batch started finalizing.
      sig { returns(T.nilable(Integer)) }
      attr_reader :finalizing_at

      sig { params(finalizing_at: Integer).void }
      attr_writer :finalizing_at

      sig { returns(String) }
      attr_accessor :id

      # The Unix timestamp (in seconds) for when the batch started processing.
      sig { returns(T.nilable(Integer)) }
      attr_reader :in_progress_at

      sig { params(in_progress_at: Integer).void }
      attr_writer :in_progress_at

      # The ID of the input file for the batch.
      sig { returns(String) }
      attr_accessor :input_file_id

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The object type, which is always `batch`.
      sig { returns(Symbol) }
      attr_accessor :object

      # The ID of the file containing the outputs of successfully executed requests.
      sig { returns(T.nilable(String)) }
      attr_reader :output_file_id

      sig { params(output_file_id: String).void }
      attr_writer :output_file_id

      # The request counts for different statuses within the batch.
      sig { returns(T.nilable(OpenAI::BatchRequestCounts)) }
      attr_reader :request_counts

      sig { params(request_counts: OpenAI::BatchRequestCounts::OrHash).void }
      attr_writer :request_counts

      # The current status of the batch.
      sig { returns(OpenAI::Batch::Status::TaggedSymbol) }
      attr_accessor :status

      sig do
        override
          .returns({
            id: String,
            completion_window: String,
            created_at: Integer,
            endpoint: String,
            input_file_id: String,
            object: Symbol,
            status: OpenAI::Batch::Status::TaggedSymbol,
            cancelled_at: Integer,
            cancelling_at: Integer,
            completed_at: Integer,
            error_file_id: String,
            errors: OpenAI::Batch::Errors,
            expired_at: Integer,
            expires_at: Integer,
            failed_at: Integer,
            finalizing_at: Integer,
            in_progress_at: Integer,
            metadata: T.nilable(T::Hash[Symbol, String]),
            output_file_id: String,
            request_counts: OpenAI::BatchRequestCounts
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            id: String,
            completion_window: String,
            created_at: Integer,
            endpoint: String,
            input_file_id: String,
            status: OpenAI::Batch::Status::OrSymbol,
            cancelled_at: Integer,
            cancelling_at: Integer,
            completed_at: Integer,
            error_file_id: String,
            errors: OpenAI::Batch::Errors::OrHash,
            expired_at: Integer,
            expires_at: Integer,
            failed_at: Integer,
            finalizing_at: Integer,
            in_progress_at: Integer,
            metadata: T.nilable(T::Hash[Symbol, String]),
            output_file_id: String,
            request_counts: OpenAI::BatchRequestCounts::OrHash,
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          id:,
          completion_window:, # The time frame within which the batch should be processed.
          created_at:, # The Unix timestamp (in seconds) for when the batch was created.
          endpoint:, # The OpenAI API endpoint used by the batch.
          input_file_id:, # The ID of the input file for the batch.
          status:, # The current status of the batch.
          cancelled_at: nil, # The Unix timestamp (in seconds) for when the batch was cancelled.
          cancelling_at: nil, # The Unix timestamp (in seconds) for when the batch started cancelling.
          completed_at: nil, # The Unix timestamp (in seconds) for when the batch was completed.
          error_file_id: nil, # The ID of the file containing the outputs of requests with errors.
          errors: nil,
          expired_at: nil, # The Unix timestamp (in seconds) for when the batch expired.
          expires_at: nil, # The Unix timestamp (in seconds) for when the batch will expire.
          failed_at: nil, # The Unix timestamp (in seconds) for when the batch failed.
          finalizing_at: nil, # The Unix timestamp (in seconds) for when the batch started finalizing.
          in_progress_at: nil, # The Unix timestamp (in seconds) for when the batch started processing.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          output_file_id: nil, # The ID of the file containing the outputs of successfully executed requests.
          request_counts: nil, # The request counts for different statuses within the batch.
          object: :batch # The object type, which is always `batch`.
); end
      end

      class Errors < OpenAI::Internal::Type::BaseModel
        sig { returns(T.nilable(T::Array[OpenAI::BatchError])) }
        attr_reader :data

        sig { params(data: T::Array[OpenAI::BatchError::OrHash]).void }
        attr_writer :data

        # The object type, which is always `list`.
        sig { returns(T.nilable(String)) }
        attr_reader :object

        sig { params(object: String).void }
        attr_writer :object

        sig { override.returns({ data: T::Array[OpenAI::BatchError], object: String }) }
        def to_hash; end

        class << self
          sig { params(data: T::Array[OpenAI::BatchError::OrHash], object: String).returns(T.attached_class) }
          def new(
            data: nil,
            object: nil # The object type, which is always `list`.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Batch::Errors, OpenAI::Internal::AnyHash)
          end
      end

      OrHash = T.type_alias { T.any(OpenAI::Batch, OpenAI::Internal::AnyHash) }

      # The current status of the batch.
      module Status
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Batch::Status::TaggedSymbol]) }
          def values; end
        end

        CANCELLED = T.let(:cancelled, OpenAI::Batch::Status::TaggedSymbol)
        CANCELLING = T.let(:cancelling, OpenAI::Batch::Status::TaggedSymbol)
        COMPLETED = T.let(:completed, OpenAI::Batch::Status::TaggedSymbol)
        EXPIRED = T.let(:expired, OpenAI::Batch::Status::TaggedSymbol)
        FAILED = T.let(:failed, OpenAI::Batch::Status::TaggedSymbol)
        FINALIZING = T.let(:finalizing, OpenAI::Batch::Status::TaggedSymbol)
        IN_PROGRESS = T.let(:in_progress, OpenAI::Batch::Status::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }
        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Batch::Status) }
        VALIDATING = T.let(:validating, OpenAI::Batch::Status::TaggedSymbol)
      end
    end

    class BatchCancelParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::BatchCancelParams, OpenAI::Internal::AnyHash)
        end
    end

    class BatchCreateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # The time frame within which the batch should be processed. Currently only `24h`
      # is supported.
      sig { returns(OpenAI::BatchCreateParams::CompletionWindow::OrSymbol) }
      attr_accessor :completion_window

      # The endpoint to be used for all requests in the batch. Currently
      # `/v1/responses`, `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions`
      # are supported. Note that `/v1/embeddings` batches are also restricted to a
      # maximum of 50,000 embedding inputs across all requests in the batch.
      sig { returns(OpenAI::BatchCreateParams::Endpoint::OrSymbol) }
      attr_accessor :endpoint

      # The ID of an uploaded file that contains requests for the new batch.
      #
      # See [upload file](https://platform.openai.com/docs/api-reference/files/create)
      # for how to upload a file.
      #
      # Your input file must be formatted as a
      # [JSONL file](https://platform.openai.com/docs/api-reference/batch/request-input),
      # and must be uploaded with the purpose `batch`. The file can contain up to 50,000
      # requests, and can be up to 200 MB in size.
      sig { returns(String) }
      attr_accessor :input_file_id

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      sig do
        override
          .returns({
            completion_window:
              OpenAI::BatchCreateParams::CompletionWindow::OrSymbol,
            endpoint: OpenAI::BatchCreateParams::Endpoint::OrSymbol,
            input_file_id: String,
            metadata: T.nilable(T::Hash[Symbol, String]),
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            completion_window: OpenAI::BatchCreateParams::CompletionWindow::OrSymbol,
            endpoint: OpenAI::BatchCreateParams::Endpoint::OrSymbol,
            input_file_id: String,
            metadata: T.nilable(T::Hash[Symbol, String]),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          completion_window:, # The time frame within which the batch should be processed. Currently only `24h`
                              # is supported.
          endpoint:, # The endpoint to be used for all requests in the batch. Currently
                     # `/v1/responses`, `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions`
                     # are supported. Note that `/v1/embeddings` batches are also restricted to a
                     # maximum of 50,000 embedding inputs across all requests in the batch.
          input_file_id:, # The ID of an uploaded file that contains requests for the new batch.
                          # See [upload file](https://platform.openai.com/docs/api-reference/files/create)
                          # for how to upload a file.
                          # Your input file must be formatted as a
                          # [JSONL file](https://platform.openai.com/docs/api-reference/batch/request-input),
                          # and must be uploaded with the purpose `batch`. The file can contain up to 50,000
                          # requests, and can be up to 200 MB in size.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          request_options: {}
); end
      end

      # The time frame within which the batch should be processed. Currently only `24h`
      # is supported.
      module CompletionWindow
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::BatchCreateParams::CompletionWindow::TaggedSymbol]) }
          def values; end
        end

        COMPLETION_WINDOW_24H = T.let(
            :"24h",
            OpenAI::BatchCreateParams::CompletionWindow::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::BatchCreateParams::CompletionWindow)
          end
      end

      # The endpoint to be used for all requests in the batch. Currently
      # `/v1/responses`, `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions`
      # are supported. Note that `/v1/embeddings` batches are also restricted to a
      # maximum of 50,000 embedding inputs across all requests in the batch.
      module Endpoint
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::BatchCreateParams::Endpoint::TaggedSymbol]) }
          def values; end
        end

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::BatchCreateParams::Endpoint) }

        V1_CHAT_COMPLETIONS = T.let(
            :"/v1/chat/completions",
            OpenAI::BatchCreateParams::Endpoint::TaggedSymbol
          )

        V1_COMPLETIONS = T.let(
            :"/v1/completions",
            OpenAI::BatchCreateParams::Endpoint::TaggedSymbol
          )

        V1_EMBEDDINGS = T.let(
            :"/v1/embeddings",
            OpenAI::BatchCreateParams::Endpoint::TaggedSymbol
          )

        V1_RESPONSES = T.let(
            :"/v1/responses",
            OpenAI::BatchCreateParams::Endpoint::TaggedSymbol
          )
      end

      OrHash = T.type_alias do
          T.any(OpenAI::BatchCreateParams, OpenAI::Internal::AnyHash)
        end
    end

    class BatchError < OpenAI::Internal::Type::BaseModel
      # An error code identifying the error type.
      sig { returns(T.nilable(String)) }
      attr_reader :code

      sig { params(code: String).void }
      attr_writer :code

      # The line number of the input file where the error occurred, if applicable.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :line

      # A human-readable message providing more details about the error.
      sig { returns(T.nilable(String)) }
      attr_reader :message

      sig { params(message: String).void }
      attr_writer :message

      # The name of the parameter that caused the error, if applicable.
      sig { returns(T.nilable(String)) }
      attr_accessor :param

      sig do
        override
          .returns({
            code: String,
            line: T.nilable(Integer),
            message: String,
            param: T.nilable(String)
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            code: String,
            line: T.nilable(Integer),
            message: String,
            param: T.nilable(String)
          ).returns(T.attached_class)
        end
        def new(
          code: nil, # An error code identifying the error type.
          line: nil, # The line number of the input file where the error occurred, if applicable.
          message: nil, # A human-readable message providing more details about the error.
          param: nil # The name of the parameter that caused the error, if applicable.
); end
      end

      OrHash = T.type_alias { T.any(OpenAI::BatchError, OpenAI::Internal::AnyHash) }
    end

    class BatchListParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # A cursor for use in pagination. `after` is an object ID that defines your place
      # in the list. For instance, if you make a list request and receive 100 objects,
      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
      # fetch the next page of the list.
      sig { returns(T.nilable(String)) }
      attr_reader :after

      sig { params(after: String).void }
      attr_writer :after

      # A limit on the number of objects to be returned. Limit can range between 1 and
      # 100, and the default is 20.
      sig { returns(T.nilable(Integer)) }
      attr_reader :limit

      sig { params(limit: Integer).void }
      attr_writer :limit

      sig do
        override
          .returns({
            after: String,
            limit: Integer,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            after: String,
            limit: Integer,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                      # in the list. For instance, if you make a list request and receive 100 objects,
                      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                      # fetch the next page of the list.
          limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                      # 100, and the default is 20.
          request_options: {}
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::BatchListParams, OpenAI::Internal::AnyHash)
        end
    end

    class BatchRequestCounts < OpenAI::Internal::Type::BaseModel
      # Number of requests that have been completed successfully.
      sig { returns(Integer) }
      attr_accessor :completed

      # Number of requests that have failed.
      sig { returns(Integer) }
      attr_accessor :failed

      # Total number of requests in the batch.
      sig { returns(Integer) }
      attr_accessor :total

      sig { override.returns({ completed: Integer, failed: Integer, total: Integer }) }
      def to_hash; end

      class << self
        # The request counts for different statuses within the batch.
        sig { params(completed: Integer, failed: Integer, total: Integer).returns(T.attached_class) }
        def new(
          completed:, # Number of requests that have been completed successfully.
          failed:, # Number of requests that have failed.
          total: # Total number of requests in the batch.
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::BatchRequestCounts, OpenAI::Internal::AnyHash)
        end
    end

    class BatchRetrieveParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::BatchRetrieveParams, OpenAI::Internal::AnyHash)
        end
    end

    module Beta
      class Assistant < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) for when the assistant was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # The description of the assistant. The maximum length is 512 characters.
        sig { returns(T.nilable(String)) }
        attr_accessor :description

        # The identifier, which can be referenced in API endpoints.
        sig { returns(String) }
        attr_accessor :id

        # The system instructions that the assistant uses. The maximum length is 256,000
        # characters.
        sig { returns(T.nilable(String)) }
        attr_accessor :instructions

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # ID of the model to use. You can use the
        # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
        # see all of your available models, or see our
        # [Model overview](https://platform.openai.com/docs/models) for descriptions of
        # them.
        sig { returns(String) }
        attr_accessor :model

        # The name of the assistant. The maximum length is 256 characters.
        sig { returns(T.nilable(String)) }
        attr_accessor :name

        # The object type, which is always `assistant`.
        sig { returns(Symbol) }
        attr_accessor :object

        # Specifies the format that the model must output. Compatible with
        # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
        # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
        # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
        #
        # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        # Outputs which ensures the model will match your supplied JSON schema. Learn more
        # in the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
        # message the model generates is valid JSON.
        #
        # **Important:** when using JSON mode, you **must** also instruct the model to
        # produce JSON yourself via a system or user message. Without this, the model may
        # generate an unending stream of whitespace until the generation reaches the token
        # limit, resulting in a long-running and seemingly "stuck" request. Also note that
        # the message content may be partially cut off if `finish_reason="length"`, which
        # indicates the generation exceeded `max_tokens` or the conversation exceeded the
        # max context length.
        sig { returns(T.nilable(OpenAI::Beta::AssistantResponseFormatOption::Variants)) }
        attr_accessor :response_format

        # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        # make the output more random, while lower values like 0.2 will make it more
        # focused and deterministic.
        sig { returns(T.nilable(Float)) }
        attr_accessor :temperature

        # A set of resources that are used by the assistant's tools. The resources are
        # specific to the type of tool. For example, the `code_interpreter` tool requires
        # a list of file IDs, while the `file_search` tool requires a list of vector store
        # IDs.
        sig { returns(T.nilable(OpenAI::Beta::Assistant::ToolResources)) }
        attr_reader :tool_resources

        sig { params(tool_resources: T.nilable(OpenAI::Beta::Assistant::ToolResources::OrHash)).void }
        attr_writer :tool_resources

        # A list of tool enabled on the assistant. There can be a maximum of 128 tools per
        # assistant. Tools can be of types `code_interpreter`, `file_search`, or
        # `function`.
        sig { returns(T::Array[OpenAI::Beta::AssistantTool::Variants]) }
        attr_accessor :tools

        # An alternative to sampling with temperature, called nucleus sampling, where the
        # model considers the results of the tokens with top_p probability mass. So 0.1
        # means only the tokens comprising the top 10% probability mass are considered.
        #
        # We generally recommend altering this or temperature but not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :top_p

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              description: T.nilable(String),
              instructions: T.nilable(String),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: T.nilable(String),
              object: Symbol,
              tools: T::Array[OpenAI::Beta::AssistantTool::Variants],
              response_format:
                T.nilable(
                  OpenAI::Beta::AssistantResponseFormatOption::Variants
                ),
              temperature: T.nilable(Float),
              tool_resources: T.nilable(OpenAI::Beta::Assistant::ToolResources),
              top_p: T.nilable(Float)
            })
        end
        def to_hash; end

        class << self
          # Represents an `assistant` that can call the model and use tools.
          sig do
            params(
              id: String,
              created_at: Integer,
              description: T.nilable(String),
              instructions: T.nilable(String),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: T.nilable(String),
              tools: T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool::OrHash,
                  OpenAI::Beta::FileSearchTool::OrHash,
                  OpenAI::Beta::FunctionTool::OrHash
                )
              ],
              response_format: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash
                )
              ),
              temperature: T.nilable(Float),
              tool_resources: T.nilable(OpenAI::Beta::Assistant::ToolResources::OrHash),
              top_p: T.nilable(Float),
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The identifier, which can be referenced in API endpoints.
            created_at:, # The Unix timestamp (in seconds) for when the assistant was created.
            description:, # The description of the assistant. The maximum length is 512 characters.
            instructions:, # The system instructions that the assistant uses. The maximum length is 256,000
                           # characters.
            metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
            model:, # ID of the model to use. You can use the
                    # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                    # see all of your available models, or see our
                    # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                    # them.
            name:, # The name of the assistant. The maximum length is 256 characters.
            tools:, # A list of tool enabled on the assistant. There can be a maximum of 128 tools per
                    # assistant. Tools can be of types `code_interpreter`, `file_search`, or
                    # `function`.
            response_format: nil, # Specifies the format that the model must output. Compatible with
                                  # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                  # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                  # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                  # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                  # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                  # in the
                                  # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                  # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                  # message the model generates is valid JSON.
                                  # **Important:** when using JSON mode, you **must** also instruct the model to
                                  # produce JSON yourself via a system or user message. Without this, the model may
                                  # generate an unending stream of whitespace until the generation reaches the token
                                  # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                  # the message content may be partially cut off if `finish_reason="length"`, which
                                  # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                  # max context length.
            temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                              # make the output more random, while lower values like 0.2 will make it more
                              # focused and deterministic.
            tool_resources: nil, # A set of resources that are used by the assistant's tools. The resources are
                                 # specific to the type of tool. For example, the `code_interpreter` tool requires
                                 # a list of file IDs, while the `file_search` tool requires a list of vector store
                                 # IDs.
            top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                        # model considers the results of the tokens with top_p probability mass. So 0.1
                        # means only the tokens comprising the top 10% probability mass are considered.
                        # We generally recommend altering this or temperature but not both.
            object: :assistant # The object type, which is always `assistant`.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::Assistant, OpenAI::Internal::AnyHash)
          end

        class ToolResources < OpenAI::Internal::Type::BaseModel
          sig { returns(T.nilable(OpenAI::Beta::Assistant::ToolResources::CodeInterpreter)) }
          attr_reader :code_interpreter

          sig { params(code_interpreter: OpenAI::Beta::Assistant::ToolResources::CodeInterpreter::OrHash).void }
          attr_writer :code_interpreter

          sig { returns(T.nilable(OpenAI::Beta::Assistant::ToolResources::FileSearch)) }
          attr_reader :file_search

          sig { params(file_search: OpenAI::Beta::Assistant::ToolResources::FileSearch::OrHash).void }
          attr_writer :file_search

          sig do
            override
              .returns({
                code_interpreter:
                  OpenAI::Beta::Assistant::ToolResources::CodeInterpreter,
                file_search: OpenAI::Beta::Assistant::ToolResources::FileSearch
              })
          end
          def to_hash; end

          class << self
            # A set of resources that are used by the assistant's tools. The resources are
            # specific to the type of tool. For example, the `code_interpreter` tool requires
            # a list of file IDs, while the `file_search` tool requires a list of vector store
            # IDs.
            sig do
              params(
                code_interpreter: OpenAI::Beta::Assistant::ToolResources::CodeInterpreter::OrHash,
                file_search: OpenAI::Beta::Assistant::ToolResources::FileSearch::OrHash
              ).returns(T.attached_class)
            end
            def new(code_interpreter: nil, file_search: nil); end
          end

          class CodeInterpreter < OpenAI::Internal::Type::BaseModel
            # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
            # available to the `code_interpreter`` tool. There can be a maximum of 20 files
            # associated with the tool.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :file_ids

            sig { params(file_ids: T::Array[String]).void }
            attr_writer :file_ids

            sig { override.returns({ file_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(file_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                file_ids: nil # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
                              # available to the `code_interpreter`` tool. There can be a maximum of 20 files
                              # associated with the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Assistant::ToolResources::CodeInterpreter,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileSearch < OpenAI::Internal::Type::BaseModel
            # The ID of the
            # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
            # attached to this assistant. There can be a maximum of 1 vector store attached to
            # the assistant.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :vector_store_ids

            sig { params(vector_store_ids: T::Array[String]).void }
            attr_writer :vector_store_ids

            sig { override.returns({ vector_store_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(vector_store_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                vector_store_ids: nil # The ID of the
                                      # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                      # attached to this assistant. There can be a maximum of 1 vector store attached to
                                      # the assistant.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Assistant::ToolResources::FileSearch,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Assistant::ToolResources,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class AssistantCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # The description of the assistant. The maximum length is 512 characters.
        sig { returns(T.nilable(String)) }
        attr_accessor :description

        # The system instructions that the assistant uses. The maximum length is 256,000
        # characters.
        sig { returns(T.nilable(String)) }
        attr_accessor :instructions

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # ID of the model to use. You can use the
        # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
        # see all of your available models, or see our
        # [Model overview](https://platform.openai.com/docs/models) for descriptions of
        # them.
        sig { returns(T.any(String, OpenAI::ChatModel::OrSymbol)) }
        attr_accessor :model

        # The name of the assistant. The maximum length is 256 characters.
        sig { returns(T.nilable(String)) }
        attr_accessor :name

        # **o-series models only**
        #
        # Constrains effort on reasoning for
        # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
        # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
        # result in faster responses and fewer tokens used on reasoning in a response.
        sig { returns(T.nilable(OpenAI::ReasoningEffort::OrSymbol)) }
        attr_accessor :reasoning_effort

        # Specifies the format that the model must output. Compatible with
        # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
        # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
        # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
        #
        # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        # Outputs which ensures the model will match your supplied JSON schema. Learn more
        # in the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
        # message the model generates is valid JSON.
        #
        # **Important:** when using JSON mode, you **must** also instruct the model to
        # produce JSON yourself via a system or user message. Without this, the model may
        # generate an unending stream of whitespace until the generation reaches the token
        # limit, resulting in a long-running and seemingly "stuck" request. Also note that
        # the message content may be partially cut off if `finish_reason="length"`, which
        # indicates the generation exceeded `max_tokens` or the conversation exceeded the
        # max context length.
        sig do
          returns(T.nilable(
              T.any(
                Symbol,
                OpenAI::ResponseFormatText,
                OpenAI::ResponseFormatJSONObject,
                OpenAI::ResponseFormatJSONSchema
              )
            ))
        end
        attr_accessor :response_format

        # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        # make the output more random, while lower values like 0.2 will make it more
        # focused and deterministic.
        sig { returns(T.nilable(Float)) }
        attr_accessor :temperature

        # A set of resources that are used by the assistant's tools. The resources are
        # specific to the type of tool. For example, the `code_interpreter` tool requires
        # a list of file IDs, while the `file_search` tool requires a list of vector store
        # IDs.
        sig { returns(T.nilable(OpenAI::Beta::AssistantCreateParams::ToolResources)) }
        attr_reader :tool_resources

        sig do
          params(
            tool_resources: T.nilable(
                OpenAI::Beta::AssistantCreateParams::ToolResources::OrHash
              )
          ).void
        end
        attr_writer :tool_resources

        # A list of tool enabled on the assistant. There can be a maximum of 128 tools per
        # assistant. Tools can be of types `code_interpreter`, `file_search`, or
        # `function`.
        sig do
          returns(T.nilable(
              T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool,
                  OpenAI::Beta::FileSearchTool,
                  OpenAI::Beta::FunctionTool
                )
              ]
            ))
        end
        attr_reader :tools

        sig do
          params(
            tools: T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool::OrHash,
                  OpenAI::Beta::FileSearchTool::OrHash,
                  OpenAI::Beta::FunctionTool::OrHash
                )
              ]
          ).void
        end
        attr_writer :tools

        # An alternative to sampling with temperature, called nucleus sampling, where the
        # model considers the results of the tokens with top_p probability mass. So 0.1
        # means only the tokens comprising the top 10% probability mass are considered.
        #
        # We generally recommend altering this or temperature but not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :top_p

        sig do
          override
            .returns({
              model: T.any(String, OpenAI::ChatModel::OrSymbol),
              description: T.nilable(String),
              instructions: T.nilable(String),
              metadata: T.nilable(T::Hash[Symbol, String]),
              name: T.nilable(String),
              reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
              response_format:
                T.nilable(
                  T.any(
                    Symbol,
                    OpenAI::ResponseFormatText,
                    OpenAI::ResponseFormatJSONObject,
                    OpenAI::ResponseFormatJSONSchema
                  )
                ),
              temperature: T.nilable(Float),
              tool_resources:
                T.nilable(OpenAI::Beta::AssistantCreateParams::ToolResources),
              tools:
                T::Array[
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool,
                    OpenAI::Beta::FileSearchTool,
                    OpenAI::Beta::FunctionTool
                  )
                ],
              top_p: T.nilable(Float),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              model: T.any(String, OpenAI::ChatModel::OrSymbol),
              description: T.nilable(String),
              instructions: T.nilable(String),
              metadata: T.nilable(T::Hash[Symbol, String]),
              name: T.nilable(String),
              reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
              response_format: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash
                )
              ),
              temperature: T.nilable(Float),
              tool_resources: T.nilable(
                OpenAI::Beta::AssistantCreateParams::ToolResources::OrHash
              ),
              tools: T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool::OrHash,
                  OpenAI::Beta::FileSearchTool::OrHash,
                  OpenAI::Beta::FunctionTool::OrHash
                )
              ],
              top_p: T.nilable(Float),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            model:, # ID of the model to use. You can use the
                    # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                    # see all of your available models, or see our
                    # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                    # them.
            description: nil, # The description of the assistant. The maximum length is 512 characters.
            instructions: nil, # The system instructions that the assistant uses. The maximum length is 256,000
                               # characters.
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            name: nil, # The name of the assistant. The maximum length is 256 characters.
            reasoning_effort: nil, # **o-series models only**
                                   # Constrains effort on reasoning for
                                   # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                   # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                   # result in faster responses and fewer tokens used on reasoning in a response.
            response_format: nil, # Specifies the format that the model must output. Compatible with
                                  # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                  # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                  # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                  # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                  # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                  # in the
                                  # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                  # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                  # message the model generates is valid JSON.
                                  # **Important:** when using JSON mode, you **must** also instruct the model to
                                  # produce JSON yourself via a system or user message. Without this, the model may
                                  # generate an unending stream of whitespace until the generation reaches the token
                                  # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                  # the message content may be partially cut off if `finish_reason="length"`, which
                                  # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                  # max context length.
            temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                              # make the output more random, while lower values like 0.2 will make it more
                              # focused and deterministic.
            tool_resources: nil, # A set of resources that are used by the assistant's tools. The resources are
                                 # specific to the type of tool. For example, the `code_interpreter` tool requires
                                 # a list of file IDs, while the `file_search` tool requires a list of vector store
                                 # IDs.
            tools: nil, # A list of tool enabled on the assistant. There can be a maximum of 128 tools per
                        # assistant. Tools can be of types `code_interpreter`, `file_search`, or
                        # `function`.
            top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                        # model considers the results of the tokens with top_p probability mass. So 0.1
                        # means only the tokens comprising the top 10% probability mass are considered.
                        # We generally recommend altering this or temperature but not both.
            request_options: {}
); end
        end

        # ID of the model to use. You can use the
        # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
        # see all of your available models, or see our
        # [Model overview](https://platform.openai.com/docs/models) for descriptions of
        # them.
        module Model
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::AssistantCreateParams::Model::Variants]) }
            def variants; end
          end

          Variants = T.type_alias { T.any(String, OpenAI::ChatModel::TaggedSymbol) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Beta::AssistantCreateParams,
              OpenAI::Internal::AnyHash
            )
          end

        class ToolResources < OpenAI::Internal::Type::BaseModel
          sig do
            returns(T.nilable(
                OpenAI::Beta::AssistantCreateParams::ToolResources::CodeInterpreter
              ))
          end
          attr_reader :code_interpreter

          sig do
            params(
              code_interpreter: OpenAI::Beta::AssistantCreateParams::ToolResources::CodeInterpreter::OrHash
            ).void
          end
          attr_writer :code_interpreter

          sig do
            returns(T.nilable(
                OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch
              ))
          end
          attr_reader :file_search

          sig { params(file_search: OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::OrHash).void }
          attr_writer :file_search

          sig do
            override
              .returns({
                code_interpreter:
                  OpenAI::Beta::AssistantCreateParams::ToolResources::CodeInterpreter,
                file_search:
                  OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch
              })
          end
          def to_hash; end

          class << self
            # A set of resources that are used by the assistant's tools. The resources are
            # specific to the type of tool. For example, the `code_interpreter` tool requires
            # a list of file IDs, while the `file_search` tool requires a list of vector store
            # IDs.
            sig do
              params(
                code_interpreter: OpenAI::Beta::AssistantCreateParams::ToolResources::CodeInterpreter::OrHash,
                file_search: OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::OrHash
              ).returns(T.attached_class)
            end
            def new(code_interpreter: nil, file_search: nil); end
          end

          class CodeInterpreter < OpenAI::Internal::Type::BaseModel
            # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
            # available to the `code_interpreter` tool. There can be a maximum of 20 files
            # associated with the tool.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :file_ids

            sig { params(file_ids: T::Array[String]).void }
            attr_writer :file_ids

            sig { override.returns({ file_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(file_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                file_ids: nil # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
                              # available to the `code_interpreter` tool. There can be a maximum of 20 files
                              # associated with the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::AssistantCreateParams::ToolResources::CodeInterpreter,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileSearch < OpenAI::Internal::Type::BaseModel
            # The
            # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
            # attached to this assistant. There can be a maximum of 1 vector store attached to
            # the assistant.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :vector_store_ids

            sig { params(vector_store_ids: T::Array[String]).void }
            attr_writer :vector_store_ids

            # A helper to create a
            # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
            # with file_ids and attach it to this assistant. There can be a maximum of 1
            # vector store attached to the assistant.
            sig do
              returns(T.nilable(
                  T::Array[
                    OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore
                  ]
                ))
            end
            attr_reader :vector_stores

            sig do
              params(
                vector_stores: T::Array[
                    OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::OrHash
                  ]
              ).void
            end
            attr_writer :vector_stores

            sig do
              override
                .returns({
                  vector_store_ids: T::Array[String],
                  vector_stores:
                    T::Array[
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore
                    ]
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  vector_store_ids: T::Array[String],
                  vector_stores: T::Array[
                    OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::OrHash
                  ]
                ).returns(T.attached_class)
              end
              def new(
                vector_store_ids: nil, # The
                                       # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                       # attached to this assistant. There can be a maximum of 1 vector store attached to
                                       # the assistant.
                vector_stores: nil # A helper to create a
                                   # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                   # with file_ids and attach it to this assistant. There can be a maximum of 1
                                   # vector store attached to the assistant.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch,
                  OpenAI::Internal::AnyHash
                )
              end

            class VectorStore < OpenAI::Internal::Type::BaseModel
              # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
              # strategy.
              sig do
                returns(T.nilable(
                    T.any(
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static
                    )
                  ))
              end
              attr_reader :chunking_strategy

              sig do
                params(
                  chunking_strategy: T.any(
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto::OrHash,
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::OrHash
                    )
                ).void
              end
              attr_writer :chunking_strategy

              # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to
              # add to the vector store. There can be a maximum of 10000 files in a vector
              # store.
              sig { returns(T.nilable(T::Array[String])) }
              attr_reader :file_ids

              sig { params(file_ids: T::Array[String]).void }
              attr_writer :file_ids

              # Set of 16 key-value pairs that can be attached to an object. This can be useful
              # for storing additional information about the object in a structured format, and
              # querying for objects via API or the dashboard.
              #
              # Keys are strings with a maximum length of 64 characters. Values are strings with
              # a maximum length of 512 characters.
              sig { returns(T.nilable(T::Hash[Symbol, String])) }
              attr_accessor :metadata

              sig do
                override
                  .returns({
                    chunking_strategy:
                      T.any(
                        OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                        OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static
                      ),
                    file_ids: T::Array[String],
                    metadata: T.nilable(T::Hash[Symbol, String])
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    chunking_strategy: T.any(
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto::OrHash,
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::OrHash
                    ),
                    file_ids: T::Array[String],
                    metadata: T.nilable(T::Hash[Symbol, String])
                  ).returns(T.attached_class)
                end
                def new(
                  chunking_strategy: nil, # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                                          # strategy.
                  file_ids: nil, # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to
                                 # add to the vector store. There can be a maximum of 10000 files in a vector
                                 # store.
                  metadata: nil # Set of 16 key-value pairs that can be attached to an object. This can be useful
                                # for storing additional information about the object in a structured format, and
                                # querying for objects via API or the dashboard.
                                # Keys are strings with a maximum length of 64 characters. Values are strings with
                                # a maximum length of 512 characters.
); end
              end

              # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
              # strategy.
              module ChunkingStrategy
                extend OpenAI::Internal::Type::Union

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Variants
                    ])
                  end
                  def variants; end
                end

                class Auto < OpenAI::Internal::Type::BaseModel
                  # Always `auto`.
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig { override.returns({ type: Symbol }) }
                  def to_hash; end

                  class << self
                    # The default strategy. This strategy currently uses a `max_chunk_size_tokens` of
                    # `800` and `chunk_overlap_tokens` of `400`.
                    sig { params(type: Symbol).returns(T.attached_class) }
                    def new(
                      type: :auto # Always `auto`.
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                class Static < OpenAI::Internal::Type::BaseModel
                  sig do
                    returns(OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static)
                  end
                  attr_reader :static

                  sig do
                    params(
                      static: OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static::OrHash
                    ).void
                  end
                  attr_writer :static

                  # Always `static`.
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig do
                    override
                      .returns({
                        static:
                          OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static,
                        type: Symbol
                      })
                  end
                  def to_hash; end

                  class << self
                    sig do
                      params(
                        static: OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static::OrHash,
                        type: Symbol
                      ).returns(T.attached_class)
                    end
                    def new(
                      static:,
                      type: :static # Always `static`.
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static,
                        OpenAI::Internal::AnyHash
                      )
                    end

                  class Static < OpenAI::Internal::Type::BaseModel
                    # The number of tokens that overlap between chunks. The default value is `400`.
                    #
                    # Note that the overlap must not exceed half of `max_chunk_size_tokens`.
                    sig { returns(Integer) }
                    attr_accessor :chunk_overlap_tokens

                    # The maximum number of tokens in each chunk. The default value is `800`. The
                    # minimum value is `100` and the maximum value is `4096`.
                    sig { returns(Integer) }
                    attr_accessor :max_chunk_size_tokens

                    sig do
                      override
                        .returns({
                          chunk_overlap_tokens: Integer,
                          max_chunk_size_tokens: Integer
                        })
                    end
                    def to_hash; end

                    class << self
                      sig do
                        params(
                          chunk_overlap_tokens: Integer,
                          max_chunk_size_tokens: Integer
                        ).returns(T.attached_class)
                      end
                      def new(
                        chunk_overlap_tokens:, # The number of tokens that overlap between chunks. The default value is `400`.
                                               # Note that the overlap must not exceed half of `max_chunk_size_tokens`.
                        max_chunk_size_tokens: # The maximum number of tokens in each chunk. The default value is `800`. The
                                               # minimum value is `100` and the maximum value is `4096`.
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end
                end

                Variants = T.type_alias do
                    T.any(
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                      OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static
                    )
                  end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::AssistantCreateParams::ToolResources::FileSearch::VectorStore,
                    OpenAI::Internal::AnyHash
                  )
                end
            end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantCreateParams::ToolResources,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class AssistantDeleteParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Beta::AssistantDeleteParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class AssistantDeleted < OpenAI::Internal::Type::BaseModel
        sig { returns(T::Boolean) }
        attr_accessor :deleted

        sig { returns(String) }
        attr_accessor :id

        sig { returns(Symbol) }
        attr_accessor :object

        sig { override.returns({ id: String, deleted: T::Boolean, object: Symbol }) }
        def to_hash; end

        class << self
          sig { params(id: String, deleted: T::Boolean, object: Symbol).returns(T.attached_class) }
          def new(id:, deleted:, object: :"assistant.deleted"); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::AssistantDeleted, OpenAI::Internal::AnyHash)
          end
      end

      class AssistantListParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # A cursor for use in pagination. `after` is an object ID that defines your place
        # in the list. For instance, if you make a list request and receive 100 objects,
        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
        # fetch the next page of the list.
        sig { returns(T.nilable(String)) }
        attr_reader :after

        sig { params(after: String).void }
        attr_writer :after

        # A cursor for use in pagination. `before` is an object ID that defines your place
        # in the list. For instance, if you make a list request and receive 100 objects,
        # starting with obj_foo, your subsequent call can include before=obj_foo in order
        # to fetch the previous page of the list.
        sig { returns(T.nilable(String)) }
        attr_reader :before

        sig { params(before: String).void }
        attr_writer :before

        # A limit on the number of objects to be returned. Limit can range between 1 and
        # 100, and the default is 20.
        sig { returns(T.nilable(Integer)) }
        attr_reader :limit

        sig { params(limit: Integer).void }
        attr_writer :limit

        # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
        # order and `desc` for descending order.
        sig { returns(T.nilable(OpenAI::Beta::AssistantListParams::Order::OrSymbol)) }
        attr_reader :order

        sig { params(order: OpenAI::Beta::AssistantListParams::Order::OrSymbol).void }
        attr_writer :order

        sig do
          override
            .returns({
              after: String,
              before: String,
              limit: Integer,
              order: OpenAI::Beta::AssistantListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              after: String,
              before: String,
              limit: Integer,
              order: OpenAI::Beta::AssistantListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                        # in the list. For instance, if you make a list request and receive 100 objects,
                        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                        # fetch the next page of the list.
            before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                         # in the list. For instance, if you make a list request and receive 100 objects,
                         # starting with obj_foo, your subsequent call can include before=obj_foo in order
                         # to fetch the previous page of the list.
            limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                        # 100, and the default is 20.
            order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                        # order and `desc` for descending order.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::AssistantListParams, OpenAI::Internal::AnyHash)
          end

        # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
        # order and `desc` for descending order.
        module Order
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::AssistantListParams::Order::TaggedSymbol]) }
            def values; end
          end

          ASC = T.let(:asc, OpenAI::Beta::AssistantListParams::Order::TaggedSymbol)

          DESC = T.let(:desc, OpenAI::Beta::AssistantListParams::Order::TaggedSymbol)

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Beta::AssistantListParams::Order)
            end
        end
      end

      # Specifies the format that the model must output. Compatible with
      # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
      # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
      # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
      #
      # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
      # Outputs which ensures the model will match your supplied JSON schema. Learn more
      # in the
      # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
      #
      # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
      # message the model generates is valid JSON.
      #
      # **Important:** when using JSON mode, you **must** also instruct the model to
      # produce JSON yourself via a system or user message. Without this, the model may
      # generate an unending stream of whitespace until the generation reaches the token
      # limit, resulting in a long-running and seemingly "stuck" request. Also note that
      # the message content may be partially cut off if `finish_reason="length"`, which
      # indicates the generation exceeded `max_tokens` or the conversation exceeded the
      # max context length.
      module AssistantResponseFormatOption
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Beta::AssistantResponseFormatOption::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              Symbol,
              OpenAI::ResponseFormatText,
              OpenAI::ResponseFormatJSONObject,
              OpenAI::ResponseFormatJSONSchema
            )
          end
      end

      class AssistantRetrieveParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Beta::AssistantRetrieveParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # Represents an event emitted when streaming a Run.
      #
      # Each event in a server-sent events stream has an `event` and `data` property:
      #
      # ```
      # event: thread.created
      # data: {"id": "thread_123", "object": "thread", ...}
      # ```
      #
      # We emit events whenever a new object is created, transitions to a new state, or
      # is being streamed in parts (deltas). For example, we emit `thread.run.created`
      # when a new run is created, `thread.run.completed` when a run completes, and so
      # on. When an Assistant chooses to create a message during a run, we emit a
      # `thread.message.created event`, a `thread.message.in_progress` event, many
      # `thread.message.delta` events, and finally a `thread.message.completed` event.
      #
      # We may add additional events over time, so we recommend handling unknown events
      # gracefully in your code. See the
      # [Assistants API quickstart](https://platform.openai.com/docs/assistants/overview)
      # to learn how to integrate the Assistants API with streaming.
      module AssistantStreamEvent
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Beta::AssistantStreamEvent::Variants]) }
          def variants; end
        end

        class ErrorEvent < OpenAI::Internal::Type::BaseModel
          sig { returns(OpenAI::ErrorObject) }
          attr_reader :data

          sig { params(data: OpenAI::ErrorObject::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::ErrorObject, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when an
            # [error](https://platform.openai.com/docs/guides/error-codes#api-errors) occurs.
            # This can happen due to an internal server error or a timeout.
            sig { params(data: OpenAI::ErrorObject::OrHash, event: Symbol).returns(T.attached_class) }
            def new(data:, event: :error); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ErrorEvent,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadCreated < OpenAI::Internal::Type::BaseModel
          # Represents a thread that contains
          # [messages](https://platform.openai.com/docs/api-reference/messages).
          sig { returns(OpenAI::Beta::Thread) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Thread::OrHash).void }
          attr_writer :data

          # Whether to enable input audio transcription.
          sig { returns(T.nilable(T::Boolean)) }
          attr_reader :enabled

          sig { params(enabled: T::Boolean).void }
          attr_writer :enabled

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Thread, event: Symbol, enabled: T::Boolean }) }
          def to_hash; end

          class << self
            # Occurs when a new
            # [thread](https://platform.openai.com/docs/api-reference/threads/object) is
            # created.
            sig do
              params(
                data: OpenAI::Beta::Thread::OrHash,
                enabled: T::Boolean,
                event: Symbol
              ).returns(T.attached_class)
            end
            def new(
              data:, # Represents a thread that contains
                     # [messages](https://platform.openai.com/docs/api-reference/messages).
              enabled: nil, # Whether to enable input audio transcription.
              event: :"thread.created"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadCreated,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadMessageCompleted < OpenAI::Internal::Type::BaseModel
          # Represents a message within a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Message) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Message::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Message, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [message](https://platform.openai.com/docs/api-reference/messages/object) is
            # completed.
            sig { params(data: OpenAI::Beta::Threads::Message::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a message within a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.message.completed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadMessageCompleted,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadMessageCreated < OpenAI::Internal::Type::BaseModel
          # Represents a message within a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Message) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Message::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Message, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [message](https://platform.openai.com/docs/api-reference/messages/object) is
            # created.
            sig { params(data: OpenAI::Beta::Threads::Message::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a message within a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.message.created"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadMessageCreated,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadMessageDelta < OpenAI::Internal::Type::BaseModel
          # Represents a message delta i.e. any changed fields on a message during
          # streaming.
          sig { returns(OpenAI::Beta::Threads::MessageDeltaEvent) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::MessageDeltaEvent::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::MessageDeltaEvent, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when parts of a
            # [Message](https://platform.openai.com/docs/api-reference/messages/object) are
            # being streamed.
            sig do
              params(
                data: OpenAI::Beta::Threads::MessageDeltaEvent::OrHash,
                event: Symbol
              ).returns(T.attached_class)
            end
            def new(
              data:, # Represents a message delta i.e. any changed fields on a message during
                     # streaming.
              event: :"thread.message.delta"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadMessageDelta,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadMessageInProgress < OpenAI::Internal::Type::BaseModel
          # Represents a message within a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Message) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Message::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Message, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [message](https://platform.openai.com/docs/api-reference/messages/object) moves
            # to an `in_progress` state.
            sig { params(data: OpenAI::Beta::Threads::Message::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a message within a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.message.in_progress"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadMessageInProgress,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadMessageIncomplete < OpenAI::Internal::Type::BaseModel
          # Represents a message within a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Message) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Message::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Message, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [message](https://platform.openai.com/docs/api-reference/messages/object) ends
            # before it is completed.
            sig { params(data: OpenAI::Beta::Threads::Message::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a message within a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.message.incomplete"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadMessageIncomplete,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunCancelled < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # is cancelled.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.cancelled"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunCancelled,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunCancelling < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # moves to a `cancelling` status.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.cancelling"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunCancelling,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunCompleted < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # is completed.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.completed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunCompleted,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunCreated < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a new
            # [run](https://platform.openai.com/docs/api-reference/runs/object) is created.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.created"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunCreated,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunExpired < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # expires.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.expired"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunExpired,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunFailed < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # fails.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.failed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunFailed,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunInProgress < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # moves to an `in_progress` status.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.in_progress"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunInProgress,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunIncomplete < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # ends with status `incomplete`.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.incomplete"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunIncomplete,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunQueued < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # moves to a `queued` status.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.queued"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunQueued,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunRequiresAction < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # moves to a `requires_action` status.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.requires_action"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunRequiresAction,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepCancelled < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # is cancelled.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.cancelled"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunStepCancelled,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepCompleted < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # is completed.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.completed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunStepCompleted,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepCreated < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # is created.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.created"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunStepCreated,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepDelta < OpenAI::Internal::Type::BaseModel
          # Represents a run step delta i.e. any changed fields on a run step during
          # streaming.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStepDeltaEvent) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStepDeltaEvent::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig do
            override
              .returns({
                data: OpenAI::Beta::Threads::Runs::RunStepDeltaEvent,
                event: Symbol
              })
          end
          def to_hash; end

          class << self
            # Occurs when parts of a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # are being streamed.
            sig do
              params(
                data: OpenAI::Beta::Threads::Runs::RunStepDeltaEvent::OrHash,
                event: Symbol
              ).returns(T.attached_class)
            end
            def new(
              data:, # Represents a run step delta i.e. any changed fields on a run step during
                     # streaming.
              event: :"thread.run.step.delta"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunStepDelta,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepExpired < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # expires.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.expired"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunStepExpired,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepFailed < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # fails.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.failed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunStepFailed,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepInProgress < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # moves to an `in_progress` state.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.in_progress"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantStreamEvent::ThreadRunStepInProgress,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Beta::AssistantStreamEvent::ThreadCreated,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunCreated,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunQueued,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunInProgress,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunRequiresAction,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunCompleted,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunIncomplete,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunFailed,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunCancelling,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunCancelled,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunExpired,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunStepCreated,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunStepInProgress,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunStepDelta,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunStepCompleted,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunStepFailed,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunStepCancelled,
              OpenAI::Beta::AssistantStreamEvent::ThreadRunStepExpired,
              OpenAI::Beta::AssistantStreamEvent::ThreadMessageCreated,
              OpenAI::Beta::AssistantStreamEvent::ThreadMessageInProgress,
              OpenAI::Beta::AssistantStreamEvent::ThreadMessageDelta,
              OpenAI::Beta::AssistantStreamEvent::ThreadMessageCompleted,
              OpenAI::Beta::AssistantStreamEvent::ThreadMessageIncomplete,
              OpenAI::Beta::AssistantStreamEvent::ErrorEvent
            )
          end
      end

      module AssistantTool
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Beta::AssistantTool::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Beta::CodeInterpreterTool,
              OpenAI::Beta::FileSearchTool,
              OpenAI::Beta::FunctionTool
            )
          end
      end

      class AssistantToolChoice < OpenAI::Internal::Type::BaseModel
        sig { returns(T.nilable(OpenAI::Beta::AssistantToolChoiceFunction)) }
        attr_reader :function

        sig { params(function: OpenAI::Beta::AssistantToolChoiceFunction::OrHash).void }
        attr_writer :function

        # The type of the tool. If type is `function`, the function name must be set
        sig { returns(OpenAI::Beta::AssistantToolChoice::Type::OrSymbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              type: OpenAI::Beta::AssistantToolChoice::Type::OrSymbol,
              function: OpenAI::Beta::AssistantToolChoiceFunction
            })
        end
        def to_hash; end

        class << self
          # Specifies a tool the model should use. Use to force the model to call a specific
          # tool.
          sig do
            params(
              type: OpenAI::Beta::AssistantToolChoice::Type::OrSymbol,
              function: OpenAI::Beta::AssistantToolChoiceFunction::OrHash
            ).returns(T.attached_class)
          end
          def new(
            type:, # The type of the tool. If type is `function`, the function name must be set
            function: nil
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::AssistantToolChoice, OpenAI::Internal::AnyHash)
          end

        # The type of the tool. If type is `function`, the function name must be set
        module Type
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::AssistantToolChoice::Type::TaggedSymbol]) }
            def values; end
          end

          CODE_INTERPRETER = T.let(
              :code_interpreter,
              OpenAI::Beta::AssistantToolChoice::Type::TaggedSymbol
            )

          FILE_SEARCH = T.let(
              :file_search,
              OpenAI::Beta::AssistantToolChoice::Type::TaggedSymbol
            )

          FUNCTION = T.let(
              :function,
              OpenAI::Beta::AssistantToolChoice::Type::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Beta::AssistantToolChoice::Type)
            end
        end
      end

      class AssistantToolChoiceFunction < OpenAI::Internal::Type::BaseModel
        # The name of the function to call.
        sig { returns(String) }
        attr_accessor :name

        sig { override.returns({ name: String }) }
        def to_hash; end

        class << self
          sig { params(name: String).returns(T.attached_class) }
          def new(
            name: # The name of the function to call.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Beta::AssistantToolChoiceFunction,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # Controls which (if any) tool is called by the model. `none` means the model will
      # not call any tools and instead generates a message. `auto` is the default value
      # and means the model can pick between generating a message or calling one or more
      # tools. `required` means the model must call one or more tools before responding
      # to the user. Specifying a particular tool like `{"type": "file_search"}` or
      # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
      # call that tool.
      module AssistantToolChoiceOption
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Beta::AssistantToolChoiceOption::Variants]) }
          def variants; end
        end

        # `none` means the model will not call any tools and instead generates a message.
        # `auto` means the model can pick between generating a message or calling one or
        # more tools. `required` means the model must call one or more tools before
        # responding to the user.
        module Auto
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Beta::AssistantToolChoiceOption::Auto::TaggedSymbol
              ])
            end
            def values; end
          end

          AUTO = T.let(
              :auto,
              OpenAI::Beta::AssistantToolChoiceOption::Auto::TaggedSymbol
            )

          NONE = T.let(
              :none,
              OpenAI::Beta::AssistantToolChoiceOption::Auto::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          REQUIRED = T.let(
              :required,
              OpenAI::Beta::AssistantToolChoiceOption::Auto::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Beta::AssistantToolChoiceOption::Auto)
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Beta::AssistantToolChoiceOption::Auto::TaggedSymbol,
              OpenAI::Beta::AssistantToolChoice
            )
          end
      end

      class AssistantUpdateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # The description of the assistant. The maximum length is 512 characters.
        sig { returns(T.nilable(String)) }
        attr_accessor :description

        # The system instructions that the assistant uses. The maximum length is 256,000
        # characters.
        sig { returns(T.nilable(String)) }
        attr_accessor :instructions

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # ID of the model to use. You can use the
        # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
        # see all of your available models, or see our
        # [Model overview](https://platform.openai.com/docs/models) for descriptions of
        # them.
        sig do
          returns(T.nilable(
              T.any(
                String,
                OpenAI::Beta::AssistantUpdateParams::Model::OrSymbol
              )
            ))
        end
        attr_reader :model

        sig do
          params(
            model: T.any(
                String,
                OpenAI::Beta::AssistantUpdateParams::Model::OrSymbol
              )
          ).void
        end
        attr_writer :model

        # The name of the assistant. The maximum length is 256 characters.
        sig { returns(T.nilable(String)) }
        attr_accessor :name

        # **o-series models only**
        #
        # Constrains effort on reasoning for
        # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
        # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
        # result in faster responses and fewer tokens used on reasoning in a response.
        sig { returns(T.nilable(OpenAI::ReasoningEffort::OrSymbol)) }
        attr_accessor :reasoning_effort

        # Specifies the format that the model must output. Compatible with
        # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
        # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
        # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
        #
        # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        # Outputs which ensures the model will match your supplied JSON schema. Learn more
        # in the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
        # message the model generates is valid JSON.
        #
        # **Important:** when using JSON mode, you **must** also instruct the model to
        # produce JSON yourself via a system or user message. Without this, the model may
        # generate an unending stream of whitespace until the generation reaches the token
        # limit, resulting in a long-running and seemingly "stuck" request. Also note that
        # the message content may be partially cut off if `finish_reason="length"`, which
        # indicates the generation exceeded `max_tokens` or the conversation exceeded the
        # max context length.
        sig do
          returns(T.nilable(
              T.any(
                Symbol,
                OpenAI::ResponseFormatText,
                OpenAI::ResponseFormatJSONObject,
                OpenAI::ResponseFormatJSONSchema
              )
            ))
        end
        attr_accessor :response_format

        # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        # make the output more random, while lower values like 0.2 will make it more
        # focused and deterministic.
        sig { returns(T.nilable(Float)) }
        attr_accessor :temperature

        # A set of resources that are used by the assistant's tools. The resources are
        # specific to the type of tool. For example, the `code_interpreter` tool requires
        # a list of file IDs, while the `file_search` tool requires a list of vector store
        # IDs.
        sig { returns(T.nilable(OpenAI::Beta::AssistantUpdateParams::ToolResources)) }
        attr_reader :tool_resources

        sig do
          params(
            tool_resources: T.nilable(
                OpenAI::Beta::AssistantUpdateParams::ToolResources::OrHash
              )
          ).void
        end
        attr_writer :tool_resources

        # A list of tool enabled on the assistant. There can be a maximum of 128 tools per
        # assistant. Tools can be of types `code_interpreter`, `file_search`, or
        # `function`.
        sig do
          returns(T.nilable(
              T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool,
                  OpenAI::Beta::FileSearchTool,
                  OpenAI::Beta::FunctionTool
                )
              ]
            ))
        end
        attr_reader :tools

        sig do
          params(
            tools: T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool::OrHash,
                  OpenAI::Beta::FileSearchTool::OrHash,
                  OpenAI::Beta::FunctionTool::OrHash
                )
              ]
          ).void
        end
        attr_writer :tools

        # An alternative to sampling with temperature, called nucleus sampling, where the
        # model considers the results of the tokens with top_p probability mass. So 0.1
        # means only the tokens comprising the top 10% probability mass are considered.
        #
        # We generally recommend altering this or temperature but not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :top_p

        sig do
          override
            .returns({
              description: T.nilable(String),
              instructions: T.nilable(String),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model:
                T.any(
                  String,
                  OpenAI::Beta::AssistantUpdateParams::Model::OrSymbol
                ),
              name: T.nilable(String),
              reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
              response_format:
                T.nilable(
                  T.any(
                    Symbol,
                    OpenAI::ResponseFormatText,
                    OpenAI::ResponseFormatJSONObject,
                    OpenAI::ResponseFormatJSONSchema
                  )
                ),
              temperature: T.nilable(Float),
              tool_resources:
                T.nilable(OpenAI::Beta::AssistantUpdateParams::ToolResources),
              tools:
                T::Array[
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool,
                    OpenAI::Beta::FileSearchTool,
                    OpenAI::Beta::FunctionTool
                  )
                ],
              top_p: T.nilable(Float),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              description: T.nilable(String),
              instructions: T.nilable(String),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: T.any(
                String,
                OpenAI::Beta::AssistantUpdateParams::Model::OrSymbol
              ),
              name: T.nilable(String),
              reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
              response_format: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash
                )
              ),
              temperature: T.nilable(Float),
              tool_resources: T.nilable(
                OpenAI::Beta::AssistantUpdateParams::ToolResources::OrHash
              ),
              tools: T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool::OrHash,
                  OpenAI::Beta::FileSearchTool::OrHash,
                  OpenAI::Beta::FunctionTool::OrHash
                )
              ],
              top_p: T.nilable(Float),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            description: nil, # The description of the assistant. The maximum length is 512 characters.
            instructions: nil, # The system instructions that the assistant uses. The maximum length is 256,000
                               # characters.
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            model: nil, # ID of the model to use. You can use the
                        # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                        # see all of your available models, or see our
                        # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                        # them.
            name: nil, # The name of the assistant. The maximum length is 256 characters.
            reasoning_effort: nil, # **o-series models only**
                                   # Constrains effort on reasoning for
                                   # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                   # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                   # result in faster responses and fewer tokens used on reasoning in a response.
            response_format: nil, # Specifies the format that the model must output. Compatible with
                                  # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                  # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                  # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                  # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                  # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                  # in the
                                  # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                  # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                  # message the model generates is valid JSON.
                                  # **Important:** when using JSON mode, you **must** also instruct the model to
                                  # produce JSON yourself via a system or user message. Without this, the model may
                                  # generate an unending stream of whitespace until the generation reaches the token
                                  # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                  # the message content may be partially cut off if `finish_reason="length"`, which
                                  # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                  # max context length.
            temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                              # make the output more random, while lower values like 0.2 will make it more
                              # focused and deterministic.
            tool_resources: nil, # A set of resources that are used by the assistant's tools. The resources are
                                 # specific to the type of tool. For example, the `code_interpreter` tool requires
                                 # a list of file IDs, while the `file_search` tool requires a list of vector store
                                 # IDs.
            tools: nil, # A list of tool enabled on the assistant. There can be a maximum of 128 tools per
                        # assistant. Tools can be of types `code_interpreter`, `file_search`, or
                        # `function`.
            top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                        # model considers the results of the tokens with top_p probability mass. So 0.1
                        # means only the tokens comprising the top 10% probability mass are considered.
                        # We generally recommend altering this or temperature but not both.
            request_options: {}
); end
        end

        # ID of the model to use. You can use the
        # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
        # see all of your available models, or see our
        # [Model overview](https://platform.openai.com/docs/models) for descriptions of
        # them.
        module Model
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::AssistantUpdateParams::Model::Variants]) }
            def variants; end
          end

          GPT_3_5_TURBO = T.let(
              :"gpt-3.5-turbo",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_3_5_TURBO_0125 = T.let(
              :"gpt-3.5-turbo-0125",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_3_5_TURBO_0613 = T.let(
              :"gpt-3.5-turbo-0613",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_3_5_TURBO_1106 = T.let(
              :"gpt-3.5-turbo-1106",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_3_5_TURBO_16K = T.let(
              :"gpt-3.5-turbo-16k",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_3_5_TURBO_16K_0613 = T.let(
              :"gpt-3.5-turbo-16k-0613",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4 = T.let(
              :"gpt-4",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4O = T.let(
              :"gpt-4o",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4O_2024_05_13 = T.let(
              :"gpt-4o-2024-05-13",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4O_2024_08_06 = T.let(
              :"gpt-4o-2024-08-06",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4O_2024_11_20 = T.let(
              :"gpt-4o-2024-11-20",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4O_MINI = T.let(
              :"gpt-4o-mini",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4O_MINI_2024_07_18 = T.let(
              :"gpt-4o-mini-2024-07-18",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_0125_PREVIEW = T.let(
              :"gpt-4-0125-preview",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_0314 = T.let(
              :"gpt-4-0314",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_0613 = T.let(
              :"gpt-4-0613",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_1 = T.let(
              :"gpt-4.1",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_1106_PREVIEW = T.let(
              :"gpt-4-1106-preview",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_1_2025_04_14 = T.let(
              :"gpt-4.1-2025-04-14",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_1_MINI = T.let(
              :"gpt-4.1-mini",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_1_MINI_2025_04_14 = T.let(
              :"gpt-4.1-mini-2025-04-14",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_1_NANO = T.let(
              :"gpt-4.1-nano",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_1_NANO_2025_04_14 = T.let(
              :"gpt-4.1-nano-2025-04-14",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_32K = T.let(
              :"gpt-4-32k",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_32K_0314 = T.let(
              :"gpt-4-32k-0314",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_32K_0613 = T.let(
              :"gpt-4-32k-0613",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_5_PREVIEW = T.let(
              :"gpt-4.5-preview",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_5_PREVIEW_2025_02_27 = T.let(
              :"gpt-4.5-preview-2025-02-27",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_TURBO = T.let(
              :"gpt-4-turbo",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_TURBO_2024_04_09 = T.let(
              :"gpt-4-turbo-2024-04-09",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_TURBO_PREVIEW = T.let(
              :"gpt-4-turbo-preview",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          GPT_4_VISION_PREVIEW = T.let(
              :"gpt-4-vision-preview",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          O1 = T.let(:o1, OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol)

          O1_2024_12_17 = T.let(
              :"o1-2024-12-17",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          O3_MINI = T.let(
              :"o3-mini",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          O3_MINI_2025_01_31 = T.let(
              :"o3-mini-2025-01-31",
              OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Beta::AssistantUpdateParams::Model)
            end

          Variants = T.type_alias do
              T.any(
                String,
                OpenAI::Beta::AssistantUpdateParams::Model::TaggedSymbol
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Beta::AssistantUpdateParams,
              OpenAI::Internal::AnyHash
            )
          end

        class ToolResources < OpenAI::Internal::Type::BaseModel
          sig do
            returns(T.nilable(
                OpenAI::Beta::AssistantUpdateParams::ToolResources::CodeInterpreter
              ))
          end
          attr_reader :code_interpreter

          sig do
            params(
              code_interpreter: OpenAI::Beta::AssistantUpdateParams::ToolResources::CodeInterpreter::OrHash
            ).void
          end
          attr_writer :code_interpreter

          sig do
            returns(T.nilable(
                OpenAI::Beta::AssistantUpdateParams::ToolResources::FileSearch
              ))
          end
          attr_reader :file_search

          sig { params(file_search: OpenAI::Beta::AssistantUpdateParams::ToolResources::FileSearch::OrHash).void }
          attr_writer :file_search

          sig do
            override
              .returns({
                code_interpreter:
                  OpenAI::Beta::AssistantUpdateParams::ToolResources::CodeInterpreter,
                file_search:
                  OpenAI::Beta::AssistantUpdateParams::ToolResources::FileSearch
              })
          end
          def to_hash; end

          class << self
            # A set of resources that are used by the assistant's tools. The resources are
            # specific to the type of tool. For example, the `code_interpreter` tool requires
            # a list of file IDs, while the `file_search` tool requires a list of vector store
            # IDs.
            sig do
              params(
                code_interpreter: OpenAI::Beta::AssistantUpdateParams::ToolResources::CodeInterpreter::OrHash,
                file_search: OpenAI::Beta::AssistantUpdateParams::ToolResources::FileSearch::OrHash
              ).returns(T.attached_class)
            end
            def new(code_interpreter: nil, file_search: nil); end
          end

          class CodeInterpreter < OpenAI::Internal::Type::BaseModel
            # Overrides the list of
            # [file](https://platform.openai.com/docs/api-reference/files) IDs made available
            # to the `code_interpreter` tool. There can be a maximum of 20 files associated
            # with the tool.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :file_ids

            sig { params(file_ids: T::Array[String]).void }
            attr_writer :file_ids

            sig { override.returns({ file_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(file_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                file_ids: nil # Overrides the list of
                              # [file](https://platform.openai.com/docs/api-reference/files) IDs made available
                              # to the `code_interpreter` tool. There can be a maximum of 20 files associated
                              # with the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::AssistantUpdateParams::ToolResources::CodeInterpreter,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileSearch < OpenAI::Internal::Type::BaseModel
            # Overrides the
            # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
            # attached to this assistant. There can be a maximum of 1 vector store attached to
            # the assistant.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :vector_store_ids

            sig { params(vector_store_ids: T::Array[String]).void }
            attr_writer :vector_store_ids

            sig { override.returns({ vector_store_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(vector_store_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                vector_store_ids: nil # Overrides the
                                      # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                      # attached to this assistant. There can be a maximum of 1 vector store attached to
                                      # the assistant.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::AssistantUpdateParams::ToolResources::FileSearch,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::AssistantUpdateParams::ToolResources,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class CodeInterpreterTool < OpenAI::Internal::Type::BaseModel
        # The type of tool being defined: `code_interpreter`
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ type: Symbol }) }
        def to_hash; end

        class << self
          sig { params(type: Symbol).returns(T.attached_class) }
          def new(
            type: :code_interpreter # The type of tool being defined: `code_interpreter`
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::CodeInterpreterTool, OpenAI::Internal::AnyHash)
          end
      end

      class FileSearchTool < OpenAI::Internal::Type::BaseModel
        # Overrides for the file search tool.
        sig { returns(T.nilable(OpenAI::Beta::FileSearchTool::FileSearch)) }
        attr_reader :file_search

        sig { params(file_search: OpenAI::Beta::FileSearchTool::FileSearch::OrHash).void }
        attr_writer :file_search

        # The type of tool being defined: `file_search`
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              type: Symbol,
              file_search: OpenAI::Beta::FileSearchTool::FileSearch
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              file_search: OpenAI::Beta::FileSearchTool::FileSearch::OrHash,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            file_search: nil, # Overrides for the file search tool.
            type: :file_search # The type of tool being defined: `file_search`
); end
        end

        class FileSearch < OpenAI::Internal::Type::BaseModel
          # The maximum number of results the file search tool should output. The default is
          # 20 for `gpt-4*` models and 5 for `gpt-3.5-turbo`. This number should be between
          # 1 and 50 inclusive.
          #
          # Note that the file search tool may output fewer than `max_num_results` results.
          # See the
          # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
          # for more information.
          sig { returns(T.nilable(Integer)) }
          attr_reader :max_num_results

          sig { params(max_num_results: Integer).void }
          attr_writer :max_num_results

          # The ranking options for the file search. If not specified, the file search tool
          # will use the `auto` ranker and a score_threshold of 0.
          #
          # See the
          # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
          # for more information.
          sig do
            returns(T.nilable(
                OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions
              ))
          end
          attr_reader :ranking_options

          sig { params(ranking_options: OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::OrHash).void }
          attr_writer :ranking_options

          sig do
            override
              .returns({
                max_num_results: Integer,
                ranking_options:
                  OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions
              })
          end
          def to_hash; end

          class << self
            # Overrides for the file search tool.
            sig do
              params(
                max_num_results: Integer,
                ranking_options: OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              max_num_results: nil, # The maximum number of results the file search tool should output. The default is
                                    # 20 for `gpt-4*` models and 5 for `gpt-3.5-turbo`. This number should be between
                                    # 1 and 50 inclusive.
                                    # Note that the file search tool may output fewer than `max_num_results` results.
                                    # See the
                                    # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
                                    # for more information.
              ranking_options: nil # The ranking options for the file search. If not specified, the file search tool
                                   # will use the `auto` ranker and a score_threshold of 0.
                                   # See the
                                   # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
                                   # for more information.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::FileSearchTool::FileSearch,
                OpenAI::Internal::AnyHash
              )
            end

          class RankingOptions < OpenAI::Internal::Type::BaseModel
            # The ranker to use for the file search. If not specified will use the `auto`
            # ranker.
            sig do
              returns(T.nilable(
                  OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::Ranker::OrSymbol
                ))
            end
            attr_reader :ranker

            sig { params(ranker: OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::Ranker::OrSymbol).void }
            attr_writer :ranker

            # The score threshold for the file search. All values must be a floating point
            # number between 0 and 1.
            sig { returns(Float) }
            attr_accessor :score_threshold

            sig do
              override
                .returns({
                  score_threshold: Float,
                  ranker:
                    OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::Ranker::OrSymbol
                })
            end
            def to_hash; end

            class << self
              # The ranking options for the file search. If not specified, the file search tool
              # will use the `auto` ranker and a score_threshold of 0.
              #
              # See the
              # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
              # for more information.
              sig do
                params(
                  score_threshold: Float,
                  ranker: OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::Ranker::OrSymbol
                ).returns(T.attached_class)
              end
              def new(
                score_threshold:, # The score threshold for the file search. All values must be a floating point
                                  # number between 0 and 1.
                ranker: nil # The ranker to use for the file search. If not specified will use the `auto`
                            # ranker.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions,
                  OpenAI::Internal::AnyHash
                )
              end

            # The ranker to use for the file search. If not specified will use the `auto`
            # ranker.
            module Ranker
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::Ranker::TaggedSymbol
                  ])
                end
                def values; end
              end

              AUTO = T.let(
                  :auto,
                  OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::Ranker::TaggedSymbol
                )

              DEFAULT_2024_08_21 = T.let(
                  :default_2024_08_21,
                  OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::Ranker::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Beta::FileSearchTool::FileSearch::RankingOptions::Ranker
                  )
                end
            end
          end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::FileSearchTool, OpenAI::Internal::AnyHash)
          end
      end

      class FunctionTool < OpenAI::Internal::Type::BaseModel
        sig { returns(OpenAI::FunctionDefinition) }
        attr_reader :function

        sig { params(function: OpenAI::FunctionDefinition::OrHash).void }
        attr_writer :function

        # The type of tool being defined: `function`
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ function: OpenAI::FunctionDefinition, type: Symbol }) }
        def to_hash; end

        class << self
          sig { params(function: OpenAI::FunctionDefinition::OrHash, type: Symbol).returns(T.attached_class) }
          def new(
            function:,
            type: :function # The type of tool being defined: `function`
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::FunctionTool, OpenAI::Internal::AnyHash)
          end
      end

      # Occurs when a
      # [message](https://platform.openai.com/docs/api-reference/messages/object) is
      # created.
      module MessageStreamEvent
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Beta::MessageStreamEvent::Variants]) }
          def variants; end
        end

        class ThreadMessageCompleted < OpenAI::Internal::Type::BaseModel
          # Represents a message within a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Message) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Message::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Message, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [message](https://platform.openai.com/docs/api-reference/messages/object) is
            # completed.
            sig { params(data: OpenAI::Beta::Threads::Message::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a message within a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.message.completed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::MessageStreamEvent::ThreadMessageCompleted,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadMessageCreated < OpenAI::Internal::Type::BaseModel
          # Represents a message within a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Message) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Message::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Message, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [message](https://platform.openai.com/docs/api-reference/messages/object) is
            # created.
            sig { params(data: OpenAI::Beta::Threads::Message::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a message within a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.message.created"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::MessageStreamEvent::ThreadMessageCreated,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadMessageDelta < OpenAI::Internal::Type::BaseModel
          # Represents a message delta i.e. any changed fields on a message during
          # streaming.
          sig { returns(OpenAI::Beta::Threads::MessageDeltaEvent) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::MessageDeltaEvent::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::MessageDeltaEvent, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when parts of a
            # [Message](https://platform.openai.com/docs/api-reference/messages/object) are
            # being streamed.
            sig do
              params(
                data: OpenAI::Beta::Threads::MessageDeltaEvent::OrHash,
                event: Symbol
              ).returns(T.attached_class)
            end
            def new(
              data:, # Represents a message delta i.e. any changed fields on a message during
                     # streaming.
              event: :"thread.message.delta"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::MessageStreamEvent::ThreadMessageDelta,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadMessageInProgress < OpenAI::Internal::Type::BaseModel
          # Represents a message within a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Message) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Message::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Message, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [message](https://platform.openai.com/docs/api-reference/messages/object) moves
            # to an `in_progress` state.
            sig { params(data: OpenAI::Beta::Threads::Message::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a message within a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.message.in_progress"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::MessageStreamEvent::ThreadMessageInProgress,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadMessageIncomplete < OpenAI::Internal::Type::BaseModel
          # Represents a message within a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Message) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Message::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Message, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [message](https://platform.openai.com/docs/api-reference/messages/object) ends
            # before it is completed.
            sig { params(data: OpenAI::Beta::Threads::Message::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a message within a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.message.incomplete"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::MessageStreamEvent::ThreadMessageIncomplete,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Beta::MessageStreamEvent::ThreadMessageCreated,
              OpenAI::Beta::MessageStreamEvent::ThreadMessageInProgress,
              OpenAI::Beta::MessageStreamEvent::ThreadMessageDelta,
              OpenAI::Beta::MessageStreamEvent::ThreadMessageCompleted,
              OpenAI::Beta::MessageStreamEvent::ThreadMessageIncomplete
            )
          end
      end

      # Occurs when a
      # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
      # is created.
      module RunStepStreamEvent
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Beta::RunStepStreamEvent::Variants]) }
          def variants; end
        end

        class ThreadRunStepCancelled < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # is cancelled.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.cancelled"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStepStreamEvent::ThreadRunStepCancelled,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepCompleted < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # is completed.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.completed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStepStreamEvent::ThreadRunStepCompleted,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepCreated < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # is created.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.created"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStepStreamEvent::ThreadRunStepCreated,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepDelta < OpenAI::Internal::Type::BaseModel
          # Represents a run step delta i.e. any changed fields on a run step during
          # streaming.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStepDeltaEvent) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStepDeltaEvent::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig do
            override
              .returns({
                data: OpenAI::Beta::Threads::Runs::RunStepDeltaEvent,
                event: Symbol
              })
          end
          def to_hash; end

          class << self
            # Occurs when parts of a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # are being streamed.
            sig do
              params(
                data: OpenAI::Beta::Threads::Runs::RunStepDeltaEvent::OrHash,
                event: Symbol
              ).returns(T.attached_class)
            end
            def new(
              data:, # Represents a run step delta i.e. any changed fields on a run step during
                     # streaming.
              event: :"thread.run.step.delta"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStepStreamEvent::ThreadRunStepDelta,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepExpired < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # expires.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.expired"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStepStreamEvent::ThreadRunStepExpired,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepFailed < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # fails.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.failed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStepStreamEvent::ThreadRunStepFailed,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunStepInProgress < OpenAI::Internal::Type::BaseModel
          # Represents a step in execution of a run.
          sig { returns(OpenAI::Beta::Threads::Runs::RunStep) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Runs::RunStep, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a
            # [run step](https://platform.openai.com/docs/api-reference/run-steps/step-object)
            # moves to an `in_progress` state.
            sig { params(data: OpenAI::Beta::Threads::Runs::RunStep::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents a step in execution of a run.
              event: :"thread.run.step.in_progress"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStepStreamEvent::ThreadRunStepInProgress,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Beta::RunStepStreamEvent::ThreadRunStepCreated,
              OpenAI::Beta::RunStepStreamEvent::ThreadRunStepInProgress,
              OpenAI::Beta::RunStepStreamEvent::ThreadRunStepDelta,
              OpenAI::Beta::RunStepStreamEvent::ThreadRunStepCompleted,
              OpenAI::Beta::RunStepStreamEvent::ThreadRunStepFailed,
              OpenAI::Beta::RunStepStreamEvent::ThreadRunStepCancelled,
              OpenAI::Beta::RunStepStreamEvent::ThreadRunStepExpired
            )
          end
      end

      # Occurs when a new
      # [run](https://platform.openai.com/docs/api-reference/runs/object) is created.
      module RunStreamEvent
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Beta::RunStreamEvent::Variants]) }
          def variants; end
        end

        class ThreadRunCancelled < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # is cancelled.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.cancelled"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunCancelled,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunCancelling < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # moves to a `cancelling` status.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.cancelling"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunCancelling,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunCompleted < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # is completed.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.completed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunCompleted,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunCreated < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a new
            # [run](https://platform.openai.com/docs/api-reference/runs/object) is created.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.created"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunCreated,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunExpired < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # expires.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.expired"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunExpired,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunFailed < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # fails.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.failed"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunFailed,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunInProgress < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # moves to an `in_progress` status.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.in_progress"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunInProgress,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunIncomplete < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # ends with status `incomplete`.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.incomplete"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunIncomplete,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunQueued < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # moves to a `queued` status.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.queued"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunQueued,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ThreadRunRequiresAction < OpenAI::Internal::Type::BaseModel
          # Represents an execution run on a
          # [thread](https://platform.openai.com/docs/api-reference/threads).
          sig { returns(OpenAI::Beta::Threads::Run) }
          attr_reader :data

          sig { params(data: OpenAI::Beta::Threads::Run::OrHash).void }
          attr_writer :data

          sig { returns(Symbol) }
          attr_accessor :event

          sig { override.returns({ data: OpenAI::Beta::Threads::Run, event: Symbol }) }
          def to_hash; end

          class << self
            # Occurs when a [run](https://platform.openai.com/docs/api-reference/runs/object)
            # moves to a `requires_action` status.
            sig { params(data: OpenAI::Beta::Threads::Run::OrHash, event: Symbol).returns(T.attached_class) }
            def new(
              data:, # Represents an execution run on a
                     # [thread](https://platform.openai.com/docs/api-reference/threads).
              event: :"thread.run.requires_action"
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::RunStreamEvent::ThreadRunRequiresAction,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Beta::RunStreamEvent::ThreadRunCreated,
              OpenAI::Beta::RunStreamEvent::ThreadRunQueued,
              OpenAI::Beta::RunStreamEvent::ThreadRunInProgress,
              OpenAI::Beta::RunStreamEvent::ThreadRunRequiresAction,
              OpenAI::Beta::RunStreamEvent::ThreadRunCompleted,
              OpenAI::Beta::RunStreamEvent::ThreadRunIncomplete,
              OpenAI::Beta::RunStreamEvent::ThreadRunFailed,
              OpenAI::Beta::RunStreamEvent::ThreadRunCancelling,
              OpenAI::Beta::RunStreamEvent::ThreadRunCancelled,
              OpenAI::Beta::RunStreamEvent::ThreadRunExpired
            )
          end
      end

      class Thread < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) for when the thread was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # The identifier, which can be referenced in API endpoints.
        sig { returns(String) }
        attr_accessor :id

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The object type, which is always `thread`.
        sig { returns(Symbol) }
        attr_accessor :object

        # A set of resources that are made available to the assistant's tools in this
        # thread. The resources are specific to the type of tool. For example, the
        # `code_interpreter` tool requires a list of file IDs, while the `file_search`
        # tool requires a list of vector store IDs.
        sig { returns(T.nilable(OpenAI::Beta::Thread::ToolResources)) }
        attr_reader :tool_resources

        sig { params(tool_resources: T.nilable(OpenAI::Beta::Thread::ToolResources::OrHash)).void }
        attr_writer :tool_resources

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              metadata: T.nilable(T::Hash[Symbol, String]),
              object: Symbol,
              tool_resources: T.nilable(OpenAI::Beta::Thread::ToolResources)
            })
        end
        def to_hash; end

        class << self
          # Represents a thread that contains
          # [messages](https://platform.openai.com/docs/api-reference/messages).
          sig do
            params(
              id: String,
              created_at: Integer,
              metadata: T.nilable(T::Hash[Symbol, String]),
              tool_resources: T.nilable(OpenAI::Beta::Thread::ToolResources::OrHash),
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The identifier, which can be referenced in API endpoints.
            created_at:, # The Unix timestamp (in seconds) for when the thread was created.
            metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
            tool_resources:, # A set of resources that are made available to the assistant's tools in this
                             # thread. The resources are specific to the type of tool. For example, the
                             # `code_interpreter` tool requires a list of file IDs, while the `file_search`
                             # tool requires a list of vector store IDs.
            object: :thread # The object type, which is always `thread`.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::Thread, OpenAI::Internal::AnyHash)
          end

        class ToolResources < OpenAI::Internal::Type::BaseModel
          sig { returns(T.nilable(OpenAI::Beta::Thread::ToolResources::CodeInterpreter)) }
          attr_reader :code_interpreter

          sig { params(code_interpreter: OpenAI::Beta::Thread::ToolResources::CodeInterpreter::OrHash).void }
          attr_writer :code_interpreter

          sig { returns(T.nilable(OpenAI::Beta::Thread::ToolResources::FileSearch)) }
          attr_reader :file_search

          sig { params(file_search: OpenAI::Beta::Thread::ToolResources::FileSearch::OrHash).void }
          attr_writer :file_search

          sig do
            override
              .returns({
                code_interpreter:
                  OpenAI::Beta::Thread::ToolResources::CodeInterpreter,
                file_search: OpenAI::Beta::Thread::ToolResources::FileSearch
              })
          end
          def to_hash; end

          class << self
            # A set of resources that are made available to the assistant's tools in this
            # thread. The resources are specific to the type of tool. For example, the
            # `code_interpreter` tool requires a list of file IDs, while the `file_search`
            # tool requires a list of vector store IDs.
            sig do
              params(
                code_interpreter: OpenAI::Beta::Thread::ToolResources::CodeInterpreter::OrHash,
                file_search: OpenAI::Beta::Thread::ToolResources::FileSearch::OrHash
              ).returns(T.attached_class)
            end
            def new(code_interpreter: nil, file_search: nil); end
          end

          class CodeInterpreter < OpenAI::Internal::Type::BaseModel
            # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
            # available to the `code_interpreter` tool. There can be a maximum of 20 files
            # associated with the tool.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :file_ids

            sig { params(file_ids: T::Array[String]).void }
            attr_writer :file_ids

            sig { override.returns({ file_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(file_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                file_ids: nil # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
                              # available to the `code_interpreter` tool. There can be a maximum of 20 files
                              # associated with the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Thread::ToolResources::CodeInterpreter,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileSearch < OpenAI::Internal::Type::BaseModel
            # The
            # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
            # attached to this thread. There can be a maximum of 1 vector store attached to
            # the thread.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :vector_store_ids

            sig { params(vector_store_ids: T::Array[String]).void }
            attr_writer :vector_store_ids

            sig { override.returns({ vector_store_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(vector_store_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                vector_store_ids: nil # The
                                      # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                      # attached to this thread. There can be a maximum of 1 vector store attached to
                                      # the thread.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Thread::ToolResources::FileSearch,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Thread::ToolResources,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class ThreadCreateAndRunParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # The ID of the
        # [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to
        # execute this run.
        sig { returns(String) }
        attr_accessor :assistant_id

        # Override the default system message of the assistant. This is useful for
        # modifying the behavior on a per-run basis.
        sig { returns(T.nilable(String)) }
        attr_accessor :instructions

        # The maximum number of completion tokens that may be used over the course of the
        # run. The run will make a best effort to use only the number of completion tokens
        # specified, across multiple turns of the run. If the run exceeds the number of
        # completion tokens specified, the run will end with status `incomplete`. See
        # `incomplete_details` for more info.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_completion_tokens

        # The maximum number of prompt tokens that may be used over the course of the run.
        # The run will make a best effort to use only the number of prompt tokens
        # specified, across multiple turns of the run. If the run exceeds the number of
        # prompt tokens specified, the run will end with status `incomplete`. See
        # `incomplete_details` for more info.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_prompt_tokens

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to
        # be used to execute this run. If a value is provided here, it will override the
        # model associated with the assistant. If not, the model associated with the
        # assistant will be used.
        sig { returns(T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol))) }
        attr_accessor :model

        # Whether to enable
        # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
        # during tool use.
        sig { returns(T.nilable(T::Boolean)) }
        attr_reader :parallel_tool_calls

        sig { params(parallel_tool_calls: T::Boolean).void }
        attr_writer :parallel_tool_calls

        # Specifies the format that the model must output. Compatible with
        # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
        # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
        # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
        #
        # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        # Outputs which ensures the model will match your supplied JSON schema. Learn more
        # in the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
        # message the model generates is valid JSON.
        #
        # **Important:** when using JSON mode, you **must** also instruct the model to
        # produce JSON yourself via a system or user message. Without this, the model may
        # generate an unending stream of whitespace until the generation reaches the token
        # limit, resulting in a long-running and seemingly "stuck" request. Also note that
        # the message content may be partially cut off if `finish_reason="length"`, which
        # indicates the generation exceeded `max_tokens` or the conversation exceeded the
        # max context length.
        sig do
          returns(T.nilable(
              T.any(
                Symbol,
                OpenAI::ResponseFormatText,
                OpenAI::ResponseFormatJSONObject,
                OpenAI::ResponseFormatJSONSchema
              )
            ))
        end
        attr_accessor :response_format

        # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        # make the output more random, while lower values like 0.2 will make it more
        # focused and deterministic.
        sig { returns(T.nilable(Float)) }
        attr_accessor :temperature

        # Options to create a new thread. If no thread is provided when running a request,
        # an empty thread will be created.
        sig { returns(T.nilable(OpenAI::Beta::ThreadCreateAndRunParams::Thread)) }
        attr_reader :thread

        sig { params(thread: OpenAI::Beta::ThreadCreateAndRunParams::Thread::OrHash).void }
        attr_writer :thread

        # Controls which (if any) tool is called by the model. `none` means the model will
        # not call any tools and instead generates a message. `auto` is the default value
        # and means the model can pick between generating a message or calling one or more
        # tools. `required` means the model must call one or more tools before responding
        # to the user. Specifying a particular tool like `{"type": "file_search"}` or
        # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
        # call that tool.
        sig do
          returns(T.nilable(
              T.any(
                OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                OpenAI::Beta::AssistantToolChoice
              )
            ))
        end
        attr_accessor :tool_choice

        # A set of resources that are used by the assistant's tools. The resources are
        # specific to the type of tool. For example, the `code_interpreter` tool requires
        # a list of file IDs, while the `file_search` tool requires a list of vector store
        # IDs.
        sig { returns(T.nilable(OpenAI::Beta::ThreadCreateAndRunParams::ToolResources)) }
        attr_reader :tool_resources

        sig do
          params(
            tool_resources: T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::OrHash
              )
          ).void
        end
        attr_writer :tool_resources

        # Override the tools the assistant can use for this run. This is useful for
        # modifying the behavior on a per-run basis.
        sig do
          returns(T.nilable(
              T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool,
                  OpenAI::Beta::FileSearchTool,
                  OpenAI::Beta::FunctionTool
                )
              ]
            ))
        end
        attr_accessor :tools

        # An alternative to sampling with temperature, called nucleus sampling, where the
        # model considers the results of the tokens with top_p probability mass. So 0.1
        # means only the tokens comprising the top 10% probability mass are considered.
        #
        # We generally recommend altering this or temperature but not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :top_p

        # Controls for how a thread will be truncated prior to the run. Use this to
        # control the intial context window of the run.
        sig do
          returns(T.nilable(
              OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy
            ))
        end
        attr_reader :truncation_strategy

        sig do
          params(
            truncation_strategy: T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::OrHash
              )
          ).void
        end
        attr_writer :truncation_strategy

        sig do
          override
            .returns({
              assistant_id: String,
              instructions: T.nilable(String),
              max_completion_tokens: T.nilable(Integer),
              max_prompt_tokens: T.nilable(Integer),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol)),
              parallel_tool_calls: T::Boolean,
              response_format:
                T.nilable(
                  T.any(
                    Symbol,
                    OpenAI::ResponseFormatText,
                    OpenAI::ResponseFormatJSONObject,
                    OpenAI::ResponseFormatJSONSchema
                  )
                ),
              temperature: T.nilable(Float),
              thread: OpenAI::Beta::ThreadCreateAndRunParams::Thread,
              tool_choice:
                T.nilable(
                  T.any(
                    OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                    OpenAI::Beta::AssistantToolChoice
                  )
                ),
              tool_resources:
                T.nilable(
                  OpenAI::Beta::ThreadCreateAndRunParams::ToolResources
                ),
              tools:
                T.nilable(
                  T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool,
                      OpenAI::Beta::FileSearchTool,
                      OpenAI::Beta::FunctionTool
                    )
                  ]
                ),
              top_p: T.nilable(Float),
              truncation_strategy:
                T.nilable(
                  OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy
                ),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              assistant_id: String,
              instructions: T.nilable(String),
              max_completion_tokens: T.nilable(Integer),
              max_prompt_tokens: T.nilable(Integer),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol)),
              parallel_tool_calls: T::Boolean,
              response_format: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash
                )
              ),
              temperature: T.nilable(Float),
              thread: OpenAI::Beta::ThreadCreateAndRunParams::Thread::OrHash,
              tool_choice: T.nilable(
                T.any(
                  OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                  OpenAI::Beta::AssistantToolChoice::OrHash
                )
              ),
              tool_resources: T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::OrHash
              ),
              tools: T.nilable(
                T::Array[
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool::OrHash,
                    OpenAI::Beta::FileSearchTool::OrHash,
                    OpenAI::Beta::FunctionTool::OrHash
                  )
                ]
              ),
              top_p: T.nilable(Float),
              truncation_strategy: T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::OrHash
              ),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            assistant_id:, # The ID of the
                           # [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to
                           # execute this run.
            instructions: nil, # Override the default system message of the assistant. This is useful for
                               # modifying the behavior on a per-run basis.
            max_completion_tokens: nil, # The maximum number of completion tokens that may be used over the course of the
                                        # run. The run will make a best effort to use only the number of completion tokens
                                        # specified, across multiple turns of the run. If the run exceeds the number of
                                        # completion tokens specified, the run will end with status `incomplete`. See
                                        # `incomplete_details` for more info.
            max_prompt_tokens: nil, # The maximum number of prompt tokens that may be used over the course of the run.
                                    # The run will make a best effort to use only the number of prompt tokens
                                    # specified, across multiple turns of the run. If the run exceeds the number of
                                    # prompt tokens specified, the run will end with status `incomplete`. See
                                    # `incomplete_details` for more info.
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            model: nil, # The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to
                        # be used to execute this run. If a value is provided here, it will override the
                        # model associated with the assistant. If not, the model associated with the
                        # assistant will be used.
            parallel_tool_calls: nil, # Whether to enable
                                      # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                      # during tool use.
            response_format: nil, # Specifies the format that the model must output. Compatible with
                                  # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                  # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                  # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                  # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                  # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                  # in the
                                  # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                  # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                  # message the model generates is valid JSON.
                                  # **Important:** when using JSON mode, you **must** also instruct the model to
                                  # produce JSON yourself via a system or user message. Without this, the model may
                                  # generate an unending stream of whitespace until the generation reaches the token
                                  # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                  # the message content may be partially cut off if `finish_reason="length"`, which
                                  # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                  # max context length.
            temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                              # make the output more random, while lower values like 0.2 will make it more
                              # focused and deterministic.
            thread: nil, # Options to create a new thread. If no thread is provided when running a request,
                         # an empty thread will be created.
            tool_choice: nil, # Controls which (if any) tool is called by the model. `none` means the model will
                              # not call any tools and instead generates a message. `auto` is the default value
                              # and means the model can pick between generating a message or calling one or more
                              # tools. `required` means the model must call one or more tools before responding
                              # to the user. Specifying a particular tool like `{"type": "file_search"}` or
                              # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                              # call that tool.
            tool_resources: nil, # A set of resources that are used by the assistant's tools. The resources are
                                 # specific to the type of tool. For example, the `code_interpreter` tool requires
                                 # a list of file IDs, while the `file_search` tool requires a list of vector store
                                 # IDs.
            tools: nil, # Override the tools the assistant can use for this run. This is useful for
                        # modifying the behavior on a per-run basis.
            top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                        # model considers the results of the tokens with top_p probability mass. So 0.1
                        # means only the tokens comprising the top 10% probability mass are considered.
                        # We generally recommend altering this or temperature but not both.
            truncation_strategy: nil, # Controls for how a thread will be truncated prior to the run. Use this to
                                      # control the intial context window of the run.
            request_options: {}
); end
        end

        # The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to
        # be used to execute this run. If a value is provided here, it will override the
        # model associated with the assistant. If not, the model associated with the
        # assistant will be used.
        module Model
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::ThreadCreateAndRunParams::Model::Variants]) }
            def variants; end
          end

          Variants = T.type_alias { T.any(String, OpenAI::ChatModel::TaggedSymbol) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Beta::ThreadCreateAndRunParams,
              OpenAI::Internal::AnyHash
            )
          end

        class Thread < OpenAI::Internal::Type::BaseModel
          # A list of [messages](https://platform.openai.com/docs/api-reference/messages) to
          # start the thread with.
          sig do
            returns(T.nilable(
                T::Array[
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message
                ]
              ))
          end
          attr_reader :messages

          sig do
            params(
              messages: T::Array[
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::OrHash
                ]
            ).void
          end
          attr_writer :messages

          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # A set of resources that are made available to the assistant's tools in this
          # thread. The resources are specific to the type of tool. For example, the
          # `code_interpreter` tool requires a list of file IDs, while the `file_search`
          # tool requires a list of vector store IDs.
          sig do
            returns(T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources
              ))
          end
          attr_reader :tool_resources

          sig do
            params(
              tool_resources: T.nilable(
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::OrHash
                )
            ).void
          end
          attr_writer :tool_resources

          sig do
            override
              .returns({
                messages:
                  T::Array[
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message
                  ],
                metadata: T.nilable(T::Hash[Symbol, String]),
                tool_resources:
                  T.nilable(
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources
                  )
              })
          end
          def to_hash; end

          class << self
            # Options to create a new thread. If no thread is provided when running a request,
            # an empty thread will be created.
            sig do
              params(
                messages: T::Array[
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::OrHash
                ],
                metadata: T.nilable(T::Hash[Symbol, String]),
                tool_resources: T.nilable(
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::OrHash
                )
              ).returns(T.attached_class)
            end
            def new(
              messages: nil, # A list of [messages](https://platform.openai.com/docs/api-reference/messages) to
                             # start the thread with.
              metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard.
                             # Keys are strings with a maximum length of 64 characters. Values are strings with
                             # a maximum length of 512 characters.
              tool_resources: nil # A set of resources that are made available to the assistant's tools in this
                                  # thread. The resources are specific to the type of tool. For example, the
                                  # `code_interpreter` tool requires a list of file IDs, while the `file_search`
                                  # tool requires a list of vector store IDs.
); end
          end

          class Message < OpenAI::Internal::Type::BaseModel
            # A list of files attached to the message, and the tools they should be added to.
            sig do
              returns(T.nilable(
                  T::Array[
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment
                  ]
                ))
            end
            attr_accessor :attachments

            # The text contents of the message.
            sig { returns(OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Content::Variants) }
            attr_accessor :content

            # Set of 16 key-value pairs that can be attached to an object. This can be useful
            # for storing additional information about the object in a structured format, and
            # querying for objects via API or the dashboard.
            #
            # Keys are strings with a maximum length of 64 characters. Values are strings with
            # a maximum length of 512 characters.
            sig { returns(T.nilable(T::Hash[Symbol, String])) }
            attr_accessor :metadata

            # The role of the entity that is creating the message. Allowed values include:
            #
            # - `user`: Indicates the message is sent by an actual user and should be used in
            #   most cases to represent user-generated messages.
            # - `assistant`: Indicates the message is generated by the assistant. Use this
            #   value to insert messages from the assistant into the conversation.
            sig { returns(OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Role::OrSymbol) }
            attr_accessor :role

            sig do
              override
                .returns({
                  content:
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Content::Variants,
                  role:
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Role::OrSymbol,
                  attachments:
                    T.nilable(
                      T::Array[
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment
                      ]
                    ),
                  metadata: T.nilable(T::Hash[Symbol, String])
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  content: OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Content::Variants,
                  role: OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Role::OrSymbol,
                  attachments: T.nilable(
                    T::Array[
                      OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment::OrHash
                    ]
                  ),
                  metadata: T.nilable(T::Hash[Symbol, String])
                ).returns(T.attached_class)
              end
              def new(
                content:, # The text contents of the message.
                role:, # The role of the entity that is creating the message. Allowed values include:
                       # - `user`: Indicates the message is sent by an actual user and should be used in
                       #   most cases to represent user-generated messages.
                       # - `assistant`: Indicates the message is generated by the assistant. Use this
                       #   value to insert messages from the assistant into the conversation.
                attachments: nil, # A list of files attached to the message, and the tools they should be added to.
                metadata: nil # Set of 16 key-value pairs that can be attached to an object. This can be useful
                              # for storing additional information about the object in a structured format, and
                              # querying for objects via API or the dashboard.
                              # Keys are strings with a maximum length of 64 characters. Values are strings with
                              # a maximum length of 512 characters.
); end
            end

            class Attachment < OpenAI::Internal::Type::BaseModel
              # The ID of the file to attach to the message.
              sig { returns(T.nilable(String)) }
              attr_reader :file_id

              sig { params(file_id: String).void }
              attr_writer :file_id

              # The tools to add this file to.
              sig do
                returns(T.nilable(
                    T::Array[
                      T.any(
                        OpenAI::Beta::CodeInterpreterTool,
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment::Tool::FileSearch
                      )
                    ]
                  ))
              end
              attr_reader :tools

              sig do
                params(
                  tools: T::Array[
                      T.any(
                        OpenAI::Beta::CodeInterpreterTool::OrHash,
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment::Tool::FileSearch::OrHash
                      )
                    ]
                ).void
              end
              attr_writer :tools

              sig do
                override
                  .returns({
                    file_id: String,
                    tools:
                      T::Array[
                        T.any(
                          OpenAI::Beta::CodeInterpreterTool,
                          OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment::Tool::FileSearch
                        )
                      ]
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    file_id: String,
                    tools: T::Array[
                      T.any(
                        OpenAI::Beta::CodeInterpreterTool::OrHash,
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment::Tool::FileSearch::OrHash
                      )
                    ]
                  ).returns(T.attached_class)
                end
                def new(
                  file_id: nil, # The ID of the file to attach to the message.
                  tools: nil # The tools to add this file to.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment,
                    OpenAI::Internal::AnyHash
                  )
                end

              module Tool
                extend OpenAI::Internal::Type::Union

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment::Tool::Variants
                    ])
                  end
                  def variants; end
                end

                class FileSearch < OpenAI::Internal::Type::BaseModel
                  # The type of tool being defined: `file_search`
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig { override.returns({ type: Symbol }) }
                  def to_hash; end

                  class << self
                    sig { params(type: Symbol).returns(T.attached_class) }
                    def new(
                      type: :file_search # The type of tool being defined: `file_search`
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment::Tool::FileSearch,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                Variants = T.type_alias do
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool,
                      OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Attachment::Tool::FileSearch
                    )
                  end
              end
            end

            # The text contents of the message.
            module Content
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Content::Variants
                  ])
                end
                def variants; end
              end

              MessageContentPartParamArray = T.let(
                  OpenAI::Internal::Type::ArrayOf[
                    union: OpenAI::Beta::Threads::MessageContentPartParam
                  ],
                  OpenAI::Internal::Type::Converter
                )

              Variants = T.type_alias do
                  T.any(
                    String,
                    T::Array[
                      OpenAI::Beta::Threads::MessageContentPartParam::Variants
                    ]
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message,
                  OpenAI::Internal::AnyHash
                )
              end

            # The role of the entity that is creating the message. Allowed values include:
            #
            # - `user`: Indicates the message is sent by an actual user and should be used in
            #   most cases to represent user-generated messages.
            # - `assistant`: Indicates the message is generated by the assistant. Use this
            #   value to insert messages from the assistant into the conversation.
            module Role
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Role::TaggedSymbol
                  ])
                end
                def values; end
              end

              ASSISTANT = T.let(
                  :assistant,
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Role::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Role
                  )
                end

              USER = T.let(
                  :user,
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::Message::Role::TaggedSymbol
                )
            end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::ThreadCreateAndRunParams::Thread,
                OpenAI::Internal::AnyHash
              )
            end

          class ToolResources < OpenAI::Internal::Type::BaseModel
            sig do
              returns(T.nilable(
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::CodeInterpreter
                ))
            end
            attr_reader :code_interpreter

            sig do
              params(
                code_interpreter: OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::CodeInterpreter::OrHash
              ).void
            end
            attr_writer :code_interpreter

            sig do
              returns(T.nilable(
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch
                ))
            end
            attr_reader :file_search

            sig do
              params(
                file_search: OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::OrHash
              ).void
            end
            attr_writer :file_search

            sig do
              override
                .returns({
                  code_interpreter:
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::CodeInterpreter,
                  file_search:
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch
                })
            end
            def to_hash; end

            class << self
              # A set of resources that are made available to the assistant's tools in this
              # thread. The resources are specific to the type of tool. For example, the
              # `code_interpreter` tool requires a list of file IDs, while the `file_search`
              # tool requires a list of vector store IDs.
              sig do
                params(
                  code_interpreter: OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::CodeInterpreter::OrHash,
                  file_search: OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::OrHash
                ).returns(T.attached_class)
              end
              def new(code_interpreter: nil, file_search: nil); end
            end

            class CodeInterpreter < OpenAI::Internal::Type::BaseModel
              # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
              # available to the `code_interpreter` tool. There can be a maximum of 20 files
              # associated with the tool.
              sig { returns(T.nilable(T::Array[String])) }
              attr_reader :file_ids

              sig { params(file_ids: T::Array[String]).void }
              attr_writer :file_ids

              sig { override.returns({ file_ids: T::Array[String] }) }
              def to_hash; end

              class << self
                sig { params(file_ids: T::Array[String]).returns(T.attached_class) }
                def new(
                  file_ids: nil # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
                                # available to the `code_interpreter` tool. There can be a maximum of 20 files
                                # associated with the tool.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::CodeInterpreter,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            class FileSearch < OpenAI::Internal::Type::BaseModel
              # The
              # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
              # attached to this thread. There can be a maximum of 1 vector store attached to
              # the thread.
              sig { returns(T.nilable(T::Array[String])) }
              attr_reader :vector_store_ids

              sig { params(vector_store_ids: T::Array[String]).void }
              attr_writer :vector_store_ids

              # A helper to create a
              # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
              # with file_ids and attach it to this thread. There can be a maximum of 1 vector
              # store attached to the thread.
              sig do
                returns(T.nilable(
                    T::Array[
                      OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore
                    ]
                  ))
              end
              attr_reader :vector_stores

              sig do
                params(
                  vector_stores: T::Array[
                      OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::OrHash
                    ]
                ).void
              end
              attr_writer :vector_stores

              sig do
                override
                  .returns({
                    vector_store_ids: T::Array[String],
                    vector_stores:
                      T::Array[
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore
                      ]
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    vector_store_ids: T::Array[String],
                    vector_stores: T::Array[
                      OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::OrHash
                    ]
                  ).returns(T.attached_class)
                end
                def new(
                  vector_store_ids: nil, # The
                                         # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                         # attached to this thread. There can be a maximum of 1 vector store attached to
                                         # the thread.
                  vector_stores: nil # A helper to create a
                                     # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                     # with file_ids and attach it to this thread. There can be a maximum of 1 vector
                                     # store attached to the thread.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch,
                    OpenAI::Internal::AnyHash
                  )
                end

              class VectorStore < OpenAI::Internal::Type::BaseModel
                # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                # strategy.
                sig do
                  returns(T.nilable(
                      T.any(
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static
                      )
                    ))
                end
                attr_reader :chunking_strategy

                sig do
                  params(
                    chunking_strategy: T.any(
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto::OrHash,
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::OrHash
                      )
                  ).void
                end
                attr_writer :chunking_strategy

                # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to
                # add to the vector store. There can be a maximum of 10000 files in a vector
                # store.
                sig { returns(T.nilable(T::Array[String])) }
                attr_reader :file_ids

                sig { params(file_ids: T::Array[String]).void }
                attr_writer :file_ids

                # Set of 16 key-value pairs that can be attached to an object. This can be useful
                # for storing additional information about the object in a structured format, and
                # querying for objects via API or the dashboard.
                #
                # Keys are strings with a maximum length of 64 characters. Values are strings with
                # a maximum length of 512 characters.
                sig { returns(T.nilable(T::Hash[Symbol, String])) }
                attr_accessor :metadata

                sig do
                  override
                    .returns({
                      chunking_strategy:
                        T.any(
                          OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                          OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static
                        ),
                      file_ids: T::Array[String],
                      metadata: T.nilable(T::Hash[Symbol, String])
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      chunking_strategy: T.any(
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto::OrHash,
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::OrHash
                      ),
                      file_ids: T::Array[String],
                      metadata: T.nilable(T::Hash[Symbol, String])
                    ).returns(T.attached_class)
                  end
                  def new(
                    chunking_strategy: nil, # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                                            # strategy.
                    file_ids: nil, # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to
                                   # add to the vector store. There can be a maximum of 10000 files in a vector
                                   # store.
                    metadata: nil # Set of 16 key-value pairs that can be attached to an object. This can be useful
                                  # for storing additional information about the object in a structured format, and
                                  # querying for objects via API or the dashboard.
                                  # Keys are strings with a maximum length of 64 characters. Values are strings with
                                  # a maximum length of 512 characters.
); end
                end

                # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                # strategy.
                module ChunkingStrategy
                  extend OpenAI::Internal::Type::Union

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Variants
                      ])
                    end
                    def variants; end
                  end

                  class Auto < OpenAI::Internal::Type::BaseModel
                    # Always `auto`.
                    sig { returns(Symbol) }
                    attr_accessor :type

                    sig { override.returns({ type: Symbol }) }
                    def to_hash; end

                    class << self
                      # The default strategy. This strategy currently uses a `max_chunk_size_tokens` of
                      # `800` and `chunk_overlap_tokens` of `400`.
                      sig { params(type: Symbol).returns(T.attached_class) }
                      def new(
                        type: :auto # Always `auto`.
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end

                  class Static < OpenAI::Internal::Type::BaseModel
                    sig do
                      returns(OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static)
                    end
                    attr_reader :static

                    sig do
                      params(
                        static: OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static::OrHash
                      ).void
                    end
                    attr_writer :static

                    # Always `static`.
                    sig { returns(Symbol) }
                    attr_accessor :type

                    sig do
                      override
                        .returns({
                          static:
                            OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static,
                          type: Symbol
                        })
                    end
                    def to_hash; end

                    class << self
                      sig do
                        params(
                          static: OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static::OrHash,
                          type: Symbol
                        ).returns(T.attached_class)
                      end
                      def new(
                        static:,
                        type: :static # Always `static`.
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static,
                          OpenAI::Internal::AnyHash
                        )
                      end

                    class Static < OpenAI::Internal::Type::BaseModel
                      # The number of tokens that overlap between chunks. The default value is `400`.
                      #
                      # Note that the overlap must not exceed half of `max_chunk_size_tokens`.
                      sig { returns(Integer) }
                      attr_accessor :chunk_overlap_tokens

                      # The maximum number of tokens in each chunk. The default value is `800`. The
                      # minimum value is `100` and the maximum value is `4096`.
                      sig { returns(Integer) }
                      attr_accessor :max_chunk_size_tokens

                      sig do
                        override
                          .returns({
                            chunk_overlap_tokens: Integer,
                            max_chunk_size_tokens: Integer
                          })
                      end
                      def to_hash; end

                      class << self
                        sig do
                          params(
                            chunk_overlap_tokens: Integer,
                            max_chunk_size_tokens: Integer
                          ).returns(T.attached_class)
                        end
                        def new(
                          chunk_overlap_tokens:, # The number of tokens that overlap between chunks. The default value is `400`.
                                                 # Note that the overlap must not exceed half of `max_chunk_size_tokens`.
                          max_chunk_size_tokens: # The maximum number of tokens in each chunk. The default value is `800`. The
                                                 # minimum value is `100` and the maximum value is `4096`.
); end
                      end

                      OrHash = T.type_alias do
                          T.any(
                            OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static,
                            OpenAI::Internal::AnyHash
                          )
                        end
                    end
                  end

                  Variants = T.type_alias do
                      T.any(
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                        OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static
                      )
                    end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources::FileSearch::VectorStore,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::ThreadCreateAndRunParams::Thread::ToolResources,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        class ToolResources < OpenAI::Internal::Type::BaseModel
          sig do
            returns(T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::CodeInterpreter
              ))
          end
          attr_reader :code_interpreter

          sig do
            params(
              code_interpreter: OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::CodeInterpreter::OrHash
            ).void
          end
          attr_writer :code_interpreter

          sig do
            returns(T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::FileSearch
              ))
          end
          attr_reader :file_search

          sig { params(file_search: OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::FileSearch::OrHash).void }
          attr_writer :file_search

          sig do
            override
              .returns({
                code_interpreter:
                  OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::CodeInterpreter,
                file_search:
                  OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::FileSearch
              })
          end
          def to_hash; end

          class << self
            # A set of resources that are used by the assistant's tools. The resources are
            # specific to the type of tool. For example, the `code_interpreter` tool requires
            # a list of file IDs, while the `file_search` tool requires a list of vector store
            # IDs.
            sig do
              params(
                code_interpreter: OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::CodeInterpreter::OrHash,
                file_search: OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::FileSearch::OrHash
              ).returns(T.attached_class)
            end
            def new(code_interpreter: nil, file_search: nil); end
          end

          class CodeInterpreter < OpenAI::Internal::Type::BaseModel
            # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
            # available to the `code_interpreter` tool. There can be a maximum of 20 files
            # associated with the tool.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :file_ids

            sig { params(file_ids: T::Array[String]).void }
            attr_writer :file_ids

            sig { override.returns({ file_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(file_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                file_ids: nil # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
                              # available to the `code_interpreter` tool. There can be a maximum of 20 files
                              # associated with the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::CodeInterpreter,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileSearch < OpenAI::Internal::Type::BaseModel
            # The ID of the
            # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
            # attached to this assistant. There can be a maximum of 1 vector store attached to
            # the assistant.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :vector_store_ids

            sig { params(vector_store_ids: T::Array[String]).void }
            attr_writer :vector_store_ids

            sig { override.returns({ vector_store_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(vector_store_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                vector_store_ids: nil # The ID of the
                                      # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                      # attached to this assistant. There can be a maximum of 1 vector store attached to
                                      # the assistant.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::FileSearch,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::ThreadCreateAndRunParams::ToolResources,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class TruncationStrategy < OpenAI::Internal::Type::BaseModel
          # The number of most recent messages from the thread when constructing the context
          # for the run.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :last_messages

          # The truncation strategy to use for the thread. The default is `auto`. If set to
          # `last_messages`, the thread will be truncated to the n most recent messages in
          # the thread. When set to `auto`, messages in the middle of the thread will be
          # dropped to fit the context length of the model, `max_prompt_tokens`.
          sig { returns(OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::Type::OrSymbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                type:
                  OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::Type::OrSymbol,
                last_messages: T.nilable(Integer)
              })
          end
          def to_hash; end

          class << self
            # Controls for how a thread will be truncated prior to the run. Use this to
            # control the intial context window of the run.
            sig do
              params(
                type: OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::Type::OrSymbol,
                last_messages: T.nilable(Integer)
              ).returns(T.attached_class)
            end
            def new(
              type:, # The truncation strategy to use for the thread. The default is `auto`. If set to
                     # `last_messages`, the thread will be truncated to the n most recent messages in
                     # the thread. When set to `auto`, messages in the middle of the thread will be
                     # dropped to fit the context length of the model, `max_prompt_tokens`.
              last_messages: nil # The number of most recent messages from the thread when constructing the context
                                 # for the run.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy,
                OpenAI::Internal::AnyHash
              )
            end

          # The truncation strategy to use for the thread. The default is `auto`. If set to
          # `last_messages`, the thread will be truncated to the n most recent messages in
          # the thread. When set to `auto`, messages in the middle of the thread will be
          # dropped to fit the context length of the model, `max_prompt_tokens`.
          module Type
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::Type::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::Type::TaggedSymbol
              )

            LAST_MESSAGES = T.let(
                :last_messages,
                OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::Type::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::Type
                )
              end
          end
        end
      end

      class ThreadCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # A list of [messages](https://platform.openai.com/docs/api-reference/messages) to
        # start the thread with.
        sig { returns(T.nilable(T::Array[OpenAI::Beta::ThreadCreateParams::Message])) }
        attr_reader :messages

        sig { params(messages: T::Array[OpenAI::Beta::ThreadCreateParams::Message::OrHash]).void }
        attr_writer :messages

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # A set of resources that are made available to the assistant's tools in this
        # thread. The resources are specific to the type of tool. For example, the
        # `code_interpreter` tool requires a list of file IDs, while the `file_search`
        # tool requires a list of vector store IDs.
        sig { returns(T.nilable(OpenAI::Beta::ThreadCreateParams::ToolResources)) }
        attr_reader :tool_resources

        sig { params(tool_resources: T.nilable(OpenAI::Beta::ThreadCreateParams::ToolResources::OrHash)).void }
        attr_writer :tool_resources

        sig do
          override
            .returns({
              messages: T::Array[OpenAI::Beta::ThreadCreateParams::Message],
              metadata: T.nilable(T::Hash[Symbol, String]),
              tool_resources:
                T.nilable(OpenAI::Beta::ThreadCreateParams::ToolResources),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              messages: T::Array[OpenAI::Beta::ThreadCreateParams::Message::OrHash],
              metadata: T.nilable(T::Hash[Symbol, String]),
              tool_resources: T.nilable(
                OpenAI::Beta::ThreadCreateParams::ToolResources::OrHash
              ),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            messages: nil, # A list of [messages](https://platform.openai.com/docs/api-reference/messages) to
                           # start the thread with.
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            tool_resources: nil, # A set of resources that are made available to the assistant's tools in this
                                 # thread. The resources are specific to the type of tool. For example, the
                                 # `code_interpreter` tool requires a list of file IDs, while the `file_search`
                                 # tool requires a list of vector store IDs.
            request_options: {}
); end
        end

        class Message < OpenAI::Internal::Type::BaseModel
          # A list of files attached to the message, and the tools they should be added to.
          sig do
            returns(T.nilable(
                T::Array[OpenAI::Beta::ThreadCreateParams::Message::Attachment]
              ))
          end
          attr_accessor :attachments

          # The text contents of the message.
          sig { returns(OpenAI::Beta::ThreadCreateParams::Message::Content::Variants) }
          attr_accessor :content

          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # The role of the entity that is creating the message. Allowed values include:
          #
          # - `user`: Indicates the message is sent by an actual user and should be used in
          #   most cases to represent user-generated messages.
          # - `assistant`: Indicates the message is generated by the assistant. Use this
          #   value to insert messages from the assistant into the conversation.
          sig { returns(OpenAI::Beta::ThreadCreateParams::Message::Role::OrSymbol) }
          attr_accessor :role

          sig do
            override
              .returns({
                content:
                  OpenAI::Beta::ThreadCreateParams::Message::Content::Variants,
                role: OpenAI::Beta::ThreadCreateParams::Message::Role::OrSymbol,
                attachments:
                  T.nilable(
                    T::Array[
                      OpenAI::Beta::ThreadCreateParams::Message::Attachment
                    ]
                  ),
                metadata: T.nilable(T::Hash[Symbol, String])
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                content: OpenAI::Beta::ThreadCreateParams::Message::Content::Variants,
                role: OpenAI::Beta::ThreadCreateParams::Message::Role::OrSymbol,
                attachments: T.nilable(
                  T::Array[
                    OpenAI::Beta::ThreadCreateParams::Message::Attachment::OrHash
                  ]
                ),
                metadata: T.nilable(T::Hash[Symbol, String])
              ).returns(T.attached_class)
            end
            def new(
              content:, # The text contents of the message.
              role:, # The role of the entity that is creating the message. Allowed values include:
                     # - `user`: Indicates the message is sent by an actual user and should be used in
                     #   most cases to represent user-generated messages.
                     # - `assistant`: Indicates the message is generated by the assistant. Use this
                     #   value to insert messages from the assistant into the conversation.
              attachments: nil, # A list of files attached to the message, and the tools they should be added to.
              metadata: nil # Set of 16 key-value pairs that can be attached to an object. This can be useful
                            # for storing additional information about the object in a structured format, and
                            # querying for objects via API or the dashboard.
                            # Keys are strings with a maximum length of 64 characters. Values are strings with
                            # a maximum length of 512 characters.
); end
          end

          class Attachment < OpenAI::Internal::Type::BaseModel
            # The ID of the file to attach to the message.
            sig { returns(T.nilable(String)) }
            attr_reader :file_id

            sig { params(file_id: String).void }
            attr_writer :file_id

            # The tools to add this file to.
            sig do
              returns(T.nilable(
                  T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool,
                      OpenAI::Beta::ThreadCreateParams::Message::Attachment::Tool::FileSearch
                    )
                  ]
                ))
            end
            attr_reader :tools

            sig do
              params(
                tools: T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool::OrHash,
                      OpenAI::Beta::ThreadCreateParams::Message::Attachment::Tool::FileSearch::OrHash
                    )
                  ]
              ).void
            end
            attr_writer :tools

            sig do
              override
                .returns({
                  file_id: String,
                  tools:
                    T::Array[
                      T.any(
                        OpenAI::Beta::CodeInterpreterTool,
                        OpenAI::Beta::ThreadCreateParams::Message::Attachment::Tool::FileSearch
                      )
                    ]
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  file_id: String,
                  tools: T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool::OrHash,
                      OpenAI::Beta::ThreadCreateParams::Message::Attachment::Tool::FileSearch::OrHash
                    )
                  ]
                ).returns(T.attached_class)
              end
              def new(
                file_id: nil, # The ID of the file to attach to the message.
                tools: nil # The tools to add this file to.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::ThreadCreateParams::Message::Attachment,
                  OpenAI::Internal::AnyHash
                )
              end

            module Tool
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::ThreadCreateParams::Message::Attachment::Tool::Variants
                  ])
                end
                def variants; end
              end

              class FileSearch < OpenAI::Internal::Type::BaseModel
                # The type of tool being defined: `file_search`
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(type: Symbol).returns(T.attached_class) }
                  def new(
                    type: :file_search # The type of tool being defined: `file_search`
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Beta::ThreadCreateParams::Message::Attachment::Tool::FileSearch,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool,
                    OpenAI::Beta::ThreadCreateParams::Message::Attachment::Tool::FileSearch
                  )
                end
            end
          end

          # The text contents of the message.
          module Content
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::ThreadCreateParams::Message::Content::Variants
                ])
              end
              def variants; end
            end

            MessageContentPartParamArray = T.let(
                OpenAI::Internal::Type::ArrayOf[
                  union: OpenAI::Beta::Threads::MessageContentPartParam
                ],
                OpenAI::Internal::Type::Converter
              )

            Variants = T.type_alias do
                T.any(
                  String,
                  T::Array[
                    OpenAI::Beta::Threads::MessageContentPartParam::Variants
                  ]
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::ThreadCreateParams::Message,
                OpenAI::Internal::AnyHash
              )
            end

          # The role of the entity that is creating the message. Allowed values include:
          #
          # - `user`: Indicates the message is sent by an actual user and should be used in
          #   most cases to represent user-generated messages.
          # - `assistant`: Indicates the message is generated by the assistant. Use this
          #   value to insert messages from the assistant into the conversation.
          module Role
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::ThreadCreateParams::Message::Role::TaggedSymbol
                ])
              end
              def values; end
            end

            ASSISTANT = T.let(
                :assistant,
                OpenAI::Beta::ThreadCreateParams::Message::Role::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::ThreadCreateParams::Message::Role)
              end

            USER = T.let(
                :user,
                OpenAI::Beta::ThreadCreateParams::Message::Role::TaggedSymbol
              )
          end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::ThreadCreateParams, OpenAI::Internal::AnyHash)
          end

        class ToolResources < OpenAI::Internal::Type::BaseModel
          sig do
            returns(T.nilable(
                OpenAI::Beta::ThreadCreateParams::ToolResources::CodeInterpreter
              ))
          end
          attr_reader :code_interpreter

          sig do
            params(
              code_interpreter: OpenAI::Beta::ThreadCreateParams::ToolResources::CodeInterpreter::OrHash
            ).void
          end
          attr_writer :code_interpreter

          sig do
            returns(T.nilable(
                OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch
              ))
          end
          attr_reader :file_search

          sig { params(file_search: OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::OrHash).void }
          attr_writer :file_search

          sig do
            override
              .returns({
                code_interpreter:
                  OpenAI::Beta::ThreadCreateParams::ToolResources::CodeInterpreter,
                file_search:
                  OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch
              })
          end
          def to_hash; end

          class << self
            # A set of resources that are made available to the assistant's tools in this
            # thread. The resources are specific to the type of tool. For example, the
            # `code_interpreter` tool requires a list of file IDs, while the `file_search`
            # tool requires a list of vector store IDs.
            sig do
              params(
                code_interpreter: OpenAI::Beta::ThreadCreateParams::ToolResources::CodeInterpreter::OrHash,
                file_search: OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::OrHash
              ).returns(T.attached_class)
            end
            def new(code_interpreter: nil, file_search: nil); end
          end

          class CodeInterpreter < OpenAI::Internal::Type::BaseModel
            # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
            # available to the `code_interpreter` tool. There can be a maximum of 20 files
            # associated with the tool.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :file_ids

            sig { params(file_ids: T::Array[String]).void }
            attr_writer :file_ids

            sig { override.returns({ file_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(file_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                file_ids: nil # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
                              # available to the `code_interpreter` tool. There can be a maximum of 20 files
                              # associated with the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::ThreadCreateParams::ToolResources::CodeInterpreter,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileSearch < OpenAI::Internal::Type::BaseModel
            # The
            # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
            # attached to this thread. There can be a maximum of 1 vector store attached to
            # the thread.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :vector_store_ids

            sig { params(vector_store_ids: T::Array[String]).void }
            attr_writer :vector_store_ids

            # A helper to create a
            # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
            # with file_ids and attach it to this thread. There can be a maximum of 1 vector
            # store attached to the thread.
            sig do
              returns(T.nilable(
                  T::Array[
                    OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore
                  ]
                ))
            end
            attr_reader :vector_stores

            sig do
              params(
                vector_stores: T::Array[
                    OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::OrHash
                  ]
              ).void
            end
            attr_writer :vector_stores

            sig do
              override
                .returns({
                  vector_store_ids: T::Array[String],
                  vector_stores:
                    T::Array[
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore
                    ]
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  vector_store_ids: T::Array[String],
                  vector_stores: T::Array[
                    OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::OrHash
                  ]
                ).returns(T.attached_class)
              end
              def new(
                vector_store_ids: nil, # The
                                       # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                       # attached to this thread. There can be a maximum of 1 vector store attached to
                                       # the thread.
                vector_stores: nil # A helper to create a
                                   # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                   # with file_ids and attach it to this thread. There can be a maximum of 1 vector
                                   # store attached to the thread.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch,
                  OpenAI::Internal::AnyHash
                )
              end

            class VectorStore < OpenAI::Internal::Type::BaseModel
              # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
              # strategy.
              sig do
                returns(T.nilable(
                    T.any(
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static
                    )
                  ))
              end
              attr_reader :chunking_strategy

              sig do
                params(
                  chunking_strategy: T.any(
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto::OrHash,
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::OrHash
                    )
                ).void
              end
              attr_writer :chunking_strategy

              # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to
              # add to the vector store. There can be a maximum of 10000 files in a vector
              # store.
              sig { returns(T.nilable(T::Array[String])) }
              attr_reader :file_ids

              sig { params(file_ids: T::Array[String]).void }
              attr_writer :file_ids

              # Set of 16 key-value pairs that can be attached to an object. This can be useful
              # for storing additional information about the object in a structured format, and
              # querying for objects via API or the dashboard.
              #
              # Keys are strings with a maximum length of 64 characters. Values are strings with
              # a maximum length of 512 characters.
              sig { returns(T.nilable(T::Hash[Symbol, String])) }
              attr_accessor :metadata

              sig do
                override
                  .returns({
                    chunking_strategy:
                      T.any(
                        OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                        OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static
                      ),
                    file_ids: T::Array[String],
                    metadata: T.nilable(T::Hash[Symbol, String])
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    chunking_strategy: T.any(
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto::OrHash,
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::OrHash
                    ),
                    file_ids: T::Array[String],
                    metadata: T.nilable(T::Hash[Symbol, String])
                  ).returns(T.attached_class)
                end
                def new(
                  chunking_strategy: nil, # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                                          # strategy.
                  file_ids: nil, # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs to
                                 # add to the vector store. There can be a maximum of 10000 files in a vector
                                 # store.
                  metadata: nil # Set of 16 key-value pairs that can be attached to an object. This can be useful
                                # for storing additional information about the object in a structured format, and
                                # querying for objects via API or the dashboard.
                                # Keys are strings with a maximum length of 64 characters. Values are strings with
                                # a maximum length of 512 characters.
); end
              end

              # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
              # strategy.
              module ChunkingStrategy
                extend OpenAI::Internal::Type::Union

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Variants
                    ])
                  end
                  def variants; end
                end

                class Auto < OpenAI::Internal::Type::BaseModel
                  # Always `auto`.
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig { override.returns({ type: Symbol }) }
                  def to_hash; end

                  class << self
                    # The default strategy. This strategy currently uses a `max_chunk_size_tokens` of
                    # `800` and `chunk_overlap_tokens` of `400`.
                    sig { params(type: Symbol).returns(T.attached_class) }
                    def new(
                      type: :auto # Always `auto`.
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                class Static < OpenAI::Internal::Type::BaseModel
                  sig do
                    returns(OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static)
                  end
                  attr_reader :static

                  sig do
                    params(
                      static: OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static::OrHash
                    ).void
                  end
                  attr_writer :static

                  # Always `static`.
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig do
                    override
                      .returns({
                        static:
                          OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static,
                        type: Symbol
                      })
                  end
                  def to_hash; end

                  class << self
                    sig do
                      params(
                        static: OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static::OrHash,
                        type: Symbol
                      ).returns(T.attached_class)
                    end
                    def new(
                      static:,
                      type: :static # Always `static`.
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static,
                        OpenAI::Internal::AnyHash
                      )
                    end

                  class Static < OpenAI::Internal::Type::BaseModel
                    # The number of tokens that overlap between chunks. The default value is `400`.
                    #
                    # Note that the overlap must not exceed half of `max_chunk_size_tokens`.
                    sig { returns(Integer) }
                    attr_accessor :chunk_overlap_tokens

                    # The maximum number of tokens in each chunk. The default value is `800`. The
                    # minimum value is `100` and the maximum value is `4096`.
                    sig { returns(Integer) }
                    attr_accessor :max_chunk_size_tokens

                    sig do
                      override
                        .returns({
                          chunk_overlap_tokens: Integer,
                          max_chunk_size_tokens: Integer
                        })
                    end
                    def to_hash; end

                    class << self
                      sig do
                        params(
                          chunk_overlap_tokens: Integer,
                          max_chunk_size_tokens: Integer
                        ).returns(T.attached_class)
                      end
                      def new(
                        chunk_overlap_tokens:, # The number of tokens that overlap between chunks. The default value is `400`.
                                               # Note that the overlap must not exceed half of `max_chunk_size_tokens`.
                        max_chunk_size_tokens: # The maximum number of tokens in each chunk. The default value is `800`. The
                                               # minimum value is `100` and the maximum value is `4096`.
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static::Static,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end
                end

                Variants = T.type_alias do
                    T.any(
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Auto,
                      OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore::ChunkingStrategy::Static
                    )
                  end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::ThreadCreateParams::ToolResources::FileSearch::VectorStore,
                    OpenAI::Internal::AnyHash
                  )
                end
            end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::ThreadCreateParams::ToolResources,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class ThreadDeleteParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::ThreadDeleteParams, OpenAI::Internal::AnyHash)
          end
      end

      class ThreadDeleted < OpenAI::Internal::Type::BaseModel
        sig { returns(T::Boolean) }
        attr_accessor :deleted

        sig { returns(String) }
        attr_accessor :id

        sig { returns(Symbol) }
        attr_accessor :object

        sig { override.returns({ id: String, deleted: T::Boolean, object: Symbol }) }
        def to_hash; end

        class << self
          sig { params(id: String, deleted: T::Boolean, object: Symbol).returns(T.attached_class) }
          def new(id:, deleted:, object: :"thread.deleted"); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::ThreadDeleted, OpenAI::Internal::AnyHash)
          end
      end

      class ThreadRetrieveParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::ThreadRetrieveParams, OpenAI::Internal::AnyHash)
          end
      end

      class ThreadStreamEvent < OpenAI::Internal::Type::BaseModel
        # Represents a thread that contains
        # [messages](https://platform.openai.com/docs/api-reference/messages).
        sig { returns(OpenAI::Beta::Thread) }
        attr_reader :data

        sig { params(data: OpenAI::Beta::Thread::OrHash).void }
        attr_writer :data

        # Whether to enable input audio transcription.
        sig { returns(T.nilable(T::Boolean)) }
        attr_reader :enabled

        sig { params(enabled: T::Boolean).void }
        attr_writer :enabled

        sig { returns(Symbol) }
        attr_accessor :event

        sig { override.returns({ data: OpenAI::Beta::Thread, event: Symbol, enabled: T::Boolean }) }
        def to_hash; end

        class << self
          # Occurs when a new
          # [thread](https://platform.openai.com/docs/api-reference/threads/object) is
          # created.
          sig do
            params(
              data: OpenAI::Beta::Thread::OrHash,
              enabled: T::Boolean,
              event: Symbol
            ).returns(T.attached_class)
          end
          def new(
            data:, # Represents a thread that contains
                   # [messages](https://platform.openai.com/docs/api-reference/messages).
            enabled: nil, # Whether to enable input audio transcription.
            event: :"thread.created"
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::ThreadStreamEvent, OpenAI::Internal::AnyHash)
          end
      end

      class ThreadUpdateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # A set of resources that are made available to the assistant's tools in this
        # thread. The resources are specific to the type of tool. For example, the
        # `code_interpreter` tool requires a list of file IDs, while the `file_search`
        # tool requires a list of vector store IDs.
        sig { returns(T.nilable(OpenAI::Beta::ThreadUpdateParams::ToolResources)) }
        attr_reader :tool_resources

        sig { params(tool_resources: T.nilable(OpenAI::Beta::ThreadUpdateParams::ToolResources::OrHash)).void }
        attr_writer :tool_resources

        sig do
          override
            .returns({
              metadata: T.nilable(T::Hash[Symbol, String]),
              tool_resources:
                T.nilable(OpenAI::Beta::ThreadUpdateParams::ToolResources),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              metadata: T.nilable(T::Hash[Symbol, String]),
              tool_resources: T.nilable(
                OpenAI::Beta::ThreadUpdateParams::ToolResources::OrHash
              ),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            tool_resources: nil, # A set of resources that are made available to the assistant's tools in this
                                 # thread. The resources are specific to the type of tool. For example, the
                                 # `code_interpreter` tool requires a list of file IDs, while the `file_search`
                                 # tool requires a list of vector store IDs.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Beta::ThreadUpdateParams, OpenAI::Internal::AnyHash)
          end

        class ToolResources < OpenAI::Internal::Type::BaseModel
          sig do
            returns(T.nilable(
                OpenAI::Beta::ThreadUpdateParams::ToolResources::CodeInterpreter
              ))
          end
          attr_reader :code_interpreter

          sig do
            params(
              code_interpreter: OpenAI::Beta::ThreadUpdateParams::ToolResources::CodeInterpreter::OrHash
            ).void
          end
          attr_writer :code_interpreter

          sig do
            returns(T.nilable(
                OpenAI::Beta::ThreadUpdateParams::ToolResources::FileSearch
              ))
          end
          attr_reader :file_search

          sig { params(file_search: OpenAI::Beta::ThreadUpdateParams::ToolResources::FileSearch::OrHash).void }
          attr_writer :file_search

          sig do
            override
              .returns({
                code_interpreter:
                  OpenAI::Beta::ThreadUpdateParams::ToolResources::CodeInterpreter,
                file_search:
                  OpenAI::Beta::ThreadUpdateParams::ToolResources::FileSearch
              })
          end
          def to_hash; end

          class << self
            # A set of resources that are made available to the assistant's tools in this
            # thread. The resources are specific to the type of tool. For example, the
            # `code_interpreter` tool requires a list of file IDs, while the `file_search`
            # tool requires a list of vector store IDs.
            sig do
              params(
                code_interpreter: OpenAI::Beta::ThreadUpdateParams::ToolResources::CodeInterpreter::OrHash,
                file_search: OpenAI::Beta::ThreadUpdateParams::ToolResources::FileSearch::OrHash
              ).returns(T.attached_class)
            end
            def new(code_interpreter: nil, file_search: nil); end
          end

          class CodeInterpreter < OpenAI::Internal::Type::BaseModel
            # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
            # available to the `code_interpreter` tool. There can be a maximum of 20 files
            # associated with the tool.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :file_ids

            sig { params(file_ids: T::Array[String]).void }
            attr_writer :file_ids

            sig { override.returns({ file_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(file_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                file_ids: nil # A list of [file](https://platform.openai.com/docs/api-reference/files) IDs made
                              # available to the `code_interpreter` tool. There can be a maximum of 20 files
                              # associated with the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::ThreadUpdateParams::ToolResources::CodeInterpreter,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileSearch < OpenAI::Internal::Type::BaseModel
            # The
            # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
            # attached to this thread. There can be a maximum of 1 vector store attached to
            # the thread.
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :vector_store_ids

            sig { params(vector_store_ids: T::Array[String]).void }
            attr_writer :vector_store_ids

            sig { override.returns({ vector_store_ids: T::Array[String] }) }
            def to_hash; end

            class << self
              sig { params(vector_store_ids: T::Array[String]).returns(T.attached_class) }
              def new(
                vector_store_ids: nil # The
                                      # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                                      # attached to this thread. There can be a maximum of 1 vector store attached to
                                      # the thread.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::ThreadUpdateParams::ToolResources::FileSearch,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::ThreadUpdateParams::ToolResources,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      module Threads
        # A citation within the message that points to a specific quote from a specific
        # File associated with the assistant or the message. Generated when the assistant
        # uses the "file_search" tool to search files.
        module Annotation
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::Threads::Annotation::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::FileCitationAnnotation,
                OpenAI::Beta::Threads::FilePathAnnotation
              )
            end
        end

        # A citation within the message that points to a specific quote from a specific
        # File associated with the assistant or the message. Generated when the assistant
        # uses the "file_search" tool to search files.
        module AnnotationDelta
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::Threads::AnnotationDelta::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::FileCitationDeltaAnnotation,
                OpenAI::Beta::Threads::FilePathDeltaAnnotation
              )
            end
        end

        class FileCitationAnnotation < OpenAI::Internal::Type::BaseModel
          sig { returns(Integer) }
          attr_accessor :end_index

          sig { returns(OpenAI::Beta::Threads::FileCitationAnnotation::FileCitation) }
          attr_reader :file_citation

          sig { params(file_citation: OpenAI::Beta::Threads::FileCitationAnnotation::FileCitation::OrHash).void }
          attr_writer :file_citation

          sig { returns(Integer) }
          attr_accessor :start_index

          # The text in the message content that needs to be replaced.
          sig { returns(String) }
          attr_accessor :text

          # Always `file_citation`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                end_index: Integer,
                file_citation:
                  OpenAI::Beta::Threads::FileCitationAnnotation::FileCitation,
                start_index: Integer,
                text: String,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A citation within the message that points to a specific quote from a specific
            # File associated with the assistant or the message. Generated when the assistant
            # uses the "file_search" tool to search files.
            sig do
              params(
                end_index: Integer,
                file_citation: OpenAI::Beta::Threads::FileCitationAnnotation::FileCitation::OrHash,
                start_index: Integer,
                text: String,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              end_index:,
              file_citation:,
              start_index:,
              text:, # The text in the message content that needs to be replaced.
              type: :file_citation # Always `file_citation`.
); end
          end

          class FileCitation < OpenAI::Internal::Type::BaseModel
            # The ID of the specific File the citation is from.
            sig { returns(String) }
            attr_accessor :file_id

            sig { override.returns({ file_id: String }) }
            def to_hash; end

            class << self
              sig { params(file_id: String).returns(T.attached_class) }
              def new(
                file_id: # The ID of the specific File the citation is from.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::FileCitationAnnotation::FileCitation,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::FileCitationAnnotation,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class FileCitationDeltaAnnotation < OpenAI::Internal::Type::BaseModel
          sig { returns(T.nilable(Integer)) }
          attr_reader :end_index

          sig { params(end_index: Integer).void }
          attr_writer :end_index

          sig do
            returns(T.nilable(
                OpenAI::Beta::Threads::FileCitationDeltaAnnotation::FileCitation
              ))
          end
          attr_reader :file_citation

          sig { params(file_citation: OpenAI::Beta::Threads::FileCitationDeltaAnnotation::FileCitation::OrHash).void }
          attr_writer :file_citation

          # The index of the annotation in the text content part.
          sig { returns(Integer) }
          attr_accessor :index

          sig { returns(T.nilable(Integer)) }
          attr_reader :start_index

          sig { params(start_index: Integer).void }
          attr_writer :start_index

          # The text in the message content that needs to be replaced.
          sig { returns(T.nilable(String)) }
          attr_reader :text

          sig { params(text: String).void }
          attr_writer :text

          # Always `file_citation`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                index: Integer,
                type: Symbol,
                end_index: Integer,
                file_citation:
                  OpenAI::Beta::Threads::FileCitationDeltaAnnotation::FileCitation,
                start_index: Integer,
                text: String
              })
          end
          def to_hash; end

          class << self
            # A citation within the message that points to a specific quote from a specific
            # File associated with the assistant or the message. Generated when the assistant
            # uses the "file_search" tool to search files.
            sig do
              params(
                index: Integer,
                end_index: Integer,
                file_citation: OpenAI::Beta::Threads::FileCitationDeltaAnnotation::FileCitation::OrHash,
                start_index: Integer,
                text: String,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              index:, # The index of the annotation in the text content part.
              end_index: nil,
              file_citation: nil,
              start_index: nil,
              text: nil, # The text in the message content that needs to be replaced.
              type: :file_citation # Always `file_citation`.
); end
          end

          class FileCitation < OpenAI::Internal::Type::BaseModel
            # The ID of the specific File the citation is from.
            sig { returns(T.nilable(String)) }
            attr_reader :file_id

            sig { params(file_id: String).void }
            attr_writer :file_id

            # The specific quote in the file.
            sig { returns(T.nilable(String)) }
            attr_reader :quote

            sig { params(quote: String).void }
            attr_writer :quote

            sig { override.returns({ file_id: String, quote: String }) }
            def to_hash; end

            class << self
              sig { params(file_id: String, quote: String).returns(T.attached_class) }
              def new(
                file_id: nil, # The ID of the specific File the citation is from.
                quote: nil # The specific quote in the file.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::FileCitationDeltaAnnotation::FileCitation,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::FileCitationDeltaAnnotation,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class FilePathAnnotation < OpenAI::Internal::Type::BaseModel
          sig { returns(Integer) }
          attr_accessor :end_index

          sig { returns(OpenAI::Beta::Threads::FilePathAnnotation::FilePath) }
          attr_reader :file_path

          sig { params(file_path: OpenAI::Beta::Threads::FilePathAnnotation::FilePath::OrHash).void }
          attr_writer :file_path

          sig { returns(Integer) }
          attr_accessor :start_index

          # The text in the message content that needs to be replaced.
          sig { returns(String) }
          attr_accessor :text

          # Always `file_path`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                end_index: Integer,
                file_path: OpenAI::Beta::Threads::FilePathAnnotation::FilePath,
                start_index: Integer,
                text: String,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A URL for the file that's generated when the assistant used the
            # `code_interpreter` tool to generate a file.
            sig do
              params(
                end_index: Integer,
                file_path: OpenAI::Beta::Threads::FilePathAnnotation::FilePath::OrHash,
                start_index: Integer,
                text: String,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              end_index:,
              file_path:,
              start_index:,
              text:, # The text in the message content that needs to be replaced.
              type: :file_path # Always `file_path`.
); end
          end

          class FilePath < OpenAI::Internal::Type::BaseModel
            # The ID of the file that was generated.
            sig { returns(String) }
            attr_accessor :file_id

            sig { override.returns({ file_id: String }) }
            def to_hash; end

            class << self
              sig { params(file_id: String).returns(T.attached_class) }
              def new(
                file_id: # The ID of the file that was generated.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::FilePathAnnotation::FilePath,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::FilePathAnnotation,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class FilePathDeltaAnnotation < OpenAI::Internal::Type::BaseModel
          sig { returns(T.nilable(Integer)) }
          attr_reader :end_index

          sig { params(end_index: Integer).void }
          attr_writer :end_index

          sig do
            returns(T.nilable(
                OpenAI::Beta::Threads::FilePathDeltaAnnotation::FilePath
              ))
          end
          attr_reader :file_path

          sig { params(file_path: OpenAI::Beta::Threads::FilePathDeltaAnnotation::FilePath::OrHash).void }
          attr_writer :file_path

          # The index of the annotation in the text content part.
          sig { returns(Integer) }
          attr_accessor :index

          sig { returns(T.nilable(Integer)) }
          attr_reader :start_index

          sig { params(start_index: Integer).void }
          attr_writer :start_index

          # The text in the message content that needs to be replaced.
          sig { returns(T.nilable(String)) }
          attr_reader :text

          sig { params(text: String).void }
          attr_writer :text

          # Always `file_path`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                index: Integer,
                type: Symbol,
                end_index: Integer,
                file_path:
                  OpenAI::Beta::Threads::FilePathDeltaAnnotation::FilePath,
                start_index: Integer,
                text: String
              })
          end
          def to_hash; end

          class << self
            # A URL for the file that's generated when the assistant used the
            # `code_interpreter` tool to generate a file.
            sig do
              params(
                index: Integer,
                end_index: Integer,
                file_path: OpenAI::Beta::Threads::FilePathDeltaAnnotation::FilePath::OrHash,
                start_index: Integer,
                text: String,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              index:, # The index of the annotation in the text content part.
              end_index: nil,
              file_path: nil,
              start_index: nil,
              text: nil, # The text in the message content that needs to be replaced.
              type: :file_path # Always `file_path`.
); end
          end

          class FilePath < OpenAI::Internal::Type::BaseModel
            # The ID of the file that was generated.
            sig { returns(T.nilable(String)) }
            attr_reader :file_id

            sig { params(file_id: String).void }
            attr_writer :file_id

            sig { override.returns({ file_id: String }) }
            def to_hash; end

            class << self
              sig { params(file_id: String).returns(T.attached_class) }
              def new(
                file_id: nil # The ID of the file that was generated.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::FilePathDeltaAnnotation::FilePath,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::FilePathDeltaAnnotation,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ImageFile < OpenAI::Internal::Type::BaseModel
          # Specifies the detail level of the image if specified by the user. `low` uses
          # fewer tokens, you can opt in to high resolution using `high`.
          sig { returns(T.nilable(OpenAI::Beta::Threads::ImageFile::Detail::OrSymbol)) }
          attr_reader :detail

          sig { params(detail: OpenAI::Beta::Threads::ImageFile::Detail::OrSymbol).void }
          attr_writer :detail

          # The [File](https://platform.openai.com/docs/api-reference/files) ID of the image
          # in the message content. Set `purpose="vision"` when uploading the File if you
          # need to later display the file content.
          sig { returns(String) }
          attr_accessor :file_id

          sig do
            override
              .returns({
                file_id: String,
                detail: OpenAI::Beta::Threads::ImageFile::Detail::OrSymbol
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                file_id: String,
                detail: OpenAI::Beta::Threads::ImageFile::Detail::OrSymbol
              ).returns(T.attached_class)
            end
            def new(
              file_id:, # The [File](https://platform.openai.com/docs/api-reference/files) ID of the image
                        # in the message content. Set `purpose="vision"` when uploading the File if you
                        # need to later display the file content.
              detail: nil # Specifies the detail level of the image if specified by the user. `low` uses
                          # fewer tokens, you can opt in to high resolution using `high`.
); end
          end

          # Specifies the detail level of the image if specified by the user. `low` uses
          # fewer tokens, you can opt in to high resolution using `high`.
          module Detail
            extend OpenAI::Internal::Type::Enum

            class << self
              sig { override.returns(T::Array[OpenAI::Beta::Threads::ImageFile::Detail::TaggedSymbol]) }
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Beta::Threads::ImageFile::Detail::TaggedSymbol
              )

            HIGH = T.let(
                :high,
                OpenAI::Beta::Threads::ImageFile::Detail::TaggedSymbol
              )

            LOW = T.let(
                :low,
                OpenAI::Beta::Threads::ImageFile::Detail::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::ImageFile::Detail)
              end
          end

          OrHash = T.type_alias do
              T.any(OpenAI::Beta::Threads::ImageFile, OpenAI::Internal::AnyHash)
            end
        end

        class ImageFileContentBlock < OpenAI::Internal::Type::BaseModel
          sig { returns(OpenAI::Beta::Threads::ImageFile) }
          attr_reader :image_file

          sig { params(image_file: OpenAI::Beta::Threads::ImageFile::OrHash).void }
          attr_writer :image_file

          # Always `image_file`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ image_file: OpenAI::Beta::Threads::ImageFile, type: Symbol }) }
          def to_hash; end

          class << self
            # References an image [File](https://platform.openai.com/docs/api-reference/files)
            # in the content of a message.
            sig do
              params(
                image_file: OpenAI::Beta::Threads::ImageFile::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              image_file:,
              type: :image_file # Always `image_file`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::ImageFileContentBlock,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ImageFileDelta < OpenAI::Internal::Type::BaseModel
          # Specifies the detail level of the image if specified by the user. `low` uses
          # fewer tokens, you can opt in to high resolution using `high`.
          sig do
            returns(T.nilable(
                OpenAI::Beta::Threads::ImageFileDelta::Detail::TaggedSymbol
              ))
          end
          attr_reader :detail

          sig { params(detail: OpenAI::Beta::Threads::ImageFileDelta::Detail::OrSymbol).void }
          attr_writer :detail

          # The [File](https://platform.openai.com/docs/api-reference/files) ID of the image
          # in the message content. Set `purpose="vision"` when uploading the File if you
          # need to later display the file content.
          sig { returns(T.nilable(String)) }
          attr_reader :file_id

          sig { params(file_id: String).void }
          attr_writer :file_id

          sig do
            override
              .returns({
                detail:
                  OpenAI::Beta::Threads::ImageFileDelta::Detail::TaggedSymbol,
                file_id: String
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                detail: OpenAI::Beta::Threads::ImageFileDelta::Detail::OrSymbol,
                file_id: String
              ).returns(T.attached_class)
            end
            def new(
              detail: nil, # Specifies the detail level of the image if specified by the user. `low` uses
                           # fewer tokens, you can opt in to high resolution using `high`.
              file_id: nil # The [File](https://platform.openai.com/docs/api-reference/files) ID of the image
                           # in the message content. Set `purpose="vision"` when uploading the File if you
                           # need to later display the file content.
); end
          end

          # Specifies the detail level of the image if specified by the user. `low` uses
          # fewer tokens, you can opt in to high resolution using `high`.
          module Detail
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::Threads::ImageFileDelta::Detail::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Beta::Threads::ImageFileDelta::Detail::TaggedSymbol
              )

            HIGH = T.let(
                :high,
                OpenAI::Beta::Threads::ImageFileDelta::Detail::TaggedSymbol
              )

            LOW = T.let(
                :low,
                OpenAI::Beta::Threads::ImageFileDelta::Detail::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::ImageFileDelta::Detail)
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::ImageFileDelta,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ImageFileDeltaBlock < OpenAI::Internal::Type::BaseModel
          sig { returns(T.nilable(OpenAI::Beta::Threads::ImageFileDelta)) }
          attr_reader :image_file

          sig { params(image_file: OpenAI::Beta::Threads::ImageFileDelta::OrHash).void }
          attr_writer :image_file

          # The index of the content part in the message.
          sig { returns(Integer) }
          attr_accessor :index

          # Always `image_file`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                index: Integer,
                type: Symbol,
                image_file: OpenAI::Beta::Threads::ImageFileDelta
              })
          end
          def to_hash; end

          class << self
            # References an image [File](https://platform.openai.com/docs/api-reference/files)
            # in the content of a message.
            sig do
              params(
                index: Integer,
                image_file: OpenAI::Beta::Threads::ImageFileDelta::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              index:, # The index of the content part in the message.
              image_file: nil,
              type: :image_file # Always `image_file`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::ImageFileDeltaBlock,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ImageURL < OpenAI::Internal::Type::BaseModel
          # Specifies the detail level of the image. `low` uses fewer tokens, you can opt in
          # to high resolution using `high`. Default value is `auto`
          sig { returns(T.nilable(OpenAI::Beta::Threads::ImageURL::Detail::OrSymbol)) }
          attr_reader :detail

          sig { params(detail: OpenAI::Beta::Threads::ImageURL::Detail::OrSymbol).void }
          attr_writer :detail

          # The external URL of the image, must be a supported image types: jpeg, jpg, png,
          # gif, webp.
          sig { returns(String) }
          attr_accessor :url

          sig do
            override
              .returns({
                url: String,
                detail: OpenAI::Beta::Threads::ImageURL::Detail::OrSymbol
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                url: String,
                detail: OpenAI::Beta::Threads::ImageURL::Detail::OrSymbol
              ).returns(T.attached_class)
            end
            def new(
              url:, # The external URL of the image, must be a supported image types: jpeg, jpg, png,
                    # gif, webp.
              detail: nil # Specifies the detail level of the image. `low` uses fewer tokens, you can opt in
                          # to high resolution using `high`. Default value is `auto`
); end
          end

          # Specifies the detail level of the image. `low` uses fewer tokens, you can opt in
          # to high resolution using `high`. Default value is `auto`
          module Detail
            extend OpenAI::Internal::Type::Enum

            class << self
              sig { override.returns(T::Array[OpenAI::Beta::Threads::ImageURL::Detail::TaggedSymbol]) }
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Beta::Threads::ImageURL::Detail::TaggedSymbol
              )

            HIGH = T.let(
                :high,
                OpenAI::Beta::Threads::ImageURL::Detail::TaggedSymbol
              )

            LOW = T.let(:low, OpenAI::Beta::Threads::ImageURL::Detail::TaggedSymbol)

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::ImageURL::Detail)
              end
          end

          OrHash = T.type_alias do
              T.any(OpenAI::Beta::Threads::ImageURL, OpenAI::Internal::AnyHash)
            end
        end

        class ImageURLContentBlock < OpenAI::Internal::Type::BaseModel
          sig { returns(OpenAI::Beta::Threads::ImageURL) }
          attr_reader :image_url

          sig { params(image_url: OpenAI::Beta::Threads::ImageURL::OrHash).void }
          attr_writer :image_url

          # The type of the content part.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ image_url: OpenAI::Beta::Threads::ImageURL, type: Symbol }) }
          def to_hash; end

          class << self
            # References an image URL in the content of a message.
            sig { params(image_url: OpenAI::Beta::Threads::ImageURL::OrHash, type: Symbol).returns(T.attached_class) }
            def new(
              image_url:,
              type: :image_url # The type of the content part.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::ImageURLContentBlock,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ImageURLDelta < OpenAI::Internal::Type::BaseModel
          # Specifies the detail level of the image. `low` uses fewer tokens, you can opt in
          # to high resolution using `high`.
          sig do
            returns(T.nilable(
                OpenAI::Beta::Threads::ImageURLDelta::Detail::TaggedSymbol
              ))
          end
          attr_reader :detail

          sig { params(detail: OpenAI::Beta::Threads::ImageURLDelta::Detail::OrSymbol).void }
          attr_writer :detail

          # The URL of the image, must be a supported image types: jpeg, jpg, png, gif,
          # webp.
          sig { returns(T.nilable(String)) }
          attr_reader :url

          sig { params(url: String).void }
          attr_writer :url

          sig do
            override
              .returns({
                detail:
                  OpenAI::Beta::Threads::ImageURLDelta::Detail::TaggedSymbol,
                url: String
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                detail: OpenAI::Beta::Threads::ImageURLDelta::Detail::OrSymbol,
                url: String
              ).returns(T.attached_class)
            end
            def new(
              detail: nil, # Specifies the detail level of the image. `low` uses fewer tokens, you can opt in
                           # to high resolution using `high`.
              url: nil # The URL of the image, must be a supported image types: jpeg, jpg, png, gif,
                       # webp.
); end
          end

          # Specifies the detail level of the image. `low` uses fewer tokens, you can opt in
          # to high resolution using `high`.
          module Detail
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::Threads::ImageURLDelta::Detail::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Beta::Threads::ImageURLDelta::Detail::TaggedSymbol
              )

            HIGH = T.let(
                :high,
                OpenAI::Beta::Threads::ImageURLDelta::Detail::TaggedSymbol
              )

            LOW = T.let(
                :low,
                OpenAI::Beta::Threads::ImageURLDelta::Detail::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::ImageURLDelta::Detail)
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::ImageURLDelta,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ImageURLDeltaBlock < OpenAI::Internal::Type::BaseModel
          sig { returns(T.nilable(OpenAI::Beta::Threads::ImageURLDelta)) }
          attr_reader :image_url

          sig { params(image_url: OpenAI::Beta::Threads::ImageURLDelta::OrHash).void }
          attr_writer :image_url

          # The index of the content part in the message.
          sig { returns(Integer) }
          attr_accessor :index

          # Always `image_url`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                index: Integer,
                type: Symbol,
                image_url: OpenAI::Beta::Threads::ImageURLDelta
              })
          end
          def to_hash; end

          class << self
            # References an image URL in the content of a message.
            sig do
              params(
                index: Integer,
                image_url: OpenAI::Beta::Threads::ImageURLDelta::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              index:, # The index of the content part in the message.
              image_url: nil,
              type: :image_url # Always `image_url`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::ImageURLDeltaBlock,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class Message < OpenAI::Internal::Type::BaseModel
          # If applicable, the ID of the
          # [assistant](https://platform.openai.com/docs/api-reference/assistants) that
          # authored this message.
          sig { returns(T.nilable(String)) }
          attr_accessor :assistant_id

          # A list of files attached to the message, and the tools they were added to.
          sig { returns(T.nilable(T::Array[OpenAI::Beta::Threads::Message::Attachment])) }
          attr_accessor :attachments

          # The Unix timestamp (in seconds) for when the message was completed.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :completed_at

          # The content of the message in array of text and/or images.
          sig { returns(T::Array[OpenAI::Beta::Threads::MessageContent::Variants]) }
          attr_accessor :content

          # The Unix timestamp (in seconds) for when the message was created.
          sig { returns(Integer) }
          attr_accessor :created_at

          # The identifier, which can be referenced in API endpoints.
          sig { returns(String) }
          attr_accessor :id

          # The Unix timestamp (in seconds) for when the message was marked as incomplete.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :incomplete_at

          # On an incomplete message, details about why the message is incomplete.
          sig { returns(T.nilable(OpenAI::Beta::Threads::Message::IncompleteDetails)) }
          attr_reader :incomplete_details

          sig do
            params(
              incomplete_details: T.nilable(
                  OpenAI::Beta::Threads::Message::IncompleteDetails::OrHash
                )
            ).void
          end
          attr_writer :incomplete_details

          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # The object type, which is always `thread.message`.
          sig { returns(Symbol) }
          attr_accessor :object

          # The entity that produced the message. One of `user` or `assistant`.
          sig { returns(OpenAI::Beta::Threads::Message::Role::TaggedSymbol) }
          attr_accessor :role

          # The ID of the [run](https://platform.openai.com/docs/api-reference/runs)
          # associated with the creation of this message. Value is `null` when messages are
          # created manually using the create message or create thread endpoints.
          sig { returns(T.nilable(String)) }
          attr_accessor :run_id

          # The status of the message, which can be either `in_progress`, `incomplete`, or
          # `completed`.
          sig { returns(OpenAI::Beta::Threads::Message::Status::TaggedSymbol) }
          attr_accessor :status

          # The [thread](https://platform.openai.com/docs/api-reference/threads) ID that
          # this message belongs to.
          sig { returns(String) }
          attr_accessor :thread_id

          sig do
            override
              .returns({
                id: String,
                assistant_id: T.nilable(String),
                attachments:
                  T.nilable(
                    T::Array[OpenAI::Beta::Threads::Message::Attachment]
                  ),
                completed_at: T.nilable(Integer),
                content:
                  T::Array[OpenAI::Beta::Threads::MessageContent::Variants],
                created_at: Integer,
                incomplete_at: T.nilable(Integer),
                incomplete_details:
                  T.nilable(OpenAI::Beta::Threads::Message::IncompleteDetails),
                metadata: T.nilable(T::Hash[Symbol, String]),
                object: Symbol,
                role: OpenAI::Beta::Threads::Message::Role::TaggedSymbol,
                run_id: T.nilable(String),
                status: OpenAI::Beta::Threads::Message::Status::TaggedSymbol,
                thread_id: String
              })
          end
          def to_hash; end

          class << self
            # Represents a message within a
            # [thread](https://platform.openai.com/docs/api-reference/threads).
            sig do
              params(
                id: String,
                assistant_id: T.nilable(String),
                attachments: T.nilable(
                  T::Array[OpenAI::Beta::Threads::Message::Attachment::OrHash]
                ),
                completed_at: T.nilable(Integer),
                content: T::Array[
                  T.any(
                    OpenAI::Beta::Threads::ImageFileContentBlock::OrHash,
                    OpenAI::Beta::Threads::ImageURLContentBlock::OrHash,
                    OpenAI::Beta::Threads::TextContentBlock::OrHash,
                    OpenAI::Beta::Threads::RefusalContentBlock::OrHash
                  )
                ],
                created_at: Integer,
                incomplete_at: T.nilable(Integer),
                incomplete_details: T.nilable(
                  OpenAI::Beta::Threads::Message::IncompleteDetails::OrHash
                ),
                metadata: T.nilable(T::Hash[Symbol, String]),
                role: OpenAI::Beta::Threads::Message::Role::OrSymbol,
                run_id: T.nilable(String),
                status: OpenAI::Beta::Threads::Message::Status::OrSymbol,
                thread_id: String,
                object: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The identifier, which can be referenced in API endpoints.
              assistant_id:, # If applicable, the ID of the
                             # [assistant](https://platform.openai.com/docs/api-reference/assistants) that
                             # authored this message.
              attachments:, # A list of files attached to the message, and the tools they were added to.
              completed_at:, # The Unix timestamp (in seconds) for when the message was completed.
              content:, # The content of the message in array of text and/or images.
              created_at:, # The Unix timestamp (in seconds) for when the message was created.
              incomplete_at:, # The Unix timestamp (in seconds) for when the message was marked as incomplete.
              incomplete_details:, # On an incomplete message, details about why the message is incomplete.
              metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
              role:, # The entity that produced the message. One of `user` or `assistant`.
              run_id:, # The ID of the [run](https://platform.openai.com/docs/api-reference/runs)
                       # associated with the creation of this message. Value is `null` when messages are
                       # created manually using the create message or create thread endpoints.
              status:, # The status of the message, which can be either `in_progress`, `incomplete`, or
                       # `completed`.
              thread_id:, # The [thread](https://platform.openai.com/docs/api-reference/threads) ID that
                          # this message belongs to.
              object: :"thread.message" # The object type, which is always `thread.message`.
); end
          end

          class Attachment < OpenAI::Internal::Type::BaseModel
            # The ID of the file to attach to the message.
            sig { returns(T.nilable(String)) }
            attr_reader :file_id

            sig { params(file_id: String).void }
            attr_writer :file_id

            # The tools to add this file to.
            sig do
              returns(T.nilable(
                  T::Array[
                    OpenAI::Beta::Threads::Message::Attachment::Tool::Variants
                  ]
                ))
            end
            attr_reader :tools

            sig do
              params(
                tools: T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool::OrHash,
                      OpenAI::Beta::Threads::Message::Attachment::Tool::AssistantToolsFileSearchTypeOnly::OrHash
                    )
                  ]
              ).void
            end
            attr_writer :tools

            sig do
              override
                .returns({
                  file_id: String,
                  tools:
                    T::Array[
                      OpenAI::Beta::Threads::Message::Attachment::Tool::Variants
                    ]
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  file_id: String,
                  tools: T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool::OrHash,
                      OpenAI::Beta::Threads::Message::Attachment::Tool::AssistantToolsFileSearchTypeOnly::OrHash
                    )
                  ]
                ).returns(T.attached_class)
              end
              def new(
                file_id: nil, # The ID of the file to attach to the message.
                tools: nil # The tools to add this file to.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Message::Attachment,
                  OpenAI::Internal::AnyHash
                )
              end

            module Tool
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Message::Attachment::Tool::Variants
                  ])
                end
                def variants; end
              end

              class AssistantToolsFileSearchTypeOnly < OpenAI::Internal::Type::BaseModel
                # The type of tool being defined: `file_search`
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(type: Symbol).returns(T.attached_class) }
                  def new(
                    type: :file_search # The type of tool being defined: `file_search`
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Beta::Threads::Message::Attachment::Tool::AssistantToolsFileSearchTypeOnly,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool,
                    OpenAI::Beta::Threads::Message::Attachment::Tool::AssistantToolsFileSearchTypeOnly
                  )
                end
            end
          end

          class IncompleteDetails < OpenAI::Internal::Type::BaseModel
            # The reason the message is incomplete.
            sig { returns(OpenAI::Beta::Threads::Message::IncompleteDetails::Reason::TaggedSymbol) }
            attr_accessor :reason

            sig do
              override
                .returns({
                  reason:
                    OpenAI::Beta::Threads::Message::IncompleteDetails::Reason::TaggedSymbol
                })
            end
            def to_hash; end

            class << self
              # On an incomplete message, details about why the message is incomplete.
              sig do
                params(
                  reason: OpenAI::Beta::Threads::Message::IncompleteDetails::Reason::OrSymbol
                ).returns(T.attached_class)
              end
              def new(
                reason: # The reason the message is incomplete.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Message::IncompleteDetails,
                  OpenAI::Internal::AnyHash
                )
              end

            # The reason the message is incomplete.
            module Reason
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Message::IncompleteDetails::Reason::TaggedSymbol
                  ])
                end
                def values; end
              end

              CONTENT_FILTER = T.let(
                  :content_filter,
                  OpenAI::Beta::Threads::Message::IncompleteDetails::Reason::TaggedSymbol
                )

              MAX_TOKENS = T.let(
                  :max_tokens,
                  OpenAI::Beta::Threads::Message::IncompleteDetails::Reason::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              RUN_CANCELLED = T.let(
                  :run_cancelled,
                  OpenAI::Beta::Threads::Message::IncompleteDetails::Reason::TaggedSymbol
                )

              RUN_EXPIRED = T.let(
                  :run_expired,
                  OpenAI::Beta::Threads::Message::IncompleteDetails::Reason::TaggedSymbol
                )

              RUN_FAILED = T.let(
                  :run_failed,
                  OpenAI::Beta::Threads::Message::IncompleteDetails::Reason::TaggedSymbol
                )

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Beta::Threads::Message::IncompleteDetails::Reason
                  )
                end
            end
          end

          OrHash = T.type_alias do
              T.any(OpenAI::Beta::Threads::Message, OpenAI::Internal::AnyHash)
            end

          # The entity that produced the message. One of `user` or `assistant`.
          module Role
            extend OpenAI::Internal::Type::Enum

            class << self
              sig { override.returns(T::Array[OpenAI::Beta::Threads::Message::Role::TaggedSymbol]) }
              def values; end
            end

            ASSISTANT = T.let(
                :assistant,
                OpenAI::Beta::Threads::Message::Role::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::Message::Role)
              end

            USER = T.let(:user, OpenAI::Beta::Threads::Message::Role::TaggedSymbol)
          end

          # The status of the message, which can be either `in_progress`, `incomplete`, or
          # `completed`.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig { override.returns(T::Array[OpenAI::Beta::Threads::Message::Status::TaggedSymbol]) }
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Beta::Threads::Message::Status::TaggedSymbol
              )

            INCOMPLETE = T.let(
                :incomplete,
                OpenAI::Beta::Threads::Message::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Beta::Threads::Message::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::Message::Status)
              end
          end
        end

        # References an image [File](https://platform.openai.com/docs/api-reference/files)
        # in the content of a message.
        module MessageContent
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::Threads::MessageContent::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::ImageFileContentBlock,
                OpenAI::Beta::Threads::ImageURLContentBlock,
                OpenAI::Beta::Threads::TextContentBlock,
                OpenAI::Beta::Threads::RefusalContentBlock
              )
            end
        end

        # References an image [File](https://platform.openai.com/docs/api-reference/files)
        # in the content of a message.
        module MessageContentDelta
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::Threads::MessageContentDelta::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::ImageFileDeltaBlock,
                OpenAI::Beta::Threads::TextDeltaBlock,
                OpenAI::Beta::Threads::RefusalDeltaBlock,
                OpenAI::Beta::Threads::ImageURLDeltaBlock
              )
            end
        end

        # References an image [File](https://platform.openai.com/docs/api-reference/files)
        # in the content of a message.
        module MessageContentPartParam
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::Threads::MessageContentPartParam::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::ImageFileContentBlock,
                OpenAI::Beta::Threads::ImageURLContentBlock,
                OpenAI::Beta::Threads::TextContentBlockParam
              )
            end
        end

        class MessageCreateParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # A list of files attached to the message, and the tools they should be added to.
          sig do
            returns(T.nilable(
                T::Array[OpenAI::Beta::Threads::MessageCreateParams::Attachment]
              ))
          end
          attr_accessor :attachments

          # The text contents of the message.
          sig { returns(OpenAI::Beta::Threads::MessageCreateParams::Content::Variants) }
          attr_accessor :content

          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # The role of the entity that is creating the message. Allowed values include:
          #
          # - `user`: Indicates the message is sent by an actual user and should be used in
          #   most cases to represent user-generated messages.
          # - `assistant`: Indicates the message is generated by the assistant. Use this
          #   value to insert messages from the assistant into the conversation.
          sig { returns(OpenAI::Beta::Threads::MessageCreateParams::Role::OrSymbol) }
          attr_accessor :role

          sig do
            override
              .returns({
                content:
                  OpenAI::Beta::Threads::MessageCreateParams::Content::Variants,
                role:
                  OpenAI::Beta::Threads::MessageCreateParams::Role::OrSymbol,
                attachments:
                  T.nilable(
                    T::Array[
                      OpenAI::Beta::Threads::MessageCreateParams::Attachment
                    ]
                  ),
                metadata: T.nilable(T::Hash[Symbol, String]),
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                content: OpenAI::Beta::Threads::MessageCreateParams::Content::Variants,
                role: OpenAI::Beta::Threads::MessageCreateParams::Role::OrSymbol,
                attachments: T.nilable(
                  T::Array[
                    OpenAI::Beta::Threads::MessageCreateParams::Attachment::OrHash
                  ]
                ),
                metadata: T.nilable(T::Hash[Symbol, String]),
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              content:, # The text contents of the message.
              role:, # The role of the entity that is creating the message. Allowed values include:
                     # - `user`: Indicates the message is sent by an actual user and should be used in
                     #   most cases to represent user-generated messages.
                     # - `assistant`: Indicates the message is generated by the assistant. Use this
                     #   value to insert messages from the assistant into the conversation.
              attachments: nil, # A list of files attached to the message, and the tools they should be added to.
              metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard.
                             # Keys are strings with a maximum length of 64 characters. Values are strings with
                             # a maximum length of 512 characters.
              request_options: {}
); end
          end

          class Attachment < OpenAI::Internal::Type::BaseModel
            # The ID of the file to attach to the message.
            sig { returns(T.nilable(String)) }
            attr_reader :file_id

            sig { params(file_id: String).void }
            attr_writer :file_id

            # The tools to add this file to.
            sig do
              returns(T.nilable(
                  T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool,
                      OpenAI::Beta::Threads::MessageCreateParams::Attachment::Tool::FileSearch
                    )
                  ]
                ))
            end
            attr_reader :tools

            sig do
              params(
                tools: T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool::OrHash,
                      OpenAI::Beta::Threads::MessageCreateParams::Attachment::Tool::FileSearch::OrHash
                    )
                  ]
              ).void
            end
            attr_writer :tools

            sig do
              override
                .returns({
                  file_id: String,
                  tools:
                    T::Array[
                      T.any(
                        OpenAI::Beta::CodeInterpreterTool,
                        OpenAI::Beta::Threads::MessageCreateParams::Attachment::Tool::FileSearch
                      )
                    ]
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  file_id: String,
                  tools: T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool::OrHash,
                      OpenAI::Beta::Threads::MessageCreateParams::Attachment::Tool::FileSearch::OrHash
                    )
                  ]
                ).returns(T.attached_class)
              end
              def new(
                file_id: nil, # The ID of the file to attach to the message.
                tools: nil # The tools to add this file to.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::MessageCreateParams::Attachment,
                  OpenAI::Internal::AnyHash
                )
              end

            module Tool
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::MessageCreateParams::Attachment::Tool::Variants
                  ])
                end
                def variants; end
              end

              class FileSearch < OpenAI::Internal::Type::BaseModel
                # The type of tool being defined: `file_search`
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(type: Symbol).returns(T.attached_class) }
                  def new(
                    type: :file_search # The type of tool being defined: `file_search`
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Beta::Threads::MessageCreateParams::Attachment::Tool::FileSearch,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool,
                    OpenAI::Beta::Threads::MessageCreateParams::Attachment::Tool::FileSearch
                  )
                end
            end
          end

          # The text contents of the message.
          module Content
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::Threads::MessageCreateParams::Content::Variants
                ])
              end
              def variants; end
            end

            MessageContentPartParamArray = T.let(
                OpenAI::Internal::Type::ArrayOf[
                  union: OpenAI::Beta::Threads::MessageContentPartParam
                ],
                OpenAI::Internal::Type::Converter
              )

            Variants = T.type_alias do
                T.any(
                  String,
                  T::Array[
                    OpenAI::Beta::Threads::MessageContentPartParam::Variants
                  ]
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::MessageCreateParams,
                OpenAI::Internal::AnyHash
              )
            end

          # The role of the entity that is creating the message. Allowed values include:
          #
          # - `user`: Indicates the message is sent by an actual user and should be used in
          #   most cases to represent user-generated messages.
          # - `assistant`: Indicates the message is generated by the assistant. Use this
          #   value to insert messages from the assistant into the conversation.
          module Role
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::Threads::MessageCreateParams::Role::TaggedSymbol
                ])
              end
              def values; end
            end

            ASSISTANT = T.let(
                :assistant,
                OpenAI::Beta::Threads::MessageCreateParams::Role::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::MessageCreateParams::Role)
              end

            USER = T.let(
                :user,
                OpenAI::Beta::Threads::MessageCreateParams::Role::TaggedSymbol
              )
          end
        end

        class MessageDeleteParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          sig { returns(String) }
          attr_accessor :thread_id

          sig { override.returns({ thread_id: String, request_options: OpenAI::RequestOptions }) }
          def to_hash; end

          class << self
            sig do
              params(
                thread_id: String,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(thread_id:, request_options: {}); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::MessageDeleteParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class MessageDeleted < OpenAI::Internal::Type::BaseModel
          sig { returns(T::Boolean) }
          attr_accessor :deleted

          sig { returns(String) }
          attr_accessor :id

          sig { returns(Symbol) }
          attr_accessor :object

          sig { override.returns({ id: String, deleted: T::Boolean, object: Symbol }) }
          def to_hash; end

          class << self
            sig { params(id: String, deleted: T::Boolean, object: Symbol).returns(T.attached_class) }
            def new(id:, deleted:, object: :"thread.message.deleted"); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::MessageDeleted,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class MessageDelta < OpenAI::Internal::Type::BaseModel
          # The content of the message in array of text and/or images.
          sig do
            returns(T.nilable(
                T::Array[OpenAI::Beta::Threads::MessageContentDelta::Variants]
              ))
          end
          attr_reader :content

          sig do
            params(
              content: T::Array[
                  T.any(
                    OpenAI::Beta::Threads::ImageFileDeltaBlock::OrHash,
                    OpenAI::Beta::Threads::TextDeltaBlock::OrHash,
                    OpenAI::Beta::Threads::RefusalDeltaBlock::OrHash,
                    OpenAI::Beta::Threads::ImageURLDeltaBlock::OrHash
                  )
                ]
            ).void
          end
          attr_writer :content

          # The entity that produced the message. One of `user` or `assistant`.
          sig { returns(T.nilable(OpenAI::Beta::Threads::MessageDelta::Role::TaggedSymbol)) }
          attr_reader :role

          sig { params(role: OpenAI::Beta::Threads::MessageDelta::Role::OrSymbol).void }
          attr_writer :role

          sig do
            override
              .returns({
                content:
                  T::Array[
                    OpenAI::Beta::Threads::MessageContentDelta::Variants
                  ],
                role: OpenAI::Beta::Threads::MessageDelta::Role::TaggedSymbol
              })
          end
          def to_hash; end

          class << self
            # The delta containing the fields that have changed on the Message.
            sig do
              params(
                content: T::Array[
                  T.any(
                    OpenAI::Beta::Threads::ImageFileDeltaBlock::OrHash,
                    OpenAI::Beta::Threads::TextDeltaBlock::OrHash,
                    OpenAI::Beta::Threads::RefusalDeltaBlock::OrHash,
                    OpenAI::Beta::Threads::ImageURLDeltaBlock::OrHash
                  )
                ],
                role: OpenAI::Beta::Threads::MessageDelta::Role::OrSymbol
              ).returns(T.attached_class)
            end
            def new(
              content: nil, # The content of the message in array of text and/or images.
              role: nil # The entity that produced the message. One of `user` or `assistant`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::MessageDelta,
                OpenAI::Internal::AnyHash
              )
            end

          # The entity that produced the message. One of `user` or `assistant`.
          module Role
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::Threads::MessageDelta::Role::TaggedSymbol
                ])
              end
              def values; end
            end

            ASSISTANT = T.let(
                :assistant,
                OpenAI::Beta::Threads::MessageDelta::Role::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::MessageDelta::Role)
              end

            USER = T.let(
                :user,
                OpenAI::Beta::Threads::MessageDelta::Role::TaggedSymbol
              )
          end
        end

        class MessageDeltaEvent < OpenAI::Internal::Type::BaseModel
          # The delta containing the fields that have changed on the Message.
          sig { returns(OpenAI::Beta::Threads::MessageDelta) }
          attr_reader :delta

          sig { params(delta: OpenAI::Beta::Threads::MessageDelta::OrHash).void }
          attr_writer :delta

          # The identifier of the message, which can be referenced in API endpoints.
          sig { returns(String) }
          attr_accessor :id

          # The object type, which is always `thread.message.delta`.
          sig { returns(Symbol) }
          attr_accessor :object

          sig do
            override
              .returns({
                id: String,
                delta: OpenAI::Beta::Threads::MessageDelta,
                object: Symbol
              })
          end
          def to_hash; end

          class << self
            # Represents a message delta i.e. any changed fields on a message during
            # streaming.
            sig do
              params(
                id: String,
                delta: OpenAI::Beta::Threads::MessageDelta::OrHash,
                object: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The identifier of the message, which can be referenced in API endpoints.
              delta:, # The delta containing the fields that have changed on the Message.
              object: :"thread.message.delta" # The object type, which is always `thread.message.delta`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::MessageDeltaEvent,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class MessageListParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # A cursor for use in pagination. `after` is an object ID that defines your place
          # in the list. For instance, if you make a list request and receive 100 objects,
          # ending with obj_foo, your subsequent call can include after=obj_foo in order to
          # fetch the next page of the list.
          sig { returns(T.nilable(String)) }
          attr_reader :after

          sig { params(after: String).void }
          attr_writer :after

          # A cursor for use in pagination. `before` is an object ID that defines your place
          # in the list. For instance, if you make a list request and receive 100 objects,
          # starting with obj_foo, your subsequent call can include before=obj_foo in order
          # to fetch the previous page of the list.
          sig { returns(T.nilable(String)) }
          attr_reader :before

          sig { params(before: String).void }
          attr_writer :before

          # A limit on the number of objects to be returned. Limit can range between 1 and
          # 100, and the default is 20.
          sig { returns(T.nilable(Integer)) }
          attr_reader :limit

          sig { params(limit: Integer).void }
          attr_writer :limit

          # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
          # order and `desc` for descending order.
          sig do
            returns(T.nilable(
                OpenAI::Beta::Threads::MessageListParams::Order::OrSymbol
              ))
          end
          attr_reader :order

          sig { params(order: OpenAI::Beta::Threads::MessageListParams::Order::OrSymbol).void }
          attr_writer :order

          # Filter messages by the run ID that generated them.
          sig { returns(T.nilable(String)) }
          attr_reader :run_id

          sig { params(run_id: String).void }
          attr_writer :run_id

          sig do
            override
              .returns({
                after: String,
                before: String,
                limit: Integer,
                order:
                  OpenAI::Beta::Threads::MessageListParams::Order::OrSymbol,
                run_id: String,
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                after: String,
                before: String,
                limit: Integer,
                order: OpenAI::Beta::Threads::MessageListParams::Order::OrSymbol,
                run_id: String,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                          # in the list. For instance, if you make a list request and receive 100 objects,
                          # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                          # fetch the next page of the list.
              before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                           # in the list. For instance, if you make a list request and receive 100 objects,
                           # starting with obj_foo, your subsequent call can include before=obj_foo in order
                           # to fetch the previous page of the list.
              limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                          # 100, and the default is 20.
              order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                          # order and `desc` for descending order.
              run_id: nil, # Filter messages by the run ID that generated them.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::MessageListParams,
                OpenAI::Internal::AnyHash
              )
            end

          # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
          # order and `desc` for descending order.
          module Order
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::Threads::MessageListParams::Order::TaggedSymbol
                ])
              end
              def values; end
            end

            ASC = T.let(
                :asc,
                OpenAI::Beta::Threads::MessageListParams::Order::TaggedSymbol
              )

            DESC = T.let(
                :desc,
                OpenAI::Beta::Threads::MessageListParams::Order::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::MessageListParams::Order)
              end
          end
        end

        class MessageRetrieveParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          sig { returns(String) }
          attr_accessor :thread_id

          sig { override.returns({ thread_id: String, request_options: OpenAI::RequestOptions }) }
          def to_hash; end

          class << self
            sig do
              params(
                thread_id: String,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(thread_id:, request_options: {}); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::MessageRetrieveParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class MessageUpdateParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          sig { returns(String) }
          attr_accessor :thread_id

          sig do
            override
              .returns({
                thread_id: String,
                metadata: T.nilable(T::Hash[Symbol, String]),
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                thread_id: String,
                metadata: T.nilable(T::Hash[Symbol, String]),
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              thread_id:,
              metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard.
                             # Keys are strings with a maximum length of 64 characters. Values are strings with
                             # a maximum length of 512 characters.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::MessageUpdateParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class RefusalContentBlock < OpenAI::Internal::Type::BaseModel
          sig { returns(String) }
          attr_accessor :refusal

          # Always `refusal`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ refusal: String, type: Symbol }) }
          def to_hash; end

          class << self
            # The refusal content generated by the assistant.
            sig { params(refusal: String, type: Symbol).returns(T.attached_class) }
            def new(
              refusal:,
              type: :refusal # Always `refusal`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::RefusalContentBlock,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class RefusalDeltaBlock < OpenAI::Internal::Type::BaseModel
          # The index of the refusal part in the message.
          sig { returns(Integer) }
          attr_accessor :index

          sig { returns(T.nilable(String)) }
          attr_reader :refusal

          sig { params(refusal: String).void }
          attr_writer :refusal

          # Always `refusal`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ index: Integer, type: Symbol, refusal: String }) }
          def to_hash; end

          class << self
            # The refusal content that is part of a message.
            sig { params(index: Integer, refusal: String, type: Symbol).returns(T.attached_class) }
            def new(
              index:, # The index of the refusal part in the message.
              refusal: nil,
              type: :refusal # Always `refusal`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::RefusalDeltaBlock,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class RequiredActionFunctionToolCall < OpenAI::Internal::Type::BaseModel
          # The function definition.
          sig { returns(OpenAI::Beta::Threads::RequiredActionFunctionToolCall::Function) }
          attr_reader :function

          sig { params(function: OpenAI::Beta::Threads::RequiredActionFunctionToolCall::Function::OrHash).void }
          attr_writer :function

          # The ID of the tool call. This ID must be referenced when you submit the tool
          # outputs in using the
          # [Submit tool outputs to run](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)
          # endpoint.
          sig { returns(String) }
          attr_accessor :id

          # The type of tool call the output is required for. For now, this is always
          # `function`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                function:
                  OpenAI::Beta::Threads::RequiredActionFunctionToolCall::Function,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # Tool call objects
            sig do
              params(
                id: String,
                function: OpenAI::Beta::Threads::RequiredActionFunctionToolCall::Function::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The ID of the tool call. This ID must be referenced when you submit the tool
                   # outputs in using the
                   # [Submit tool outputs to run](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)
                   # endpoint.
              function:, # The function definition.
              type: :function # The type of tool call the output is required for. For now, this is always
                              # `function`.
); end
          end

          class Function < OpenAI::Internal::Type::BaseModel
            # The arguments that the model expects you to pass to the function.
            sig { returns(String) }
            attr_accessor :arguments

            # The name of the function.
            sig { returns(String) }
            attr_accessor :name

            sig { override.returns({ arguments: String, name: String }) }
            def to_hash; end

            class << self
              # The function definition.
              sig { params(arguments: String, name: String).returns(T.attached_class) }
              def new(
                arguments:, # The arguments that the model expects you to pass to the function.
                name: # The name of the function.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::RequiredActionFunctionToolCall::Function,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::RequiredActionFunctionToolCall,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class Run < OpenAI::Internal::Type::BaseModel
          # The ID of the
          # [assistant](https://platform.openai.com/docs/api-reference/assistants) used for
          # execution of this run.
          sig { returns(String) }
          attr_accessor :assistant_id

          # The Unix timestamp (in seconds) for when the run was cancelled.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :cancelled_at

          # The Unix timestamp (in seconds) for when the run was completed.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :completed_at

          # The Unix timestamp (in seconds) for when the run was created.
          sig { returns(Integer) }
          attr_accessor :created_at

          # The Unix timestamp (in seconds) for when the run will expire.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :expires_at

          # The Unix timestamp (in seconds) for when the run failed.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :failed_at

          # The identifier, which can be referenced in API endpoints.
          sig { returns(String) }
          attr_accessor :id

          # Details on why the run is incomplete. Will be `null` if the run is not
          # incomplete.
          sig { returns(T.nilable(OpenAI::Beta::Threads::Run::IncompleteDetails)) }
          attr_reader :incomplete_details

          sig { params(incomplete_details: T.nilable(OpenAI::Beta::Threads::Run::IncompleteDetails::OrHash)).void }
          attr_writer :incomplete_details

          # The instructions that the
          # [assistant](https://platform.openai.com/docs/api-reference/assistants) used for
          # this run.
          sig { returns(String) }
          attr_accessor :instructions

          # The last error associated with this run. Will be `null` if there are no errors.
          sig { returns(T.nilable(OpenAI::Beta::Threads::Run::LastError)) }
          attr_reader :last_error

          sig { params(last_error: T.nilable(OpenAI::Beta::Threads::Run::LastError::OrHash)).void }
          attr_writer :last_error

          # The maximum number of completion tokens specified to have been used over the
          # course of the run.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :max_completion_tokens

          # The maximum number of prompt tokens specified to have been used over the course
          # of the run.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :max_prompt_tokens

          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # The model that the
          # [assistant](https://platform.openai.com/docs/api-reference/assistants) used for
          # this run.
          sig { returns(String) }
          attr_accessor :model

          # The object type, which is always `thread.run`.
          sig { returns(Symbol) }
          attr_accessor :object

          # Whether to enable
          # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
          # during tool use.
          sig { returns(T::Boolean) }
          attr_accessor :parallel_tool_calls

          # Details on the action required to continue the run. Will be `null` if no action
          # is required.
          sig { returns(T.nilable(OpenAI::Beta::Threads::Run::RequiredAction)) }
          attr_reader :required_action

          sig { params(required_action: T.nilable(OpenAI::Beta::Threads::Run::RequiredAction::OrHash)).void }
          attr_writer :required_action

          # Specifies the format that the model must output. Compatible with
          # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
          # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
          # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
          #
          # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
          # Outputs which ensures the model will match your supplied JSON schema. Learn more
          # in the
          # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
          #
          # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
          # message the model generates is valid JSON.
          #
          # **Important:** when using JSON mode, you **must** also instruct the model to
          # produce JSON yourself via a system or user message. Without this, the model may
          # generate an unending stream of whitespace until the generation reaches the token
          # limit, resulting in a long-running and seemingly "stuck" request. Also note that
          # the message content may be partially cut off if `finish_reason="length"`, which
          # indicates the generation exceeded `max_tokens` or the conversation exceeded the
          # max context length.
          sig { returns(T.nilable(OpenAI::Beta::AssistantResponseFormatOption::Variants)) }
          attr_accessor :response_format

          # The Unix timestamp (in seconds) for when the run was started.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :started_at

          # The status of the run, which can be either `queued`, `in_progress`,
          # `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`,
          # `incomplete`, or `expired`.
          sig { returns(OpenAI::Beta::Threads::RunStatus::TaggedSymbol) }
          attr_accessor :status

          # The sampling temperature used for this run. If not set, defaults to 1.
          sig { returns(T.nilable(Float)) }
          attr_accessor :temperature

          # The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)
          # that was executed on as a part of this run.
          sig { returns(String) }
          attr_accessor :thread_id

          # Controls which (if any) tool is called by the model. `none` means the model will
          # not call any tools and instead generates a message. `auto` is the default value
          # and means the model can pick between generating a message or calling one or more
          # tools. `required` means the model must call one or more tools before responding
          # to the user. Specifying a particular tool like `{"type": "file_search"}` or
          # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
          # call that tool.
          sig { returns(T.nilable(OpenAI::Beta::AssistantToolChoiceOption::Variants)) }
          attr_accessor :tool_choice

          # The list of tools that the
          # [assistant](https://platform.openai.com/docs/api-reference/assistants) used for
          # this run.
          sig { returns(T::Array[OpenAI::Beta::AssistantTool::Variants]) }
          attr_accessor :tools

          # The nucleus sampling value used for this run. If not set, defaults to 1.
          sig { returns(T.nilable(Float)) }
          attr_accessor :top_p

          # Controls for how a thread will be truncated prior to the run. Use this to
          # control the intial context window of the run.
          sig { returns(T.nilable(OpenAI::Beta::Threads::Run::TruncationStrategy)) }
          attr_reader :truncation_strategy

          sig do
            params(
              truncation_strategy: T.nilable(
                  OpenAI::Beta::Threads::Run::TruncationStrategy::OrHash
                )
            ).void
          end
          attr_writer :truncation_strategy

          # Usage statistics related to the run. This value will be `null` if the run is not
          # in a terminal state (i.e. `in_progress`, `queued`, etc.).
          sig { returns(T.nilable(OpenAI::Beta::Threads::Run::Usage)) }
          attr_reader :usage

          sig { params(usage: T.nilable(OpenAI::Beta::Threads::Run::Usage::OrHash)).void }
          attr_writer :usage

          sig do
            override
              .returns({
                id: String,
                assistant_id: String,
                cancelled_at: T.nilable(Integer),
                completed_at: T.nilable(Integer),
                created_at: Integer,
                expires_at: T.nilable(Integer),
                failed_at: T.nilable(Integer),
                incomplete_details:
                  T.nilable(OpenAI::Beta::Threads::Run::IncompleteDetails),
                instructions: String,
                last_error: T.nilable(OpenAI::Beta::Threads::Run::LastError),
                max_completion_tokens: T.nilable(Integer),
                max_prompt_tokens: T.nilable(Integer),
                metadata: T.nilable(T::Hash[Symbol, String]),
                model: String,
                object: Symbol,
                parallel_tool_calls: T::Boolean,
                required_action:
                  T.nilable(OpenAI::Beta::Threads::Run::RequiredAction),
                response_format:
                  T.nilable(
                    OpenAI::Beta::AssistantResponseFormatOption::Variants
                  ),
                started_at: T.nilable(Integer),
                status: OpenAI::Beta::Threads::RunStatus::TaggedSymbol,
                thread_id: String,
                tool_choice:
                  T.nilable(OpenAI::Beta::AssistantToolChoiceOption::Variants),
                tools: T::Array[OpenAI::Beta::AssistantTool::Variants],
                truncation_strategy:
                  T.nilable(OpenAI::Beta::Threads::Run::TruncationStrategy),
                usage: T.nilable(OpenAI::Beta::Threads::Run::Usage),
                temperature: T.nilable(Float),
                top_p: T.nilable(Float)
              })
          end
          def to_hash; end

          class << self
            # Represents an execution run on a
            # [thread](https://platform.openai.com/docs/api-reference/threads).
            sig do
              params(
                id: String,
                assistant_id: String,
                cancelled_at: T.nilable(Integer),
                completed_at: T.nilable(Integer),
                created_at: Integer,
                expires_at: T.nilable(Integer),
                failed_at: T.nilable(Integer),
                incomplete_details: T.nilable(
                  OpenAI::Beta::Threads::Run::IncompleteDetails::OrHash
                ),
                instructions: String,
                last_error: T.nilable(OpenAI::Beta::Threads::Run::LastError::OrHash),
                max_completion_tokens: T.nilable(Integer),
                max_prompt_tokens: T.nilable(Integer),
                metadata: T.nilable(T::Hash[Symbol, String]),
                model: String,
                parallel_tool_calls: T::Boolean,
                required_action: T.nilable(OpenAI::Beta::Threads::Run::RequiredAction::OrHash),
                response_format: T.nilable(
                  T.any(
                    Symbol,
                    OpenAI::ResponseFormatText::OrHash,
                    OpenAI::ResponseFormatJSONObject::OrHash,
                    OpenAI::ResponseFormatJSONSchema::OrHash
                  )
                ),
                started_at: T.nilable(Integer),
                status: OpenAI::Beta::Threads::RunStatus::OrSymbol,
                thread_id: String,
                tool_choice: T.nilable(
                  T.any(
                    OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                    OpenAI::Beta::AssistantToolChoice::OrHash
                  )
                ),
                tools: T::Array[
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool::OrHash,
                    OpenAI::Beta::FileSearchTool::OrHash,
                    OpenAI::Beta::FunctionTool::OrHash
                  )
                ],
                truncation_strategy: T.nilable(
                  OpenAI::Beta::Threads::Run::TruncationStrategy::OrHash
                ),
                usage: T.nilable(OpenAI::Beta::Threads::Run::Usage::OrHash),
                temperature: T.nilable(Float),
                top_p: T.nilable(Float),
                object: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The identifier, which can be referenced in API endpoints.
              assistant_id:, # The ID of the
                             # [assistant](https://platform.openai.com/docs/api-reference/assistants) used for
                             # execution of this run.
              cancelled_at:, # The Unix timestamp (in seconds) for when the run was cancelled.
              completed_at:, # The Unix timestamp (in seconds) for when the run was completed.
              created_at:, # The Unix timestamp (in seconds) for when the run was created.
              expires_at:, # The Unix timestamp (in seconds) for when the run will expire.
              failed_at:, # The Unix timestamp (in seconds) for when the run failed.
              incomplete_details:, # Details on why the run is incomplete. Will be `null` if the run is not
                                   # incomplete.
              instructions:, # The instructions that the
                             # [assistant](https://platform.openai.com/docs/api-reference/assistants) used for
                             # this run.
              last_error:, # The last error associated with this run. Will be `null` if there are no errors.
              max_completion_tokens:, # The maximum number of completion tokens specified to have been used over the
                                      # course of the run.
              max_prompt_tokens:, # The maximum number of prompt tokens specified to have been used over the course
                                  # of the run.
              metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
              model:, # The model that the
                      # [assistant](https://platform.openai.com/docs/api-reference/assistants) used for
                      # this run.
              parallel_tool_calls:, # Whether to enable
                                    # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                    # during tool use.
              required_action:, # Details on the action required to continue the run. Will be `null` if no action
                                # is required.
              response_format:, # Specifies the format that the model must output. Compatible with
                                # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                # in the
                                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                # message the model generates is valid JSON.
                                # **Important:** when using JSON mode, you **must** also instruct the model to
                                # produce JSON yourself via a system or user message. Without this, the model may
                                # generate an unending stream of whitespace until the generation reaches the token
                                # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                # the message content may be partially cut off if `finish_reason="length"`, which
                                # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                # max context length.
              started_at:, # The Unix timestamp (in seconds) for when the run was started.
              status:, # The status of the run, which can be either `queued`, `in_progress`,
                       # `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`,
                       # `incomplete`, or `expired`.
              thread_id:, # The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)
                          # that was executed on as a part of this run.
              tool_choice:, # Controls which (if any) tool is called by the model. `none` means the model will
                            # not call any tools and instead generates a message. `auto` is the default value
                            # and means the model can pick between generating a message or calling one or more
                            # tools. `required` means the model must call one or more tools before responding
                            # to the user. Specifying a particular tool like `{"type": "file_search"}` or
                            # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                            # call that tool.
              tools:, # The list of tools that the
                      # [assistant](https://platform.openai.com/docs/api-reference/assistants) used for
                      # this run.
              truncation_strategy:, # Controls for how a thread will be truncated prior to the run. Use this to
                                    # control the intial context window of the run.
              usage:, # Usage statistics related to the run. This value will be `null` if the run is not
                      # in a terminal state (i.e. `in_progress`, `queued`, etc.).
              temperature: nil, # The sampling temperature used for this run. If not set, defaults to 1.
              top_p: nil, # The nucleus sampling value used for this run. If not set, defaults to 1.
              object: :"thread.run" # The object type, which is always `thread.run`.
); end
          end

          class IncompleteDetails < OpenAI::Internal::Type::BaseModel
            # The reason why the run is incomplete. This will point to which specific token
            # limit was reached over the course of the run.
            sig do
              returns(T.nilable(
                  OpenAI::Beta::Threads::Run::IncompleteDetails::Reason::TaggedSymbol
                ))
            end
            attr_reader :reason

            sig { params(reason: OpenAI::Beta::Threads::Run::IncompleteDetails::Reason::OrSymbol).void }
            attr_writer :reason

            sig do
              override
                .returns({
                  reason:
                    OpenAI::Beta::Threads::Run::IncompleteDetails::Reason::TaggedSymbol
                })
            end
            def to_hash; end

            class << self
              # Details on why the run is incomplete. Will be `null` if the run is not
              # incomplete.
              sig do
                params(
                  reason: OpenAI::Beta::Threads::Run::IncompleteDetails::Reason::OrSymbol
                ).returns(T.attached_class)
              end
              def new(
                reason: nil # The reason why the run is incomplete. This will point to which specific token
                            # limit was reached over the course of the run.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Run::IncompleteDetails,
                  OpenAI::Internal::AnyHash
                )
              end

            # The reason why the run is incomplete. This will point to which specific token
            # limit was reached over the course of the run.
            module Reason
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Run::IncompleteDetails::Reason::TaggedSymbol
                  ])
                end
                def values; end
              end

              MAX_COMPLETION_TOKENS = T.let(
                  :max_completion_tokens,
                  OpenAI::Beta::Threads::Run::IncompleteDetails::Reason::TaggedSymbol
                )

              MAX_PROMPT_TOKENS = T.let(
                  :max_prompt_tokens,
                  OpenAI::Beta::Threads::Run::IncompleteDetails::Reason::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Beta::Threads::Run::IncompleteDetails::Reason
                  )
                end
            end
          end

          class LastError < OpenAI::Internal::Type::BaseModel
            # One of `server_error`, `rate_limit_exceeded`, or `invalid_prompt`.
            sig { returns(OpenAI::Beta::Threads::Run::LastError::Code::TaggedSymbol) }
            attr_accessor :code

            # A human-readable description of the error.
            sig { returns(String) }
            attr_accessor :message

            sig do
              override
                .returns({
                  code:
                    OpenAI::Beta::Threads::Run::LastError::Code::TaggedSymbol,
                  message: String
                })
            end
            def to_hash; end

            class << self
              # The last error associated with this run. Will be `null` if there are no errors.
              sig do
                params(
                  code: OpenAI::Beta::Threads::Run::LastError::Code::OrSymbol,
                  message: String
                ).returns(T.attached_class)
              end
              def new(
                code:, # One of `server_error`, `rate_limit_exceeded`, or `invalid_prompt`.
                message: # A human-readable description of the error.
); end
            end

            # One of `server_error`, `rate_limit_exceeded`, or `invalid_prompt`.
            module Code
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Run::LastError::Code::TaggedSymbol
                  ])
                end
                def values; end
              end

              INVALID_PROMPT = T.let(
                  :invalid_prompt,
                  OpenAI::Beta::Threads::Run::LastError::Code::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              RATE_LIMIT_EXCEEDED = T.let(
                  :rate_limit_exceeded,
                  OpenAI::Beta::Threads::Run::LastError::Code::TaggedSymbol
                )

              SERVER_ERROR = T.let(
                  :server_error,
                  OpenAI::Beta::Threads::Run::LastError::Code::TaggedSymbol
                )

              TaggedSymbol = T.type_alias do
                  T.all(Symbol, OpenAI::Beta::Threads::Run::LastError::Code)
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Run::LastError,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(OpenAI::Beta::Threads::Run, OpenAI::Internal::AnyHash)
            end

          class RequiredAction < OpenAI::Internal::Type::BaseModel
            # Details on the tool outputs needed for this run to continue.
            sig { returns(OpenAI::Beta::Threads::Run::RequiredAction::SubmitToolOutputs) }
            attr_reader :submit_tool_outputs

            sig do
              params(
                submit_tool_outputs: OpenAI::Beta::Threads::Run::RequiredAction::SubmitToolOutputs::OrHash
              ).void
            end
            attr_writer :submit_tool_outputs

            # For now, this is always `submit_tool_outputs`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  submit_tool_outputs:
                    OpenAI::Beta::Threads::Run::RequiredAction::SubmitToolOutputs,
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              # Details on the action required to continue the run. Will be `null` if no action
              # is required.
              sig do
                params(
                  submit_tool_outputs: OpenAI::Beta::Threads::Run::RequiredAction::SubmitToolOutputs::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                submit_tool_outputs:, # Details on the tool outputs needed for this run to continue.
                type: :submit_tool_outputs # For now, this is always `submit_tool_outputs`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Run::RequiredAction,
                  OpenAI::Internal::AnyHash
                )
              end

            class SubmitToolOutputs < OpenAI::Internal::Type::BaseModel
              # A list of the relevant tool calls.
              sig do
                returns(T::Array[
                    OpenAI::Beta::Threads::RequiredActionFunctionToolCall
                  ])
              end
              attr_accessor :tool_calls

              sig do
                override
                  .returns({
                    tool_calls:
                      T::Array[
                        OpenAI::Beta::Threads::RequiredActionFunctionToolCall
                      ]
                  })
              end
              def to_hash; end

              class << self
                # Details on the tool outputs needed for this run to continue.
                sig do
                  params(
                    tool_calls: T::Array[
                      OpenAI::Beta::Threads::RequiredActionFunctionToolCall::OrHash
                    ]
                  ).returns(T.attached_class)
                end
                def new(
                  tool_calls: # A list of the relevant tool calls.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Run::RequiredAction::SubmitToolOutputs,
                    OpenAI::Internal::AnyHash
                  )
                end
            end
          end

          class TruncationStrategy < OpenAI::Internal::Type::BaseModel
            # The number of most recent messages from the thread when constructing the context
            # for the run.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :last_messages

            # The truncation strategy to use for the thread. The default is `auto`. If set to
            # `last_messages`, the thread will be truncated to the n most recent messages in
            # the thread. When set to `auto`, messages in the middle of the thread will be
            # dropped to fit the context length of the model, `max_prompt_tokens`.
            sig { returns(OpenAI::Beta::Threads::Run::TruncationStrategy::Type::TaggedSymbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  type:
                    OpenAI::Beta::Threads::Run::TruncationStrategy::Type::TaggedSymbol,
                  last_messages: T.nilable(Integer)
                })
            end
            def to_hash; end

            class << self
              # Controls for how a thread will be truncated prior to the run. Use this to
              # control the intial context window of the run.
              sig do
                params(
                  type: OpenAI::Beta::Threads::Run::TruncationStrategy::Type::OrSymbol,
                  last_messages: T.nilable(Integer)
                ).returns(T.attached_class)
              end
              def new(
                type:, # The truncation strategy to use for the thread. The default is `auto`. If set to
                       # `last_messages`, the thread will be truncated to the n most recent messages in
                       # the thread. When set to `auto`, messages in the middle of the thread will be
                       # dropped to fit the context length of the model, `max_prompt_tokens`.
                last_messages: nil # The number of most recent messages from the thread when constructing the context
                                   # for the run.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Run::TruncationStrategy,
                  OpenAI::Internal::AnyHash
                )
              end

            # The truncation strategy to use for the thread. The default is `auto`. If set to
            # `last_messages`, the thread will be truncated to the n most recent messages in
            # the thread. When set to `auto`, messages in the middle of the thread will be
            # dropped to fit the context length of the model, `max_prompt_tokens`.
            module Type
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Run::TruncationStrategy::Type::TaggedSymbol
                  ])
                end
                def values; end
              end

              AUTO = T.let(
                  :auto,
                  OpenAI::Beta::Threads::Run::TruncationStrategy::Type::TaggedSymbol
                )

              LAST_MESSAGES = T.let(
                  :last_messages,
                  OpenAI::Beta::Threads::Run::TruncationStrategy::Type::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Beta::Threads::Run::TruncationStrategy::Type
                  )
                end
            end
          end

          class Usage < OpenAI::Internal::Type::BaseModel
            # Number of completion tokens used over the course of the run.
            sig { returns(Integer) }
            attr_accessor :completion_tokens

            # Number of prompt tokens used over the course of the run.
            sig { returns(Integer) }
            attr_accessor :prompt_tokens

            # Total number of tokens used (prompt + completion).
            sig { returns(Integer) }
            attr_accessor :total_tokens

            sig do
              override
                .returns({
                  completion_tokens: Integer,
                  prompt_tokens: Integer,
                  total_tokens: Integer
                })
            end
            def to_hash; end

            class << self
              # Usage statistics related to the run. This value will be `null` if the run is not
              # in a terminal state (i.e. `in_progress`, `queued`, etc.).
              sig do
                params(
                  completion_tokens: Integer,
                  prompt_tokens: Integer,
                  total_tokens: Integer
                ).returns(T.attached_class)
              end
              def new(
                completion_tokens:, # Number of completion tokens used over the course of the run.
                prompt_tokens:, # Number of prompt tokens used over the course of the run.
                total_tokens: # Total number of tokens used (prompt + completion).
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Run::Usage,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        class RunCancelParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          sig { returns(String) }
          attr_accessor :thread_id

          sig { override.returns({ thread_id: String, request_options: OpenAI::RequestOptions }) }
          def to_hash; end

          class << self
            sig do
              params(
                thread_id: String,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(thread_id:, request_options: {}); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::RunCancelParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class RunCreateParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # Appends additional instructions at the end of the instructions for the run. This
          # is useful for modifying the behavior on a per-run basis without overriding other
          # instructions.
          sig { returns(T.nilable(String)) }
          attr_accessor :additional_instructions

          # Adds additional messages to the thread before creating the run.
          sig do
            returns(T.nilable(
                T::Array[
                  OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage
                ]
              ))
          end
          attr_accessor :additional_messages

          # The ID of the
          # [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to
          # execute this run.
          sig { returns(String) }
          attr_accessor :assistant_id

          # A list of additional fields to include in the response. Currently the only
          # supported value is `step_details.tool_calls[*].file_search.results[*].content`
          # to fetch the file search result content.
          #
          # See the
          # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
          # for more information.
          sig do
            returns(T.nilable(
                T::Array[OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol]
              ))
          end
          attr_reader :include

          sig { params(include: T::Array[OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol]).void }
          attr_writer :include

          # Overrides the
          # [instructions](https://platform.openai.com/docs/api-reference/assistants/createAssistant)
          # of the assistant. This is useful for modifying the behavior on a per-run basis.
          sig { returns(T.nilable(String)) }
          attr_accessor :instructions

          # The maximum number of completion tokens that may be used over the course of the
          # run. The run will make a best effort to use only the number of completion tokens
          # specified, across multiple turns of the run. If the run exceeds the number of
          # completion tokens specified, the run will end with status `incomplete`. See
          # `incomplete_details` for more info.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :max_completion_tokens

          # The maximum number of prompt tokens that may be used over the course of the run.
          # The run will make a best effort to use only the number of prompt tokens
          # specified, across multiple turns of the run. If the run exceeds the number of
          # prompt tokens specified, the run will end with status `incomplete`. See
          # `incomplete_details` for more info.
          sig { returns(T.nilable(Integer)) }
          attr_accessor :max_prompt_tokens

          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to
          # be used to execute this run. If a value is provided here, it will override the
          # model associated with the assistant. If not, the model associated with the
          # assistant will be used.
          sig { returns(T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol))) }
          attr_accessor :model

          # Whether to enable
          # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
          # during tool use.
          sig { returns(T.nilable(T::Boolean)) }
          attr_reader :parallel_tool_calls

          sig { params(parallel_tool_calls: T::Boolean).void }
          attr_writer :parallel_tool_calls

          # **o-series models only**
          #
          # Constrains effort on reasoning for
          # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
          # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
          # result in faster responses and fewer tokens used on reasoning in a response.
          sig { returns(T.nilable(OpenAI::ReasoningEffort::OrSymbol)) }
          attr_accessor :reasoning_effort

          # Specifies the format that the model must output. Compatible with
          # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
          # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
          # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
          #
          # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
          # Outputs which ensures the model will match your supplied JSON schema. Learn more
          # in the
          # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
          #
          # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
          # message the model generates is valid JSON.
          #
          # **Important:** when using JSON mode, you **must** also instruct the model to
          # produce JSON yourself via a system or user message. Without this, the model may
          # generate an unending stream of whitespace until the generation reaches the token
          # limit, resulting in a long-running and seemingly "stuck" request. Also note that
          # the message content may be partially cut off if `finish_reason="length"`, which
          # indicates the generation exceeded `max_tokens` or the conversation exceeded the
          # max context length.
          sig do
            returns(T.nilable(
                T.any(
                  Symbol,
                  OpenAI::ResponseFormatText,
                  OpenAI::ResponseFormatJSONObject,
                  OpenAI::ResponseFormatJSONSchema
                )
              ))
          end
          attr_accessor :response_format

          # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
          # make the output more random, while lower values like 0.2 will make it more
          # focused and deterministic.
          sig { returns(T.nilable(Float)) }
          attr_accessor :temperature

          # Controls which (if any) tool is called by the model. `none` means the model will
          # not call any tools and instead generates a message. `auto` is the default value
          # and means the model can pick between generating a message or calling one or more
          # tools. `required` means the model must call one or more tools before responding
          # to the user. Specifying a particular tool like `{"type": "file_search"}` or
          # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
          # call that tool.
          sig do
            returns(T.nilable(
                T.any(
                  OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                  OpenAI::Beta::AssistantToolChoice
                )
              ))
          end
          attr_accessor :tool_choice

          # Override the tools the assistant can use for this run. This is useful for
          # modifying the behavior on a per-run basis.
          sig do
            returns(T.nilable(
                T::Array[
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool,
                    OpenAI::Beta::FileSearchTool,
                    OpenAI::Beta::FunctionTool
                  )
                ]
              ))
          end
          attr_accessor :tools

          # An alternative to sampling with temperature, called nucleus sampling, where the
          # model considers the results of the tokens with top_p probability mass. So 0.1
          # means only the tokens comprising the top 10% probability mass are considered.
          #
          # We generally recommend altering this or temperature but not both.
          sig { returns(T.nilable(Float)) }
          attr_accessor :top_p

          # Controls for how a thread will be truncated prior to the run. Use this to
          # control the intial context window of the run.
          sig do
            returns(T.nilable(
                OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy
              ))
          end
          attr_reader :truncation_strategy

          sig do
            params(
              truncation_strategy: T.nilable(
                  OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::OrHash
                )
            ).void
          end
          attr_writer :truncation_strategy

          sig do
            override
              .returns({
                assistant_id: String,
                include:
                  T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                  ],
                additional_instructions: T.nilable(String),
                additional_messages:
                  T.nilable(
                    T::Array[
                      OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage
                    ]
                  ),
                instructions: T.nilable(String),
                max_completion_tokens: T.nilable(Integer),
                max_prompt_tokens: T.nilable(Integer),
                metadata: T.nilable(T::Hash[Symbol, String]),
                model: T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol)),
                parallel_tool_calls: T::Boolean,
                reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
                response_format:
                  T.nilable(
                    T.any(
                      Symbol,
                      OpenAI::ResponseFormatText,
                      OpenAI::ResponseFormatJSONObject,
                      OpenAI::ResponseFormatJSONSchema
                    )
                  ),
                temperature: T.nilable(Float),
                tool_choice:
                  T.nilable(
                    T.any(
                      OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                      OpenAI::Beta::AssistantToolChoice
                    )
                  ),
                tools:
                  T.nilable(
                    T::Array[
                      T.any(
                        OpenAI::Beta::CodeInterpreterTool,
                        OpenAI::Beta::FileSearchTool,
                        OpenAI::Beta::FunctionTool
                      )
                    ]
                  ),
                top_p: T.nilable(Float),
                truncation_strategy:
                  T.nilable(
                    OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy
                  ),
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                assistant_id: String,
                include: T::Array[OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol],
                additional_instructions: T.nilable(String),
                additional_messages: T.nilable(
                  T::Array[
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::OrHash
                  ]
                ),
                instructions: T.nilable(String),
                max_completion_tokens: T.nilable(Integer),
                max_prompt_tokens: T.nilable(Integer),
                metadata: T.nilable(T::Hash[Symbol, String]),
                model: T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol)),
                parallel_tool_calls: T::Boolean,
                reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
                response_format: T.nilable(
                  T.any(
                    Symbol,
                    OpenAI::ResponseFormatText::OrHash,
                    OpenAI::ResponseFormatJSONObject::OrHash,
                    OpenAI::ResponseFormatJSONSchema::OrHash
                  )
                ),
                temperature: T.nilable(Float),
                tool_choice: T.nilable(
                  T.any(
                    OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                    OpenAI::Beta::AssistantToolChoice::OrHash
                  )
                ),
                tools: T.nilable(
                  T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool::OrHash,
                      OpenAI::Beta::FileSearchTool::OrHash,
                      OpenAI::Beta::FunctionTool::OrHash
                    )
                  ]
                ),
                top_p: T.nilable(Float),
                truncation_strategy: T.nilable(
                  OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::OrHash
                ),
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              assistant_id:, # The ID of the
                             # [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to
                             # execute this run.
              include: nil, # A list of additional fields to include in the response. Currently the only
                            # supported value is `step_details.tool_calls[*].file_search.results[*].content`
                            # to fetch the file search result content.
                            # See the
                            # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
                            # for more information.
              additional_instructions: nil, # Appends additional instructions at the end of the instructions for the run. This
                                            # is useful for modifying the behavior on a per-run basis without overriding other
                                            # instructions.
              additional_messages: nil, # Adds additional messages to the thread before creating the run.
              instructions: nil, # Overrides the
                                 # [instructions](https://platform.openai.com/docs/api-reference/assistants/createAssistant)
                                 # of the assistant. This is useful for modifying the behavior on a per-run basis.
              max_completion_tokens: nil, # The maximum number of completion tokens that may be used over the course of the
                                          # run. The run will make a best effort to use only the number of completion tokens
                                          # specified, across multiple turns of the run. If the run exceeds the number of
                                          # completion tokens specified, the run will end with status `incomplete`. See
                                          # `incomplete_details` for more info.
              max_prompt_tokens: nil, # The maximum number of prompt tokens that may be used over the course of the run.
                                      # The run will make a best effort to use only the number of prompt tokens
                                      # specified, across multiple turns of the run. If the run exceeds the number of
                                      # prompt tokens specified, the run will end with status `incomplete`. See
                                      # `incomplete_details` for more info.
              metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard.
                             # Keys are strings with a maximum length of 64 characters. Values are strings with
                             # a maximum length of 512 characters.
              model: nil, # The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to
                          # be used to execute this run. If a value is provided here, it will override the
                          # model associated with the assistant. If not, the model associated with the
                          # assistant will be used.
              parallel_tool_calls: nil, # Whether to enable
                                        # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                        # during tool use.
              reasoning_effort: nil, # **o-series models only**
                                     # Constrains effort on reasoning for
                                     # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                     # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                     # result in faster responses and fewer tokens used on reasoning in a response.
              response_format: nil, # Specifies the format that the model must output. Compatible with
                                    # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                    # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                    # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                    # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                    # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                    # in the
                                    # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                    # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                    # message the model generates is valid JSON.
                                    # **Important:** when using JSON mode, you **must** also instruct the model to
                                    # produce JSON yourself via a system or user message. Without this, the model may
                                    # generate an unending stream of whitespace until the generation reaches the token
                                    # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                    # the message content may be partially cut off if `finish_reason="length"`, which
                                    # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                    # max context length.
              temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                                # make the output more random, while lower values like 0.2 will make it more
                                # focused and deterministic.
              tool_choice: nil, # Controls which (if any) tool is called by the model. `none` means the model will
                                # not call any tools and instead generates a message. `auto` is the default value
                                # and means the model can pick between generating a message or calling one or more
                                # tools. `required` means the model must call one or more tools before responding
                                # to the user. Specifying a particular tool like `{"type": "file_search"}` or
                                # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                                # call that tool.
              tools: nil, # Override the tools the assistant can use for this run. This is useful for
                          # modifying the behavior on a per-run basis.
              top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                          # model considers the results of the tokens with top_p probability mass. So 0.1
                          # means only the tokens comprising the top 10% probability mass are considered.
                          # We generally recommend altering this or temperature but not both.
              truncation_strategy: nil, # Controls for how a thread will be truncated prior to the run. Use this to
                                        # control the intial context window of the run.
              request_options: {}
); end
          end

          class AdditionalMessage < OpenAI::Internal::Type::BaseModel
            # A list of files attached to the message, and the tools they should be added to.
            sig do
              returns(T.nilable(
                  T::Array[
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment
                  ]
                ))
            end
            attr_accessor :attachments

            # The text contents of the message.
            sig { returns(OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Content::Variants) }
            attr_accessor :content

            # Set of 16 key-value pairs that can be attached to an object. This can be useful
            # for storing additional information about the object in a structured format, and
            # querying for objects via API or the dashboard.
            #
            # Keys are strings with a maximum length of 64 characters. Values are strings with
            # a maximum length of 512 characters.
            sig { returns(T.nilable(T::Hash[Symbol, String])) }
            attr_accessor :metadata

            # The role of the entity that is creating the message. Allowed values include:
            #
            # - `user`: Indicates the message is sent by an actual user and should be used in
            #   most cases to represent user-generated messages.
            # - `assistant`: Indicates the message is generated by the assistant. Use this
            #   value to insert messages from the assistant into the conversation.
            sig { returns(OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Role::OrSymbol) }
            attr_accessor :role

            sig do
              override
                .returns({
                  content:
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Content::Variants,
                  role:
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Role::OrSymbol,
                  attachments:
                    T.nilable(
                      T::Array[
                        OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment
                      ]
                    ),
                  metadata: T.nilable(T::Hash[Symbol, String])
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  content: OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Content::Variants,
                  role: OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Role::OrSymbol,
                  attachments: T.nilable(
                    T::Array[
                      OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment::OrHash
                    ]
                  ),
                  metadata: T.nilable(T::Hash[Symbol, String])
                ).returns(T.attached_class)
              end
              def new(
                content:, # The text contents of the message.
                role:, # The role of the entity that is creating the message. Allowed values include:
                       # - `user`: Indicates the message is sent by an actual user and should be used in
                       #   most cases to represent user-generated messages.
                       # - `assistant`: Indicates the message is generated by the assistant. Use this
                       #   value to insert messages from the assistant into the conversation.
                attachments: nil, # A list of files attached to the message, and the tools they should be added to.
                metadata: nil # Set of 16 key-value pairs that can be attached to an object. This can be useful
                              # for storing additional information about the object in a structured format, and
                              # querying for objects via API or the dashboard.
                              # Keys are strings with a maximum length of 64 characters. Values are strings with
                              # a maximum length of 512 characters.
); end
            end

            class Attachment < OpenAI::Internal::Type::BaseModel
              # The ID of the file to attach to the message.
              sig { returns(T.nilable(String)) }
              attr_reader :file_id

              sig { params(file_id: String).void }
              attr_writer :file_id

              # The tools to add this file to.
              sig do
                returns(T.nilable(
                    T::Array[
                      T.any(
                        OpenAI::Beta::CodeInterpreterTool,
                        OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment::Tool::FileSearch
                      )
                    ]
                  ))
              end
              attr_reader :tools

              sig do
                params(
                  tools: T::Array[
                      T.any(
                        OpenAI::Beta::CodeInterpreterTool::OrHash,
                        OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment::Tool::FileSearch::OrHash
                      )
                    ]
                ).void
              end
              attr_writer :tools

              sig do
                override
                  .returns({
                    file_id: String,
                    tools:
                      T::Array[
                        T.any(
                          OpenAI::Beta::CodeInterpreterTool,
                          OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment::Tool::FileSearch
                        )
                      ]
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    file_id: String,
                    tools: T::Array[
                      T.any(
                        OpenAI::Beta::CodeInterpreterTool::OrHash,
                        OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment::Tool::FileSearch::OrHash
                      )
                    ]
                  ).returns(T.attached_class)
                end
                def new(
                  file_id: nil, # The ID of the file to attach to the message.
                  tools: nil # The tools to add this file to.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment,
                    OpenAI::Internal::AnyHash
                  )
                end

              module Tool
                extend OpenAI::Internal::Type::Union

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment::Tool::Variants
                    ])
                  end
                  def variants; end
                end

                class FileSearch < OpenAI::Internal::Type::BaseModel
                  # The type of tool being defined: `file_search`
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig { override.returns({ type: Symbol }) }
                  def to_hash; end

                  class << self
                    sig { params(type: Symbol).returns(T.attached_class) }
                    def new(
                      type: :file_search # The type of tool being defined: `file_search`
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment::Tool::FileSearch,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                Variants = T.type_alias do
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool,
                      OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Attachment::Tool::FileSearch
                    )
                  end
              end
            end

            # The text contents of the message.
            module Content
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Content::Variants
                  ])
                end
                def variants; end
              end

              MessageContentPartParamArray = T.let(
                  OpenAI::Internal::Type::ArrayOf[
                    union: OpenAI::Beta::Threads::MessageContentPartParam
                  ],
                  OpenAI::Internal::Type::Converter
                )

              Variants = T.type_alias do
                  T.any(
                    String,
                    T::Array[
                      OpenAI::Beta::Threads::MessageContentPartParam::Variants
                    ]
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage,
                  OpenAI::Internal::AnyHash
                )
              end

            # The role of the entity that is creating the message. Allowed values include:
            #
            # - `user`: Indicates the message is sent by an actual user and should be used in
            #   most cases to represent user-generated messages.
            # - `assistant`: Indicates the message is generated by the assistant. Use this
            #   value to insert messages from the assistant into the conversation.
            module Role
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Role::TaggedSymbol
                  ])
                end
                def values; end
              end

              ASSISTANT = T.let(
                  :assistant,
                  OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Role::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Role
                  )
                end

              USER = T.let(
                  :user,
                  OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::Role::TaggedSymbol
                )
            end
          end

          # The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to
          # be used to execute this run. If a value is provided here, it will override the
          # model associated with the assistant. If not, the model associated with the
          # assistant will be used.
          module Model
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::Threads::RunCreateParams::Model::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias { T.any(String, OpenAI::ChatModel::TaggedSymbol) }
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::RunCreateParams,
                OpenAI::Internal::AnyHash
              )
            end

          class TruncationStrategy < OpenAI::Internal::Type::BaseModel
            # The number of most recent messages from the thread when constructing the context
            # for the run.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :last_messages

            # The truncation strategy to use for the thread. The default is `auto`. If set to
            # `last_messages`, the thread will be truncated to the n most recent messages in
            # the thread. When set to `auto`, messages in the middle of the thread will be
            # dropped to fit the context length of the model, `max_prompt_tokens`.
            sig { returns(OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::Type::OrSymbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  type:
                    OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::Type::OrSymbol,
                  last_messages: T.nilable(Integer)
                })
            end
            def to_hash; end

            class << self
              # Controls for how a thread will be truncated prior to the run. Use this to
              # control the intial context window of the run.
              sig do
                params(
                  type: OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::Type::OrSymbol,
                  last_messages: T.nilable(Integer)
                ).returns(T.attached_class)
              end
              def new(
                type:, # The truncation strategy to use for the thread. The default is `auto`. If set to
                       # `last_messages`, the thread will be truncated to the n most recent messages in
                       # the thread. When set to `auto`, messages in the middle of the thread will be
                       # dropped to fit the context length of the model, `max_prompt_tokens`.
                last_messages: nil # The number of most recent messages from the thread when constructing the context
                                   # for the run.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy,
                  OpenAI::Internal::AnyHash
                )
              end

            # The truncation strategy to use for the thread. The default is `auto`. If set to
            # `last_messages`, the thread will be truncated to the n most recent messages in
            # the thread. When set to `auto`, messages in the middle of the thread will be
            # dropped to fit the context length of the model, `max_prompt_tokens`.
            module Type
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::Type::TaggedSymbol
                  ])
                end
                def values; end
              end

              AUTO = T.let(
                  :auto,
                  OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::Type::TaggedSymbol
                )

              LAST_MESSAGES = T.let(
                  :last_messages,
                  OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::Type::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::Type
                  )
                end
            end
          end
        end

        class RunListParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # A cursor for use in pagination. `after` is an object ID that defines your place
          # in the list. For instance, if you make a list request and receive 100 objects,
          # ending with obj_foo, your subsequent call can include after=obj_foo in order to
          # fetch the next page of the list.
          sig { returns(T.nilable(String)) }
          attr_reader :after

          sig { params(after: String).void }
          attr_writer :after

          # A cursor for use in pagination. `before` is an object ID that defines your place
          # in the list. For instance, if you make a list request and receive 100 objects,
          # starting with obj_foo, your subsequent call can include before=obj_foo in order
          # to fetch the previous page of the list.
          sig { returns(T.nilable(String)) }
          attr_reader :before

          sig { params(before: String).void }
          attr_writer :before

          # A limit on the number of objects to be returned. Limit can range between 1 and
          # 100, and the default is 20.
          sig { returns(T.nilable(Integer)) }
          attr_reader :limit

          sig { params(limit: Integer).void }
          attr_writer :limit

          # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
          # order and `desc` for descending order.
          sig { returns(T.nilable(OpenAI::Beta::Threads::RunListParams::Order::OrSymbol)) }
          attr_reader :order

          sig { params(order: OpenAI::Beta::Threads::RunListParams::Order::OrSymbol).void }
          attr_writer :order

          sig do
            override
              .returns({
                after: String,
                before: String,
                limit: Integer,
                order: OpenAI::Beta::Threads::RunListParams::Order::OrSymbol,
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                after: String,
                before: String,
                limit: Integer,
                order: OpenAI::Beta::Threads::RunListParams::Order::OrSymbol,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                          # in the list. For instance, if you make a list request and receive 100 objects,
                          # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                          # fetch the next page of the list.
              before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                           # in the list. For instance, if you make a list request and receive 100 objects,
                           # starting with obj_foo, your subsequent call can include before=obj_foo in order
                           # to fetch the previous page of the list.
              limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                          # 100, and the default is 20.
              order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                          # order and `desc` for descending order.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::RunListParams,
                OpenAI::Internal::AnyHash
              )
            end

          # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
          # order and `desc` for descending order.
          module Order
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::Threads::RunListParams::Order::TaggedSymbol
                ])
              end
              def values; end
            end

            ASC = T.let(
                :asc,
                OpenAI::Beta::Threads::RunListParams::Order::TaggedSymbol
              )

            DESC = T.let(
                :desc,
                OpenAI::Beta::Threads::RunListParams::Order::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::RunListParams::Order)
              end
          end
        end

        class RunRetrieveParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          sig { returns(String) }
          attr_accessor :thread_id

          sig { override.returns({ thread_id: String, request_options: OpenAI::RequestOptions }) }
          def to_hash; end

          class << self
            sig do
              params(
                thread_id: String,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(thread_id:, request_options: {}); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::RunRetrieveParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The status of the run, which can be either `queued`, `in_progress`,
        # `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`,
        # `incomplete`, or `expired`.
        module RunStatus
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Beta::Threads::RunStatus::TaggedSymbol]) }
            def values; end
          end

          CANCELLED = T.let(:cancelled, OpenAI::Beta::Threads::RunStatus::TaggedSymbol)

          CANCELLING = T.let(:cancelling, OpenAI::Beta::Threads::RunStatus::TaggedSymbol)

          COMPLETED = T.let(:completed, OpenAI::Beta::Threads::RunStatus::TaggedSymbol)

          EXPIRED = T.let(:expired, OpenAI::Beta::Threads::RunStatus::TaggedSymbol)

          FAILED = T.let(:failed, OpenAI::Beta::Threads::RunStatus::TaggedSymbol)

          INCOMPLETE = T.let(:incomplete, OpenAI::Beta::Threads::RunStatus::TaggedSymbol)

          IN_PROGRESS = T.let(:in_progress, OpenAI::Beta::Threads::RunStatus::TaggedSymbol)

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          QUEUED = T.let(:queued, OpenAI::Beta::Threads::RunStatus::TaggedSymbol)

          REQUIRES_ACTION = T.let(
              :requires_action,
              OpenAI::Beta::Threads::RunStatus::TaggedSymbol
            )

          TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Beta::Threads::RunStatus) }
        end

        RunStep = Runs::RunStep
        RunStepDelta = Runs::RunStepDelta
        RunStepDeltaEvent = Runs::RunStepDeltaEvent
        RunStepDeltaMessageDelta = Runs::RunStepDeltaMessageDelta
        RunStepInclude = Runs::RunStepInclude

        class RunSubmitToolOutputsParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          sig { returns(String) }
          attr_accessor :thread_id

          # A list of tools for which the outputs are being submitted.
          sig do
            returns(T::Array[
                OpenAI::Beta::Threads::RunSubmitToolOutputsParams::ToolOutput
              ])
          end
          attr_accessor :tool_outputs

          sig do
            override
              .returns({
                thread_id: String,
                tool_outputs:
                  T::Array[
                    OpenAI::Beta::Threads::RunSubmitToolOutputsParams::ToolOutput
                  ],
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                thread_id: String,
                tool_outputs: T::Array[
                  OpenAI::Beta::Threads::RunSubmitToolOutputsParams::ToolOutput::OrHash
                ],
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              thread_id:,
              tool_outputs:, # A list of tools for which the outputs are being submitted.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::RunSubmitToolOutputsParams,
                OpenAI::Internal::AnyHash
              )
            end

          class ToolOutput < OpenAI::Internal::Type::BaseModel
            # The output of the tool call to be submitted to continue the run.
            sig { returns(T.nilable(String)) }
            attr_reader :output

            sig { params(output: String).void }
            attr_writer :output

            # The ID of the tool call in the `required_action` object within the run object
            # the output is being submitted for.
            sig { returns(T.nilable(String)) }
            attr_reader :tool_call_id

            sig { params(tool_call_id: String).void }
            attr_writer :tool_call_id

            sig { override.returns({ output: String, tool_call_id: String }) }
            def to_hash; end

            class << self
              sig { params(output: String, tool_call_id: String).returns(T.attached_class) }
              def new(
                output: nil, # The output of the tool call to be submitted to continue the run.
                tool_call_id: nil # The ID of the tool call in the `required_action` object within the run object
                                  # the output is being submitted for.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::RunSubmitToolOutputsParams::ToolOutput,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        class RunUpdateParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          sig { returns(String) }
          attr_accessor :thread_id

          sig do
            override
              .returns({
                thread_id: String,
                metadata: T.nilable(T::Hash[Symbol, String]),
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                thread_id: String,
                metadata: T.nilable(T::Hash[Symbol, String]),
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              thread_id:,
              metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard.
                             # Keys are strings with a maximum length of 64 characters. Values are strings with
                             # a maximum length of 512 characters.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::RunUpdateParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        module Runs
          class CodeInterpreterLogs < OpenAI::Internal::Type::BaseModel
            # The index of the output in the outputs array.
            sig { returns(Integer) }
            attr_accessor :index

            # The text output from the Code Interpreter tool call.
            sig { returns(T.nilable(String)) }
            attr_reader :logs

            sig { params(logs: String).void }
            attr_writer :logs

            # Always `logs`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ index: Integer, type: Symbol, logs: String }) }
            def to_hash; end

            class << self
              # Text output from the Code Interpreter tool call as part of a run step.
              sig { params(index: Integer, logs: String, type: Symbol).returns(T.attached_class) }
              def new(
                index:, # The index of the output in the outputs array.
                logs: nil, # The text output from the Code Interpreter tool call.
                type: :logs # Always `logs`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::CodeInterpreterLogs,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class CodeInterpreterOutputImage < OpenAI::Internal::Type::BaseModel
            sig do
              returns(T.nilable(
                  OpenAI::Beta::Threads::Runs::CodeInterpreterOutputImage::Image
                ))
            end
            attr_reader :image

            sig { params(image: OpenAI::Beta::Threads::Runs::CodeInterpreterOutputImage::Image::OrHash).void }
            attr_writer :image

            # The index of the output in the outputs array.
            sig { returns(Integer) }
            attr_accessor :index

            # Always `image`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  index: Integer,
                  type: Symbol,
                  image:
                    OpenAI::Beta::Threads::Runs::CodeInterpreterOutputImage::Image
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  index: Integer,
                  image: OpenAI::Beta::Threads::Runs::CodeInterpreterOutputImage::Image::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                index:, # The index of the output in the outputs array.
                image: nil,
                type: :image # Always `image`.
); end
            end

            class Image < OpenAI::Internal::Type::BaseModel
              # The [file](https://platform.openai.com/docs/api-reference/files) ID of the
              # image.
              sig { returns(T.nilable(String)) }
              attr_reader :file_id

              sig { params(file_id: String).void }
              attr_writer :file_id

              sig { override.returns({ file_id: String }) }
              def to_hash; end

              class << self
                sig { params(file_id: String).returns(T.attached_class) }
                def new(
                  file_id: nil # The [file](https://platform.openai.com/docs/api-reference/files) ID of the
                               # image.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::CodeInterpreterOutputImage::Image,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::CodeInterpreterOutputImage,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class CodeInterpreterToolCall < OpenAI::Internal::Type::BaseModel
            # The Code Interpreter tool call definition.
            sig { returns(OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter) }
            attr_reader :code_interpreter

            sig do
              params(
                code_interpreter: OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::OrHash
              ).void
            end
            attr_writer :code_interpreter

            # The ID of the tool call.
            sig { returns(String) }
            attr_accessor :id

            # The type of tool call. This is always going to be `code_interpreter` for this
            # type of tool call.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  id: String,
                  code_interpreter:
                    OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter,
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              # Details of the Code Interpreter tool call the run step was involved in.
              sig do
                params(
                  id: String,
                  code_interpreter: OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                id:, # The ID of the tool call.
                code_interpreter:, # The Code Interpreter tool call definition.
                type: :code_interpreter # The type of tool call. This is always going to be `code_interpreter` for this
                                        # type of tool call.
); end
            end

            class CodeInterpreter < OpenAI::Internal::Type::BaseModel
              # The input to the Code Interpreter tool call.
              sig { returns(String) }
              attr_accessor :input

              # The outputs from the Code Interpreter tool call. Code Interpreter can output one
              # or more items, including text (`logs`) or images (`image`). Each of these are
              # represented by a different object type.
              sig do
                returns(T::Array[
                    OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Variants
                  ])
              end
              attr_accessor :outputs

              sig do
                override
                  .returns({
                    input: String,
                    outputs:
                      T::Array[
                        OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Variants
                      ]
                  })
              end
              def to_hash; end

              class << self
                # The Code Interpreter tool call definition.
                sig do
                  params(
                    input: String,
                    outputs: T::Array[
                      T.any(
                        OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Logs::OrHash,
                        OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Image::OrHash
                      )
                    ]
                  ).returns(T.attached_class)
                end
                def new(
                  input:, # The input to the Code Interpreter tool call.
                  outputs: # The outputs from the Code Interpreter tool call. Code Interpreter can output one
                           # or more items, including text (`logs`) or images (`image`). Each of these are
                           # represented by a different object type.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter,
                    OpenAI::Internal::AnyHash
                  )
                end

              # Text output from the Code Interpreter tool call as part of a run step.
              module Output
                extend OpenAI::Internal::Type::Union

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Variants
                    ])
                  end
                  def variants; end
                end

                class Image < OpenAI::Internal::Type::BaseModel
                  sig do
                    returns(OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Image::Image)
                  end
                  attr_reader :image

                  sig do
                    params(
                      image: OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Image::Image::OrHash
                    ).void
                  end
                  attr_writer :image

                  # Always `image`.
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig do
                    override
                      .returns({
                        image:
                          OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Image::Image,
                        type: Symbol
                      })
                  end
                  def to_hash; end

                  class << self
                    sig do
                      params(
                        image: OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Image::Image::OrHash,
                        type: Symbol
                      ).returns(T.attached_class)
                    end
                    def new(
                      image:,
                      type: :image # Always `image`.
); end
                  end

                  class Image < OpenAI::Internal::Type::BaseModel
                    # The [file](https://platform.openai.com/docs/api-reference/files) ID of the
                    # image.
                    sig { returns(String) }
                    attr_accessor :file_id

                    sig { override.returns({ file_id: String }) }
                    def to_hash; end

                    class << self
                      sig { params(file_id: String).returns(T.attached_class) }
                      def new(
                        file_id: # The [file](https://platform.openai.com/docs/api-reference/files) ID of the
                                 # image.
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Image::Image,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Image,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                class Logs < OpenAI::Internal::Type::BaseModel
                  # The text output from the Code Interpreter tool call.
                  sig { returns(String) }
                  attr_accessor :logs

                  # Always `logs`.
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig { override.returns({ logs: String, type: Symbol }) }
                  def to_hash; end

                  class << self
                    # Text output from the Code Interpreter tool call as part of a run step.
                    sig { params(logs: String, type: Symbol).returns(T.attached_class) }
                    def new(
                      logs:, # The text output from the Code Interpreter tool call.
                      type: :logs # Always `logs`.
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Logs,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                Variants = T.type_alias do
                    T.any(
                      OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Logs,
                      OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::CodeInterpreter::Output::Image
                    )
                  end
              end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class CodeInterpreterToolCallDelta < OpenAI::Internal::Type::BaseModel
            # The Code Interpreter tool call definition.
            sig do
              returns(T.nilable(
                  OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::CodeInterpreter
                ))
            end
            attr_reader :code_interpreter

            sig do
              params(
                code_interpreter: OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::CodeInterpreter::OrHash
              ).void
            end
            attr_writer :code_interpreter

            # The ID of the tool call.
            sig { returns(T.nilable(String)) }
            attr_reader :id

            sig { params(id: String).void }
            attr_writer :id

            # The index of the tool call in the tool calls array.
            sig { returns(Integer) }
            attr_accessor :index

            # The type of tool call. This is always going to be `code_interpreter` for this
            # type of tool call.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  index: Integer,
                  type: Symbol,
                  id: String,
                  code_interpreter:
                    OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::CodeInterpreter
                })
            end
            def to_hash; end

            class << self
              # Details of the Code Interpreter tool call the run step was involved in.
              sig do
                params(
                  index: Integer,
                  id: String,
                  code_interpreter: OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::CodeInterpreter::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                index:, # The index of the tool call in the tool calls array.
                id: nil, # The ID of the tool call.
                code_interpreter: nil, # The Code Interpreter tool call definition.
                type: :code_interpreter # The type of tool call. This is always going to be `code_interpreter` for this
                                        # type of tool call.
); end
            end

            class CodeInterpreter < OpenAI::Internal::Type::BaseModel
              # The input to the Code Interpreter tool call.
              sig { returns(T.nilable(String)) }
              attr_reader :input

              sig { params(input: String).void }
              attr_writer :input

              # The outputs from the Code Interpreter tool call. Code Interpreter can output one
              # or more items, including text (`logs`) or images (`image`). Each of these are
              # represented by a different object type.
              sig do
                returns(T.nilable(
                    T::Array[
                      OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::CodeInterpreter::Output::Variants
                    ]
                  ))
              end
              attr_reader :outputs

              sig do
                params(
                  outputs: T::Array[
                      T.any(
                        OpenAI::Beta::Threads::Runs::CodeInterpreterLogs::OrHash,
                        OpenAI::Beta::Threads::Runs::CodeInterpreterOutputImage::OrHash
                      )
                    ]
                ).void
              end
              attr_writer :outputs

              sig do
                override
                  .returns({
                    input: String,
                    outputs:
                      T::Array[
                        OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::CodeInterpreter::Output::Variants
                      ]
                  })
              end
              def to_hash; end

              class << self
                # The Code Interpreter tool call definition.
                sig do
                  params(
                    input: String,
                    outputs: T::Array[
                      T.any(
                        OpenAI::Beta::Threads::Runs::CodeInterpreterLogs::OrHash,
                        OpenAI::Beta::Threads::Runs::CodeInterpreterOutputImage::OrHash
                      )
                    ]
                  ).returns(T.attached_class)
                end
                def new(
                  input: nil, # The input to the Code Interpreter tool call.
                  outputs: nil # The outputs from the Code Interpreter tool call. Code Interpreter can output one
                               # or more items, including text (`logs`) or images (`image`). Each of these are
                               # represented by a different object type.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::CodeInterpreter,
                    OpenAI::Internal::AnyHash
                  )
                end

              # Text output from the Code Interpreter tool call as part of a run step.
              module Output
                extend OpenAI::Internal::Type::Union

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::CodeInterpreter::Output::Variants
                    ])
                  end
                  def variants; end
                end

                Variants = T.type_alias do
                    T.any(
                      OpenAI::Beta::Threads::Runs::CodeInterpreterLogs,
                      OpenAI::Beta::Threads::Runs::CodeInterpreterOutputImage
                    )
                  end
              end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileSearchToolCall < OpenAI::Internal::Type::BaseModel
            # For now, this is always going to be an empty object.
            sig { returns(OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch) }
            attr_reader :file_search

            sig { params(file_search: OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::OrHash).void }
            attr_writer :file_search

            # The ID of the tool call object.
            sig { returns(String) }
            attr_accessor :id

            # The type of tool call. This is always going to be `file_search` for this type of
            # tool call.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  id: String,
                  file_search:
                    OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch,
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  id: String,
                  file_search: OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                id:, # The ID of the tool call object.
                file_search:, # For now, this is always going to be an empty object.
                type: :file_search # The type of tool call. This is always going to be `file_search` for this type of
                                   # tool call.
); end
            end

            class FileSearch < OpenAI::Internal::Type::BaseModel
              # The ranking options for the file search.
              sig do
                returns(T.nilable(
                    OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions
                  ))
              end
              attr_reader :ranking_options

              sig do
                params(
                  ranking_options: OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions::OrHash
                ).void
              end
              attr_writer :ranking_options

              # The results of the file search.
              sig do
                returns(T.nilable(
                    T::Array[
                      OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result
                    ]
                  ))
              end
              attr_reader :results

              sig do
                params(
                  results: T::Array[
                      OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::OrHash
                    ]
                ).void
              end
              attr_writer :results

              sig do
                override
                  .returns({
                    ranking_options:
                      OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions,
                    results:
                      T::Array[
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result
                      ]
                  })
              end
              def to_hash; end

              class << self
                # For now, this is always going to be an empty object.
                sig do
                  params(
                    ranking_options: OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions::OrHash,
                    results: T::Array[
                      OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::OrHash
                    ]
                  ).returns(T.attached_class)
                end
                def new(
                  ranking_options: nil, # The ranking options for the file search.
                  results: nil # The results of the file search.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch,
                    OpenAI::Internal::AnyHash
                  )
                end

              class RankingOptions < OpenAI::Internal::Type::BaseModel
                # The ranker to use for the file search. If not specified will use the `auto`
                # ranker.
                sig do
                  returns(OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions::Ranker::TaggedSymbol)
                end
                attr_accessor :ranker

                # The score threshold for the file search. All values must be a floating point
                # number between 0 and 1.
                sig { returns(Float) }
                attr_accessor :score_threshold

                sig do
                  override
                    .returns({
                      ranker:
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions::Ranker::TaggedSymbol,
                      score_threshold: Float
                    })
                end
                def to_hash; end

                class << self
                  # The ranking options for the file search.
                  sig do
                    params(
                      ranker: OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions::Ranker::OrSymbol,
                      score_threshold: Float
                    ).returns(T.attached_class)
                  end
                  def new(
                    ranker:, # The ranker to use for the file search. If not specified will use the `auto`
                             # ranker.
                    score_threshold: # The score threshold for the file search. All values must be a floating point
                                     # number between 0 and 1.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions,
                      OpenAI::Internal::AnyHash
                    )
                  end

                # The ranker to use for the file search. If not specified will use the `auto`
                # ranker.
                module Ranker
                  extend OpenAI::Internal::Type::Enum

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions::Ranker::TaggedSymbol
                      ])
                    end
                    def values; end
                  end

                  AUTO = T.let(
                      :auto,
                      OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions::Ranker::TaggedSymbol
                    )

                  DEFAULT_2024_08_21 = T.let(
                      :default_2024_08_21,
                      OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions::Ranker::TaggedSymbol
                    )

                  OrSymbol = T.type_alias { T.any(Symbol, String) }

                  TaggedSymbol = T.type_alias do
                      T.all(
                        Symbol,
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::RankingOptions::Ranker
                      )
                    end
                end
              end

              class Result < OpenAI::Internal::Type::BaseModel
                # The content of the result that was found. The content is only included if
                # requested via the include query parameter.
                sig do
                  returns(T.nilable(
                      T::Array[
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content
                      ]
                    ))
                end
                attr_reader :content

                sig do
                  params(
                    content: T::Array[
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content::OrHash
                      ]
                  ).void
                end
                attr_writer :content

                # The ID of the file that result was found in.
                sig { returns(String) }
                attr_accessor :file_id

                # The name of the file that result was found in.
                sig { returns(String) }
                attr_accessor :file_name

                # The score of the result. All values must be a floating point number between 0
                # and 1.
                sig { returns(Float) }
                attr_accessor :score

                sig do
                  override
                    .returns({
                      file_id: String,
                      file_name: String,
                      score: Float,
                      content:
                        T::Array[
                          OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content
                        ]
                    })
                end
                def to_hash; end

                class << self
                  # A result instance of the file search.
                  sig do
                    params(
                      file_id: String,
                      file_name: String,
                      score: Float,
                      content: T::Array[
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content::OrHash
                      ]
                    ).returns(T.attached_class)
                  end
                  def new(
                    file_id:, # The ID of the file that result was found in.
                    file_name:, # The name of the file that result was found in.
                    score:, # The score of the result. All values must be a floating point number between 0
                            # and 1.
                    content: nil # The content of the result that was found. The content is only included if
                                 # requested via the include query parameter.
); end
                end

                class Content < OpenAI::Internal::Type::BaseModel
                  # The text content of the file.
                  sig { returns(T.nilable(String)) }
                  attr_reader :text

                  sig { params(text: String).void }
                  attr_writer :text

                  # The type of the content.
                  sig do
                    returns(T.nilable(
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content::Type::TaggedSymbol
                      ))
                  end
                  attr_reader :type

                  sig do
                    params(
                      type: OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content::Type::OrSymbol
                    ).void
                  end
                  attr_writer :type

                  sig do
                    override
                      .returns({
                        text: String,
                        type:
                          OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content::Type::TaggedSymbol
                      })
                  end
                  def to_hash; end

                  class << self
                    sig do
                      params(
                        text: String,
                        type: OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content::Type::OrSymbol
                      ).returns(T.attached_class)
                    end
                    def new(
                      text: nil, # The text content of the file.
                      type: nil # The type of the content.
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content,
                        OpenAI::Internal::AnyHash
                      )
                    end

                  # The type of the content.
                  module Type
                    extend OpenAI::Internal::Type::Enum

                    class << self
                      sig do
                        override
                          .returns(T::Array[
                          OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content::Type::TaggedSymbol
                        ])
                      end
                      def values; end
                    end

                    OrSymbol = T.type_alias { T.any(Symbol, String) }

                    TEXT = T.let(
                        :text,
                        OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content::Type::TaggedSymbol
                      )

                    TaggedSymbol = T.type_alias do
                        T.all(
                          Symbol,
                          OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result::Content::Type
                        )
                      end
                  end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Beta::Threads::Runs::FileSearchToolCall::FileSearch::Result,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::FileSearchToolCall,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileSearchToolCallDelta < OpenAI::Internal::Type::BaseModel
            # For now, this is always going to be an empty object.
            sig { returns(T.anything) }
            attr_accessor :file_search

            # The ID of the tool call object.
            sig { returns(T.nilable(String)) }
            attr_reader :id

            sig { params(id: String).void }
            attr_writer :id

            # The index of the tool call in the tool calls array.
            sig { returns(Integer) }
            attr_accessor :index

            # The type of tool call. This is always going to be `file_search` for this type of
            # tool call.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  file_search: T.anything,
                  index: Integer,
                  type: Symbol,
                  id: String
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  file_search: T.anything,
                  index: Integer,
                  id: String,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                file_search:, # For now, this is always going to be an empty object.
                index:, # The index of the tool call in the tool calls array.
                id: nil, # The ID of the tool call object.
                type: :file_search # The type of tool call. This is always going to be `file_search` for this type of
                                   # tool call.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::FileSearchToolCallDelta,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FunctionToolCall < OpenAI::Internal::Type::BaseModel
            # The definition of the function that was called.
            sig { returns(OpenAI::Beta::Threads::Runs::FunctionToolCall::Function) }
            attr_reader :function

            sig { params(function: OpenAI::Beta::Threads::Runs::FunctionToolCall::Function::OrHash).void }
            attr_writer :function

            # The ID of the tool call object.
            sig { returns(String) }
            attr_accessor :id

            # The type of tool call. This is always going to be `function` for this type of
            # tool call.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  id: String,
                  function:
                    OpenAI::Beta::Threads::Runs::FunctionToolCall::Function,
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  id: String,
                  function: OpenAI::Beta::Threads::Runs::FunctionToolCall::Function::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                id:, # The ID of the tool call object.
                function:, # The definition of the function that was called.
                type: :function # The type of tool call. This is always going to be `function` for this type of
                                # tool call.
); end
            end

            class Function < OpenAI::Internal::Type::BaseModel
              # The arguments passed to the function.
              sig { returns(String) }
              attr_accessor :arguments

              # The name of the function.
              sig { returns(String) }
              attr_accessor :name

              # The output of the function. This will be `null` if the outputs have not been
              # [submitted](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)
              # yet.
              sig { returns(T.nilable(String)) }
              attr_accessor :output

              sig { override.returns({ arguments: String, name: String, output: T.nilable(String) }) }
              def to_hash; end

              class << self
                # The definition of the function that was called.
                sig { params(arguments: String, name: String, output: T.nilable(String)).returns(T.attached_class) }
                def new(
                  arguments:, # The arguments passed to the function.
                  name:, # The name of the function.
                  output: # The output of the function. This will be `null` if the outputs have not been
                          # [submitted](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)
                          # yet.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::FunctionToolCall::Function,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::FunctionToolCall,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FunctionToolCallDelta < OpenAI::Internal::Type::BaseModel
            # The definition of the function that was called.
            sig do
              returns(T.nilable(
                  OpenAI::Beta::Threads::Runs::FunctionToolCallDelta::Function
                ))
            end
            attr_reader :function

            sig { params(function: OpenAI::Beta::Threads::Runs::FunctionToolCallDelta::Function::OrHash).void }
            attr_writer :function

            # The ID of the tool call object.
            sig { returns(T.nilable(String)) }
            attr_reader :id

            sig { params(id: String).void }
            attr_writer :id

            # The index of the tool call in the tool calls array.
            sig { returns(Integer) }
            attr_accessor :index

            # The type of tool call. This is always going to be `function` for this type of
            # tool call.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  index: Integer,
                  type: Symbol,
                  id: String,
                  function:
                    OpenAI::Beta::Threads::Runs::FunctionToolCallDelta::Function
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  index: Integer,
                  id: String,
                  function: OpenAI::Beta::Threads::Runs::FunctionToolCallDelta::Function::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                index:, # The index of the tool call in the tool calls array.
                id: nil, # The ID of the tool call object.
                function: nil, # The definition of the function that was called.
                type: :function # The type of tool call. This is always going to be `function` for this type of
                                # tool call.
); end
            end

            class Function < OpenAI::Internal::Type::BaseModel
              # The arguments passed to the function.
              sig { returns(T.nilable(String)) }
              attr_reader :arguments

              sig { params(arguments: String).void }
              attr_writer :arguments

              # The name of the function.
              sig { returns(T.nilable(String)) }
              attr_reader :name

              sig { params(name: String).void }
              attr_writer :name

              # The output of the function. This will be `null` if the outputs have not been
              # [submitted](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)
              # yet.
              sig { returns(T.nilable(String)) }
              attr_accessor :output

              sig { override.returns({ arguments: String, name: String, output: T.nilable(String) }) }
              def to_hash; end

              class << self
                # The definition of the function that was called.
                sig { params(arguments: String, name: String, output: T.nilable(String)).returns(T.attached_class) }
                def new(
                  arguments: nil, # The arguments passed to the function.
                  name: nil, # The name of the function.
                  output: nil # The output of the function. This will be `null` if the outputs have not been
                              # [submitted](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)
                              # yet.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::FunctionToolCallDelta::Function,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::FunctionToolCallDelta,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class MessageCreationStepDetails < OpenAI::Internal::Type::BaseModel
            sig { returns(OpenAI::Beta::Threads::Runs::MessageCreationStepDetails::MessageCreation) }
            attr_reader :message_creation

            sig do
              params(
                message_creation: OpenAI::Beta::Threads::Runs::MessageCreationStepDetails::MessageCreation::OrHash
              ).void
            end
            attr_writer :message_creation

            # Always `message_creation`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  message_creation:
                    OpenAI::Beta::Threads::Runs::MessageCreationStepDetails::MessageCreation,
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              # Details of the message creation by the run step.
              sig do
                params(
                  message_creation: OpenAI::Beta::Threads::Runs::MessageCreationStepDetails::MessageCreation::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                message_creation:,
                type: :message_creation # Always `message_creation`.
); end
            end

            class MessageCreation < OpenAI::Internal::Type::BaseModel
              # The ID of the message that was created by this run step.
              sig { returns(String) }
              attr_accessor :message_id

              sig { override.returns({ message_id: String }) }
              def to_hash; end

              class << self
                sig { params(message_id: String).returns(T.attached_class) }
                def new(
                  message_id: # The ID of the message that was created by this run step.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::MessageCreationStepDetails::MessageCreation,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::MessageCreationStepDetails,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class RunStep < OpenAI::Internal::Type::BaseModel
            # The ID of the
            # [assistant](https://platform.openai.com/docs/api-reference/assistants)
            # associated with the run step.
            sig { returns(String) }
            attr_accessor :assistant_id

            # The Unix timestamp (in seconds) for when the run step was cancelled.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :cancelled_at

            # The Unix timestamp (in seconds) for when the run step completed.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :completed_at

            # The Unix timestamp (in seconds) for when the run step was created.
            sig { returns(Integer) }
            attr_accessor :created_at

            # The Unix timestamp (in seconds) for when the run step expired. A step is
            # considered expired if the parent run is expired.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :expired_at

            # The Unix timestamp (in seconds) for when the run step failed.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :failed_at

            # The identifier of the run step, which can be referenced in API endpoints.
            sig { returns(String) }
            attr_accessor :id

            # The last error associated with this run step. Will be `null` if there are no
            # errors.
            sig { returns(T.nilable(OpenAI::Beta::Threads::Runs::RunStep::LastError)) }
            attr_reader :last_error

            sig do
              params(
                last_error: T.nilable(
                    OpenAI::Beta::Threads::Runs::RunStep::LastError::OrHash
                  )
              ).void
            end
            attr_writer :last_error

            # Set of 16 key-value pairs that can be attached to an object. This can be useful
            # for storing additional information about the object in a structured format, and
            # querying for objects via API or the dashboard.
            #
            # Keys are strings with a maximum length of 64 characters. Values are strings with
            # a maximum length of 512 characters.
            sig { returns(T.nilable(T::Hash[Symbol, String])) }
            attr_accessor :metadata

            # The object type, which is always `thread.run.step`.
            sig { returns(Symbol) }
            attr_accessor :object

            # The ID of the [run](https://platform.openai.com/docs/api-reference/runs) that
            # this run step is a part of.
            sig { returns(String) }
            attr_accessor :run_id

            # The status of the run step, which can be either `in_progress`, `cancelled`,
            # `failed`, `completed`, or `expired`.
            sig { returns(OpenAI::Beta::Threads::Runs::RunStep::Status::TaggedSymbol) }
            attr_accessor :status

            # The details of the run step.
            sig { returns(OpenAI::Beta::Threads::Runs::RunStep::StepDetails::Variants) }
            attr_accessor :step_details

            # The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)
            # that was run.
            sig { returns(String) }
            attr_accessor :thread_id

            # The type of run step, which can be either `message_creation` or `tool_calls`.
            sig { returns(OpenAI::Beta::Threads::Runs::RunStep::Type::TaggedSymbol) }
            attr_accessor :type

            # Usage statistics related to the run step. This value will be `null` while the
            # run step's status is `in_progress`.
            sig { returns(T.nilable(OpenAI::Beta::Threads::Runs::RunStep::Usage)) }
            attr_reader :usage

            sig { params(usage: T.nilable(OpenAI::Beta::Threads::Runs::RunStep::Usage::OrHash)).void }
            attr_writer :usage

            sig do
              override
                .returns({
                  id: String,
                  assistant_id: String,
                  cancelled_at: T.nilable(Integer),
                  completed_at: T.nilable(Integer),
                  created_at: Integer,
                  expired_at: T.nilable(Integer),
                  failed_at: T.nilable(Integer),
                  last_error:
                    T.nilable(OpenAI::Beta::Threads::Runs::RunStep::LastError),
                  metadata: T.nilable(T::Hash[Symbol, String]),
                  object: Symbol,
                  run_id: String,
                  status:
                    OpenAI::Beta::Threads::Runs::RunStep::Status::TaggedSymbol,
                  step_details:
                    OpenAI::Beta::Threads::Runs::RunStep::StepDetails::Variants,
                  thread_id: String,
                  type:
                    OpenAI::Beta::Threads::Runs::RunStep::Type::TaggedSymbol,
                  usage: T.nilable(OpenAI::Beta::Threads::Runs::RunStep::Usage)
                })
            end
            def to_hash; end

            class << self
              # Represents a step in execution of a run.
              sig do
                params(
                  id: String,
                  assistant_id: String,
                  cancelled_at: T.nilable(Integer),
                  completed_at: T.nilable(Integer),
                  created_at: Integer,
                  expired_at: T.nilable(Integer),
                  failed_at: T.nilable(Integer),
                  last_error: T.nilable(
                    OpenAI::Beta::Threads::Runs::RunStep::LastError::OrHash
                  ),
                  metadata: T.nilable(T::Hash[Symbol, String]),
                  run_id: String,
                  status: OpenAI::Beta::Threads::Runs::RunStep::Status::OrSymbol,
                  step_details: T.any(
                    OpenAI::Beta::Threads::Runs::MessageCreationStepDetails::OrHash,
                    OpenAI::Beta::Threads::Runs::ToolCallsStepDetails::OrHash
                  ),
                  thread_id: String,
                  type: OpenAI::Beta::Threads::Runs::RunStep::Type::OrSymbol,
                  usage: T.nilable(
                    OpenAI::Beta::Threads::Runs::RunStep::Usage::OrHash
                  ),
                  object: Symbol
                ).returns(T.attached_class)
              end
              def new(
                id:, # The identifier of the run step, which can be referenced in API endpoints.
                assistant_id:, # The ID of the
                               # [assistant](https://platform.openai.com/docs/api-reference/assistants)
                               # associated with the run step.
                cancelled_at:, # The Unix timestamp (in seconds) for when the run step was cancelled.
                completed_at:, # The Unix timestamp (in seconds) for when the run step completed.
                created_at:, # The Unix timestamp (in seconds) for when the run step was created.
                expired_at:, # The Unix timestamp (in seconds) for when the run step expired. A step is
                             # considered expired if the parent run is expired.
                failed_at:, # The Unix timestamp (in seconds) for when the run step failed.
                last_error:, # The last error associated with this run step. Will be `null` if there are no
                             # errors.
                metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
                run_id:, # The ID of the [run](https://platform.openai.com/docs/api-reference/runs) that
                         # this run step is a part of.
                status:, # The status of the run step, which can be either `in_progress`, `cancelled`,
                         # `failed`, `completed`, or `expired`.
                step_details:, # The details of the run step.
                thread_id:, # The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)
                            # that was run.
                type:, # The type of run step, which can be either `message_creation` or `tool_calls`.
                usage:, # Usage statistics related to the run step. This value will be `null` while the
                        # run step's status is `in_progress`.
                object: :"thread.run.step" # The object type, which is always `thread.run.step`.
); end
            end

            class LastError < OpenAI::Internal::Type::BaseModel
              # One of `server_error` or `rate_limit_exceeded`.
              sig { returns(OpenAI::Beta::Threads::Runs::RunStep::LastError::Code::TaggedSymbol) }
              attr_accessor :code

              # A human-readable description of the error.
              sig { returns(String) }
              attr_accessor :message

              sig do
                override
                  .returns({
                    code:
                      OpenAI::Beta::Threads::Runs::RunStep::LastError::Code::TaggedSymbol,
                    message: String
                  })
              end
              def to_hash; end

              class << self
                # The last error associated with this run step. Will be `null` if there are no
                # errors.
                sig do
                  params(
                    code: OpenAI::Beta::Threads::Runs::RunStep::LastError::Code::OrSymbol,
                    message: String
                  ).returns(T.attached_class)
                end
                def new(
                  code:, # One of `server_error` or `rate_limit_exceeded`.
                  message: # A human-readable description of the error.
); end
              end

              # One of `server_error` or `rate_limit_exceeded`.
              module Code
                extend OpenAI::Internal::Type::Enum

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::Beta::Threads::Runs::RunStep::LastError::Code::TaggedSymbol
                    ])
                  end
                  def values; end
                end

                OrSymbol = T.type_alias { T.any(Symbol, String) }

                RATE_LIMIT_EXCEEDED = T.let(
                    :rate_limit_exceeded,
                    OpenAI::Beta::Threads::Runs::RunStep::LastError::Code::TaggedSymbol
                  )

                SERVER_ERROR = T.let(
                    :server_error,
                    OpenAI::Beta::Threads::Runs::RunStep::LastError::Code::TaggedSymbol
                  )

                TaggedSymbol = T.type_alias do
                    T.all(
                      Symbol,
                      OpenAI::Beta::Threads::Runs::RunStep::LastError::Code
                    )
                  end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::RunStep::LastError,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::RunStep,
                  OpenAI::Internal::AnyHash
                )
              end

            # The status of the run step, which can be either `in_progress`, `cancelled`,
            # `failed`, `completed`, or `expired`.
            module Status
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Runs::RunStep::Status::TaggedSymbol
                  ])
                end
                def values; end
              end

              CANCELLED = T.let(
                  :cancelled,
                  OpenAI::Beta::Threads::Runs::RunStep::Status::TaggedSymbol
                )

              COMPLETED = T.let(
                  :completed,
                  OpenAI::Beta::Threads::Runs::RunStep::Status::TaggedSymbol
                )

              EXPIRED = T.let(
                  :expired,
                  OpenAI::Beta::Threads::Runs::RunStep::Status::TaggedSymbol
                )

              FAILED = T.let(
                  :failed,
                  OpenAI::Beta::Threads::Runs::RunStep::Status::TaggedSymbol
                )

              IN_PROGRESS = T.let(
                  :in_progress,
                  OpenAI::Beta::Threads::Runs::RunStep::Status::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TaggedSymbol = T.type_alias do
                  T.all(Symbol, OpenAI::Beta::Threads::Runs::RunStep::Status)
                end
            end

            # The details of the run step.
            module StepDetails
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Runs::RunStep::StepDetails::Variants
                  ])
                end
                def variants; end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::MessageCreationStepDetails,
                    OpenAI::Beta::Threads::Runs::ToolCallsStepDetails
                  )
                end
            end

            # The type of run step, which can be either `message_creation` or `tool_calls`.
            module Type
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Runs::RunStep::Type::TaggedSymbol
                  ])
                end
                def values; end
              end

              MESSAGE_CREATION = T.let(
                  :message_creation,
                  OpenAI::Beta::Threads::Runs::RunStep::Type::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TOOL_CALLS = T.let(
                  :tool_calls,
                  OpenAI::Beta::Threads::Runs::RunStep::Type::TaggedSymbol
                )

              TaggedSymbol = T.type_alias do
                  T.all(Symbol, OpenAI::Beta::Threads::Runs::RunStep::Type)
                end
            end

            class Usage < OpenAI::Internal::Type::BaseModel
              # Number of completion tokens used over the course of the run step.
              sig { returns(Integer) }
              attr_accessor :completion_tokens

              # Number of prompt tokens used over the course of the run step.
              sig { returns(Integer) }
              attr_accessor :prompt_tokens

              # Total number of tokens used (prompt + completion).
              sig { returns(Integer) }
              attr_accessor :total_tokens

              sig do
                override
                  .returns({
                    completion_tokens: Integer,
                    prompt_tokens: Integer,
                    total_tokens: Integer
                  })
              end
              def to_hash; end

              class << self
                # Usage statistics related to the run step. This value will be `null` while the
                # run step's status is `in_progress`.
                sig do
                  params(
                    completion_tokens: Integer,
                    prompt_tokens: Integer,
                    total_tokens: Integer
                  ).returns(T.attached_class)
                end
                def new(
                  completion_tokens:, # Number of completion tokens used over the course of the run step.
                  prompt_tokens:, # Number of prompt tokens used over the course of the run step.
                  total_tokens: # Total number of tokens used (prompt + completion).
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::RunStep::Usage,
                    OpenAI::Internal::AnyHash
                  )
                end
            end
          end

          class RunStepDelta < OpenAI::Internal::Type::BaseModel
            # The details of the run step.
            sig do
              returns(T.nilable(
                  OpenAI::Beta::Threads::Runs::RunStepDelta::StepDetails::Variants
                ))
            end
            attr_reader :step_details

            sig do
              params(
                step_details: T.any(
                    OpenAI::Beta::Threads::Runs::RunStepDeltaMessageDelta::OrHash,
                    OpenAI::Beta::Threads::Runs::ToolCallDeltaObject::OrHash
                  )
              ).void
            end
            attr_writer :step_details

            sig do
              override
                .returns({
                  step_details:
                    OpenAI::Beta::Threads::Runs::RunStepDelta::StepDetails::Variants
                })
            end
            def to_hash; end

            class << self
              # The delta containing the fields that have changed on the run step.
              sig do
                params(
                  step_details: T.any(
                    OpenAI::Beta::Threads::Runs::RunStepDeltaMessageDelta::OrHash,
                    OpenAI::Beta::Threads::Runs::ToolCallDeltaObject::OrHash
                  )
                ).returns(T.attached_class)
              end
              def new(
                step_details: nil # The details of the run step.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::RunStepDelta,
                  OpenAI::Internal::AnyHash
                )
              end

            # The details of the run step.
            module StepDetails
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepDelta::StepDetails::Variants
                  ])
                end
                def variants; end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::RunStepDeltaMessageDelta,
                    OpenAI::Beta::Threads::Runs::ToolCallDeltaObject
                  )
                end
            end
          end

          class RunStepDeltaEvent < OpenAI::Internal::Type::BaseModel
            # The delta containing the fields that have changed on the run step.
            sig { returns(OpenAI::Beta::Threads::Runs::RunStepDelta) }
            attr_reader :delta

            sig { params(delta: OpenAI::Beta::Threads::Runs::RunStepDelta::OrHash).void }
            attr_writer :delta

            # The identifier of the run step, which can be referenced in API endpoints.
            sig { returns(String) }
            attr_accessor :id

            # The object type, which is always `thread.run.step.delta`.
            sig { returns(Symbol) }
            attr_accessor :object

            sig do
              override
                .returns({
                  id: String,
                  delta: OpenAI::Beta::Threads::Runs::RunStepDelta,
                  object: Symbol
                })
            end
            def to_hash; end

            class << self
              # Represents a run step delta i.e. any changed fields on a run step during
              # streaming.
              sig do
                params(
                  id: String,
                  delta: OpenAI::Beta::Threads::Runs::RunStepDelta::OrHash,
                  object: Symbol
                ).returns(T.attached_class)
              end
              def new(
                id:, # The identifier of the run step, which can be referenced in API endpoints.
                delta:, # The delta containing the fields that have changed on the run step.
                object: :"thread.run.step.delta" # The object type, which is always `thread.run.step.delta`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::RunStepDeltaEvent,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class RunStepDeltaMessageDelta < OpenAI::Internal::Type::BaseModel
            sig do
              returns(T.nilable(
                  OpenAI::Beta::Threads::Runs::RunStepDeltaMessageDelta::MessageCreation
                ))
            end
            attr_reader :message_creation

            sig do
              params(
                message_creation: OpenAI::Beta::Threads::Runs::RunStepDeltaMessageDelta::MessageCreation::OrHash
              ).void
            end
            attr_writer :message_creation

            # Always `message_creation`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  type: Symbol,
                  message_creation:
                    OpenAI::Beta::Threads::Runs::RunStepDeltaMessageDelta::MessageCreation
                })
            end
            def to_hash; end

            class << self
              # Details of the message creation by the run step.
              sig do
                params(
                  message_creation: OpenAI::Beta::Threads::Runs::RunStepDeltaMessageDelta::MessageCreation::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                message_creation: nil,
                type: :message_creation # Always `message_creation`.
); end
            end

            class MessageCreation < OpenAI::Internal::Type::BaseModel
              # The ID of the message that was created by this run step.
              sig { returns(T.nilable(String)) }
              attr_reader :message_id

              sig { params(message_id: String).void }
              attr_writer :message_id

              sig { override.returns({ message_id: String }) }
              def to_hash; end

              class << self
                sig { params(message_id: String).returns(T.attached_class) }
                def new(
                  message_id: nil # The ID of the message that was created by this run step.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Beta::Threads::Runs::RunStepDeltaMessageDelta::MessageCreation,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::RunStepDeltaMessageDelta,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          module RunStepInclude
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Beta::Threads::Runs::RunStepInclude::TaggedSymbol
                ])
              end
              def values; end
            end

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            STEP_DETAILS_TOOL_CALLS_FILE_SEARCH_RESULTS_CONTENT = T.let(
                :"step_details.tool_calls[*].file_search.results[*].content",
                OpenAI::Beta::Threads::Runs::RunStepInclude::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Beta::Threads::Runs::RunStepInclude)
              end
          end

          class StepListParams < OpenAI::Internal::Type::BaseModel
            extend OpenAI::Internal::Type::RequestParameters::Converter
            include OpenAI::Internal::Type::RequestParameters

            # A cursor for use in pagination. `after` is an object ID that defines your place
            # in the list. For instance, if you make a list request and receive 100 objects,
            # ending with obj_foo, your subsequent call can include after=obj_foo in order to
            # fetch the next page of the list.
            sig { returns(T.nilable(String)) }
            attr_reader :after

            sig { params(after: String).void }
            attr_writer :after

            # A cursor for use in pagination. `before` is an object ID that defines your place
            # in the list. For instance, if you make a list request and receive 100 objects,
            # starting with obj_foo, your subsequent call can include before=obj_foo in order
            # to fetch the previous page of the list.
            sig { returns(T.nilable(String)) }
            attr_reader :before

            sig { params(before: String).void }
            attr_writer :before

            # A list of additional fields to include in the response. Currently the only
            # supported value is `step_details.tool_calls[*].file_search.results[*].content`
            # to fetch the file search result content.
            #
            # See the
            # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
            # for more information.
            sig do
              returns(T.nilable(
                  T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                  ]
                ))
            end
            attr_reader :include

            sig do
              params(
                include: T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                  ]
              ).void
            end
            attr_writer :include

            # A limit on the number of objects to be returned. Limit can range between 1 and
            # 100, and the default is 20.
            sig { returns(T.nilable(Integer)) }
            attr_reader :limit

            sig { params(limit: Integer).void }
            attr_writer :limit

            # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
            # order and `desc` for descending order.
            sig do
              returns(T.nilable(
                  OpenAI::Beta::Threads::Runs::StepListParams::Order::OrSymbol
                ))
            end
            attr_reader :order

            sig { params(order: OpenAI::Beta::Threads::Runs::StepListParams::Order::OrSymbol).void }
            attr_writer :order

            sig { returns(String) }
            attr_accessor :thread_id

            sig do
              override
                .returns({
                  thread_id: String,
                  after: String,
                  before: String,
                  include:
                    T::Array[
                      OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                    ],
                  limit: Integer,
                  order:
                    OpenAI::Beta::Threads::Runs::StepListParams::Order::OrSymbol,
                  request_options: OpenAI::RequestOptions
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  thread_id: String,
                  after: String,
                  before: String,
                  include: T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                  ],
                  limit: Integer,
                  order: OpenAI::Beta::Threads::Runs::StepListParams::Order::OrSymbol,
                  request_options: OpenAI::RequestOptions::OrHash
                ).returns(T.attached_class)
              end
              def new(
                thread_id:,
                after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                            # in the list. For instance, if you make a list request and receive 100 objects,
                            # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                            # fetch the next page of the list.
                before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                             # in the list. For instance, if you make a list request and receive 100 objects,
                             # starting with obj_foo, your subsequent call can include before=obj_foo in order
                             # to fetch the previous page of the list.
                include: nil, # A list of additional fields to include in the response. Currently the only
                              # supported value is `step_details.tool_calls[*].file_search.results[*].content`
                              # to fetch the file search result content.
                              # See the
                              # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
                              # for more information.
                limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                            # 100, and the default is 20.
                order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                            # order and `desc` for descending order.
                request_options: {}
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::StepListParams,
                  OpenAI::Internal::AnyHash
                )
              end

            # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
            # order and `desc` for descending order.
            module Order
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Beta::Threads::Runs::StepListParams::Order::TaggedSymbol
                  ])
                end
                def values; end
              end

              ASC = T.let(
                  :asc,
                  OpenAI::Beta::Threads::Runs::StepListParams::Order::TaggedSymbol
                )

              DESC = T.let(
                  :desc,
                  OpenAI::Beta::Threads::Runs::StepListParams::Order::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Beta::Threads::Runs::StepListParams::Order
                  )
                end
            end
          end

          class StepRetrieveParams < OpenAI::Internal::Type::BaseModel
            extend OpenAI::Internal::Type::RequestParameters::Converter
            include OpenAI::Internal::Type::RequestParameters

            # A list of additional fields to include in the response. Currently the only
            # supported value is `step_details.tool_calls[*].file_search.results[*].content`
            # to fetch the file search result content.
            #
            # See the
            # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
            # for more information.
            sig do
              returns(T.nilable(
                  T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                  ]
                ))
            end
            attr_reader :include

            sig do
              params(
                include: T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                  ]
              ).void
            end
            attr_writer :include

            sig { returns(String) }
            attr_accessor :run_id

            sig { returns(String) }
            attr_accessor :thread_id

            sig do
              override
                .returns({
                  thread_id: String,
                  run_id: String,
                  include:
                    T::Array[
                      OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                    ],
                  request_options: OpenAI::RequestOptions
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  thread_id: String,
                  run_id: String,
                  include: T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                  ],
                  request_options: OpenAI::RequestOptions::OrHash
                ).returns(T.attached_class)
              end
              def new(
                thread_id:,
                run_id:,
                include: nil, # A list of additional fields to include in the response. Currently the only
                              # supported value is `step_details.tool_calls[*].file_search.results[*].content`
                              # to fetch the file search result content.
                              # See the
                              # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
                              # for more information.
                request_options: {}
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::StepRetrieveParams,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          # Details of the Code Interpreter tool call the run step was involved in.
          module ToolCall
            extend OpenAI::Internal::Type::Union

            class << self
              sig { override.returns(T::Array[OpenAI::Beta::Threads::Runs::ToolCall::Variants]) }
              def variants; end
            end

            Variants = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall,
                  OpenAI::Beta::Threads::Runs::FileSearchToolCall,
                  OpenAI::Beta::Threads::Runs::FunctionToolCall
                )
              end
          end

          # Details of the Code Interpreter tool call the run step was involved in.
          module ToolCallDelta
            extend OpenAI::Internal::Type::Union

            class << self
              sig { override.returns(T::Array[OpenAI::Beta::Threads::Runs::ToolCallDelta::Variants]) }
              def variants; end
            end

            Variants = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta,
                  OpenAI::Beta::Threads::Runs::FileSearchToolCallDelta,
                  OpenAI::Beta::Threads::Runs::FunctionToolCallDelta
                )
              end
          end

          class ToolCallDeltaObject < OpenAI::Internal::Type::BaseModel
            # An array of tool calls the run step was involved in. These can be associated
            # with one of three types of tools: `code_interpreter`, `file_search`, or
            # `function`.
            sig do
              returns(T.nilable(
                  T::Array[OpenAI::Beta::Threads::Runs::ToolCallDelta::Variants]
                ))
            end
            attr_reader :tool_calls

            sig do
              params(
                tool_calls: T::Array[
                    T.any(
                      OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::OrHash,
                      OpenAI::Beta::Threads::Runs::FileSearchToolCallDelta::OrHash,
                      OpenAI::Beta::Threads::Runs::FunctionToolCallDelta::OrHash
                    )
                  ]
              ).void
            end
            attr_writer :tool_calls

            # Always `tool_calls`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  type: Symbol,
                  tool_calls:
                    T::Array[
                      OpenAI::Beta::Threads::Runs::ToolCallDelta::Variants
                    ]
                })
            end
            def to_hash; end

            class << self
              # Details of the tool call.
              sig do
                params(
                  tool_calls: T::Array[
                    T.any(
                      OpenAI::Beta::Threads::Runs::CodeInterpreterToolCallDelta::OrHash,
                      OpenAI::Beta::Threads::Runs::FileSearchToolCallDelta::OrHash,
                      OpenAI::Beta::Threads::Runs::FunctionToolCallDelta::OrHash
                    )
                  ],
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                tool_calls: nil, # An array of tool calls the run step was involved in. These can be associated
                                 # with one of three types of tools: `code_interpreter`, `file_search`, or
                                 # `function`.
                type: :tool_calls # Always `tool_calls`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::ToolCallDeltaObject,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class ToolCallsStepDetails < OpenAI::Internal::Type::BaseModel
            # An array of tool calls the run step was involved in. These can be associated
            # with one of three types of tools: `code_interpreter`, `file_search`, or
            # `function`.
            sig { returns(T::Array[OpenAI::Beta::Threads::Runs::ToolCall::Variants]) }
            attr_accessor :tool_calls

            # Always `tool_calls`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  tool_calls:
                    T::Array[OpenAI::Beta::Threads::Runs::ToolCall::Variants],
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              # Details of the tool call.
              sig do
                params(
                  tool_calls: T::Array[
                    T.any(
                      OpenAI::Beta::Threads::Runs::CodeInterpreterToolCall::OrHash,
                      OpenAI::Beta::Threads::Runs::FileSearchToolCall::OrHash,
                      OpenAI::Beta::Threads::Runs::FunctionToolCall::OrHash
                    )
                  ],
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                tool_calls:, # An array of tool calls the run step was involved in. These can be associated
                             # with one of three types of tools: `code_interpreter`, `file_search`, or
                             # `function`.
                type: :tool_calls # Always `tool_calls`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Beta::Threads::Runs::ToolCallsStepDetails,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        class Text < OpenAI::Internal::Type::BaseModel
          sig { returns(T::Array[OpenAI::Beta::Threads::Annotation::Variants]) }
          attr_accessor :annotations

          # The data that makes up the text.
          sig { returns(String) }
          attr_accessor :value

          sig do
            override
              .returns({
                annotations:
                  T::Array[OpenAI::Beta::Threads::Annotation::Variants],
                value: String
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                annotations: T::Array[
                  T.any(
                    OpenAI::Beta::Threads::FileCitationAnnotation::OrHash,
                    OpenAI::Beta::Threads::FilePathAnnotation::OrHash
                  )
                ],
                value: String
              ).returns(T.attached_class)
            end
            def new(
              annotations:,
              value: # The data that makes up the text.
); end
          end

          OrHash = T.type_alias do
              T.any(OpenAI::Beta::Threads::Text, OpenAI::Internal::AnyHash)
            end
        end

        class TextContentBlock < OpenAI::Internal::Type::BaseModel
          sig { returns(OpenAI::Beta::Threads::Text) }
          attr_reader :text

          sig { params(text: OpenAI::Beta::Threads::Text::OrHash).void }
          attr_writer :text

          # Always `text`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ text: OpenAI::Beta::Threads::Text, type: Symbol }) }
          def to_hash; end

          class << self
            # The text content that is part of a message.
            sig { params(text: OpenAI::Beta::Threads::Text::OrHash, type: Symbol).returns(T.attached_class) }
            def new(
              text:,
              type: :text # Always `text`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::TextContentBlock,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class TextContentBlockParam < OpenAI::Internal::Type::BaseModel
          # Text content to be sent to the model
          sig { returns(String) }
          attr_accessor :text

          # Always `text`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ text: String, type: Symbol }) }
          def to_hash; end

          class << self
            # The text content that is part of a message.
            sig { params(text: String, type: Symbol).returns(T.attached_class) }
            def new(
              text:, # Text content to be sent to the model
              type: :text # Always `text`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::TextContentBlockParam,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class TextDelta < OpenAI::Internal::Type::BaseModel
          sig do
            returns(T.nilable(
                T::Array[OpenAI::Beta::Threads::AnnotationDelta::Variants]
              ))
          end
          attr_reader :annotations

          sig do
            params(
              annotations: T::Array[
                  T.any(
                    OpenAI::Beta::Threads::FileCitationDeltaAnnotation::OrHash,
                    OpenAI::Beta::Threads::FilePathDeltaAnnotation::OrHash
                  )
                ]
            ).void
          end
          attr_writer :annotations

          # The data that makes up the text.
          sig { returns(T.nilable(String)) }
          attr_reader :value

          sig { params(value: String).void }
          attr_writer :value

          sig do
            override
              .returns({
                annotations:
                  T::Array[OpenAI::Beta::Threads::AnnotationDelta::Variants],
                value: String
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                annotations: T::Array[
                  T.any(
                    OpenAI::Beta::Threads::FileCitationDeltaAnnotation::OrHash,
                    OpenAI::Beta::Threads::FilePathDeltaAnnotation::OrHash
                  )
                ],
                value: String
              ).returns(T.attached_class)
            end
            def new(
              annotations: nil,
              value: nil # The data that makes up the text.
); end
          end

          OrHash = T.type_alias do
              T.any(OpenAI::Beta::Threads::TextDelta, OpenAI::Internal::AnyHash)
            end
        end

        class TextDeltaBlock < OpenAI::Internal::Type::BaseModel
          # The index of the content part in the message.
          sig { returns(Integer) }
          attr_accessor :index

          sig { returns(T.nilable(OpenAI::Beta::Threads::TextDelta)) }
          attr_reader :text

          sig { params(text: OpenAI::Beta::Threads::TextDelta::OrHash).void }
          attr_writer :text

          # Always `text`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                index: Integer,
                type: Symbol,
                text: OpenAI::Beta::Threads::TextDelta
              })
          end
          def to_hash; end

          class << self
            # The text content that is part of a message.
            sig do
              params(
                index: Integer,
                text: OpenAI::Beta::Threads::TextDelta::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              index:, # The index of the content part in the message.
              text: nil,
              type: :text # Always `text`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Beta::Threads::TextDeltaBlock,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end
    end

    module Chat
      class ChatCompletion < OpenAI::Internal::Type::BaseModel
        # A list of chat completion choices. Can be more than one if `n` is greater
        # than 1.
        sig { returns(T::Array[OpenAI::Chat::ChatCompletion::Choice]) }
        attr_accessor :choices

        # The Unix timestamp (in seconds) of when the chat completion was created.
        sig { returns(Integer) }
        attr_accessor :created

        # A unique identifier for the chat completion.
        sig { returns(String) }
        attr_accessor :id

        # The model used for the chat completion.
        sig { returns(String) }
        attr_accessor :model

        # The object type, which is always `chat.completion`.
        sig { returns(Symbol) }
        attr_accessor :object

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        sig { returns(T.nilable(OpenAI::Chat::ChatCompletion::ServiceTier::TaggedSymbol)) }
        attr_accessor :service_tier

        # This fingerprint represents the backend configuration that the model runs with.
        #
        # Can be used in conjunction with the `seed` request parameter to understand when
        # backend changes have been made that might impact determinism.
        sig { returns(T.nilable(String)) }
        attr_reader :system_fingerprint

        sig { params(system_fingerprint: String).void }
        attr_writer :system_fingerprint

        # Usage statistics for the completion request.
        sig { returns(T.nilable(OpenAI::CompletionUsage)) }
        attr_reader :usage

        sig { params(usage: OpenAI::CompletionUsage::OrHash).void }
        attr_writer :usage

        sig do
          override
            .returns({
              id: String,
              choices: T::Array[OpenAI::Chat::ChatCompletion::Choice],
              created: Integer,
              model: String,
              object: Symbol,
              service_tier:
                T.nilable(
                  OpenAI::Chat::ChatCompletion::ServiceTier::TaggedSymbol
                ),
              system_fingerprint: String,
              usage: OpenAI::CompletionUsage
            })
        end
        def to_hash; end

        class << self
          # Represents a chat completion response returned by model, based on the provided
          # input.
          sig do
            params(
              id: String,
              choices: T::Array[OpenAI::Chat::ChatCompletion::Choice::OrHash],
              created: Integer,
              model: String,
              service_tier: T.nilable(OpenAI::Chat::ChatCompletion::ServiceTier::OrSymbol),
              system_fingerprint: String,
              usage: OpenAI::CompletionUsage::OrHash,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # A unique identifier for the chat completion.
            choices:, # A list of chat completion choices. Can be more than one if `n` is greater
                      # than 1.
            created:, # The Unix timestamp (in seconds) of when the chat completion was created.
            model:, # The model used for the chat completion.
            service_tier: nil, # Specifies the processing type used for serving the request.
                               # - If set to 'auto', then the request will be processed with the service tier
                               #   configured in the Project settings. Unless otherwise configured, the Project
                               #   will use 'default'.
                               # - If set to 'default', then the request will be processed with the standard
                               #   pricing and performance for the selected model.
                               # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
                               #   'priority', then the request will be processed with the corresponding service
                               #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
                               #   Priority processing.
                               # - When not set, the default behavior is 'auto'.
                               # When the `service_tier` parameter is set, the response body will include the
                               # `service_tier` value based on the processing mode actually used to serve the
                               # request. This response value may be different from the value set in the
                               # parameter.
            system_fingerprint: nil, # This fingerprint represents the backend configuration that the model runs with.
                                     # Can be used in conjunction with the `seed` request parameter to understand when
                                     # backend changes have been made that might impact determinism.
            usage: nil, # Usage statistics for the completion request.
            object: :"chat.completion" # The object type, which is always `chat.completion`.
); end
        end

        class Choice < OpenAI::Internal::Type::BaseModel
          # The reason the model stopped generating tokens. This will be `stop` if the model
          # hit a natural stop point or a provided stop sequence, `length` if the maximum
          # number of tokens specified in the request was reached, `content_filter` if
          # content was omitted due to a flag from our content filters, `tool_calls` if the
          # model called a tool, or `function_call` (deprecated) if the model called a
          # function.
          sig { returns(OpenAI::Chat::ChatCompletion::Choice::FinishReason::TaggedSymbol) }
          attr_accessor :finish_reason

          # The index of the choice in the list of choices.
          sig { returns(Integer) }
          attr_accessor :index

          # Log probability information for the choice.
          sig { returns(T.nilable(OpenAI::Chat::ChatCompletion::Choice::Logprobs)) }
          attr_reader :logprobs

          sig do
            params(
              logprobs: T.nilable(
                  OpenAI::Chat::ChatCompletion::Choice::Logprobs::OrHash
                )
            ).void
          end
          attr_writer :logprobs

          # A chat completion message generated by the model.
          sig { returns(OpenAI::Chat::ChatCompletionMessage) }
          attr_reader :message

          sig { params(message: OpenAI::Chat::ChatCompletionMessage::OrHash).void }
          attr_writer :message

          sig do
            override
              .returns({
                finish_reason:
                  OpenAI::Chat::ChatCompletion::Choice::FinishReason::TaggedSymbol,
                index: Integer,
                logprobs:
                  T.nilable(OpenAI::Chat::ChatCompletion::Choice::Logprobs),
                message: OpenAI::Chat::ChatCompletionMessage
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                finish_reason: OpenAI::Chat::ChatCompletion::Choice::FinishReason::OrSymbol,
                index: Integer,
                logprobs: T.nilable(
                  OpenAI::Chat::ChatCompletion::Choice::Logprobs::OrHash
                ),
                message: OpenAI::Chat::ChatCompletionMessage::OrHash
              ).returns(T.attached_class)
            end
            def new(
              finish_reason:, # The reason the model stopped generating tokens. This will be `stop` if the model
                              # hit a natural stop point or a provided stop sequence, `length` if the maximum
                              # number of tokens specified in the request was reached, `content_filter` if
                              # content was omitted due to a flag from our content filters, `tool_calls` if the
                              # model called a tool, or `function_call` (deprecated) if the model called a
                              # function.
              index:, # The index of the choice in the list of choices.
              logprobs:, # Log probability information for the choice.
              message: # A chat completion message generated by the model.
); end
          end

          # The reason the model stopped generating tokens. This will be `stop` if the model
          # hit a natural stop point or a provided stop sequence, `length` if the maximum
          # number of tokens specified in the request was reached, `content_filter` if
          # content was omitted due to a flag from our content filters, `tool_calls` if the
          # model called a tool, or `function_call` (deprecated) if the model called a
          # function.
          module FinishReason
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Chat::ChatCompletion::Choice::FinishReason::TaggedSymbol
                ])
              end
              def values; end
            end

            CONTENT_FILTER = T.let(
                :content_filter,
                OpenAI::Chat::ChatCompletion::Choice::FinishReason::TaggedSymbol
              )

            FUNCTION_CALL = T.let(
                :function_call,
                OpenAI::Chat::ChatCompletion::Choice::FinishReason::TaggedSymbol
              )

            LENGTH = T.let(
                :length,
                OpenAI::Chat::ChatCompletion::Choice::FinishReason::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            STOP = T.let(
                :stop,
                OpenAI::Chat::ChatCompletion::Choice::FinishReason::TaggedSymbol
              )

            TOOL_CALLS = T.let(
                :tool_calls,
                OpenAI::Chat::ChatCompletion::Choice::FinishReason::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Chat::ChatCompletion::Choice::FinishReason
                )
              end
          end

          class Logprobs < OpenAI::Internal::Type::BaseModel
            # A list of message content tokens with log probability information.
            sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionTokenLogprob])) }
            attr_accessor :content

            # A list of message refusal tokens with log probability information.
            sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionTokenLogprob])) }
            attr_accessor :refusal

            sig do
              override
                .returns({
                  content:
                    T.nilable(
                      T::Array[OpenAI::Chat::ChatCompletionTokenLogprob]
                    ),
                  refusal:
                    T.nilable(
                      T::Array[OpenAI::Chat::ChatCompletionTokenLogprob]
                    )
                })
            end
            def to_hash; end

            class << self
              # Log probability information for the choice.
              sig do
                params(
                  content: T.nilable(
                    T::Array[OpenAI::Chat::ChatCompletionTokenLogprob::OrHash]
                  ),
                  refusal: T.nilable(
                    T::Array[OpenAI::Chat::ChatCompletionTokenLogprob::OrHash]
                  )
                ).returns(T.attached_class)
              end
              def new(
                content:, # A list of message content tokens with log probability information.
                refusal: # A list of message refusal tokens with log probability information.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Chat::ChatCompletion::Choice::Logprobs,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletion::Choice,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Chat::ChatCompletion, OpenAI::Internal::AnyHash)
          end

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        module ServiceTier
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Chat::ChatCompletion::ServiceTier::TaggedSymbol]) }
            def values; end
          end

          AUTO = T.let(
              :auto,
              OpenAI::Chat::ChatCompletion::ServiceTier::TaggedSymbol
            )

          DEFAULT = T.let(
              :default,
              OpenAI::Chat::ChatCompletion::ServiceTier::TaggedSymbol
            )

          FLEX = T.let(
              :flex,
              OpenAI::Chat::ChatCompletion::ServiceTier::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          PRIORITY = T.let(
              :priority,
              OpenAI::Chat::ChatCompletion::ServiceTier::TaggedSymbol
            )

          SCALE = T.let(
              :scale,
              OpenAI::Chat::ChatCompletion::ServiceTier::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Chat::ChatCompletion::ServiceTier)
            end
        end
      end

      class ChatCompletionAssistantMessageParam < OpenAI::Internal::Type::BaseModel
        # Data about a previous audio response from the model.
        # [Learn more](https://platform.openai.com/docs/guides/audio).
        sig { returns(T.nilable(OpenAI::Chat::ChatCompletionAssistantMessageParam::Audio)) }
        attr_reader :audio

        sig do
          params(
            audio: T.nilable(
                OpenAI::Chat::ChatCompletionAssistantMessageParam::Audio::OrHash
              )
          ).void
        end
        attr_writer :audio

        # The contents of the assistant message. Required unless `tool_calls` or
        # `function_call` is specified.
        sig do
          returns(T.nilable(
              OpenAI::Chat::ChatCompletionAssistantMessageParam::Content::Variants
            ))
        end
        attr_accessor :content

        # Deprecated and replaced by `tool_calls`. The name and arguments of a function
        # that should be called, as generated by the model.
        sig do
          returns(T.nilable(
              OpenAI::Chat::ChatCompletionAssistantMessageParam::FunctionCall
            ))
        end
        attr_reader :function_call

        sig do
          params(
            function_call: T.nilable(
                OpenAI::Chat::ChatCompletionAssistantMessageParam::FunctionCall::OrHash
              )
          ).void
        end
        attr_writer :function_call

        # An optional name for the participant. Provides the model information to
        # differentiate between participants of the same role.
        sig { returns(T.nilable(String)) }
        attr_reader :name

        sig { params(name: String).void }
        attr_writer :name

        # The refusal message by the assistant.
        sig { returns(T.nilable(String)) }
        attr_accessor :refusal

        # The role of the messages author, in this case `assistant`.
        sig { returns(Symbol) }
        attr_accessor :role

        # The tool calls generated by the model, such as function calls.
        sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionMessageToolCall])) }
        attr_reader :tool_calls

        sig { params(tool_calls: T::Array[OpenAI::Chat::ChatCompletionMessageToolCall::OrHash]).void }
        attr_writer :tool_calls

        sig do
          override
            .returns({
              role: Symbol,
              audio:
                T.nilable(
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::Audio
                ),
              content:
                T.nilable(
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::Content::Variants
                ),
              function_call:
                T.nilable(
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::FunctionCall
                ),
              name: String,
              refusal: T.nilable(String),
              tool_calls: T::Array[OpenAI::Chat::ChatCompletionMessageToolCall]
            })
        end
        def to_hash; end

        class << self
          # Messages sent by the model in response to user messages.
          sig do
            params(
              audio: T.nilable(
                OpenAI::Chat::ChatCompletionAssistantMessageParam::Audio::OrHash
              ),
              content: T.nilable(
                OpenAI::Chat::ChatCompletionAssistantMessageParam::Content::Variants
              ),
              function_call: T.nilable(
                OpenAI::Chat::ChatCompletionAssistantMessageParam::FunctionCall::OrHash
              ),
              name: String,
              refusal: T.nilable(String),
              tool_calls: T::Array[OpenAI::Chat::ChatCompletionMessageToolCall::OrHash],
              role: Symbol
            ).returns(T.attached_class)
          end
          def new(
            audio: nil, # Data about a previous audio response from the model.
                        # [Learn more](https://platform.openai.com/docs/guides/audio).
            content: nil, # The contents of the assistant message. Required unless `tool_calls` or
                          # `function_call` is specified.
            function_call: nil, # Deprecated and replaced by `tool_calls`. The name and arguments of a function
                                # that should be called, as generated by the model.
            name: nil, # An optional name for the participant. Provides the model information to
                       # differentiate between participants of the same role.
            refusal: nil, # The refusal message by the assistant.
            tool_calls: nil, # The tool calls generated by the model, such as function calls.
            role: :assistant # The role of the messages author, in this case `assistant`.
); end
        end

        class Audio < OpenAI::Internal::Type::BaseModel
          # Unique identifier for a previous audio response from the model.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Data about a previous audio response from the model.
            # [Learn more](https://platform.openai.com/docs/guides/audio).
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # Unique identifier for a previous audio response from the model.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionAssistantMessageParam::Audio,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The contents of the assistant message. Required unless `tool_calls` or
        # `function_call` is specified.
        module Content
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionAssistantMessageParam::Content::Variants
              ])
            end
            def variants; end
          end

          # Learn about
          # [text inputs](https://platform.openai.com/docs/guides/text-generation).
          module ArrayOfContentPart
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::Content::ArrayOfContentPart::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias do
                T.any(
                  OpenAI::Chat::ChatCompletionContentPartText,
                  OpenAI::Chat::ChatCompletionContentPartRefusal
                )
              end
          end

          ArrayOfContentPartArray = T.let(
              OpenAI::Internal::Type::ArrayOf[
                union:
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::Content::ArrayOfContentPart
              ],
              OpenAI::Internal::Type::Converter
            )

          Variants = T.type_alias do
              T.any(
                String,
                T::Array[
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::Content::ArrayOfContentPart::Variants
                ]
              )
            end
        end

        class FunctionCall < OpenAI::Internal::Type::BaseModel
          # The arguments to call the function with, as generated by the model in JSON
          # format. Note that the model does not always generate valid JSON, and may
          # hallucinate parameters not defined by your function schema. Validate the
          # arguments in your code before calling your function.
          sig { returns(String) }
          attr_accessor :arguments

          # The name of the function to call.
          sig { returns(String) }
          attr_accessor :name

          sig { override.returns({ arguments: String, name: String }) }
          def to_hash; end

          class << self
            # Deprecated and replaced by `tool_calls`. The name and arguments of a function
            # that should be called, as generated by the model.
            sig { params(arguments: String, name: String).returns(T.attached_class) }
            def new(
              arguments:, # The arguments to call the function with, as generated by the model in JSON
                          # format. Note that the model does not always generate valid JSON, and may
                          # hallucinate parameters not defined by your function schema. Validate the
                          # arguments in your code before calling your function.
              name: # The name of the function to call.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionAssistantMessageParam::FunctionCall,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionAssistantMessageParam,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionAudio < OpenAI::Internal::Type::BaseModel
        # Base64 encoded audio bytes generated by the model, in the format specified in
        # the request.
        sig { returns(String) }
        attr_accessor :data

        # The Unix timestamp (in seconds) for when this audio response will no longer be
        # accessible on the server for use in multi-turn conversations.
        sig { returns(Integer) }
        attr_accessor :expires_at

        # Unique identifier for this audio response.
        sig { returns(String) }
        attr_accessor :id

        # Transcript of the audio generated by the model.
        sig { returns(String) }
        attr_accessor :transcript

        sig do
          override
            .returns({
              id: String,
              data: String,
              expires_at: Integer,
              transcript: String
            })
        end
        def to_hash; end

        class << self
          # If the audio output modality is requested, this object contains data about the
          # audio response from the model.
          # [Learn more](https://platform.openai.com/docs/guides/audio).
          sig { params(id: String, data: String, expires_at: Integer, transcript: String).returns(T.attached_class) }
          def new(
            id:, # Unique identifier for this audio response.
            data:, # Base64 encoded audio bytes generated by the model, in the format specified in
                   # the request.
            expires_at:, # The Unix timestamp (in seconds) for when this audio response will no longer be
                         # accessible on the server for use in multi-turn conversations.
            transcript: # Transcript of the audio generated by the model.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Chat::ChatCompletionAudio, OpenAI::Internal::AnyHash)
          end
      end

      class ChatCompletionAudioParam < OpenAI::Internal::Type::BaseModel
        # Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`, `opus`,
        # or `pcm16`.
        sig { returns(OpenAI::Chat::ChatCompletionAudioParam::Format::OrSymbol) }
        attr_accessor :format_

        # The voice the model uses to respond. Supported voices are `alloy`, `ash`,
        # `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, and `shimmer`.
        sig do
          returns(T.any(
              String,
              OpenAI::Chat::ChatCompletionAudioParam::Voice::OrSymbol
            ))
        end
        attr_accessor :voice

        sig do
          override
            .returns({
              format_: OpenAI::Chat::ChatCompletionAudioParam::Format::OrSymbol,
              voice:
                T.any(
                  String,
                  OpenAI::Chat::ChatCompletionAudioParam::Voice::OrSymbol
                )
            })
        end
        def to_hash; end

        class << self
          # Parameters for audio output. Required when audio output is requested with
          # `modalities: ["audio"]`.
          # [Learn more](https://platform.openai.com/docs/guides/audio).
          sig do
            params(
              format_: OpenAI::Chat::ChatCompletionAudioParam::Format::OrSymbol,
              voice: T.any(
                String,
                OpenAI::Chat::ChatCompletionAudioParam::Voice::OrSymbol
              )
            ).returns(T.attached_class)
          end
          def new(
            format_:, # Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`, `opus`,
                      # or `pcm16`.
            voice: # The voice the model uses to respond. Supported voices are `alloy`, `ash`,
                   # `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, and `shimmer`.
); end
        end

        # Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`, `opus`,
        # or `pcm16`.
        module Format
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionAudioParam::Format::TaggedSymbol
              ])
            end
            def values; end
          end

          AAC = T.let(
              :aac,
              OpenAI::Chat::ChatCompletionAudioParam::Format::TaggedSymbol
            )

          FLAC = T.let(
              :flac,
              OpenAI::Chat::ChatCompletionAudioParam::Format::TaggedSymbol
            )

          MP3 = T.let(
              :mp3,
              OpenAI::Chat::ChatCompletionAudioParam::Format::TaggedSymbol
            )

          OPUS = T.let(
              :opus,
              OpenAI::Chat::ChatCompletionAudioParam::Format::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          PCM16 = T.let(
              :pcm16,
              OpenAI::Chat::ChatCompletionAudioParam::Format::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Chat::ChatCompletionAudioParam::Format)
            end

          WAV = T.let(
              :wav,
              OpenAI::Chat::ChatCompletionAudioParam::Format::TaggedSymbol
            )
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionAudioParam,
              OpenAI::Internal::AnyHash
            )
          end

        # The voice the model uses to respond. Supported voices are `alloy`, `ash`,
        # `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, and `shimmer`.
        module Voice
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Chat::ChatCompletionAudioParam::Voice::Variants]) }
            def variants; end
          end

          ALLOY = T.let(
              :alloy,
              OpenAI::Chat::ChatCompletionAudioParam::Voice::TaggedSymbol
            )

          ASH = T.let(
              :ash,
              OpenAI::Chat::ChatCompletionAudioParam::Voice::TaggedSymbol
            )

          BALLAD = T.let(
              :ballad,
              OpenAI::Chat::ChatCompletionAudioParam::Voice::TaggedSymbol
            )

          CORAL = T.let(
              :coral,
              OpenAI::Chat::ChatCompletionAudioParam::Voice::TaggedSymbol
            )

          ECHO = T.let(
              :echo,
              OpenAI::Chat::ChatCompletionAudioParam::Voice::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          SAGE = T.let(
              :sage,
              OpenAI::Chat::ChatCompletionAudioParam::Voice::TaggedSymbol
            )

          SHIMMER = T.let(
              :shimmer,
              OpenAI::Chat::ChatCompletionAudioParam::Voice::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Chat::ChatCompletionAudioParam::Voice)
            end

          VERSE = T.let(
              :verse,
              OpenAI::Chat::ChatCompletionAudioParam::Voice::TaggedSymbol
            )

          Variants = T.type_alias do
              T.any(
                String,
                OpenAI::Chat::ChatCompletionAudioParam::Voice::TaggedSymbol
              )
            end
        end
      end

      class ChatCompletionChunk < OpenAI::Internal::Type::BaseModel
        # A list of chat completion choices. Can contain more than one elements if `n` is
        # greater than 1. Can also be empty for the last chunk if you set
        # `stream_options: {"include_usage": true}`.
        sig { returns(T::Array[OpenAI::Chat::ChatCompletionChunk::Choice]) }
        attr_accessor :choices

        # The Unix timestamp (in seconds) of when the chat completion was created. Each
        # chunk has the same timestamp.
        sig { returns(Integer) }
        attr_accessor :created

        # A unique identifier for the chat completion. Each chunk has the same ID.
        sig { returns(String) }
        attr_accessor :id

        # The model to generate the completion.
        sig { returns(String) }
        attr_accessor :model

        # The object type, which is always `chat.completion.chunk`.
        sig { returns(Symbol) }
        attr_accessor :object

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        sig do
          returns(T.nilable(
              OpenAI::Chat::ChatCompletionChunk::ServiceTier::TaggedSymbol
            ))
        end
        attr_accessor :service_tier

        # This fingerprint represents the backend configuration that the model runs with.
        # Can be used in conjunction with the `seed` request parameter to understand when
        # backend changes have been made that might impact determinism.
        sig { returns(T.nilable(String)) }
        attr_reader :system_fingerprint

        sig { params(system_fingerprint: String).void }
        attr_writer :system_fingerprint

        # An optional field that will only be present when you set
        # `stream_options: {"include_usage": true}` in your request. When present, it
        # contains a null value **except for the last chunk** which contains the token
        # usage statistics for the entire request.
        #
        # **NOTE:** If the stream is interrupted or cancelled, you may not receive the
        # final usage chunk which contains the total token usage for the request.
        sig { returns(T.nilable(OpenAI::CompletionUsage)) }
        attr_reader :usage

        sig { params(usage: T.nilable(OpenAI::CompletionUsage::OrHash)).void }
        attr_writer :usage

        sig do
          override
            .returns({
              id: String,
              choices: T::Array[OpenAI::Chat::ChatCompletionChunk::Choice],
              created: Integer,
              model: String,
              object: Symbol,
              service_tier:
                T.nilable(
                  OpenAI::Chat::ChatCompletionChunk::ServiceTier::TaggedSymbol
                ),
              system_fingerprint: String,
              usage: T.nilable(OpenAI::CompletionUsage)
            })
        end
        def to_hash; end

        class << self
          # Represents a streamed chunk of a chat completion response returned by the model,
          # based on the provided input.
          # [Learn more](https://platform.openai.com/docs/guides/streaming-responses).
          sig do
            params(
              id: String,
              choices: T::Array[OpenAI::Chat::ChatCompletionChunk::Choice::OrHash],
              created: Integer,
              model: String,
              service_tier: T.nilable(
                OpenAI::Chat::ChatCompletionChunk::ServiceTier::OrSymbol
              ),
              system_fingerprint: String,
              usage: T.nilable(OpenAI::CompletionUsage::OrHash),
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # A unique identifier for the chat completion. Each chunk has the same ID.
            choices:, # A list of chat completion choices. Can contain more than one elements if `n` is
                      # greater than 1. Can also be empty for the last chunk if you set
                      # `stream_options: {"include_usage": true}`.
            created:, # The Unix timestamp (in seconds) of when the chat completion was created. Each
                      # chunk has the same timestamp.
            model:, # The model to generate the completion.
            service_tier: nil, # Specifies the processing type used for serving the request.
                               # - If set to 'auto', then the request will be processed with the service tier
                               #   configured in the Project settings. Unless otherwise configured, the Project
                               #   will use 'default'.
                               # - If set to 'default', then the request will be processed with the standard
                               #   pricing and performance for the selected model.
                               # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
                               #   'priority', then the request will be processed with the corresponding service
                               #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
                               #   Priority processing.
                               # - When not set, the default behavior is 'auto'.
                               # When the `service_tier` parameter is set, the response body will include the
                               # `service_tier` value based on the processing mode actually used to serve the
                               # request. This response value may be different from the value set in the
                               # parameter.
            system_fingerprint: nil, # This fingerprint represents the backend configuration that the model runs with.
                                     # Can be used in conjunction with the `seed` request parameter to understand when
                                     # backend changes have been made that might impact determinism.
            usage: nil, # An optional field that will only be present when you set
                        # `stream_options: {"include_usage": true}` in your request. When present, it
                        # contains a null value **except for the last chunk** which contains the token
                        # usage statistics for the entire request.
                        # **NOTE:** If the stream is interrupted or cancelled, you may not receive the
                        # final usage chunk which contains the total token usage for the request.
            object: :"chat.completion.chunk" # The object type, which is always `chat.completion.chunk`.
); end
        end

        class Choice < OpenAI::Internal::Type::BaseModel
          # A chat completion delta generated by streamed model responses.
          sig { returns(OpenAI::Chat::ChatCompletionChunk::Choice::Delta) }
          attr_reader :delta

          sig { params(delta: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::OrHash).void }
          attr_writer :delta

          # The reason the model stopped generating tokens. This will be `stop` if the model
          # hit a natural stop point or a provided stop sequence, `length` if the maximum
          # number of tokens specified in the request was reached, `content_filter` if
          # content was omitted due to a flag from our content filters, `tool_calls` if the
          # model called a tool, or `function_call` (deprecated) if the model called a
          # function.
          sig do
            returns(T.nilable(
                OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason::TaggedSymbol
              ))
          end
          attr_accessor :finish_reason

          # The index of the choice in the list of choices.
          sig { returns(Integer) }
          attr_accessor :index

          # Log probability information for the choice.
          sig { returns(T.nilable(OpenAI::Chat::ChatCompletionChunk::Choice::Logprobs)) }
          attr_reader :logprobs

          sig do
            params(
              logprobs: T.nilable(
                  OpenAI::Chat::ChatCompletionChunk::Choice::Logprobs::OrHash
                )
            ).void
          end
          attr_writer :logprobs

          sig do
            override
              .returns({
                delta: OpenAI::Chat::ChatCompletionChunk::Choice::Delta,
                finish_reason:
                  T.nilable(
                    OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason::TaggedSymbol
                  ),
                index: Integer,
                logprobs:
                  T.nilable(OpenAI::Chat::ChatCompletionChunk::Choice::Logprobs)
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                delta: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::OrHash,
                finish_reason: T.nilable(
                  OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason::OrSymbol
                ),
                index: Integer,
                logprobs: T.nilable(
                  OpenAI::Chat::ChatCompletionChunk::Choice::Logprobs::OrHash
                )
              ).returns(T.attached_class)
            end
            def new(
              delta:, # A chat completion delta generated by streamed model responses.
              finish_reason:, # The reason the model stopped generating tokens. This will be `stop` if the model
                              # hit a natural stop point or a provided stop sequence, `length` if the maximum
                              # number of tokens specified in the request was reached, `content_filter` if
                              # content was omitted due to a flag from our content filters, `tool_calls` if the
                              # model called a tool, or `function_call` (deprecated) if the model called a
                              # function.
              index:, # The index of the choice in the list of choices.
              logprobs: nil # Log probability information for the choice.
); end
          end

          class Delta < OpenAI::Internal::Type::BaseModel
            # The contents of the chunk message.
            sig { returns(T.nilable(String)) }
            attr_accessor :content

            # Deprecated and replaced by `tool_calls`. The name and arguments of a function
            # that should be called, as generated by the model.
            sig do
              returns(T.nilable(
                  OpenAI::Chat::ChatCompletionChunk::Choice::Delta::FunctionCall
                ))
            end
            attr_reader :function_call

            sig { params(function_call: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::FunctionCall::OrHash).void }
            attr_writer :function_call

            # The refusal message generated by the model.
            sig { returns(T.nilable(String)) }
            attr_accessor :refusal

            # The role of the author of this message.
            sig do
              returns(T.nilable(
                  OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::TaggedSymbol
                ))
            end
            attr_reader :role

            sig { params(role: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::OrSymbol).void }
            attr_writer :role

            sig do
              returns(T.nilable(
                  T::Array[
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall
                  ]
                ))
            end
            attr_reader :tool_calls

            sig do
              params(
                tool_calls: T::Array[
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::OrHash
                  ]
              ).void
            end
            attr_writer :tool_calls

            sig do
              override
                .returns({
                  content: T.nilable(String),
                  function_call:
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::FunctionCall,
                  refusal: T.nilable(String),
                  role:
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::TaggedSymbol,
                  tool_calls:
                    T::Array[
                      OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall
                    ]
                })
            end
            def to_hash; end

            class << self
              # A chat completion delta generated by streamed model responses.
              sig do
                params(
                  content: T.nilable(String),
                  function_call: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::FunctionCall::OrHash,
                  refusal: T.nilable(String),
                  role: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::OrSymbol,
                  tool_calls: T::Array[
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::OrHash
                  ]
                ).returns(T.attached_class)
              end
              def new(
                content: nil, # The contents of the chunk message.
                function_call: nil, # Deprecated and replaced by `tool_calls`. The name and arguments of a function
                                    # that should be called, as generated by the model.
                refusal: nil, # The refusal message generated by the model.
                role: nil, # The role of the author of this message.
                tool_calls: nil
); end
            end

            class FunctionCall < OpenAI::Internal::Type::BaseModel
              # The arguments to call the function with, as generated by the model in JSON
              # format. Note that the model does not always generate valid JSON, and may
              # hallucinate parameters not defined by your function schema. Validate the
              # arguments in your code before calling your function.
              sig { returns(T.nilable(String)) }
              attr_reader :arguments

              sig { params(arguments: String).void }
              attr_writer :arguments

              # The name of the function to call.
              sig { returns(T.nilable(String)) }
              attr_reader :name

              sig { params(name: String).void }
              attr_writer :name

              sig { override.returns({ arguments: String, name: String }) }
              def to_hash; end

              class << self
                # Deprecated and replaced by `tool_calls`. The name and arguments of a function
                # that should be called, as generated by the model.
                sig { params(arguments: String, name: String).returns(T.attached_class) }
                def new(
                  arguments: nil, # The arguments to call the function with, as generated by the model in JSON
                                  # format. Note that the model does not always generate valid JSON, and may
                                  # hallucinate parameters not defined by your function schema. Validate the
                                  # arguments in your code before calling your function.
                  name: nil # The name of the function to call.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::FunctionCall,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Chat::ChatCompletionChunk::Choice::Delta,
                  OpenAI::Internal::AnyHash
                )
              end

            # The role of the author of this message.
            module Role
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::TaggedSymbol
                  ])
                end
                def values; end
              end

              ASSISTANT = T.let(
                  :assistant,
                  OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::TaggedSymbol
                )

              DEVELOPER = T.let(
                  :developer,
                  OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              SYSTEM = T.let(
                  :system,
                  OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::TaggedSymbol
                )

              TOOL = T.let(
                  :tool,
                  OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::TaggedSymbol
                )

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role
                  )
                end

              USER = T.let(
                  :user,
                  OpenAI::Chat::ChatCompletionChunk::Choice::Delta::Role::TaggedSymbol
                )
            end

            class ToolCall < OpenAI::Internal::Type::BaseModel
              sig do
                returns(T.nilable(
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Function
                  ))
              end
              attr_reader :function

              sig do
                params(
                  function: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Function::OrHash
                ).void
              end
              attr_writer :function

              # The ID of the tool call.
              sig { returns(T.nilable(String)) }
              attr_reader :id

              sig { params(id: String).void }
              attr_writer :id

              sig { returns(Integer) }
              attr_accessor :index

              # The type of the tool. Currently, only `function` is supported.
              sig do
                returns(T.nilable(
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Type::TaggedSymbol
                  ))
              end
              attr_reader :type

              sig { params(type: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Type::OrSymbol).void }
              attr_writer :type

              sig do
                override
                  .returns({
                    index: Integer,
                    id: String,
                    function:
                      OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Function,
                    type:
                      OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Type::TaggedSymbol
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    index: Integer,
                    id: String,
                    function: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Function::OrHash,
                    type: OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Type::OrSymbol
                  ).returns(T.attached_class)
                end
                def new(
                  index:,
                  id: nil, # The ID of the tool call.
                  function: nil,
                  type: nil # The type of the tool. Currently, only `function` is supported.
); end
              end

              class Function < OpenAI::Internal::Type::BaseModel
                # The arguments to call the function with, as generated by the model in JSON
                # format. Note that the model does not always generate valid JSON, and may
                # hallucinate parameters not defined by your function schema. Validate the
                # arguments in your code before calling your function.
                sig { returns(T.nilable(String)) }
                attr_reader :arguments

                sig { params(arguments: String).void }
                attr_writer :arguments

                # The name of the function to call.
                sig { returns(T.nilable(String)) }
                attr_reader :name

                sig { params(name: String).void }
                attr_writer :name

                sig { override.returns({ arguments: String, name: String }) }
                def to_hash; end

                class << self
                  sig { params(arguments: String, name: String).returns(T.attached_class) }
                  def new(
                    arguments: nil, # The arguments to call the function with, as generated by the model in JSON
                                    # format. Note that the model does not always generate valid JSON, and may
                                    # hallucinate parameters not defined by your function schema. Validate the
                                    # arguments in your code before calling your function.
                    name: nil # The name of the function to call.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Function,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall,
                    OpenAI::Internal::AnyHash
                  )
                end

              # The type of the tool. Currently, only `function` is supported.
              module Type
                extend OpenAI::Internal::Type::Enum

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Type::TaggedSymbol
                    ])
                  end
                  def values; end
                end

                FUNCTION = T.let(
                    :function,
                    OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Type::TaggedSymbol
                  )

                OrSymbol = T.type_alias { T.any(Symbol, String) }

                TaggedSymbol = T.type_alias do
                    T.all(
                      Symbol,
                      OpenAI::Chat::ChatCompletionChunk::Choice::Delta::ToolCall::Type
                    )
                  end
              end
            end
          end

          # The reason the model stopped generating tokens. This will be `stop` if the model
          # hit a natural stop point or a provided stop sequence, `length` if the maximum
          # number of tokens specified in the request was reached, `content_filter` if
          # content was omitted due to a flag from our content filters, `tool_calls` if the
          # model called a tool, or `function_call` (deprecated) if the model called a
          # function.
          module FinishReason
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason::TaggedSymbol
                ])
              end
              def values; end
            end

            CONTENT_FILTER = T.let(
                :content_filter,
                OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason::TaggedSymbol
              )

            FUNCTION_CALL = T.let(
                :function_call,
                OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason::TaggedSymbol
              )

            LENGTH = T.let(
                :length,
                OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            STOP = T.let(
                :stop,
                OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason::TaggedSymbol
              )

            TOOL_CALLS = T.let(
                :tool_calls,
                OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Chat::ChatCompletionChunk::Choice::FinishReason
                )
              end
          end

          class Logprobs < OpenAI::Internal::Type::BaseModel
            # A list of message content tokens with log probability information.
            sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionTokenLogprob])) }
            attr_accessor :content

            # A list of message refusal tokens with log probability information.
            sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionTokenLogprob])) }
            attr_accessor :refusal

            sig do
              override
                .returns({
                  content:
                    T.nilable(
                      T::Array[OpenAI::Chat::ChatCompletionTokenLogprob]
                    ),
                  refusal:
                    T.nilable(
                      T::Array[OpenAI::Chat::ChatCompletionTokenLogprob]
                    )
                })
            end
            def to_hash; end

            class << self
              # Log probability information for the choice.
              sig do
                params(
                  content: T.nilable(
                    T::Array[OpenAI::Chat::ChatCompletionTokenLogprob::OrHash]
                  ),
                  refusal: T.nilable(
                    T::Array[OpenAI::Chat::ChatCompletionTokenLogprob::OrHash]
                  )
                ).returns(T.attached_class)
              end
              def new(
                content:, # A list of message content tokens with log probability information.
                refusal: # A list of message refusal tokens with log probability information.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Chat::ChatCompletionChunk::Choice::Logprobs,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionChunk::Choice,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Chat::ChatCompletionChunk, OpenAI::Internal::AnyHash)
          end

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        module ServiceTier
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionChunk::ServiceTier::TaggedSymbol
              ])
            end
            def values; end
          end

          AUTO = T.let(
              :auto,
              OpenAI::Chat::ChatCompletionChunk::ServiceTier::TaggedSymbol
            )

          DEFAULT = T.let(
              :default,
              OpenAI::Chat::ChatCompletionChunk::ServiceTier::TaggedSymbol
            )

          FLEX = T.let(
              :flex,
              OpenAI::Chat::ChatCompletionChunk::ServiceTier::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          PRIORITY = T.let(
              :priority,
              OpenAI::Chat::ChatCompletionChunk::ServiceTier::TaggedSymbol
            )

          SCALE = T.let(
              :scale,
              OpenAI::Chat::ChatCompletionChunk::ServiceTier::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Chat::ChatCompletionChunk::ServiceTier)
            end
        end
      end

      # Learn about
      # [text inputs](https://platform.openai.com/docs/guides/text-generation).
      module ChatCompletionContentPart
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Chat::ChatCompletionContentPart::Variants]) }
          def variants; end
        end

        class File < OpenAI::Internal::Type::BaseModel
          sig { returns(OpenAI::Chat::ChatCompletionContentPart::File::File) }
          attr_reader :file

          sig { params(file: OpenAI::Chat::ChatCompletionContentPart::File::File::OrHash).void }
          attr_writer :file

          # The type of the content part. Always `file`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                file: OpenAI::Chat::ChatCompletionContentPart::File::File,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # Learn about [file inputs](https://platform.openai.com/docs/guides/text) for text
            # generation.
            sig do
              params(
                file: OpenAI::Chat::ChatCompletionContentPart::File::File::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              file:,
              type: :file # The type of the content part. Always `file`.
); end
          end

          class File < OpenAI::Internal::Type::BaseModel
            # The base64 encoded file data, used when passing the file to the model as a
            # string.
            sig { returns(T.nilable(String)) }
            attr_reader :file_data

            sig { params(file_data: String).void }
            attr_writer :file_data

            # The ID of an uploaded file to use as input.
            sig { returns(T.nilable(String)) }
            attr_reader :file_id

            sig { params(file_id: String).void }
            attr_writer :file_id

            # The name of the file, used when passing the file to the model as a string.
            sig { returns(T.nilable(String)) }
            attr_reader :filename

            sig { params(filename: String).void }
            attr_writer :filename

            sig { override.returns({ file_data: String, file_id: String, filename: String }) }
            def to_hash; end

            class << self
              sig { params(file_data: String, file_id: String, filename: String).returns(T.attached_class) }
              def new(
                file_data: nil, # The base64 encoded file data, used when passing the file to the model as a
                                # string.
                file_id: nil, # The ID of an uploaded file to use as input.
                filename: nil # The name of the file, used when passing the file to the model as a string.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Chat::ChatCompletionContentPart::File::File,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionContentPart::File,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionContentPartText,
              OpenAI::Chat::ChatCompletionContentPartImage,
              OpenAI::Chat::ChatCompletionContentPartInputAudio,
              OpenAI::Chat::ChatCompletionContentPart::File
            )
          end
      end

      class ChatCompletionContentPartImage < OpenAI::Internal::Type::BaseModel
        sig { returns(OpenAI::Chat::ChatCompletionContentPartImage::ImageURL) }
        attr_reader :image_url

        sig { params(image_url: OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::OrHash).void }
        attr_writer :image_url

        # The type of the content part.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              image_url: OpenAI::Chat::ChatCompletionContentPartImage::ImageURL,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Learn about [image inputs](https://platform.openai.com/docs/guides/vision).
          sig do
            params(
              image_url: OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::OrHash,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            image_url:,
            type: :image_url # The type of the content part.
); end
        end

        class ImageURL < OpenAI::Internal::Type::BaseModel
          # Specifies the detail level of the image. Learn more in the
          # [Vision guide](https://platform.openai.com/docs/guides/vision#low-or-high-fidelity-image-understanding).
          sig do
            returns(T.nilable(
                OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::Detail::OrSymbol
              ))
          end
          attr_reader :detail

          sig { params(detail: OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::Detail::OrSymbol).void }
          attr_writer :detail

          # Either a URL of the image or the base64 encoded image data.
          sig { returns(String) }
          attr_accessor :url

          sig do
            override
              .returns({
                url: String,
                detail:
                  OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::Detail::OrSymbol
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                url: String,
                detail: OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::Detail::OrSymbol
              ).returns(T.attached_class)
            end
            def new(
              url:, # Either a URL of the image or the base64 encoded image data.
              detail: nil # Specifies the detail level of the image. Learn more in the
                          # [Vision guide](https://platform.openai.com/docs/guides/vision#low-or-high-fidelity-image-understanding).
); end
          end

          # Specifies the detail level of the image. Learn more in the
          # [Vision guide](https://platform.openai.com/docs/guides/vision#low-or-high-fidelity-image-understanding).
          module Detail
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::Detail::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::Detail::TaggedSymbol
              )

            HIGH = T.let(
                :high,
                OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::Detail::TaggedSymbol
              )

            LOW = T.let(
                :low,
                OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::Detail::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Chat::ChatCompletionContentPartImage::ImageURL::Detail
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionContentPartImage::ImageURL,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionContentPartImage,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionContentPartInputAudio < OpenAI::Internal::Type::BaseModel
        sig { returns(OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio) }
        attr_reader :input_audio

        sig { params(input_audio: OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio::OrHash).void }
        attr_writer :input_audio

        # The type of the content part. Always `input_audio`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              input_audio:
                OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Learn about [audio inputs](https://platform.openai.com/docs/guides/audio).
          sig do
            params(
              input_audio: OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio::OrHash,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            input_audio:,
            type: :input_audio # The type of the content part. Always `input_audio`.
); end
        end

        class InputAudio < OpenAI::Internal::Type::BaseModel
          # Base64 encoded audio data.
          sig { returns(String) }
          attr_accessor :data

          # The format of the encoded audio data. Currently supports "wav" and "mp3".
          sig { returns(OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio::Format::OrSymbol) }
          attr_accessor :format_

          sig do
            override
              .returns({
                data: String,
                format_:
                  OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio::Format::OrSymbol
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                data: String,
                format_: OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio::Format::OrSymbol
              ).returns(T.attached_class)
            end
            def new(
              data:, # Base64 encoded audio data.
              format_: # The format of the encoded audio data. Currently supports "wav" and "mp3".
); end
          end

          # The format of the encoded audio data. Currently supports "wav" and "mp3".
          module Format
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio::Format::TaggedSymbol
                ])
              end
              def values; end
            end

            MP3 = T.let(
                :mp3,
                OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio::Format::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio::Format
                )
              end

            WAV = T.let(
                :wav,
                OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio::Format::TaggedSymbol
              )
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionContentPartInputAudio::InputAudio,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionContentPartInputAudio,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionContentPartRefusal < OpenAI::Internal::Type::BaseModel
        # The refusal message generated by the model.
        sig { returns(String) }
        attr_accessor :refusal

        # The type of the content part.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ refusal: String, type: Symbol }) }
        def to_hash; end

        class << self
          sig { params(refusal: String, type: Symbol).returns(T.attached_class) }
          def new(
            refusal:, # The refusal message generated by the model.
            type: :refusal # The type of the content part.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionContentPartRefusal,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionContentPartText < OpenAI::Internal::Type::BaseModel
        # The text content.
        sig { returns(String) }
        attr_accessor :text

        # The type of the content part.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ text: String, type: Symbol }) }
        def to_hash; end

        class << self
          # Learn about
          # [text inputs](https://platform.openai.com/docs/guides/text-generation).
          sig { params(text: String, type: Symbol).returns(T.attached_class) }
          def new(
            text:, # The text content.
            type: :text # The type of the content part.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionContentPartText,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionDeleted < OpenAI::Internal::Type::BaseModel
        # Whether the chat completion was deleted.
        sig { returns(T::Boolean) }
        attr_accessor :deleted

        # The ID of the chat completion that was deleted.
        sig { returns(String) }
        attr_accessor :id

        # The type of object being deleted.
        sig { returns(Symbol) }
        attr_accessor :object

        sig { override.returns({ id: String, deleted: T::Boolean, object: Symbol }) }
        def to_hash; end

        class << self
          sig { params(id: String, deleted: T::Boolean, object: Symbol).returns(T.attached_class) }
          def new(
            id:, # The ID of the chat completion that was deleted.
            deleted:, # Whether the chat completion was deleted.
            object: :"chat.completion.deleted" # The type of object being deleted.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionDeleted,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionDeveloperMessageParam < OpenAI::Internal::Type::BaseModel
        # The contents of the developer message.
        sig { returns(OpenAI::Chat::ChatCompletionDeveloperMessageParam::Content::Variants) }
        attr_accessor :content

        # An optional name for the participant. Provides the model information to
        # differentiate between participants of the same role.
        sig { returns(T.nilable(String)) }
        attr_reader :name

        sig { params(name: String).void }
        attr_writer :name

        # The role of the messages author, in this case `developer`.
        sig { returns(Symbol) }
        attr_accessor :role

        sig do
          override
            .returns({
              content:
                OpenAI::Chat::ChatCompletionDeveloperMessageParam::Content::Variants,
              role: Symbol,
              name: String
            })
        end
        def to_hash; end

        class << self
          # Developer-provided instructions that the model should follow, regardless of
          # messages sent by the user. With o1 models and newer, `developer` messages
          # replace the previous `system` messages.
          sig do
            params(
              content: OpenAI::Chat::ChatCompletionDeveloperMessageParam::Content::Variants,
              name: String,
              role: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content:, # The contents of the developer message.
            name: nil, # An optional name for the participant. Provides the model information to
                       # differentiate between participants of the same role.
            role: :developer # The role of the messages author, in this case `developer`.
); end
        end

        # The contents of the developer message.
        module Content
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionDeveloperMessageParam::Content::Variants
              ])
            end
            def variants; end
          end

          ChatCompletionContentPartTextArray = T.let(
              OpenAI::Internal::Type::ArrayOf[
                OpenAI::Chat::ChatCompletionContentPartText
              ],
              OpenAI::Internal::Type::Converter
            )

          Variants = T.type_alias do
              T.any(
                String,
                T::Array[OpenAI::Chat::ChatCompletionContentPartText]
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionDeveloperMessageParam,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionFunctionCallOption < OpenAI::Internal::Type::BaseModel
        # The name of the function to call.
        sig { returns(String) }
        attr_accessor :name

        sig { override.returns({ name: String }) }
        def to_hash; end

        class << self
          # Specifying a particular function via `{"name": "my_function"}` forces the model
          # to call that function.
          sig { params(name: String).returns(T.attached_class) }
          def new(
            name: # The name of the function to call.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionFunctionCallOption,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionFunctionMessageParam < OpenAI::Internal::Type::BaseModel
        # The contents of the function message.
        sig { returns(T.nilable(String)) }
        attr_accessor :content

        # The name of the function to call.
        sig { returns(String) }
        attr_accessor :name

        # The role of the messages author, in this case `function`.
        sig { returns(Symbol) }
        attr_accessor :role

        sig { override.returns({ content: T.nilable(String), name: String, role: Symbol }) }
        def to_hash; end

        class << self
          sig { params(content: T.nilable(String), name: String, role: Symbol).returns(T.attached_class) }
          def new(
            content:, # The contents of the function message.
            name:, # The name of the function to call.
            role: :function # The role of the messages author, in this case `function`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionFunctionMessageParam,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionMessage < OpenAI::Internal::Type::BaseModel
        # Annotations for the message, when applicable, as when using the
        # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
        sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionMessage::Annotation])) }
        attr_reader :annotations

        sig { params(annotations: T::Array[OpenAI::Chat::ChatCompletionMessage::Annotation::OrHash]).void }
        attr_writer :annotations

        # If the audio output modality is requested, this object contains data about the
        # audio response from the model.
        # [Learn more](https://platform.openai.com/docs/guides/audio).
        sig { returns(T.nilable(OpenAI::Chat::ChatCompletionAudio)) }
        attr_reader :audio

        sig { params(audio: T.nilable(OpenAI::Chat::ChatCompletionAudio::OrHash)).void }
        attr_writer :audio

        # The contents of the message.
        sig { returns(T.nilable(String)) }
        attr_accessor :content

        # Deprecated and replaced by `tool_calls`. The name and arguments of a function
        # that should be called, as generated by the model.
        sig { returns(T.nilable(OpenAI::Chat::ChatCompletionMessage::FunctionCall)) }
        attr_reader :function_call

        sig { params(function_call: OpenAI::Chat::ChatCompletionMessage::FunctionCall::OrHash).void }
        attr_writer :function_call

        # The parsed contents of the message, if JSON schema is specified.
        sig { returns(T.nilable(T.anything)) }
        attr_accessor :parsed

        # The refusal message generated by the model.
        sig { returns(T.nilable(String)) }
        attr_accessor :refusal

        # The role of the author of this message.
        sig { returns(Symbol) }
        attr_accessor :role

        # The tool calls generated by the model, such as function calls.
        sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionMessageToolCall])) }
        attr_reader :tool_calls

        sig { params(tool_calls: T::Array[OpenAI::Chat::ChatCompletionMessageToolCall::OrHash]).void }
        attr_writer :tool_calls

        sig do
          override
            .returns({
              content: T.nilable(String),
              refusal: T.nilable(String),
              role: Symbol,
              annotations:
                T::Array[OpenAI::Chat::ChatCompletionMessage::Annotation],
              audio: T.nilable(OpenAI::Chat::ChatCompletionAudio),
              function_call: OpenAI::Chat::ChatCompletionMessage::FunctionCall,
              tool_calls: T::Array[OpenAI::Chat::ChatCompletionMessageToolCall]
            })
        end
        def to_hash; end

        class << self
          # A chat completion message generated by the model.
          sig do
            params(
              content: T.nilable(String),
              refusal: T.nilable(String),
              annotations: T::Array[OpenAI::Chat::ChatCompletionMessage::Annotation::OrHash],
              audio: T.nilable(OpenAI::Chat::ChatCompletionAudio::OrHash),
              function_call: OpenAI::Chat::ChatCompletionMessage::FunctionCall::OrHash,
              tool_calls: T::Array[OpenAI::Chat::ChatCompletionMessageToolCall::OrHash],
              role: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content:, # The contents of the message.
            refusal:, # The refusal message generated by the model.
            annotations: nil, # Annotations for the message, when applicable, as when using the
                              # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
            audio: nil, # If the audio output modality is requested, this object contains data about the
                        # audio response from the model.
                        # [Learn more](https://platform.openai.com/docs/guides/audio).
            function_call: nil, # Deprecated and replaced by `tool_calls`. The name and arguments of a function
                                # that should be called, as generated by the model.
            tool_calls: nil, # The tool calls generated by the model, such as function calls.
            role: :assistant # The role of the author of this message.
); end
        end

        class Annotation < OpenAI::Internal::Type::BaseModel
          # The type of the URL citation. Always `url_citation`.
          sig { returns(Symbol) }
          attr_accessor :type

          # A URL citation when using web search.
          sig { returns(OpenAI::Chat::ChatCompletionMessage::Annotation::URLCitation) }
          attr_reader :url_citation

          sig { params(url_citation: OpenAI::Chat::ChatCompletionMessage::Annotation::URLCitation::OrHash).void }
          attr_writer :url_citation

          sig do
            override
              .returns({
                type: Symbol,
                url_citation:
                  OpenAI::Chat::ChatCompletionMessage::Annotation::URLCitation
              })
          end
          def to_hash; end

          class << self
            # A URL citation when using web search.
            sig do
              params(
                url_citation: OpenAI::Chat::ChatCompletionMessage::Annotation::URLCitation::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              url_citation:, # A URL citation when using web search.
              type: :url_citation # The type of the URL citation. Always `url_citation`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionMessage::Annotation,
                OpenAI::Internal::AnyHash
              )
            end

          class URLCitation < OpenAI::Internal::Type::BaseModel
            # The index of the last character of the URL citation in the message.
            sig { returns(Integer) }
            attr_accessor :end_index

            # The index of the first character of the URL citation in the message.
            sig { returns(Integer) }
            attr_accessor :start_index

            # The title of the web resource.
            sig { returns(String) }
            attr_accessor :title

            # The URL of the web resource.
            sig { returns(String) }
            attr_accessor :url

            sig do
              override
                .returns({
                  end_index: Integer,
                  start_index: Integer,
                  title: String,
                  url: String
                })
            end
            def to_hash; end

            class << self
              # A URL citation when using web search.
              sig do
                params(
                  end_index: Integer,
                  start_index: Integer,
                  title: String,
                  url: String
                ).returns(T.attached_class)
              end
              def new(
                end_index:, # The index of the last character of the URL citation in the message.
                start_index:, # The index of the first character of the URL citation in the message.
                title:, # The title of the web resource.
                url: # The URL of the web resource.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Chat::ChatCompletionMessage::Annotation::URLCitation,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        class FunctionCall < OpenAI::Internal::Type::BaseModel
          # The arguments to call the function with, as generated by the model in JSON
          # format. Note that the model does not always generate valid JSON, and may
          # hallucinate parameters not defined by your function schema. Validate the
          # arguments in your code before calling your function.
          sig { returns(String) }
          attr_accessor :arguments

          # The name of the function to call.
          sig { returns(String) }
          attr_accessor :name

          sig { override.returns({ arguments: String, name: String }) }
          def to_hash; end

          class << self
            # Deprecated and replaced by `tool_calls`. The name and arguments of a function
            # that should be called, as generated by the model.
            sig { params(arguments: String, name: String).returns(T.attached_class) }
            def new(
              arguments:, # The arguments to call the function with, as generated by the model in JSON
                          # format. Note that the model does not always generate valid JSON, and may
                          # hallucinate parameters not defined by your function schema. Validate the
                          # arguments in your code before calling your function.
              name: # The name of the function to call.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionMessage::FunctionCall,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionMessage,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # Developer-provided instructions that the model should follow, regardless of
      # messages sent by the user. With o1 models and newer, `developer` messages
      # replace the previous `system` messages.
      module ChatCompletionMessageParam
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Chat::ChatCompletionMessageParam::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionDeveloperMessageParam,
              OpenAI::Chat::ChatCompletionSystemMessageParam,
              OpenAI::Chat::ChatCompletionUserMessageParam,
              OpenAI::Chat::ChatCompletionAssistantMessageParam,
              OpenAI::Chat::ChatCompletionToolMessageParam,
              OpenAI::Chat::ChatCompletionFunctionMessageParam
            )
          end
      end

      class ChatCompletionMessageToolCall < OpenAI::Internal::Type::BaseModel
        # The function that the model called.
        sig { returns(OpenAI::Chat::ChatCompletionMessageToolCall::Function) }
        attr_reader :function

        sig { params(function: OpenAI::Chat::ChatCompletionMessageToolCall::Function::OrHash).void }
        attr_writer :function

        # The ID of the tool call.
        sig { returns(String) }
        attr_accessor :id

        # The type of the tool. Currently, only `function` is supported.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              function: OpenAI::Chat::ChatCompletionMessageToolCall::Function,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              id: String,
              function: OpenAI::Chat::ChatCompletionMessageToolCall::Function::OrHash,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The ID of the tool call.
            function:, # The function that the model called.
            type: :function # The type of the tool. Currently, only `function` is supported.
); end
        end

        class Function < OpenAI::Internal::Type::BaseModel
          # The arguments to call the function with, as generated by the model in JSON
          # format. Note that the model does not always generate valid JSON, and may
          # hallucinate parameters not defined by your function schema. Validate the
          # arguments in your code before calling your function.
          sig { returns(String) }
          attr_accessor :arguments

          # The name of the function to call.
          sig { returns(String) }
          attr_accessor :name

          # The parsed contents of the arguments.
          sig { returns(T.anything) }
          attr_accessor :parsed

          sig { override.returns({ arguments: String, name: String }) }
          def to_hash; end

          class << self
            # The function that the model called.
            sig { params(arguments: String, name: String).returns(T.attached_class) }
            def new(
              arguments:, # The arguments to call the function with, as generated by the model in JSON
                          # format. Note that the model does not always generate valid JSON, and may
                          # hallucinate parameters not defined by your function schema. Validate the
                          # arguments in your code before calling your function.
              name: # The name of the function to call.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionMessageToolCall::Function,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionMessageToolCall,
              OpenAI::Internal::AnyHash
            )
          end
      end

      module ChatCompletionModality
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Chat::ChatCompletionModality::TaggedSymbol]) }
          def values; end
        end

        AUDIO = T.let(:audio, OpenAI::Chat::ChatCompletionModality::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }
        TEXT = T.let(:text, OpenAI::Chat::ChatCompletionModality::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Chat::ChatCompletionModality) }
      end

      class ChatCompletionNamedToolChoice < OpenAI::Internal::Type::BaseModel
        sig { returns(OpenAI::Chat::ChatCompletionNamedToolChoice::Function) }
        attr_reader :function

        sig { params(function: OpenAI::Chat::ChatCompletionNamedToolChoice::Function::OrHash).void }
        attr_writer :function

        # The type of the tool. Currently, only `function` is supported.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              function: OpenAI::Chat::ChatCompletionNamedToolChoice::Function,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Specifies a tool the model should use. Use to force the model to call a specific
          # function.
          sig do
            params(
              function: OpenAI::Chat::ChatCompletionNamedToolChoice::Function::OrHash,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            function:,
            type: :function # The type of the tool. Currently, only `function` is supported.
); end
        end

        class Function < OpenAI::Internal::Type::BaseModel
          # The name of the function to call.
          sig { returns(String) }
          attr_accessor :name

          sig { override.returns({ name: String }) }
          def to_hash; end

          class << self
            sig { params(name: String).returns(T.attached_class) }
            def new(
              name: # The name of the function to call.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionNamedToolChoice::Function,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionNamedToolChoice,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionPredictionContent < OpenAI::Internal::Type::BaseModel
        # The content that should be matched when generating a model response. If
        # generated tokens would match this content, the entire model response can be
        # returned much more quickly.
        sig { returns(OpenAI::Chat::ChatCompletionPredictionContent::Content::Variants) }
        attr_accessor :content

        # The type of the predicted content you want to provide. This type is currently
        # always `content`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              content:
                OpenAI::Chat::ChatCompletionPredictionContent::Content::Variants,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Static predicted output content, such as the content of a text file that is
          # being regenerated.
          sig do
            params(
              content: OpenAI::Chat::ChatCompletionPredictionContent::Content::Variants,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content:, # The content that should be matched when generating a model response. If
                      # generated tokens would match this content, the entire model response can be
                      # returned much more quickly.
            type: :content # The type of the predicted content you want to provide. This type is currently
                           # always `content`.
); end
        end

        # The content that should be matched when generating a model response. If
        # generated tokens would match this content, the entire model response can be
        # returned much more quickly.
        module Content
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionPredictionContent::Content::Variants
              ])
            end
            def variants; end
          end

          ChatCompletionContentPartTextArray = T.let(
              OpenAI::Internal::Type::ArrayOf[
                OpenAI::Chat::ChatCompletionContentPartText
              ],
              OpenAI::Internal::Type::Converter
            )

          Variants = T.type_alias do
              T.any(
                String,
                T::Array[OpenAI::Chat::ChatCompletionContentPartText]
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionPredictionContent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      ChatCompletionReasoningEffort = OpenAI::Models::ReasoningEffort

      # The role of the author of a message
      module ChatCompletionRole
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Chat::ChatCompletionRole::TaggedSymbol]) }
          def values; end
        end

        ASSISTANT = T.let(:assistant, OpenAI::Chat::ChatCompletionRole::TaggedSymbol)

        DEVELOPER = T.let(:developer, OpenAI::Chat::ChatCompletionRole::TaggedSymbol)

        FUNCTION = T.let(:function, OpenAI::Chat::ChatCompletionRole::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }
        SYSTEM = T.let(:system, OpenAI::Chat::ChatCompletionRole::TaggedSymbol)
        TOOL = T.let(:tool, OpenAI::Chat::ChatCompletionRole::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Chat::ChatCompletionRole) }

        USER = T.let(:user, OpenAI::Chat::ChatCompletionRole::TaggedSymbol)
      end

      class ChatCompletionStoreMessage < OpenAI::Models::Chat::ChatCompletionMessage
        # If a content parts array was provided, this is an array of `text` and
        # `image_url` parts. Otherwise, null.
        sig do
          returns(T.nilable(
              T::Array[
                OpenAI::Chat::ChatCompletionStoreMessage::ContentPart::Variants
              ]
            ))
        end
        attr_accessor :content_parts

        # The identifier of the chat message.
        sig { returns(String) }
        attr_accessor :id

        sig do
          override
            .returns({
              id: String,
              content_parts:
                T.nilable(
                  T::Array[
                    OpenAI::Chat::ChatCompletionStoreMessage::ContentPart::Variants
                  ]
                )
            })
        end
        def to_hash; end

        class << self
          # A chat completion message generated by the model.
          sig do
            params(
              id: String,
              content_parts: T.nilable(
                T::Array[
                  T.any(
                    OpenAI::Chat::ChatCompletionContentPartText::OrHash,
                    OpenAI::Chat::ChatCompletionContentPartImage::OrHash
                  )
                ]
              )
            ).returns(T.attached_class)
          end
          def new(
            id:, # The identifier of the chat message.
            content_parts: nil # If a content parts array was provided, this is an array of `text` and
                               # `image_url` parts. Otherwise, null.
); end
        end

        # Learn about
        # [text inputs](https://platform.openai.com/docs/guides/text-generation).
        module ContentPart
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionStoreMessage::ContentPart::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionContentPartText,
                OpenAI::Chat::ChatCompletionContentPartImage
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionStoreMessage,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionStreamOptions < OpenAI::Internal::Type::BaseModel
        # If set, an additional chunk will be streamed before the `data: [DONE]` message.
        # The `usage` field on this chunk shows the token usage statistics for the entire
        # request, and the `choices` field will always be an empty array.
        #
        # All other chunks will also include a `usage` field, but with a null value.
        # **NOTE:** If the stream is interrupted, you may not receive the final usage
        # chunk which contains the total token usage for the request.
        sig { returns(T.nilable(T::Boolean)) }
        attr_reader :include_usage

        sig { params(include_usage: T::Boolean).void }
        attr_writer :include_usage

        sig { override.returns({ include_usage: T::Boolean }) }
        def to_hash; end

        class << self
          # Options for streaming response. Only set this when you set `stream: true`.
          sig { params(include_usage: T::Boolean).returns(T.attached_class) }
          def new(
            include_usage: nil # If set, an additional chunk will be streamed before the `data: [DONE]` message.
                               # The `usage` field on this chunk shows the token usage statistics for the entire
                               # request, and the `choices` field will always be an empty array.
                               # All other chunks will also include a `usage` field, but with a null value.
                               # **NOTE:** If the stream is interrupted, you may not receive the final usage
                               # chunk which contains the total token usage for the request.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionStreamOptions,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionSystemMessageParam < OpenAI::Internal::Type::BaseModel
        # The contents of the system message.
        sig { returns(OpenAI::Chat::ChatCompletionSystemMessageParam::Content::Variants) }
        attr_accessor :content

        # An optional name for the participant. Provides the model information to
        # differentiate between participants of the same role.
        sig { returns(T.nilable(String)) }
        attr_reader :name

        sig { params(name: String).void }
        attr_writer :name

        # The role of the messages author, in this case `system`.
        sig { returns(Symbol) }
        attr_accessor :role

        sig do
          override
            .returns({
              content:
                OpenAI::Chat::ChatCompletionSystemMessageParam::Content::Variants,
              role: Symbol,
              name: String
            })
        end
        def to_hash; end

        class << self
          # Developer-provided instructions that the model should follow, regardless of
          # messages sent by the user. With o1 models and newer, use `developer` messages
          # for this purpose instead.
          sig do
            params(
              content: OpenAI::Chat::ChatCompletionSystemMessageParam::Content::Variants,
              name: String,
              role: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content:, # The contents of the system message.
            name: nil, # An optional name for the participant. Provides the model information to
                       # differentiate between participants of the same role.
            role: :system # The role of the messages author, in this case `system`.
); end
        end

        # The contents of the system message.
        module Content
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionSystemMessageParam::Content::Variants
              ])
            end
            def variants; end
          end

          ChatCompletionContentPartTextArray = T.let(
              OpenAI::Internal::Type::ArrayOf[
                OpenAI::Chat::ChatCompletionContentPartText
              ],
              OpenAI::Internal::Type::Converter
            )

          Variants = T.type_alias do
              T.any(
                String,
                T::Array[OpenAI::Chat::ChatCompletionContentPartText]
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionSystemMessageParam,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionTokenLogprob < OpenAI::Internal::Type::BaseModel
        # A list of integers representing the UTF-8 bytes representation of the token.
        # Useful in instances where characters are represented by multiple tokens and
        # their byte representations must be combined to generate the correct text
        # representation. Can be `null` if there is no bytes representation for the token.
        sig { returns(T.nilable(T::Array[Integer])) }
        attr_accessor :bytes

        # The log probability of this token, if it is within the top 20 most likely
        # tokens. Otherwise, the value `-9999.0` is used to signify that the token is very
        # unlikely.
        sig { returns(Float) }
        attr_accessor :logprob

        # The token.
        sig { returns(String) }
        attr_accessor :token

        # List of the most likely tokens and their log probability, at this token
        # position. In rare cases, there may be fewer than the number of requested
        # `top_logprobs` returned.
        sig { returns(T::Array[OpenAI::Chat::ChatCompletionTokenLogprob::TopLogprob]) }
        attr_accessor :top_logprobs

        sig do
          override
            .returns({
              token: String,
              bytes: T.nilable(T::Array[Integer]),
              logprob: Float,
              top_logprobs:
                T::Array[OpenAI::Chat::ChatCompletionTokenLogprob::TopLogprob]
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              token: String,
              bytes: T.nilable(T::Array[Integer]),
              logprob: Float,
              top_logprobs: T::Array[
                OpenAI::Chat::ChatCompletionTokenLogprob::TopLogprob::OrHash
              ]
            ).returns(T.attached_class)
          end
          def new(
            token:, # The token.
            bytes:, # A list of integers representing the UTF-8 bytes representation of the token.
                    # Useful in instances where characters are represented by multiple tokens and
                    # their byte representations must be combined to generate the correct text
                    # representation. Can be `null` if there is no bytes representation for the token.
            logprob:, # The log probability of this token, if it is within the top 20 most likely
                      # tokens. Otherwise, the value `-9999.0` is used to signify that the token is very
                      # unlikely.
            top_logprobs: # List of the most likely tokens and their log probability, at this token
                          # position. In rare cases, there may be fewer than the number of requested
                          # `top_logprobs` returned.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionTokenLogprob,
              OpenAI::Internal::AnyHash
            )
          end

        class TopLogprob < OpenAI::Internal::Type::BaseModel
          # A list of integers representing the UTF-8 bytes representation of the token.
          # Useful in instances where characters are represented by multiple tokens and
          # their byte representations must be combined to generate the correct text
          # representation. Can be `null` if there is no bytes representation for the token.
          sig { returns(T.nilable(T::Array[Integer])) }
          attr_accessor :bytes

          # The log probability of this token, if it is within the top 20 most likely
          # tokens. Otherwise, the value `-9999.0` is used to signify that the token is very
          # unlikely.
          sig { returns(Float) }
          attr_accessor :logprob

          # The token.
          sig { returns(String) }
          attr_accessor :token

          sig do
            override
              .returns({
                token: String,
                bytes: T.nilable(T::Array[Integer]),
                logprob: Float
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                token: String,
                bytes: T.nilable(T::Array[Integer]),
                logprob: Float
              ).returns(T.attached_class)
            end
            def new(
              token:, # The token.
              bytes:, # A list of integers representing the UTF-8 bytes representation of the token.
                      # Useful in instances where characters are represented by multiple tokens and
                      # their byte representations must be combined to generate the correct text
                      # representation. Can be `null` if there is no bytes representation for the token.
              logprob: # The log probability of this token, if it is within the top 20 most likely
                       # tokens. Otherwise, the value `-9999.0` is used to signify that the token is very
                       # unlikely.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::ChatCompletionTokenLogprob::TopLogprob,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class ChatCompletionTool < OpenAI::Internal::Type::BaseModel
        sig { returns(OpenAI::FunctionDefinition) }
        attr_reader :function

        sig { params(function: OpenAI::FunctionDefinition::OrHash).void }
        attr_writer :function

        # The type of the tool. Currently, only `function` is supported.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ function: OpenAI::FunctionDefinition, type: Symbol }) }
        def to_hash; end

        class << self
          sig { params(function: OpenAI::FunctionDefinition::OrHash, type: Symbol).returns(T.attached_class) }
          def new(
            function:,
            type: :function # The type of the tool. Currently, only `function` is supported.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Chat::ChatCompletionTool, OpenAI::Internal::AnyHash)
          end
      end

      # Controls which (if any) tool is called by the model. `none` means the model will
      # not call any tool and instead generates a message. `auto` means the model can
      # pick between generating a message or calling one or more tools. `required` means
      # the model must call one or more tools. Specifying a particular tool via
      # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
      # call that tool.
      #
      # `none` is the default when no tools are present. `auto` is the default if tools
      # are present.
      module ChatCompletionToolChoiceOption
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Chat::ChatCompletionToolChoiceOption::Variants]) }
          def variants; end
        end

        # `none` means the model will not call any tool and instead generates a message.
        # `auto` means the model can pick between generating a message or calling one or
        # more tools. `required` means the model must call one or more tools.
        module Auto
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::TaggedSymbol
              ])
            end
            def values; end
          end

          AUTO = T.let(
              :auto,
              OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::TaggedSymbol
            )

          NONE = T.let(
              :none,
              OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          REQUIRED = T.let(
              :required,
              OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Chat::ChatCompletionToolChoiceOption::Auto)
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::TaggedSymbol,
              OpenAI::Chat::ChatCompletionNamedToolChoice
            )
          end
      end

      class ChatCompletionToolMessageParam < OpenAI::Internal::Type::BaseModel
        # The contents of the tool message.
        sig { returns(OpenAI::Chat::ChatCompletionToolMessageParam::Content::Variants) }
        attr_accessor :content

        # The role of the messages author, in this case `tool`.
        sig { returns(Symbol) }
        attr_accessor :role

        # Tool call that this message is responding to.
        sig { returns(String) }
        attr_accessor :tool_call_id

        sig do
          override
            .returns({
              content:
                OpenAI::Chat::ChatCompletionToolMessageParam::Content::Variants,
              role: Symbol,
              tool_call_id: String
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              content: OpenAI::Chat::ChatCompletionToolMessageParam::Content::Variants,
              tool_call_id: String,
              role: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content:, # The contents of the tool message.
            tool_call_id:, # Tool call that this message is responding to.
            role: :tool # The role of the messages author, in this case `tool`.
); end
        end

        # The contents of the tool message.
        module Content
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionToolMessageParam::Content::Variants
              ])
            end
            def variants; end
          end

          ChatCompletionContentPartTextArray = T.let(
              OpenAI::Internal::Type::ArrayOf[
                OpenAI::Chat::ChatCompletionContentPartText
              ],
              OpenAI::Internal::Type::Converter
            )

          Variants = T.type_alias do
              T.any(
                String,
                T::Array[OpenAI::Chat::ChatCompletionContentPartText]
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionToolMessageParam,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ChatCompletionUserMessageParam < OpenAI::Internal::Type::BaseModel
        # The contents of the user message.
        sig { returns(OpenAI::Chat::ChatCompletionUserMessageParam::Content::Variants) }
        attr_accessor :content

        # An optional name for the participant. Provides the model information to
        # differentiate between participants of the same role.
        sig { returns(T.nilable(String)) }
        attr_reader :name

        sig { params(name: String).void }
        attr_writer :name

        # The role of the messages author, in this case `user`.
        sig { returns(Symbol) }
        attr_accessor :role

        sig do
          override
            .returns({
              content:
                OpenAI::Chat::ChatCompletionUserMessageParam::Content::Variants,
              role: Symbol,
              name: String
            })
        end
        def to_hash; end

        class << self
          # Messages sent by an end user, containing prompts or additional context
          # information.
          sig do
            params(
              content: OpenAI::Chat::ChatCompletionUserMessageParam::Content::Variants,
              name: String,
              role: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content:, # The contents of the user message.
            name: nil, # An optional name for the participant. Provides the model information to
                       # differentiate between participants of the same role.
            role: :user # The role of the messages author, in this case `user`.
); end
        end

        # The contents of the user message.
        module Content
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::ChatCompletionUserMessageParam::Content::Variants
              ])
            end
            def variants; end
          end

          ChatCompletionContentPartArray = T.let(
              OpenAI::Internal::Type::ArrayOf[
                union: OpenAI::Chat::ChatCompletionContentPart
              ],
              OpenAI::Internal::Type::Converter
            )

          Variants = T.type_alias do
              T.any(
                String,
                T::Array[OpenAI::Chat::ChatCompletionContentPart::Variants]
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::ChatCompletionUserMessageParam,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class CompletionCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Parameters for audio output. Required when audio output is requested with
        # `modalities: ["audio"]`.
        # [Learn more](https://platform.openai.com/docs/guides/audio).
        sig { returns(T.nilable(OpenAI::Chat::ChatCompletionAudioParam)) }
        attr_reader :audio

        sig { params(audio: T.nilable(OpenAI::Chat::ChatCompletionAudioParam::OrHash)).void }
        attr_writer :audio

        # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
        # existing frequency in the text so far, decreasing the model's likelihood to
        # repeat the same line verbatim.
        sig { returns(T.nilable(Float)) }
        attr_accessor :frequency_penalty

        # Deprecated in favor of `tool_choice`.
        #
        # Controls which (if any) function is called by the model.
        #
        # `none` means the model will not call a function and instead generates a message.
        #
        # `auto` means the model can pick between generating a message or calling a
        # function.
        #
        # Specifying a particular function via `{"name": "my_function"}` forces the model
        # to call that function.
        #
        # `none` is the default when no functions are present. `auto` is the default if
        # functions are present.
        sig do
          returns(T.nilable(
              T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption
              )
            ))
        end
        attr_reader :function_call

        sig do
          params(
            function_call: T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption::OrHash
              )
          ).void
        end
        attr_writer :function_call

        # Deprecated in favor of `tools`.
        #
        # A list of functions the model may generate JSON inputs for.
        sig { returns(T.nilable(T::Array[OpenAI::Chat::CompletionCreateParams::Function])) }
        attr_reader :functions

        sig { params(functions: T::Array[OpenAI::Chat::CompletionCreateParams::Function::OrHash]).void }
        attr_writer :functions

        # Modify the likelihood of specified tokens appearing in the completion.
        #
        # Accepts a JSON object that maps tokens (specified by their token ID in the
        # tokenizer) to an associated bias value from -100 to 100. Mathematically, the
        # bias is added to the logits generated by the model prior to sampling. The exact
        # effect will vary per model, but values between -1 and 1 should decrease or
        # increase likelihood of selection; values like -100 or 100 should result in a ban
        # or exclusive selection of the relevant token.
        sig { returns(T.nilable(T::Hash[Symbol, Integer])) }
        attr_accessor :logit_bias

        # Whether to return log probabilities of the output tokens or not. If true,
        # returns the log probabilities of each output token returned in the `content` of
        # `message`.
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :logprobs

        # An upper bound for the number of tokens that can be generated for a completion,
        # including visible output tokens and
        # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_completion_tokens

        # The maximum number of [tokens](/tokenizer) that can be generated in the chat
        # completion. This value can be used to control
        # [costs](https://openai.com/api/pricing/) for text generated via API.
        #
        # This value is now deprecated in favor of `max_completion_tokens`, and is not
        # compatible with
        # [o-series models](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_tokens

        # A list of messages comprising the conversation so far. Depending on the
        # [model](https://platform.openai.com/docs/models) you use, different message
        # types (modalities) are supported, like
        # [text](https://platform.openai.com/docs/guides/text-generation),
        # [images](https://platform.openai.com/docs/guides/vision), and
        # [audio](https://platform.openai.com/docs/guides/audio).
        sig do
          returns(T::Array[
              T.any(
                OpenAI::Chat::ChatCompletionDeveloperMessageParam,
                OpenAI::Chat::ChatCompletionSystemMessageParam,
                OpenAI::Chat::ChatCompletionUserMessageParam,
                OpenAI::Chat::ChatCompletionAssistantMessageParam,
                OpenAI::Chat::ChatCompletionToolMessageParam,
                OpenAI::Chat::ChatCompletionFunctionMessageParam
              )
            ])
        end
        attr_accessor :messages

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # Output types that you would like the model to generate. Most models are capable
        # of generating text, which is the default:
        #
        # `["text"]`
        #
        # The `gpt-4o-audio-preview` model can also be used to
        # [generate audio](https://platform.openai.com/docs/guides/audio). To request that
        # this model generate both text and audio responses, you can use:
        #
        # `["text", "audio"]`
        sig do
          returns(T.nilable(
              T::Array[OpenAI::Chat::CompletionCreateParams::Modality::OrSymbol]
            ))
        end
        attr_accessor :modalities

        # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
        # wide range of models with different capabilities, performance characteristics,
        # and price points. Refer to the
        # [model guide](https://platform.openai.com/docs/models) to browse and compare
        # available models.
        sig { returns(T.any(String, OpenAI::ChatModel::OrSymbol)) }
        attr_accessor :model

        # How many chat completion choices to generate for each input message. Note that
        # you will be charged based on the number of generated tokens across all of the
        # choices. Keep `n` as `1` to minimize costs.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :n

        # Whether to enable
        # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
        # during tool use.
        sig { returns(T.nilable(T::Boolean)) }
        attr_reader :parallel_tool_calls

        sig { params(parallel_tool_calls: T::Boolean).void }
        attr_writer :parallel_tool_calls

        # Static predicted output content, such as the content of a text file that is
        # being regenerated.
        sig { returns(T.nilable(OpenAI::Chat::ChatCompletionPredictionContent)) }
        attr_reader :prediction

        sig { params(prediction: T.nilable(OpenAI::Chat::ChatCompletionPredictionContent::OrHash)).void }
        attr_writer :prediction

        # Number between -2.0 and 2.0. Positive values penalize new tokens based on
        # whether they appear in the text so far, increasing the model's likelihood to
        # talk about new topics.
        sig { returns(T.nilable(Float)) }
        attr_accessor :presence_penalty

        # Used by OpenAI to cache responses for similar requests to optimize your cache
        # hit rates. Replaces the `user` field.
        # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
        sig { returns(T.nilable(String)) }
        attr_reader :prompt_cache_key

        sig { params(prompt_cache_key: String).void }
        attr_writer :prompt_cache_key

        # **o-series models only**
        #
        # Constrains effort on reasoning for
        # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
        # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
        # result in faster responses and fewer tokens used on reasoning in a response.
        sig { returns(T.nilable(OpenAI::ReasoningEffort::OrSymbol)) }
        attr_accessor :reasoning_effort

        # An object specifying the format that the model must output.
        #
        # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        # Outputs which ensures the model will match your supplied JSON schema. Learn more
        # in the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
        # ensures the message the model generates is valid JSON. Using `json_schema` is
        # preferred for models that support it.
        sig do
          returns(T.nilable(
              T.any(
                OpenAI::ResponseFormatText,
                OpenAI::ResponseFormatJSONSchema,
                OpenAI::StructuredOutput::JsonSchemaConverter,
                OpenAI::ResponseFormatJSONObject
              )
            ))
        end
        attr_reader :response_format

        sig do
          params(
            response_format: T.any(
                OpenAI::ResponseFormatText::OrHash,
                OpenAI::ResponseFormatJSONSchema::OrHash,
                OpenAI::StructuredOutput::JsonSchemaConverter,
                OpenAI::ResponseFormatJSONObject::OrHash
              )
          ).void
        end
        attr_writer :response_format

        # A stable identifier used to help detect users of your application that may be
        # violating OpenAI's usage policies. The IDs should be a string that uniquely
        # identifies each user. We recommend hashing their username or email address, in
        # order to avoid sending us any identifying information.
        # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        sig { returns(T.nilable(String)) }
        attr_reader :safety_identifier

        sig { params(safety_identifier: String).void }
        attr_writer :safety_identifier

        # This feature is in Beta. If specified, our system will make a best effort to
        # sample deterministically, such that repeated requests with the same `seed` and
        # parameters should return the same result. Determinism is not guaranteed, and you
        # should refer to the `system_fingerprint` response parameter to monitor changes
        # in the backend.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :seed

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        sig do
          returns(T.nilable(
              OpenAI::Chat::CompletionCreateParams::ServiceTier::OrSymbol
            ))
        end
        attr_accessor :service_tier

        # Not supported with latest reasoning models `o3` and `o4-mini`.
        #
        # Up to 4 sequences where the API will stop generating further tokens. The
        # returned text will not contain the stop sequence.
        sig { returns(T.nilable(OpenAI::Chat::CompletionCreateParams::Stop::Variants)) }
        attr_accessor :stop

        # Whether or not to store the output of this chat completion request for use in
        # our [model distillation](https://platform.openai.com/docs/guides/distillation)
        # or [evals](https://platform.openai.com/docs/guides/evals) products.
        #
        # Supports text and image inputs. Note: image inputs over 10MB will be dropped.
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :store

        # Options for streaming response. Only set this when you set `stream: true`.
        sig { returns(T.nilable(OpenAI::Chat::ChatCompletionStreamOptions)) }
        attr_reader :stream_options

        sig { params(stream_options: T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash)).void }
        attr_writer :stream_options

        # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        # make the output more random, while lower values like 0.2 will make it more
        # focused and deterministic. We generally recommend altering this or `top_p` but
        # not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :temperature

        # Controls which (if any) tool is called by the model. `none` means the model will
        # not call any tool and instead generates a message. `auto` means the model can
        # pick between generating a message or calling one or more tools. `required` means
        # the model must call one or more tools. Specifying a particular tool via
        # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
        # call that tool.
        #
        # `none` is the default when no tools are present. `auto` is the default if tools
        # are present.
        sig do
          returns(T.nilable(
              T.any(
                OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                OpenAI::Chat::ChatCompletionNamedToolChoice
              )
            ))
        end
        attr_reader :tool_choice

        sig do
          params(
            tool_choice: T.any(
                OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                OpenAI::Chat::ChatCompletionNamedToolChoice::OrHash
              )
          ).void
        end
        attr_writer :tool_choice

        # A list of tools the model may call. Currently, only functions are supported as a
        # tool. Use this to provide a list of functions the model may generate JSON inputs
        # for. A max of 128 functions are supported.
        sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionTool])) }
        attr_reader :tools

        sig do
          params(
            tools: T::Array[
                T.any(
                  OpenAI::Chat::ChatCompletionTool::OrHash,
                  OpenAI::StructuredOutput::JsonSchemaConverter
                )
              ]
          ).void
        end
        attr_writer :tools

        # An integer between 0 and 20 specifying the number of most likely tokens to
        # return at each token position, each with an associated log probability.
        # `logprobs` must be set to `true` if this parameter is used.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :top_logprobs

        # An alternative to sampling with temperature, called nucleus sampling, where the
        # model considers the results of the tokens with top_p probability mass. So 0.1
        # means only the tokens comprising the top 10% probability mass are considered.
        #
        # We generally recommend altering this or `temperature` but not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :top_p

        # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
        # `prompt_cache_key` instead to maintain caching optimizations. A stable
        # identifier for your end-users. Used to boost cache hit rates by better bucketing
        # similar requests and to help OpenAI detect and prevent abuse.
        # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        sig { returns(T.nilable(String)) }
        attr_reader :user

        sig { params(user: String).void }
        attr_writer :user

        # This tool searches the web for relevant results to use in a response. Learn more
        # about the
        # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
        sig { returns(T.nilable(OpenAI::Chat::CompletionCreateParams::WebSearchOptions)) }
        attr_reader :web_search_options

        sig { params(web_search_options: OpenAI::Chat::CompletionCreateParams::WebSearchOptions::OrHash).void }
        attr_writer :web_search_options

        sig do
          override
            .returns({
              messages:
                T::Array[
                  T.any(
                    OpenAI::Chat::ChatCompletionDeveloperMessageParam,
                    OpenAI::Chat::ChatCompletionSystemMessageParam,
                    OpenAI::Chat::ChatCompletionUserMessageParam,
                    OpenAI::Chat::ChatCompletionAssistantMessageParam,
                    OpenAI::Chat::ChatCompletionToolMessageParam,
                    OpenAI::Chat::ChatCompletionFunctionMessageParam
                  )
                ],
              model: T.any(String, OpenAI::ChatModel::OrSymbol),
              audio: T.nilable(OpenAI::Chat::ChatCompletionAudioParam),
              frequency_penalty: T.nilable(Float),
              function_call:
                T.any(
                  OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                  OpenAI::Chat::ChatCompletionFunctionCallOption
                ),
              functions:
                T::Array[OpenAI::Chat::CompletionCreateParams::Function],
              logit_bias: T.nilable(T::Hash[Symbol, Integer]),
              logprobs: T.nilable(T::Boolean),
              max_completion_tokens: T.nilable(Integer),
              max_tokens: T.nilable(Integer),
              metadata: T.nilable(T::Hash[Symbol, String]),
              modalities:
                T.nilable(
                  T::Array[
                    OpenAI::Chat::CompletionCreateParams::Modality::OrSymbol
                  ]
                ),
              n: T.nilable(Integer),
              parallel_tool_calls: T::Boolean,
              prediction:
                T.nilable(OpenAI::Chat::ChatCompletionPredictionContent),
              presence_penalty: T.nilable(Float),
              prompt_cache_key: String,
              reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
              response_format:
                T.any(
                  OpenAI::ResponseFormatText,
                  OpenAI::ResponseFormatJSONSchema,
                  OpenAI::ResponseFormatJSONObject
                ),
              safety_identifier: String,
              seed: T.nilable(Integer),
              service_tier:
                T.nilable(
                  OpenAI::Chat::CompletionCreateParams::ServiceTier::OrSymbol
                ),
              stop:
                T.nilable(OpenAI::Chat::CompletionCreateParams::Stop::Variants),
              store: T.nilable(T::Boolean),
              stream_options:
                T.nilable(OpenAI::Chat::ChatCompletionStreamOptions),
              temperature: T.nilable(Float),
              tool_choice:
                T.any(
                  OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                  OpenAI::Chat::ChatCompletionNamedToolChoice
                ),
              tools:
                T::Array[
                  T.any(
                    OpenAI::Chat::ChatCompletionTool,
                    OpenAI::StructuredOutput::JsonSchemaConverter
                  )
                ],
              top_logprobs: T.nilable(Integer),
              top_p: T.nilable(Float),
              user: String,
              web_search_options:
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              messages: T::Array[
                T.any(
                  OpenAI::Chat::ChatCompletionDeveloperMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionSystemMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionUserMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionToolMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionFunctionMessageParam::OrHash
                )
              ],
              model: T.any(String, OpenAI::ChatModel::OrSymbol),
              audio: T.nilable(OpenAI::Chat::ChatCompletionAudioParam::OrHash),
              frequency_penalty: T.nilable(Float),
              function_call: T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption::OrHash
              ),
              functions: T::Array[OpenAI::Chat::CompletionCreateParams::Function::OrHash],
              logit_bias: T.nilable(T::Hash[Symbol, Integer]),
              logprobs: T.nilable(T::Boolean),
              max_completion_tokens: T.nilable(Integer),
              max_tokens: T.nilable(Integer),
              metadata: T.nilable(T::Hash[Symbol, String]),
              modalities: T.nilable(
                T::Array[
                  OpenAI::Chat::CompletionCreateParams::Modality::OrSymbol
                ]
              ),
              n: T.nilable(Integer),
              parallel_tool_calls: T::Boolean,
              prediction: T.nilable(OpenAI::Chat::ChatCompletionPredictionContent::OrHash),
              presence_penalty: T.nilable(Float),
              prompt_cache_key: String,
              reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
              response_format: T.any(
                OpenAI::ResponseFormatText::OrHash,
                OpenAI::ResponseFormatJSONSchema::OrHash,
                OpenAI::StructuredOutput::JsonSchemaConverter,
                OpenAI::ResponseFormatJSONObject::OrHash
              ),
              safety_identifier: String,
              seed: T.nilable(Integer),
              service_tier: T.nilable(
                OpenAI::Chat::CompletionCreateParams::ServiceTier::OrSymbol
              ),
              stop: T.nilable(OpenAI::Chat::CompletionCreateParams::Stop::Variants),
              store: T.nilable(T::Boolean),
              stream_options: T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash),
              temperature: T.nilable(Float),
              tool_choice: T.any(
                OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                OpenAI::Chat::ChatCompletionNamedToolChoice::OrHash
              ),
              tools: T::Array[
                T.any(
                  OpenAI::Chat::ChatCompletionTool::OrHash,
                  OpenAI::StructuredOutput::JsonSchemaConverter
                )
              ],
              top_logprobs: T.nilable(Integer),
              top_p: T.nilable(Float),
              user: String,
              web_search_options: OpenAI::Chat::CompletionCreateParams::WebSearchOptions::OrHash,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            messages:, # A list of messages comprising the conversation so far. Depending on the
                       # [model](https://platform.openai.com/docs/models) you use, different message
                       # types (modalities) are supported, like
                       # [text](https://platform.openai.com/docs/guides/text-generation),
                       # [images](https://platform.openai.com/docs/guides/vision), and
                       # [audio](https://platform.openai.com/docs/guides/audio).
            model:, # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
                    # wide range of models with different capabilities, performance characteristics,
                    # and price points. Refer to the
                    # [model guide](https://platform.openai.com/docs/models) to browse and compare
                    # available models.
            audio: nil, # Parameters for audio output. Required when audio output is requested with
                        # `modalities: ["audio"]`.
                        # [Learn more](https://platform.openai.com/docs/guides/audio).
            frequency_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
                                    # existing frequency in the text so far, decreasing the model's likelihood to
                                    # repeat the same line verbatim.
            function_call: nil, # Deprecated in favor of `tool_choice`.
                                # Controls which (if any) function is called by the model.
                                # `none` means the model will not call a function and instead generates a message.
                                # `auto` means the model can pick between generating a message or calling a
                                # function.
                                # Specifying a particular function via `{"name": "my_function"}` forces the model
                                # to call that function.
                                # `none` is the default when no functions are present. `auto` is the default if
                                # functions are present.
            functions: nil, # Deprecated in favor of `tools`.
                            # A list of functions the model may generate JSON inputs for.
            logit_bias: nil, # Modify the likelihood of specified tokens appearing in the completion.
                             # Accepts a JSON object that maps tokens (specified by their token ID in the
                             # tokenizer) to an associated bias value from -100 to 100. Mathematically, the
                             # bias is added to the logits generated by the model prior to sampling. The exact
                             # effect will vary per model, but values between -1 and 1 should decrease or
                             # increase likelihood of selection; values like -100 or 100 should result in a ban
                             # or exclusive selection of the relevant token.
            logprobs: nil, # Whether to return log probabilities of the output tokens or not. If true,
                           # returns the log probabilities of each output token returned in the `content` of
                           # `message`.
            max_completion_tokens: nil, # An upper bound for the number of tokens that can be generated for a completion,
                                        # including visible output tokens and
                                        # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
            max_tokens: nil, # The maximum number of [tokens](/tokenizer) that can be generated in the chat
                             # completion. This value can be used to control
                             # [costs](https://openai.com/api/pricing/) for text generated via API.
                             # This value is now deprecated in favor of `max_completion_tokens`, and is not
                             # compatible with
                             # [o-series models](https://platform.openai.com/docs/guides/reasoning).
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            modalities: nil, # Output types that you would like the model to generate. Most models are capable
                             # of generating text, which is the default:
                             # `["text"]`
                             # The `gpt-4o-audio-preview` model can also be used to
                             # [generate audio](https://platform.openai.com/docs/guides/audio). To request that
                             # this model generate both text and audio responses, you can use:
                             # `["text", "audio"]`
            n: nil, # How many chat completion choices to generate for each input message. Note that
                    # you will be charged based on the number of generated tokens across all of the
                    # choices. Keep `n` as `1` to minimize costs.
            parallel_tool_calls: nil, # Whether to enable
                                      # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                      # during tool use.
            prediction: nil, # Static predicted output content, such as the content of a text file that is
                             # being regenerated.
            presence_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on
                                   # whether they appear in the text so far, increasing the model's likelihood to
                                   # talk about new topics.
            prompt_cache_key: nil, # Used by OpenAI to cache responses for similar requests to optimize your cache
                                   # hit rates. Replaces the `user` field.
                                   # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
            reasoning_effort: nil, # **o-series models only**
                                   # Constrains effort on reasoning for
                                   # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                   # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                   # result in faster responses and fewer tokens used on reasoning in a response.
            response_format: nil, # An object specifying the format that the model must output.
                                  # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                  # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                  # in the
                                  # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                  # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                                  # ensures the message the model generates is valid JSON. Using `json_schema` is
                                  # preferred for models that support it.
            safety_identifier: nil, # A stable identifier used to help detect users of your application that may be
                                    # violating OpenAI's usage policies. The IDs should be a string that uniquely
                                    # identifies each user. We recommend hashing their username or email address, in
                                    # order to avoid sending us any identifying information.
                                    # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
            seed: nil, # This feature is in Beta. If specified, our system will make a best effort to
                       # sample deterministically, such that repeated requests with the same `seed` and
                       # parameters should return the same result. Determinism is not guaranteed, and you
                       # should refer to the `system_fingerprint` response parameter to monitor changes
                       # in the backend.
            service_tier: nil, # Specifies the processing type used for serving the request.
                               # - If set to 'auto', then the request will be processed with the service tier
                               #   configured in the Project settings. Unless otherwise configured, the Project
                               #   will use 'default'.
                               # - If set to 'default', then the request will be processed with the standard
                               #   pricing and performance for the selected model.
                               # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
                               #   'priority', then the request will be processed with the corresponding service
                               #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
                               #   Priority processing.
                               # - When not set, the default behavior is 'auto'.
                               # When the `service_tier` parameter is set, the response body will include the
                               # `service_tier` value based on the processing mode actually used to serve the
                               # request. This response value may be different from the value set in the
                               # parameter.
            stop: nil, # Not supported with latest reasoning models `o3` and `o4-mini`.
                       # Up to 4 sequences where the API will stop generating further tokens. The
                       # returned text will not contain the stop sequence.
            store: nil, # Whether or not to store the output of this chat completion request for use in
                        # our [model distillation](https://platform.openai.com/docs/guides/distillation)
                        # or [evals](https://platform.openai.com/docs/guides/evals) products.
                        # Supports text and image inputs. Note: image inputs over 10MB will be dropped.
            stream_options: nil, # Options for streaming response. Only set this when you set `stream: true`.
            temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                              # make the output more random, while lower values like 0.2 will make it more
                              # focused and deterministic. We generally recommend altering this or `top_p` but
                              # not both.
            tool_choice: nil, # Controls which (if any) tool is called by the model. `none` means the model will
                              # not call any tool and instead generates a message. `auto` means the model can
                              # pick between generating a message or calling one or more tools. `required` means
                              # the model must call one or more tools. Specifying a particular tool via
                              # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                              # call that tool.
                              # `none` is the default when no tools are present. `auto` is the default if tools
                              # are present.
            tools: nil, # A list of tools the model may call. Currently, only functions are supported as a
                        # tool. Use this to provide a list of functions the model may generate JSON inputs
                        # for. A max of 128 functions are supported.
            top_logprobs: nil, # An integer between 0 and 20 specifying the number of most likely tokens to
                               # return at each token position, each with an associated log probability.
                               # `logprobs` must be set to `true` if this parameter is used.
            top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                        # model considers the results of the tokens with top_p probability mass. So 0.1
                        # means only the tokens comprising the top 10% probability mass are considered.
                        # We generally recommend altering this or `temperature` but not both.
            user: nil, # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
                       # `prompt_cache_key` instead to maintain caching optimizations. A stable
                       # identifier for your end-users. Used to boost cache hit rates by better bucketing
                       # similar requests and to help OpenAI detect and prevent abuse.
                       # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
            web_search_options: nil, # This tool searches the web for relevant results to use in a response. Learn more
                                     # about the
                                     # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
            request_options: {}
); end
        end

        class Function < OpenAI::Internal::Type::BaseModel
          # A description of what the function does, used by the model to choose when and
          # how to call the function.
          sig { returns(T.nilable(String)) }
          attr_reader :description

          sig { params(description: String).void }
          attr_writer :description

          # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
          # underscores and dashes, with a maximum length of 64.
          sig { returns(String) }
          attr_accessor :name

          # The parameters the functions accepts, described as a JSON Schema object. See the
          # [guide](https://platform.openai.com/docs/guides/function-calling) for examples,
          # and the
          # [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
          # documentation about the format.
          #
          # Omitting `parameters` defines a function with an empty parameter list.
          sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
          attr_reader :parameters

          sig { params(parameters: T::Hash[Symbol, T.anything]).void }
          attr_writer :parameters

          sig do
            override
              .returns({
                name: String,
                description: String,
                parameters: T::Hash[Symbol, T.anything]
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                name: String,
                description: String,
                parameters: T::Hash[Symbol, T.anything]
              ).returns(T.attached_class)
            end
            def new(
              name:, # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
                     # underscores and dashes, with a maximum length of 64.
              description: nil, # A description of what the function does, used by the model to choose when and
                                # how to call the function.
              parameters: nil # The parameters the functions accepts, described as a JSON Schema object. See the
                              # [guide](https://platform.openai.com/docs/guides/function-calling) for examples,
                              # and the
                              # [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
                              # documentation about the format.
                              # Omitting `parameters` defines a function with an empty parameter list.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::CompletionCreateParams::Function,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # Deprecated in favor of `tool_choice`.
        #
        # Controls which (if any) function is called by the model.
        #
        # `none` means the model will not call a function and instead generates a message.
        #
        # `auto` means the model can pick between generating a message or calling a
        # function.
        #
        # Specifying a particular function via `{"name": "my_function"}` forces the model
        # to call that function.
        #
        # `none` is the default when no functions are present. `auto` is the default if
        # functions are present.
        module FunctionCall
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::CompletionCreateParams::FunctionCall::Variants
              ])
            end
            def variants; end
          end

          # `none` means the model will not call a function and instead generates a message.
          # `auto` means the model can pick between generating a message or calling a
          # function.
          module FunctionCallMode
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::TaggedSymbol
              )

            NONE = T.let(
                :none,
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode
                )
              end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::TaggedSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption
              )
            end
        end

        module Modality
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::CompletionCreateParams::Modality::TaggedSymbol
              ])
            end
            def values; end
          end

          AUDIO = T.let(
              :audio,
              OpenAI::Chat::CompletionCreateParams::Modality::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Chat::CompletionCreateParams::Modality::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Chat::CompletionCreateParams::Modality)
            end
        end

        # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
        # wide range of models with different capabilities, performance characteristics,
        # and price points. Refer to the
        # [model guide](https://platform.openai.com/docs/models) to browse and compare
        # available models.
        module Model
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Chat::CompletionCreateParams::Model::Variants]) }
            def variants; end
          end

          Variants = T.type_alias { T.any(String, OpenAI::ChatModel::TaggedSymbol) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::CompletionCreateParams,
              OpenAI::Internal::AnyHash
            )
          end

        # An object specifying the format that the model must output.
        #
        # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        # Outputs which ensures the model will match your supplied JSON schema. Learn more
        # in the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
        # ensures the message the model generates is valid JSON. Using `json_schema` is
        # preferred for models that support it.
        module ResponseFormat
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::CompletionCreateParams::ResponseFormat::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::ResponseFormatText,
                OpenAI::ResponseFormatJSONSchema,
                OpenAI::StructuredOutput::JsonSchemaConverter,
                OpenAI::ResponseFormatJSONObject
              )
            end
        end

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        module ServiceTier
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
              ])
            end
            def values; end
          end

          AUTO = T.let(
              :auto,
              OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
            )

          DEFAULT = T.let(
              :default,
              OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
            )

          FLEX = T.let(
              :flex,
              OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          PRIORITY = T.let(
              :priority,
              OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
            )

          SCALE = T.let(
              :scale,
              OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Chat::CompletionCreateParams::ServiceTier)
            end
        end

        # Not supported with latest reasoning models `o3` and `o4-mini`.
        #
        # Up to 4 sequences where the API will stop generating further tokens. The
        # returned text will not contain the stop sequence.
        module Stop
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Chat::CompletionCreateParams::Stop::Variants]) }
            def variants; end
          end

          StringArray = T.let(
              OpenAI::Internal::Type::ArrayOf[String],
              OpenAI::Internal::Type::Converter
            )

          Variants = T.type_alias { T.nilable(T.any(String, T::Array[String])) }
        end

        class WebSearchOptions < OpenAI::Internal::Type::BaseModel
          # High level guidance for the amount of context window space to use for the
          # search. One of `low`, `medium`, or `high`. `medium` is the default.
          sig do
            returns(T.nilable(
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::OrSymbol
              ))
          end
          attr_reader :search_context_size

          sig do
            params(
              search_context_size: OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::OrSymbol
            ).void
          end
          attr_writer :search_context_size

          # Approximate location parameters for the search.
          sig do
            returns(T.nilable(
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation
              ))
          end
          attr_reader :user_location

          sig do
            params(
              user_location: T.nilable(
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::OrHash
                )
            ).void
          end
          attr_writer :user_location

          sig do
            override
              .returns({
                search_context_size:
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::OrSymbol,
                user_location:
                  T.nilable(
                    OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation
                  )
              })
          end
          def to_hash; end

          class << self
            # This tool searches the web for relevant results to use in a response. Learn more
            # about the
            # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
            sig do
              params(
                search_context_size: OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::OrSymbol,
                user_location: T.nilable(
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::OrHash
                )
              ).returns(T.attached_class)
            end
            def new(
              search_context_size: nil, # High level guidance for the amount of context window space to use for the
                                        # search. One of `low`, `medium`, or `high`. `medium` is the default.
              user_location: nil # Approximate location parameters for the search.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions,
                OpenAI::Internal::AnyHash
              )
            end

          # High level guidance for the amount of context window space to use for the
          # search. One of `low`, `medium`, or `high`. `medium` is the default.
          module SearchContextSize
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::TaggedSymbol
                ])
              end
              def values; end
            end

            HIGH = T.let(
                :high,
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::TaggedSymbol
              )

            LOW = T.let(
                :low,
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::TaggedSymbol
              )

            MEDIUM = T.let(
                :medium,
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize
                )
              end
          end

          class UserLocation < OpenAI::Internal::Type::BaseModel
            # Approximate location parameters for the search.
            sig { returns(OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate) }
            attr_reader :approximate

            sig do
              params(
                approximate: OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate::OrHash
              ).void
            end
            attr_writer :approximate

            # The type of location approximation. Always `approximate`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  approximate:
                    OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate,
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              # Approximate location parameters for the search.
              sig do
                params(
                  approximate: OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                approximate:, # Approximate location parameters for the search.
                type: :approximate # The type of location approximation. Always `approximate`.
); end
            end

            class Approximate < OpenAI::Internal::Type::BaseModel
              # Free text input for the city of the user, e.g. `San Francisco`.
              sig { returns(T.nilable(String)) }
              attr_reader :city

              sig { params(city: String).void }
              attr_writer :city

              # The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of
              # the user, e.g. `US`.
              sig { returns(T.nilable(String)) }
              attr_reader :country

              sig { params(country: String).void }
              attr_writer :country

              # Free text input for the region of the user, e.g. `California`.
              sig { returns(T.nilable(String)) }
              attr_reader :region

              sig { params(region: String).void }
              attr_writer :region

              # The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the
              # user, e.g. `America/Los_Angeles`.
              sig { returns(T.nilable(String)) }
              attr_reader :timezone

              sig { params(timezone: String).void }
              attr_writer :timezone

              sig do
                override
                  .returns({
                    city: String,
                    country: String,
                    region: String,
                    timezone: String
                  })
              end
              def to_hash; end

              class << self
                # Approximate location parameters for the search.
                sig do
                  params(
                    city: String,
                    country: String,
                    region: String,
                    timezone: String
                  ).returns(T.attached_class)
                end
                def new(
                  city: nil, # Free text input for the city of the user, e.g. `San Francisco`.
                  country: nil, # The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of
                                # the user, e.g. `US`.
                  region: nil, # Free text input for the region of the user, e.g. `California`.
                  timezone: nil # The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the
                                # user, e.g. `America/Los_Angeles`.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end
      end

      class CompletionDeleteParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::CompletionDeleteParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class CompletionListParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Identifier for the last chat completion from the previous pagination request.
        sig { returns(T.nilable(String)) }
        attr_reader :after

        sig { params(after: String).void }
        attr_writer :after

        # Number of Chat Completions to retrieve.
        sig { returns(T.nilable(Integer)) }
        attr_reader :limit

        sig { params(limit: Integer).void }
        attr_writer :limit

        # A list of metadata keys to filter the Chat Completions by. Example:
        #
        # `metadata[key1]=value1&metadata[key2]=value2`
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The model used to generate the Chat Completions.
        sig { returns(T.nilable(String)) }
        attr_reader :model

        sig { params(model: String).void }
        attr_writer :model

        # Sort order for Chat Completions by timestamp. Use `asc` for ascending order or
        # `desc` for descending order. Defaults to `asc`.
        sig { returns(T.nilable(OpenAI::Chat::CompletionListParams::Order::OrSymbol)) }
        attr_reader :order

        sig { params(order: OpenAI::Chat::CompletionListParams::Order::OrSymbol).void }
        attr_writer :order

        sig do
          override
            .returns({
              after: String,
              limit: Integer,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              order: OpenAI::Chat::CompletionListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              after: String,
              limit: Integer,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              order: OpenAI::Chat::CompletionListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            after: nil, # Identifier for the last chat completion from the previous pagination request.
            limit: nil, # Number of Chat Completions to retrieve.
            metadata: nil, # A list of metadata keys to filter the Chat Completions by. Example:
                           # `metadata[key1]=value1&metadata[key2]=value2`
            model: nil, # The model used to generate the Chat Completions.
            order: nil, # Sort order for Chat Completions by timestamp. Use `asc` for ascending order or
                        # `desc` for descending order. Defaults to `asc`.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Chat::CompletionListParams, OpenAI::Internal::AnyHash)
          end

        # Sort order for Chat Completions by timestamp. Use `asc` for ascending order or
        # `desc` for descending order. Defaults to `asc`.
        module Order
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Chat::CompletionListParams::Order::TaggedSymbol]) }
            def values; end
          end

          ASC = T.let(:asc, OpenAI::Chat::CompletionListParams::Order::TaggedSymbol)

          DESC = T.let(
              :desc,
              OpenAI::Chat::CompletionListParams::Order::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Chat::CompletionListParams::Order)
            end
        end
      end

      class CompletionRetrieveParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::CompletionRetrieveParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class CompletionUpdateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        sig do
          override
            .returns({
              metadata: T.nilable(T::Hash[Symbol, String]),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              metadata: T.nilable(T::Hash[Symbol, String]),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Chat::CompletionUpdateParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      module Completions
        class MessageListParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # Identifier for the last message from the previous pagination request.
          sig { returns(T.nilable(String)) }
          attr_reader :after

          sig { params(after: String).void }
          attr_writer :after

          # Number of messages to retrieve.
          sig { returns(T.nilable(Integer)) }
          attr_reader :limit

          sig { params(limit: Integer).void }
          attr_writer :limit

          # Sort order for messages by timestamp. Use `asc` for ascending order or `desc`
          # for descending order. Defaults to `asc`.
          sig do
            returns(T.nilable(
                OpenAI::Chat::Completions::MessageListParams::Order::OrSymbol
              ))
          end
          attr_reader :order

          sig { params(order: OpenAI::Chat::Completions::MessageListParams::Order::OrSymbol).void }
          attr_writer :order

          sig do
            override
              .returns({
                after: String,
                limit: Integer,
                order:
                  OpenAI::Chat::Completions::MessageListParams::Order::OrSymbol,
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                after: String,
                limit: Integer,
                order: OpenAI::Chat::Completions::MessageListParams::Order::OrSymbol,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              after: nil, # Identifier for the last message from the previous pagination request.
              limit: nil, # Number of messages to retrieve.
              order: nil, # Sort order for messages by timestamp. Use `asc` for ascending order or `desc`
                          # for descending order. Defaults to `asc`.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Chat::Completions::MessageListParams,
                OpenAI::Internal::AnyHash
              )
            end

          # Sort order for messages by timestamp. Use `asc` for ascending order or `desc`
          # for descending order. Defaults to `asc`.
          module Order
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Chat::Completions::MessageListParams::Order::TaggedSymbol
                ])
              end
              def values; end
            end

            ASC = T.let(
                :asc,
                OpenAI::Chat::Completions::MessageListParams::Order::TaggedSymbol
              )

            DESC = T.let(
                :desc,
                OpenAI::Chat::Completions::MessageListParams::Order::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Chat::Completions::MessageListParams::Order
                )
              end
          end
        end
      end
    end

    ChatCompletion = Chat::ChatCompletion

    ChatCompletionAssistantMessageParam = Chat::ChatCompletionAssistantMessageParam

    ChatCompletionAudio = Chat::ChatCompletionAudio
    ChatCompletionAudioParam = Chat::ChatCompletionAudioParam
    ChatCompletionChunk = Chat::ChatCompletionChunk
    ChatCompletionContentPart = Chat::ChatCompletionContentPart
    ChatCompletionContentPartImage = Chat::ChatCompletionContentPartImage

    ChatCompletionContentPartInputAudio = Chat::ChatCompletionContentPartInputAudio

    ChatCompletionContentPartRefusal = Chat::ChatCompletionContentPartRefusal
    ChatCompletionContentPartText = Chat::ChatCompletionContentPartText
    ChatCompletionDeleted = Chat::ChatCompletionDeleted

    ChatCompletionDeveloperMessageParam = Chat::ChatCompletionDeveloperMessageParam

    ChatCompletionFunctionCallOption = Chat::ChatCompletionFunctionCallOption

    ChatCompletionFunctionMessageParam = Chat::ChatCompletionFunctionMessageParam

    ChatCompletionMessage = Chat::ChatCompletionMessage
    ChatCompletionMessageParam = Chat::ChatCompletionMessageParam
    ChatCompletionMessageToolCall = Chat::ChatCompletionMessageToolCall
    ChatCompletionModality = Chat::ChatCompletionModality
    ChatCompletionNamedToolChoice = Chat::ChatCompletionNamedToolChoice
    ChatCompletionPredictionContent = Chat::ChatCompletionPredictionContent
    ChatCompletionReasoningEffort = Chat::ChatCompletionReasoningEffort
    ChatCompletionRole = Chat::ChatCompletionRole
    ChatCompletionStoreMessage = Chat::ChatCompletionStoreMessage
    ChatCompletionStreamOptions = Chat::ChatCompletionStreamOptions
    ChatCompletionSystemMessageParam = Chat::ChatCompletionSystemMessageParam
    ChatCompletionTokenLogprob = Chat::ChatCompletionTokenLogprob
    ChatCompletionTool = Chat::ChatCompletionTool
    ChatCompletionToolChoiceOption = Chat::ChatCompletionToolChoiceOption
    ChatCompletionToolMessageParam = Chat::ChatCompletionToolMessageParam
    ChatCompletionUserMessageParam = Chat::ChatCompletionUserMessageParam

    module ChatModel
      extend OpenAI::Internal::Type::Enum

      class << self
        sig { override.returns(T::Array[OpenAI::ChatModel::TaggedSymbol]) }
        def values; end
      end

      CHATGPT_4O_LATEST = T.let(:"chatgpt-4o-latest", OpenAI::ChatModel::TaggedSymbol)

      CODEX_MINI_LATEST = T.let(:"codex-mini-latest", OpenAI::ChatModel::TaggedSymbol)

      GPT_3_5_TURBO = T.let(:"gpt-3.5-turbo", OpenAI::ChatModel::TaggedSymbol)

      GPT_3_5_TURBO_0125 = T.let(:"gpt-3.5-turbo-0125", OpenAI::ChatModel::TaggedSymbol)

      GPT_3_5_TURBO_0301 = T.let(:"gpt-3.5-turbo-0301", OpenAI::ChatModel::TaggedSymbol)

      GPT_3_5_TURBO_0613 = T.let(:"gpt-3.5-turbo-0613", OpenAI::ChatModel::TaggedSymbol)

      GPT_3_5_TURBO_1106 = T.let(:"gpt-3.5-turbo-1106", OpenAI::ChatModel::TaggedSymbol)

      GPT_3_5_TURBO_16K = T.let(:"gpt-3.5-turbo-16k", OpenAI::ChatModel::TaggedSymbol)

      GPT_3_5_TURBO_16K_0613 = T.let(:"gpt-3.5-turbo-16k-0613", OpenAI::ChatModel::TaggedSymbol)

      GPT_4 = T.let(:"gpt-4", OpenAI::ChatModel::TaggedSymbol)
      GPT_4O = T.let(:"gpt-4o", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_2024_05_13 = T.let(:"gpt-4o-2024-05-13", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_2024_08_06 = T.let(:"gpt-4o-2024-08-06", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_2024_11_20 = T.let(:"gpt-4o-2024-11-20", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_AUDIO_PREVIEW = T.let(:"gpt-4o-audio-preview", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_AUDIO_PREVIEW_2024_10_01 = T.let(
          :"gpt-4o-audio-preview-2024-10-01",
          OpenAI::ChatModel::TaggedSymbol
        )

      GPT_4O_AUDIO_PREVIEW_2024_12_17 = T.let(
          :"gpt-4o-audio-preview-2024-12-17",
          OpenAI::ChatModel::TaggedSymbol
        )

      GPT_4O_AUDIO_PREVIEW_2025_06_03 = T.let(
          :"gpt-4o-audio-preview-2025-06-03",
          OpenAI::ChatModel::TaggedSymbol
        )

      GPT_4O_MINI = T.let(:"gpt-4o-mini", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_MINI_2024_07_18 = T.let(:"gpt-4o-mini-2024-07-18", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_MINI_AUDIO_PREVIEW = T.let(:"gpt-4o-mini-audio-preview", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_MINI_AUDIO_PREVIEW_2024_12_17 = T.let(
          :"gpt-4o-mini-audio-preview-2024-12-17",
          OpenAI::ChatModel::TaggedSymbol
        )

      GPT_4O_MINI_SEARCH_PREVIEW = T.let(:"gpt-4o-mini-search-preview", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_MINI_SEARCH_PREVIEW_2025_03_11 = T.let(
          :"gpt-4o-mini-search-preview-2025-03-11",
          OpenAI::ChatModel::TaggedSymbol
        )

      GPT_4O_SEARCH_PREVIEW = T.let(:"gpt-4o-search-preview", OpenAI::ChatModel::TaggedSymbol)

      GPT_4O_SEARCH_PREVIEW_2025_03_11 = T.let(
          :"gpt-4o-search-preview-2025-03-11",
          OpenAI::ChatModel::TaggedSymbol
        )

      GPT_4_0125_PREVIEW = T.let(:"gpt-4-0125-preview", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_0314 = T.let(:"gpt-4-0314", OpenAI::ChatModel::TaggedSymbol)
      GPT_4_0613 = T.let(:"gpt-4-0613", OpenAI::ChatModel::TaggedSymbol)
      GPT_4_1 = T.let(:"gpt-4.1", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_1106_PREVIEW = T.let(:"gpt-4-1106-preview", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_1_2025_04_14 = T.let(:"gpt-4.1-2025-04-14", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_1_MINI = T.let(:"gpt-4.1-mini", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_1_MINI_2025_04_14 = T.let(:"gpt-4.1-mini-2025-04-14", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_1_NANO = T.let(:"gpt-4.1-nano", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_1_NANO_2025_04_14 = T.let(:"gpt-4.1-nano-2025-04-14", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_32K = T.let(:"gpt-4-32k", OpenAI::ChatModel::TaggedSymbol)
      GPT_4_32K_0314 = T.let(:"gpt-4-32k-0314", OpenAI::ChatModel::TaggedSymbol)
      GPT_4_32K_0613 = T.let(:"gpt-4-32k-0613", OpenAI::ChatModel::TaggedSymbol)
      GPT_4_TURBO = T.let(:"gpt-4-turbo", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_TURBO_2024_04_09 = T.let(:"gpt-4-turbo-2024-04-09", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_TURBO_PREVIEW = T.let(:"gpt-4-turbo-preview", OpenAI::ChatModel::TaggedSymbol)

      GPT_4_VISION_PREVIEW = T.let(:"gpt-4-vision-preview", OpenAI::ChatModel::TaggedSymbol)

      O1 = T.let(:o1, OpenAI::ChatModel::TaggedSymbol)
      O1_2024_12_17 = T.let(:"o1-2024-12-17", OpenAI::ChatModel::TaggedSymbol)
      O1_MINI = T.let(:"o1-mini", OpenAI::ChatModel::TaggedSymbol)

      O1_MINI_2024_09_12 = T.let(:"o1-mini-2024-09-12", OpenAI::ChatModel::TaggedSymbol)

      O1_PREVIEW = T.let(:"o1-preview", OpenAI::ChatModel::TaggedSymbol)

      O1_PREVIEW_2024_09_12 = T.let(:"o1-preview-2024-09-12", OpenAI::ChatModel::TaggedSymbol)

      O3 = T.let(:o3, OpenAI::ChatModel::TaggedSymbol)
      O3_2025_04_16 = T.let(:"o3-2025-04-16", OpenAI::ChatModel::TaggedSymbol)
      O3_MINI = T.let(:"o3-mini", OpenAI::ChatModel::TaggedSymbol)

      O3_MINI_2025_01_31 = T.let(:"o3-mini-2025-01-31", OpenAI::ChatModel::TaggedSymbol)

      O4_MINI = T.let(:"o4-mini", OpenAI::ChatModel::TaggedSymbol)

      O4_MINI_2025_04_16 = T.let(:"o4-mini-2025-04-16", OpenAI::ChatModel::TaggedSymbol)

      OrSymbol = T.type_alias { T.any(Symbol, String) }
      TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ChatModel) }
    end

    class ComparisonFilter < OpenAI::Internal::Type::BaseModel
      # The key to compare against the value.
      sig { returns(String) }
      attr_accessor :key

      # Specifies the comparison operator: `eq`, `ne`, `gt`, `gte`, `lt`, `lte`.
      #
      # - `eq`: equals
      # - `ne`: not equal
      # - `gt`: greater than
      # - `gte`: greater than or equal
      # - `lt`: less than
      # - `lte`: less than or equal
      sig { returns(OpenAI::ComparisonFilter::Type::OrSymbol) }
      attr_accessor :type

      # The value to compare against the attribute key; supports string, number, or
      # boolean types.
      sig { returns(OpenAI::ComparisonFilter::Value::Variants) }
      attr_accessor :value

      sig do
        override
          .returns({
            key: String,
            type: OpenAI::ComparisonFilter::Type::OrSymbol,
            value: OpenAI::ComparisonFilter::Value::Variants
          })
      end
      def to_hash; end

      class << self
        # A filter used to compare a specified attribute key to a given value using a
        # defined comparison operation.
        sig do
          params(
            key: String,
            type: OpenAI::ComparisonFilter::Type::OrSymbol,
            value: OpenAI::ComparisonFilter::Value::Variants
          ).returns(T.attached_class)
        end
        def new(
          key:, # The key to compare against the value.
          type:, # Specifies the comparison operator: `eq`, `ne`, `gt`, `gte`, `lt`, `lte`.
                 # - `eq`: equals
                 # - `ne`: not equal
                 # - `gt`: greater than
                 # - `gte`: greater than or equal
                 # - `lt`: less than
                 # - `lte`: less than or equal
          value: # The value to compare against the attribute key; supports string, number, or
                 # boolean types.
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ComparisonFilter, OpenAI::Internal::AnyHash)
        end

      # Specifies the comparison operator: `eq`, `ne`, `gt`, `gte`, `lt`, `lte`.
      #
      # - `eq`: equals
      # - `ne`: not equal
      # - `gt`: greater than
      # - `gte`: greater than or equal
      # - `lt`: less than
      # - `lte`: less than or equal
      module Type
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ComparisonFilter::Type::TaggedSymbol]) }
          def values; end
        end

        EQ = T.let(:eq, OpenAI::ComparisonFilter::Type::TaggedSymbol)
        GT = T.let(:gt, OpenAI::ComparisonFilter::Type::TaggedSymbol)
        GTE = T.let(:gte, OpenAI::ComparisonFilter::Type::TaggedSymbol)
        LT = T.let(:lt, OpenAI::ComparisonFilter::Type::TaggedSymbol)
        LTE = T.let(:lte, OpenAI::ComparisonFilter::Type::TaggedSymbol)
        NE = T.let(:ne, OpenAI::ComparisonFilter::Type::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ComparisonFilter::Type) }
      end

      # The value to compare against the attribute key; supports string, number, or
      # boolean types.
      module Value
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::ComparisonFilter::Value::Variants]) }
          def variants; end
        end

        Variants = T.type_alias { T.any(String, Float, T::Boolean) }
      end
    end

    class Completion < OpenAI::Internal::Type::BaseModel
      # The list of completion choices the model generated for the input prompt.
      sig { returns(T::Array[OpenAI::CompletionChoice]) }
      attr_accessor :choices

      # The Unix timestamp (in seconds) of when the completion was created.
      sig { returns(Integer) }
      attr_accessor :created

      # A unique identifier for the completion.
      sig { returns(String) }
      attr_accessor :id

      # The model used for completion.
      sig { returns(String) }
      attr_accessor :model

      # The object type, which is always "text_completion"
      sig { returns(Symbol) }
      attr_accessor :object

      # This fingerprint represents the backend configuration that the model runs with.
      #
      # Can be used in conjunction with the `seed` request parameter to understand when
      # backend changes have been made that might impact determinism.
      sig { returns(T.nilable(String)) }
      attr_reader :system_fingerprint

      sig { params(system_fingerprint: String).void }
      attr_writer :system_fingerprint

      # Usage statistics for the completion request.
      sig { returns(T.nilable(OpenAI::CompletionUsage)) }
      attr_reader :usage

      sig { params(usage: OpenAI::CompletionUsage::OrHash).void }
      attr_writer :usage

      sig do
        override
          .returns({
            id: String,
            choices: T::Array[OpenAI::CompletionChoice],
            created: Integer,
            model: String,
            object: Symbol,
            system_fingerprint: String,
            usage: OpenAI::CompletionUsage
          })
      end
      def to_hash; end

      class << self
        # Represents a completion response from the API. Note: both the streamed and
        # non-streamed response objects share the same shape (unlike the chat endpoint).
        sig do
          params(
            id: String,
            choices: T::Array[OpenAI::CompletionChoice::OrHash],
            created: Integer,
            model: String,
            system_fingerprint: String,
            usage: OpenAI::CompletionUsage::OrHash,
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          id:, # A unique identifier for the completion.
          choices:, # The list of completion choices the model generated for the input prompt.
          created:, # The Unix timestamp (in seconds) of when the completion was created.
          model:, # The model used for completion.
          system_fingerprint: nil, # This fingerprint represents the backend configuration that the model runs with.
                                   # Can be used in conjunction with the `seed` request parameter to understand when
                                   # backend changes have been made that might impact determinism.
          usage: nil, # Usage statistics for the completion request.
          object: :text_completion # The object type, which is always "text_completion"
); end
      end

      OrHash = T.type_alias { T.any(OpenAI::Completion, OpenAI::Internal::AnyHash) }
    end

    class CompletionChoice < OpenAI::Internal::Type::BaseModel
      # The reason the model stopped generating tokens. This will be `stop` if the model
      # hit a natural stop point or a provided stop sequence, `length` if the maximum
      # number of tokens specified in the request was reached, or `content_filter` if
      # content was omitted due to a flag from our content filters.
      sig { returns(OpenAI::CompletionChoice::FinishReason::TaggedSymbol) }
      attr_accessor :finish_reason

      sig { returns(Integer) }
      attr_accessor :index

      sig { returns(T.nilable(OpenAI::CompletionChoice::Logprobs)) }
      attr_reader :logprobs

      sig { params(logprobs: T.nilable(OpenAI::CompletionChoice::Logprobs::OrHash)).void }
      attr_writer :logprobs

      sig { returns(String) }
      attr_accessor :text

      sig do
        override
          .returns({
            finish_reason: OpenAI::CompletionChoice::FinishReason::TaggedSymbol,
            index: Integer,
            logprobs: T.nilable(OpenAI::CompletionChoice::Logprobs),
            text: String
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            finish_reason: OpenAI::CompletionChoice::FinishReason::OrSymbol,
            index: Integer,
            logprobs: T.nilable(OpenAI::CompletionChoice::Logprobs::OrHash),
            text: String
          ).returns(T.attached_class)
        end
        def new(
          finish_reason:, # The reason the model stopped generating tokens. This will be `stop` if the model
                          # hit a natural stop point or a provided stop sequence, `length` if the maximum
                          # number of tokens specified in the request was reached, or `content_filter` if
                          # content was omitted due to a flag from our content filters.
          index:,
          logprobs:,
          text:
); end
      end

      # The reason the model stopped generating tokens. This will be `stop` if the model
      # hit a natural stop point or a provided stop sequence, `length` if the maximum
      # number of tokens specified in the request was reached, or `content_filter` if
      # content was omitted due to a flag from our content filters.
      module FinishReason
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::CompletionChoice::FinishReason::TaggedSymbol]) }
          def values; end
        end

        CONTENT_FILTER = T.let(
            :content_filter,
            OpenAI::CompletionChoice::FinishReason::TaggedSymbol
          )

        LENGTH = T.let(:length, OpenAI::CompletionChoice::FinishReason::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        STOP = T.let(:stop, OpenAI::CompletionChoice::FinishReason::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::CompletionChoice::FinishReason) }
      end

      class Logprobs < OpenAI::Internal::Type::BaseModel
        sig { returns(T.nilable(T::Array[Integer])) }
        attr_reader :text_offset

        sig { params(text_offset: T::Array[Integer]).void }
        attr_writer :text_offset

        sig { returns(T.nilable(T::Array[Float])) }
        attr_reader :token_logprobs

        sig { params(token_logprobs: T::Array[Float]).void }
        attr_writer :token_logprobs

        sig { returns(T.nilable(T::Array[String])) }
        attr_reader :tokens

        sig { params(tokens: T::Array[String]).void }
        attr_writer :tokens

        sig { returns(T.nilable(T::Array[T::Hash[Symbol, Float]])) }
        attr_reader :top_logprobs

        sig { params(top_logprobs: T::Array[T::Hash[Symbol, Float]]).void }
        attr_writer :top_logprobs

        sig do
          override
            .returns({
              text_offset: T::Array[Integer],
              token_logprobs: T::Array[Float],
              tokens: T::Array[String],
              top_logprobs: T::Array[T::Hash[Symbol, Float]]
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              text_offset: T::Array[Integer],
              token_logprobs: T::Array[Float],
              tokens: T::Array[String],
              top_logprobs: T::Array[T::Hash[Symbol, Float]]
            ).returns(T.attached_class)
          end
          def new(text_offset: nil, token_logprobs: nil, tokens: nil, top_logprobs: nil); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::CompletionChoice::Logprobs, OpenAI::Internal::AnyHash)
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::CompletionChoice, OpenAI::Internal::AnyHash)
        end
    end

    class CompletionCreateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # Generates `best_of` completions server-side and returns the "best" (the one with
      # the highest log probability per token). Results cannot be streamed.
      #
      # When used with `n`, `best_of` controls the number of candidate completions and
      # `n` specifies how many to return – `best_of` must be greater than `n`.
      #
      # **Note:** Because this parameter generates many completions, it can quickly
      # consume your token quota. Use carefully and ensure that you have reasonable
      # settings for `max_tokens` and `stop`.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :best_of

      # Echo back the prompt in addition to the completion
      sig { returns(T.nilable(T::Boolean)) }
      attr_accessor :echo

      # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
      # existing frequency in the text so far, decreasing the model's likelihood to
      # repeat the same line verbatim.
      #
      # [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
      sig { returns(T.nilable(Float)) }
      attr_accessor :frequency_penalty

      # Modify the likelihood of specified tokens appearing in the completion.
      #
      # Accepts a JSON object that maps tokens (specified by their token ID in the GPT
      # tokenizer) to an associated bias value from -100 to 100. You can use this
      # [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.
      # Mathematically, the bias is added to the logits generated by the model prior to
      # sampling. The exact effect will vary per model, but values between -1 and 1
      # should decrease or increase likelihood of selection; values like -100 or 100
      # should result in a ban or exclusive selection of the relevant token.
      #
      # As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token
      # from being generated.
      sig { returns(T.nilable(T::Hash[Symbol, Integer])) }
      attr_accessor :logit_bias

      # Include the log probabilities on the `logprobs` most likely output tokens, as
      # well the chosen tokens. For example, if `logprobs` is 5, the API will return a
      # list of the 5 most likely tokens. The API will always return the `logprob` of
      # the sampled token, so there may be up to `logprobs+1` elements in the response.
      #
      # The maximum value for `logprobs` is 5.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :logprobs

      # The maximum number of [tokens](/tokenizer) that can be generated in the
      # completion.
      #
      # The token count of your prompt plus `max_tokens` cannot exceed the model's
      # context length.
      # [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
      # for counting tokens.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :max_tokens

      # ID of the model to use. You can use the
      # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
      # see all of your available models, or see our
      # [Model overview](https://platform.openai.com/docs/models) for descriptions of
      # them.
      sig { returns(T.any(String, OpenAI::CompletionCreateParams::Model::OrSymbol)) }
      attr_accessor :model

      # How many completions to generate for each prompt.
      #
      # **Note:** Because this parameter generates many completions, it can quickly
      # consume your token quota. Use carefully and ensure that you have reasonable
      # settings for `max_tokens` and `stop`.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :n

      # Number between -2.0 and 2.0. Positive values penalize new tokens based on
      # whether they appear in the text so far, increasing the model's likelihood to
      # talk about new topics.
      #
      # [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
      sig { returns(T.nilable(Float)) }
      attr_accessor :presence_penalty

      # The prompt(s) to generate completions for, encoded as a string, array of
      # strings, array of tokens, or array of token arrays.
      #
      # Note that <|endoftext|> is the document separator that the model sees during
      # training, so if a prompt is not specified the model will generate as if from the
      # beginning of a new document.
      sig { returns(T.nilable(OpenAI::CompletionCreateParams::Prompt::Variants)) }
      attr_accessor :prompt

      # If specified, our system will make a best effort to sample deterministically,
      # such that repeated requests with the same `seed` and parameters should return
      # the same result.
      #
      # Determinism is not guaranteed, and you should refer to the `system_fingerprint`
      # response parameter to monitor changes in the backend.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :seed

      # Not supported with latest reasoning models `o3` and `o4-mini`.
      #
      # Up to 4 sequences where the API will stop generating further tokens. The
      # returned text will not contain the stop sequence.
      sig { returns(T.nilable(OpenAI::CompletionCreateParams::Stop::Variants)) }
      attr_accessor :stop

      # Options for streaming response. Only set this when you set `stream: true`.
      sig { returns(T.nilable(OpenAI::Chat::ChatCompletionStreamOptions)) }
      attr_reader :stream_options

      sig { params(stream_options: T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash)).void }
      attr_writer :stream_options

      # The suffix that comes after a completion of inserted text.
      #
      # This parameter is only supported for `gpt-3.5-turbo-instruct`.
      sig { returns(T.nilable(String)) }
      attr_accessor :suffix

      # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
      # make the output more random, while lower values like 0.2 will make it more
      # focused and deterministic.
      #
      # We generally recommend altering this or `top_p` but not both.
      sig { returns(T.nilable(Float)) }
      attr_accessor :temperature

      # An alternative to sampling with temperature, called nucleus sampling, where the
      # model considers the results of the tokens with top_p probability mass. So 0.1
      # means only the tokens comprising the top 10% probability mass are considered.
      #
      # We generally recommend altering this or `temperature` but not both.
      sig { returns(T.nilable(Float)) }
      attr_accessor :top_p

      # A unique identifier representing your end-user, which can help OpenAI to monitor
      # and detect abuse.
      # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
      sig { returns(T.nilable(String)) }
      attr_reader :user

      sig { params(user: String).void }
      attr_writer :user

      sig do
        override
          .returns({
            model:
              T.any(String, OpenAI::CompletionCreateParams::Model::OrSymbol),
            prompt: T.nilable(OpenAI::CompletionCreateParams::Prompt::Variants),
            best_of: T.nilable(Integer),
            echo: T.nilable(T::Boolean),
            frequency_penalty: T.nilable(Float),
            logit_bias: T.nilable(T::Hash[Symbol, Integer]),
            logprobs: T.nilable(Integer),
            max_tokens: T.nilable(Integer),
            n: T.nilable(Integer),
            presence_penalty: T.nilable(Float),
            seed: T.nilable(Integer),
            stop: T.nilable(OpenAI::CompletionCreateParams::Stop::Variants),
            stream_options:
              T.nilable(OpenAI::Chat::ChatCompletionStreamOptions),
            suffix: T.nilable(String),
            temperature: T.nilable(Float),
            top_p: T.nilable(Float),
            user: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            model: T.any(String, OpenAI::CompletionCreateParams::Model::OrSymbol),
            prompt: T.nilable(OpenAI::CompletionCreateParams::Prompt::Variants),
            best_of: T.nilable(Integer),
            echo: T.nilable(T::Boolean),
            frequency_penalty: T.nilable(Float),
            logit_bias: T.nilable(T::Hash[Symbol, Integer]),
            logprobs: T.nilable(Integer),
            max_tokens: T.nilable(Integer),
            n: T.nilable(Integer),
            presence_penalty: T.nilable(Float),
            seed: T.nilable(Integer),
            stop: T.nilable(OpenAI::CompletionCreateParams::Stop::Variants),
            stream_options: T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash),
            suffix: T.nilable(String),
            temperature: T.nilable(Float),
            top_p: T.nilable(Float),
            user: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          model:, # ID of the model to use. You can use the
                  # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                  # see all of your available models, or see our
                  # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                  # them.
          prompt:, # The prompt(s) to generate completions for, encoded as a string, array of
                   # strings, array of tokens, or array of token arrays.
                   # Note that <|endoftext|> is the document separator that the model sees during
                   # training, so if a prompt is not specified the model will generate as if from the
                   # beginning of a new document.
          best_of: nil, # Generates `best_of` completions server-side and returns the "best" (the one with
                        # the highest log probability per token). Results cannot be streamed.
                        # When used with `n`, `best_of` controls the number of candidate completions and
                        # `n` specifies how many to return – `best_of` must be greater than `n`.
                        # **Note:** Because this parameter generates many completions, it can quickly
                        # consume your token quota. Use carefully and ensure that you have reasonable
                        # settings for `max_tokens` and `stop`.
          echo: nil, # Echo back the prompt in addition to the completion
          frequency_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
                                  # existing frequency in the text so far, decreasing the model's likelihood to
                                  # repeat the same line verbatim.
                                  # [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
          logit_bias: nil, # Modify the likelihood of specified tokens appearing in the completion.
                           # Accepts a JSON object that maps tokens (specified by their token ID in the GPT
                           # tokenizer) to an associated bias value from -100 to 100. You can use this
                           # [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.
                           # Mathematically, the bias is added to the logits generated by the model prior to
                           # sampling. The exact effect will vary per model, but values between -1 and 1
                           # should decrease or increase likelihood of selection; values like -100 or 100
                           # should result in a ban or exclusive selection of the relevant token.
                           # As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token
                           # from being generated.
          logprobs: nil, # Include the log probabilities on the `logprobs` most likely output tokens, as
                         # well the chosen tokens. For example, if `logprobs` is 5, the API will return a
                         # list of the 5 most likely tokens. The API will always return the `logprob` of
                         # the sampled token, so there may be up to `logprobs+1` elements in the response.
                         # The maximum value for `logprobs` is 5.
          max_tokens: nil, # The maximum number of [tokens](/tokenizer) that can be generated in the
                           # completion.
                           # The token count of your prompt plus `max_tokens` cannot exceed the model's
                           # context length.
                           # [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
                           # for counting tokens.
          n: nil, # How many completions to generate for each prompt.
                  # **Note:** Because this parameter generates many completions, it can quickly
                  # consume your token quota. Use carefully and ensure that you have reasonable
                  # settings for `max_tokens` and `stop`.
          presence_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on
                                 # whether they appear in the text so far, increasing the model's likelihood to
                                 # talk about new topics.
                                 # [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
          seed: nil, # If specified, our system will make a best effort to sample deterministically,
                     # such that repeated requests with the same `seed` and parameters should return
                     # the same result.
                     # Determinism is not guaranteed, and you should refer to the `system_fingerprint`
                     # response parameter to monitor changes in the backend.
          stop: nil, # Not supported with latest reasoning models `o3` and `o4-mini`.
                     # Up to 4 sequences where the API will stop generating further tokens. The
                     # returned text will not contain the stop sequence.
          stream_options: nil, # Options for streaming response. Only set this when you set `stream: true`.
          suffix: nil, # The suffix that comes after a completion of inserted text.
                       # This parameter is only supported for `gpt-3.5-turbo-instruct`.
          temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                            # make the output more random, while lower values like 0.2 will make it more
                            # focused and deterministic.
                            # We generally recommend altering this or `top_p` but not both.
          top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                      # model considers the results of the tokens with top_p probability mass. So 0.1
                      # means only the tokens comprising the top 10% probability mass are considered.
                      # We generally recommend altering this or `temperature` but not both.
          user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                     # and detect abuse.
                     # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
          request_options: {}
); end
      end

      # ID of the model to use. You can use the
      # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
      # see all of your available models, or see our
      # [Model overview](https://platform.openai.com/docs/models) for descriptions of
      # them.
      module Model
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::CompletionCreateParams::Model::Variants]) }
          def variants; end
        end

        BABBAGE_002 = T.let(
            :"babbage-002",
            OpenAI::CompletionCreateParams::Model::TaggedSymbol
          )

        DAVINCI_002 = T.let(
            :"davinci-002",
            OpenAI::CompletionCreateParams::Model::TaggedSymbol
          )

        GPT_3_5_TURBO_INSTRUCT = T.let(
            :"gpt-3.5-turbo-instruct",
            OpenAI::CompletionCreateParams::Model::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::CompletionCreateParams::Model) }

        Variants = T.type_alias do
            T.any(String, OpenAI::CompletionCreateParams::Model::TaggedSymbol)
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::CompletionCreateParams, OpenAI::Internal::AnyHash)
        end

      # The prompt(s) to generate completions for, encoded as a string, array of
      # strings, array of tokens, or array of token arrays.
      #
      # Note that <|endoftext|> is the document separator that the model sees during
      # training, so if a prompt is not specified the model will generate as if from the
      # beginning of a new document.
      module Prompt
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::CompletionCreateParams::Prompt::Variants]) }
          def variants; end
        end

        ArrayOfToken2DArray = T.let(
            OpenAI::Internal::Type::ArrayOf[
              OpenAI::Internal::Type::ArrayOf[Integer]
            ],
            OpenAI::Internal::Type::Converter
          )

        IntegerArray = T.let(
            OpenAI::Internal::Type::ArrayOf[Integer],
            OpenAI::Internal::Type::Converter
          )

        StringArray = T.let(
            OpenAI::Internal::Type::ArrayOf[String],
            OpenAI::Internal::Type::Converter
          )

        Variants = T.type_alias do
            T.any(
              String,
              T::Array[String],
              T::Array[Integer],
              T::Array[T::Array[Integer]]
            )
          end
      end

      # Not supported with latest reasoning models `o3` and `o4-mini`.
      #
      # Up to 4 sequences where the API will stop generating further tokens. The
      # returned text will not contain the stop sequence.
      module Stop
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::CompletionCreateParams::Stop::Variants]) }
          def variants; end
        end

        StringArray = T.let(
            OpenAI::Internal::Type::ArrayOf[String],
            OpenAI::Internal::Type::Converter
          )

        Variants = T.type_alias { T.nilable(T.any(String, T::Array[String])) }
      end
    end

    class CompletionUsage < OpenAI::Internal::Type::BaseModel
      # Number of tokens in the generated completion.
      sig { returns(Integer) }
      attr_accessor :completion_tokens

      # Breakdown of tokens used in a completion.
      sig { returns(T.nilable(OpenAI::CompletionUsage::CompletionTokensDetails)) }
      attr_reader :completion_tokens_details

      sig { params(completion_tokens_details: OpenAI::CompletionUsage::CompletionTokensDetails::OrHash).void }
      attr_writer :completion_tokens_details

      # Number of tokens in the prompt.
      sig { returns(Integer) }
      attr_accessor :prompt_tokens

      # Breakdown of tokens used in the prompt.
      sig { returns(T.nilable(OpenAI::CompletionUsage::PromptTokensDetails)) }
      attr_reader :prompt_tokens_details

      sig { params(prompt_tokens_details: OpenAI::CompletionUsage::PromptTokensDetails::OrHash).void }
      attr_writer :prompt_tokens_details

      # Total number of tokens used in the request (prompt + completion).
      sig { returns(Integer) }
      attr_accessor :total_tokens

      sig do
        override
          .returns({
            completion_tokens: Integer,
            prompt_tokens: Integer,
            total_tokens: Integer,
            completion_tokens_details:
              OpenAI::CompletionUsage::CompletionTokensDetails,
            prompt_tokens_details: OpenAI::CompletionUsage::PromptTokensDetails
          })
      end
      def to_hash; end

      class << self
        # Usage statistics for the completion request.
        sig do
          params(
            completion_tokens: Integer,
            prompt_tokens: Integer,
            total_tokens: Integer,
            completion_tokens_details: OpenAI::CompletionUsage::CompletionTokensDetails::OrHash,
            prompt_tokens_details: OpenAI::CompletionUsage::PromptTokensDetails::OrHash
          ).returns(T.attached_class)
        end
        def new(
          completion_tokens:, # Number of tokens in the generated completion.
          prompt_tokens:, # Number of tokens in the prompt.
          total_tokens:, # Total number of tokens used in the request (prompt + completion).
          completion_tokens_details: nil, # Breakdown of tokens used in a completion.
          prompt_tokens_details: nil # Breakdown of tokens used in the prompt.
); end
      end

      class CompletionTokensDetails < OpenAI::Internal::Type::BaseModel
        # When using Predicted Outputs, the number of tokens in the prediction that
        # appeared in the completion.
        sig { returns(T.nilable(Integer)) }
        attr_reader :accepted_prediction_tokens

        sig { params(accepted_prediction_tokens: Integer).void }
        attr_writer :accepted_prediction_tokens

        # Audio input tokens generated by the model.
        sig { returns(T.nilable(Integer)) }
        attr_reader :audio_tokens

        sig { params(audio_tokens: Integer).void }
        attr_writer :audio_tokens

        # Tokens generated by the model for reasoning.
        sig { returns(T.nilable(Integer)) }
        attr_reader :reasoning_tokens

        sig { params(reasoning_tokens: Integer).void }
        attr_writer :reasoning_tokens

        # When using Predicted Outputs, the number of tokens in the prediction that did
        # not appear in the completion. However, like reasoning tokens, these tokens are
        # still counted in the total completion tokens for purposes of billing, output,
        # and context window limits.
        sig { returns(T.nilable(Integer)) }
        attr_reader :rejected_prediction_tokens

        sig { params(rejected_prediction_tokens: Integer).void }
        attr_writer :rejected_prediction_tokens

        sig do
          override
            .returns({
              accepted_prediction_tokens: Integer,
              audio_tokens: Integer,
              reasoning_tokens: Integer,
              rejected_prediction_tokens: Integer
            })
        end
        def to_hash; end

        class << self
          # Breakdown of tokens used in a completion.
          sig do
            params(
              accepted_prediction_tokens: Integer,
              audio_tokens: Integer,
              reasoning_tokens: Integer,
              rejected_prediction_tokens: Integer
            ).returns(T.attached_class)
          end
          def new(
            accepted_prediction_tokens: nil, # When using Predicted Outputs, the number of tokens in the prediction that
                                             # appeared in the completion.
            audio_tokens: nil, # Audio input tokens generated by the model.
            reasoning_tokens: nil, # Tokens generated by the model for reasoning.
            rejected_prediction_tokens: nil # When using Predicted Outputs, the number of tokens in the prediction that did
                                            # not appear in the completion. However, like reasoning tokens, these tokens are
                                            # still counted in the total completion tokens for purposes of billing, output,
                                            # and context window limits.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::CompletionUsage::CompletionTokensDetails,
              OpenAI::Internal::AnyHash
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::CompletionUsage, OpenAI::Internal::AnyHash)
        end

      class PromptTokensDetails < OpenAI::Internal::Type::BaseModel
        # Audio input tokens present in the prompt.
        sig { returns(T.nilable(Integer)) }
        attr_reader :audio_tokens

        sig { params(audio_tokens: Integer).void }
        attr_writer :audio_tokens

        # Cached tokens present in the prompt.
        sig { returns(T.nilable(Integer)) }
        attr_reader :cached_tokens

        sig { params(cached_tokens: Integer).void }
        attr_writer :cached_tokens

        sig { override.returns({ audio_tokens: Integer, cached_tokens: Integer }) }
        def to_hash; end

        class << self
          # Breakdown of tokens used in the prompt.
          sig { params(audio_tokens: Integer, cached_tokens: Integer).returns(T.attached_class) }
          def new(
            audio_tokens: nil, # Audio input tokens present in the prompt.
            cached_tokens: nil # Cached tokens present in the prompt.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::CompletionUsage::PromptTokensDetails,
              OpenAI::Internal::AnyHash
            )
          end
      end
    end

    class CompoundFilter < OpenAI::Internal::Type::BaseModel
      # Array of filters to combine. Items can be `ComparisonFilter` or
      # `CompoundFilter`.
      sig { returns(T::Array[T.any(OpenAI::ComparisonFilter, T.anything)]) }
      attr_accessor :filters

      # Type of operation: `and` or `or`.
      sig { returns(OpenAI::CompoundFilter::Type::OrSymbol) }
      attr_accessor :type

      sig do
        override
          .returns({
            filters: T::Array[T.any(OpenAI::ComparisonFilter, T.anything)],
            type: OpenAI::CompoundFilter::Type::OrSymbol
          })
      end
      def to_hash; end

      class << self
        # Combine multiple filters using `and` or `or`.
        sig do
          params(
            filters: T::Array[T.any(OpenAI::ComparisonFilter::OrHash, T.anything)],
            type: OpenAI::CompoundFilter::Type::OrSymbol
          ).returns(T.attached_class)
        end
        def new(
          filters:, # Array of filters to combine. Items can be `ComparisonFilter` or
                    # `CompoundFilter`.
          type: # Type of operation: `and` or `or`.
); end
      end

      # A filter used to compare a specified attribute key to a given value using a
      # defined comparison operation.
      module Filter
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::CompoundFilter::Filter::Variants]) }
          def variants; end
        end

        Variants = T.type_alias { T.any(OpenAI::ComparisonFilter, T.anything) }
      end

      OrHash = T.type_alias do
          T.any(OpenAI::CompoundFilter, OpenAI::Internal::AnyHash)
        end

      # Type of operation: `and` or `or`.
      module Type
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::CompoundFilter::Type::TaggedSymbol]) }
          def values; end
        end

        AND = T.let(:and, OpenAI::CompoundFilter::Type::TaggedSymbol)
        OR = T.let(:or, OpenAI::CompoundFilter::Type::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::CompoundFilter::Type) }
      end
    end

    class ContainerCreateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # Container expiration time in seconds relative to the 'anchor' time.
      sig { returns(T.nilable(OpenAI::ContainerCreateParams::ExpiresAfter)) }
      attr_reader :expires_after

      sig { params(expires_after: OpenAI::ContainerCreateParams::ExpiresAfter::OrHash).void }
      attr_writer :expires_after

      # IDs of files to copy to the container.
      sig { returns(T.nilable(T::Array[String])) }
      attr_reader :file_ids

      sig { params(file_ids: T::Array[String]).void }
      attr_writer :file_ids

      # Name of the container to create.
      sig { returns(String) }
      attr_accessor :name

      sig do
        override
          .returns({
            name: String,
            expires_after: OpenAI::ContainerCreateParams::ExpiresAfter,
            file_ids: T::Array[String],
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            name: String,
            expires_after: OpenAI::ContainerCreateParams::ExpiresAfter::OrHash,
            file_ids: T::Array[String],
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          name:, # Name of the container to create.
          expires_after: nil, # Container expiration time in seconds relative to the 'anchor' time.
          file_ids: nil, # IDs of files to copy to the container.
          request_options: {}
); end
      end

      class ExpiresAfter < OpenAI::Internal::Type::BaseModel
        # Time anchor for the expiration time. Currently only 'last_active_at' is
        # supported.
        sig { returns(OpenAI::ContainerCreateParams::ExpiresAfter::Anchor::OrSymbol) }
        attr_accessor :anchor

        sig { returns(Integer) }
        attr_accessor :minutes

        sig do
          override
            .returns({
              anchor:
                OpenAI::ContainerCreateParams::ExpiresAfter::Anchor::OrSymbol,
              minutes: Integer
            })
        end
        def to_hash; end

        class << self
          # Container expiration time in seconds relative to the 'anchor' time.
          sig do
            params(
              anchor: OpenAI::ContainerCreateParams::ExpiresAfter::Anchor::OrSymbol,
              minutes: Integer
            ).returns(T.attached_class)
          end
          def new(
            anchor:, # Time anchor for the expiration time. Currently only 'last_active_at' is
                     # supported.
            minutes:
); end
        end

        # Time anchor for the expiration time. Currently only 'last_active_at' is
        # supported.
        module Anchor
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::ContainerCreateParams::ExpiresAfter::Anchor::TaggedSymbol
              ])
            end
            def values; end
          end

          LAST_ACTIVE_AT = T.let(
              :last_active_at,
              OpenAI::ContainerCreateParams::ExpiresAfter::Anchor::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::ContainerCreateParams::ExpiresAfter::Anchor)
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::ContainerCreateParams::ExpiresAfter,
              OpenAI::Internal::AnyHash
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ContainerCreateParams, OpenAI::Internal::AnyHash)
        end
    end

    class ContainerCreateResponse < OpenAI::Internal::Type::BaseModel
      # Unix timestamp (in seconds) when the container was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The container will expire after this time period. The anchor is the reference
      # point for the expiration. The minutes is the number of minutes after the anchor
      # before the container expires.
      sig { returns(T.nilable(OpenAI::Models::ContainerCreateResponse::ExpiresAfter)) }
      attr_reader :expires_after

      sig { params(expires_after: OpenAI::Models::ContainerCreateResponse::ExpiresAfter::OrHash).void }
      attr_writer :expires_after

      # Unique identifier for the container.
      sig { returns(String) }
      attr_accessor :id

      # Name of the container.
      sig { returns(String) }
      attr_accessor :name

      # The type of this object.
      sig { returns(String) }
      attr_accessor :object

      # Status of the container (e.g., active, deleted).
      sig { returns(String) }
      attr_accessor :status

      sig do
        override
          .returns({
            id: String,
            created_at: Integer,
            name: String,
            object: String,
            status: String,
            expires_after: OpenAI::Models::ContainerCreateResponse::ExpiresAfter
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            id: String,
            created_at: Integer,
            name: String,
            object: String,
            status: String,
            expires_after: OpenAI::Models::ContainerCreateResponse::ExpiresAfter::OrHash
          ).returns(T.attached_class)
        end
        def new(
          id:, # Unique identifier for the container.
          created_at:, # Unix timestamp (in seconds) when the container was created.
          name:, # Name of the container.
          object:, # The type of this object.
          status:, # Status of the container (e.g., active, deleted).
          expires_after: nil # The container will expire after this time period. The anchor is the reference
                             # point for the expiration. The minutes is the number of minutes after the anchor
                             # before the container expires.
); end
      end

      class ExpiresAfter < OpenAI::Internal::Type::BaseModel
        # The reference point for the expiration.
        sig do
          returns(T.nilable(
              OpenAI::Models::ContainerCreateResponse::ExpiresAfter::Anchor::TaggedSymbol
            ))
        end
        attr_reader :anchor

        sig { params(anchor: OpenAI::Models::ContainerCreateResponse::ExpiresAfter::Anchor::OrSymbol).void }
        attr_writer :anchor

        # The number of minutes after the anchor before the container expires.
        sig { returns(T.nilable(Integer)) }
        attr_reader :minutes

        sig { params(minutes: Integer).void }
        attr_writer :minutes

        sig do
          override
            .returns({
              anchor:
                OpenAI::Models::ContainerCreateResponse::ExpiresAfter::Anchor::TaggedSymbol,
              minutes: Integer
            })
        end
        def to_hash; end

        class << self
          # The container will expire after this time period. The anchor is the reference
          # point for the expiration. The minutes is the number of minutes after the anchor
          # before the container expires.
          sig do
            params(
              anchor: OpenAI::Models::ContainerCreateResponse::ExpiresAfter::Anchor::OrSymbol,
              minutes: Integer
            ).returns(T.attached_class)
          end
          def new(
            anchor: nil, # The reference point for the expiration.
            minutes: nil # The number of minutes after the anchor before the container expires.
); end
        end

        # The reference point for the expiration.
        module Anchor
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Models::ContainerCreateResponse::ExpiresAfter::Anchor::TaggedSymbol
              ])
            end
            def values; end
          end

          LAST_ACTIVE_AT = T.let(
              :last_active_at,
              OpenAI::Models::ContainerCreateResponse::ExpiresAfter::Anchor::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Models::ContainerCreateResponse::ExpiresAfter::Anchor
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::ContainerCreateResponse::ExpiresAfter,
              OpenAI::Internal::AnyHash
            )
          end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::Models::ContainerCreateResponse,
            OpenAI::Internal::AnyHash
          )
        end
    end

    class ContainerDeleteParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ContainerDeleteParams, OpenAI::Internal::AnyHash)
        end
    end

    class ContainerListParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # A cursor for use in pagination. `after` is an object ID that defines your place
      # in the list. For instance, if you make a list request and receive 100 objects,
      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
      # fetch the next page of the list.
      sig { returns(T.nilable(String)) }
      attr_reader :after

      sig { params(after: String).void }
      attr_writer :after

      # A limit on the number of objects to be returned. Limit can range between 1 and
      # 100, and the default is 20.
      sig { returns(T.nilable(Integer)) }
      attr_reader :limit

      sig { params(limit: Integer).void }
      attr_writer :limit

      # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
      # order and `desc` for descending order.
      sig { returns(T.nilable(OpenAI::ContainerListParams::Order::OrSymbol)) }
      attr_reader :order

      sig { params(order: OpenAI::ContainerListParams::Order::OrSymbol).void }
      attr_writer :order

      sig do
        override
          .returns({
            after: String,
            limit: Integer,
            order: OpenAI::ContainerListParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            after: String,
            limit: Integer,
            order: OpenAI::ContainerListParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                      # in the list. For instance, if you make a list request and receive 100 objects,
                      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                      # fetch the next page of the list.
          limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                      # 100, and the default is 20.
          order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                      # order and `desc` for descending order.
          request_options: {}
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ContainerListParams, OpenAI::Internal::AnyHash)
        end

      # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
      # order and `desc` for descending order.
      module Order
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ContainerListParams::Order::TaggedSymbol]) }
          def values; end
        end

        ASC = T.let(:asc, OpenAI::ContainerListParams::Order::TaggedSymbol)
        DESC = T.let(:desc, OpenAI::ContainerListParams::Order::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ContainerListParams::Order) }
      end
    end

    class ContainerListResponse < OpenAI::Internal::Type::BaseModel
      # Unix timestamp (in seconds) when the container was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The container will expire after this time period. The anchor is the reference
      # point for the expiration. The minutes is the number of minutes after the anchor
      # before the container expires.
      sig { returns(T.nilable(OpenAI::Models::ContainerListResponse::ExpiresAfter)) }
      attr_reader :expires_after

      sig { params(expires_after: OpenAI::Models::ContainerListResponse::ExpiresAfter::OrHash).void }
      attr_writer :expires_after

      # Unique identifier for the container.
      sig { returns(String) }
      attr_accessor :id

      # Name of the container.
      sig { returns(String) }
      attr_accessor :name

      # The type of this object.
      sig { returns(String) }
      attr_accessor :object

      # Status of the container (e.g., active, deleted).
      sig { returns(String) }
      attr_accessor :status

      sig do
        override
          .returns({
            id: String,
            created_at: Integer,
            name: String,
            object: String,
            status: String,
            expires_after: OpenAI::Models::ContainerListResponse::ExpiresAfter
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            id: String,
            created_at: Integer,
            name: String,
            object: String,
            status: String,
            expires_after: OpenAI::Models::ContainerListResponse::ExpiresAfter::OrHash
          ).returns(T.attached_class)
        end
        def new(
          id:, # Unique identifier for the container.
          created_at:, # Unix timestamp (in seconds) when the container was created.
          name:, # Name of the container.
          object:, # The type of this object.
          status:, # Status of the container (e.g., active, deleted).
          expires_after: nil # The container will expire after this time period. The anchor is the reference
                             # point for the expiration. The minutes is the number of minutes after the anchor
                             # before the container expires.
); end
      end

      class ExpiresAfter < OpenAI::Internal::Type::BaseModel
        # The reference point for the expiration.
        sig do
          returns(T.nilable(
              OpenAI::Models::ContainerListResponse::ExpiresAfter::Anchor::TaggedSymbol
            ))
        end
        attr_reader :anchor

        sig { params(anchor: OpenAI::Models::ContainerListResponse::ExpiresAfter::Anchor::OrSymbol).void }
        attr_writer :anchor

        # The number of minutes after the anchor before the container expires.
        sig { returns(T.nilable(Integer)) }
        attr_reader :minutes

        sig { params(minutes: Integer).void }
        attr_writer :minutes

        sig do
          override
            .returns({
              anchor:
                OpenAI::Models::ContainerListResponse::ExpiresAfter::Anchor::TaggedSymbol,
              minutes: Integer
            })
        end
        def to_hash; end

        class << self
          # The container will expire after this time period. The anchor is the reference
          # point for the expiration. The minutes is the number of minutes after the anchor
          # before the container expires.
          sig do
            params(
              anchor: OpenAI::Models::ContainerListResponse::ExpiresAfter::Anchor::OrSymbol,
              minutes: Integer
            ).returns(T.attached_class)
          end
          def new(
            anchor: nil, # The reference point for the expiration.
            minutes: nil # The number of minutes after the anchor before the container expires.
); end
        end

        # The reference point for the expiration.
        module Anchor
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Models::ContainerListResponse::ExpiresAfter::Anchor::TaggedSymbol
              ])
            end
            def values; end
          end

          LAST_ACTIVE_AT = T.let(
              :last_active_at,
              OpenAI::Models::ContainerListResponse::ExpiresAfter::Anchor::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Models::ContainerListResponse::ExpiresAfter::Anchor
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::ContainerListResponse::ExpiresAfter,
              OpenAI::Internal::AnyHash
            )
          end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::Models::ContainerListResponse,
            OpenAI::Internal::AnyHash
          )
        end
    end

    class ContainerRetrieveParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ContainerRetrieveParams, OpenAI::Internal::AnyHash)
        end
    end

    class ContainerRetrieveResponse < OpenAI::Internal::Type::BaseModel
      # Unix timestamp (in seconds) when the container was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The container will expire after this time period. The anchor is the reference
      # point for the expiration. The minutes is the number of minutes after the anchor
      # before the container expires.
      sig { returns(T.nilable(OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter)) }
      attr_reader :expires_after

      sig { params(expires_after: OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter::OrHash).void }
      attr_writer :expires_after

      # Unique identifier for the container.
      sig { returns(String) }
      attr_accessor :id

      # Name of the container.
      sig { returns(String) }
      attr_accessor :name

      # The type of this object.
      sig { returns(String) }
      attr_accessor :object

      # Status of the container (e.g., active, deleted).
      sig { returns(String) }
      attr_accessor :status

      sig do
        override
          .returns({
            id: String,
            created_at: Integer,
            name: String,
            object: String,
            status: String,
            expires_after:
              OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            id: String,
            created_at: Integer,
            name: String,
            object: String,
            status: String,
            expires_after: OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter::OrHash
          ).returns(T.attached_class)
        end
        def new(
          id:, # Unique identifier for the container.
          created_at:, # Unix timestamp (in seconds) when the container was created.
          name:, # Name of the container.
          object:, # The type of this object.
          status:, # Status of the container (e.g., active, deleted).
          expires_after: nil # The container will expire after this time period. The anchor is the reference
                             # point for the expiration. The minutes is the number of minutes after the anchor
                             # before the container expires.
); end
      end

      class ExpiresAfter < OpenAI::Internal::Type::BaseModel
        # The reference point for the expiration.
        sig do
          returns(T.nilable(
              OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter::Anchor::TaggedSymbol
            ))
        end
        attr_reader :anchor

        sig { params(anchor: OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter::Anchor::OrSymbol).void }
        attr_writer :anchor

        # The number of minutes after the anchor before the container expires.
        sig { returns(T.nilable(Integer)) }
        attr_reader :minutes

        sig { params(minutes: Integer).void }
        attr_writer :minutes

        sig do
          override
            .returns({
              anchor:
                OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter::Anchor::TaggedSymbol,
              minutes: Integer
            })
        end
        def to_hash; end

        class << self
          # The container will expire after this time period. The anchor is the reference
          # point for the expiration. The minutes is the number of minutes after the anchor
          # before the container expires.
          sig do
            params(
              anchor: OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter::Anchor::OrSymbol,
              minutes: Integer
            ).returns(T.attached_class)
          end
          def new(
            anchor: nil, # The reference point for the expiration.
            minutes: nil # The number of minutes after the anchor before the container expires.
); end
        end

        # The reference point for the expiration.
        module Anchor
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter::Anchor::TaggedSymbol
              ])
            end
            def values; end
          end

          LAST_ACTIVE_AT = T.let(
              :last_active_at,
              OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter::Anchor::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter::Anchor
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::ContainerRetrieveResponse::ExpiresAfter,
              OpenAI::Internal::AnyHash
            )
          end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::Models::ContainerRetrieveResponse,
            OpenAI::Internal::AnyHash
          )
        end
    end

    module Containers
      class FileCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # The File object (not file name) to be uploaded.
        sig { returns(T.nilable(OpenAI::Internal::FileInput)) }
        attr_reader :file

        sig { params(file: OpenAI::Internal::FileInput).void }
        attr_writer :file

        # Name of the file to create.
        sig { returns(T.nilable(String)) }
        attr_reader :file_id

        sig { params(file_id: String).void }
        attr_writer :file_id

        sig do
          override
            .returns({
              file: OpenAI::Internal::FileInput,
              file_id: String,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              file: OpenAI::Internal::FileInput,
              file_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            file: nil, # The File object (not file name) to be uploaded.
            file_id: nil, # Name of the file to create.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Containers::FileCreateParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileCreateResponse < OpenAI::Internal::Type::BaseModel
        # Size of the file in bytes.
        sig { returns(Integer) }
        attr_accessor :bytes

        # The container this file belongs to.
        sig { returns(String) }
        attr_accessor :container_id

        # Unix timestamp (in seconds) when the file was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Unique identifier for the file.
        sig { returns(String) }
        attr_accessor :id

        # The type of this object (`container.file`).
        sig { returns(Symbol) }
        attr_accessor :object

        # Path of the file in the container.
        sig { returns(String) }
        attr_accessor :path

        # Source of the file (e.g., `user`, `assistant`).
        sig { returns(String) }
        attr_accessor :source

        sig do
          override
            .returns({
              id: String,
              bytes: Integer,
              container_id: String,
              created_at: Integer,
              object: Symbol,
              path: String,
              source: String
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              id: String,
              bytes: Integer,
              container_id: String,
              created_at: Integer,
              path: String,
              source: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # Unique identifier for the file.
            bytes:, # Size of the file in bytes.
            container_id:, # The container this file belongs to.
            created_at:, # Unix timestamp (in seconds) when the file was created.
            path:, # Path of the file in the container.
            source:, # Source of the file (e.g., `user`, `assistant`).
            object: :"container.file" # The type of this object (`container.file`).
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::Containers::FileCreateResponse,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileDeleteParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :container_id

        sig { override.returns({ container_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig do
            params(
              container_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(container_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Containers::FileDeleteParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileListParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # A cursor for use in pagination. `after` is an object ID that defines your place
        # in the list. For instance, if you make a list request and receive 100 objects,
        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
        # fetch the next page of the list.
        sig { returns(T.nilable(String)) }
        attr_reader :after

        sig { params(after: String).void }
        attr_writer :after

        # A limit on the number of objects to be returned. Limit can range between 1 and
        # 100, and the default is 20.
        sig { returns(T.nilable(Integer)) }
        attr_reader :limit

        sig { params(limit: Integer).void }
        attr_writer :limit

        # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
        # order and `desc` for descending order.
        sig { returns(T.nilable(OpenAI::Containers::FileListParams::Order::OrSymbol)) }
        attr_reader :order

        sig { params(order: OpenAI::Containers::FileListParams::Order::OrSymbol).void }
        attr_writer :order

        sig do
          override
            .returns({
              after: String,
              limit: Integer,
              order: OpenAI::Containers::FileListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              after: String,
              limit: Integer,
              order: OpenAI::Containers::FileListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                        # in the list. For instance, if you make a list request and receive 100 objects,
                        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                        # fetch the next page of the list.
            limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                        # 100, and the default is 20.
            order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                        # order and `desc` for descending order.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Containers::FileListParams, OpenAI::Internal::AnyHash)
          end

        # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
        # order and `desc` for descending order.
        module Order
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Containers::FileListParams::Order::TaggedSymbol]) }
            def values; end
          end

          ASC = T.let(:asc, OpenAI::Containers::FileListParams::Order::TaggedSymbol)

          DESC = T.let(
              :desc,
              OpenAI::Containers::FileListParams::Order::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Containers::FileListParams::Order)
            end
        end
      end

      class FileListResponse < OpenAI::Internal::Type::BaseModel
        # Size of the file in bytes.
        sig { returns(Integer) }
        attr_accessor :bytes

        # The container this file belongs to.
        sig { returns(String) }
        attr_accessor :container_id

        # Unix timestamp (in seconds) when the file was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Unique identifier for the file.
        sig { returns(String) }
        attr_accessor :id

        # The type of this object (`container.file`).
        sig { returns(Symbol) }
        attr_accessor :object

        # Path of the file in the container.
        sig { returns(String) }
        attr_accessor :path

        # Source of the file (e.g., `user`, `assistant`).
        sig { returns(String) }
        attr_accessor :source

        sig do
          override
            .returns({
              id: String,
              bytes: Integer,
              container_id: String,
              created_at: Integer,
              object: Symbol,
              path: String,
              source: String
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              id: String,
              bytes: Integer,
              container_id: String,
              created_at: Integer,
              path: String,
              source: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # Unique identifier for the file.
            bytes:, # Size of the file in bytes.
            container_id:, # The container this file belongs to.
            created_at:, # Unix timestamp (in seconds) when the file was created.
            path:, # Path of the file in the container.
            source:, # Source of the file (e.g., `user`, `assistant`).
            object: :"container.file" # The type of this object (`container.file`).
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::Containers::FileListResponse,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileRetrieveParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :container_id

        sig { override.returns({ container_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig do
            params(
              container_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(container_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Containers::FileRetrieveParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileRetrieveResponse < OpenAI::Internal::Type::BaseModel
        # Size of the file in bytes.
        sig { returns(Integer) }
        attr_accessor :bytes

        # The container this file belongs to.
        sig { returns(String) }
        attr_accessor :container_id

        # Unix timestamp (in seconds) when the file was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Unique identifier for the file.
        sig { returns(String) }
        attr_accessor :id

        # The type of this object (`container.file`).
        sig { returns(Symbol) }
        attr_accessor :object

        # Path of the file in the container.
        sig { returns(String) }
        attr_accessor :path

        # Source of the file (e.g., `user`, `assistant`).
        sig { returns(String) }
        attr_accessor :source

        sig do
          override
            .returns({
              id: String,
              bytes: Integer,
              container_id: String,
              created_at: Integer,
              object: Symbol,
              path: String,
              source: String
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              id: String,
              bytes: Integer,
              container_id: String,
              created_at: Integer,
              path: String,
              source: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # Unique identifier for the file.
            bytes:, # Size of the file in bytes.
            container_id:, # The container this file belongs to.
            created_at:, # Unix timestamp (in seconds) when the file was created.
            path:, # Path of the file in the container.
            source:, # Source of the file (e.g., `user`, `assistant`).
            object: :"container.file" # The type of this object (`container.file`).
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::Containers::FileRetrieveResponse,
              OpenAI::Internal::AnyHash
            )
          end
      end

      module Files
        class ContentRetrieveParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          sig { returns(String) }
          attr_accessor :container_id

          sig { override.returns({ container_id: String, request_options: OpenAI::RequestOptions }) }
          def to_hash; end

          class << self
            sig do
              params(
                container_id: String,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(container_id:, request_options: {}); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Containers::Files::ContentRetrieveParams,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end
    end

    class CreateEmbeddingResponse < OpenAI::Internal::Type::BaseModel
      # The list of embeddings generated by the model.
      sig { returns(T::Array[OpenAI::Embedding]) }
      attr_accessor :data

      # The name of the model used to generate the embedding.
      sig { returns(String) }
      attr_accessor :model

      # The object type, which is always "list".
      sig { returns(Symbol) }
      attr_accessor :object

      # The usage information for the request.
      sig { returns(OpenAI::CreateEmbeddingResponse::Usage) }
      attr_reader :usage

      sig { params(usage: OpenAI::CreateEmbeddingResponse::Usage::OrHash).void }
      attr_writer :usage

      sig do
        override
          .returns({
            data: T::Array[OpenAI::Embedding],
            model: String,
            object: Symbol,
            usage: OpenAI::CreateEmbeddingResponse::Usage
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            data: T::Array[OpenAI::Embedding::OrHash],
            model: String,
            usage: OpenAI::CreateEmbeddingResponse::Usage::OrHash,
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          data:, # The list of embeddings generated by the model.
          model:, # The name of the model used to generate the embedding.
          usage:, # The usage information for the request.
          object: :list # The object type, which is always "list".
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::CreateEmbeddingResponse, OpenAI::Internal::AnyHash)
        end

      class Usage < OpenAI::Internal::Type::BaseModel
        # The number of tokens used by the prompt.
        sig { returns(Integer) }
        attr_accessor :prompt_tokens

        # The total number of tokens used by the request.
        sig { returns(Integer) }
        attr_accessor :total_tokens

        sig { override.returns({ prompt_tokens: Integer, total_tokens: Integer }) }
        def to_hash; end

        class << self
          # The usage information for the request.
          sig { params(prompt_tokens: Integer, total_tokens: Integer).returns(T.attached_class) }
          def new(
            prompt_tokens:, # The number of tokens used by the prompt.
            total_tokens: # The total number of tokens used by the request.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::CreateEmbeddingResponse::Usage,
              OpenAI::Internal::AnyHash
            )
          end
      end
    end

    class Embedding < OpenAI::Internal::Type::BaseModel
      # The embedding vector, which is a list of floats. The length of vector depends on
      # the model as listed in the
      # [embedding guide](https://platform.openai.com/docs/guides/embeddings).
      sig { returns(T::Array[Float]) }
      attr_accessor :embedding

      # The index of the embedding in the list of embeddings.
      sig { returns(Integer) }
      attr_accessor :index

      # The object type, which is always "embedding".
      sig { returns(Symbol) }
      attr_accessor :object

      sig { override.returns({ embedding: T::Array[Float], index: Integer, object: Symbol }) }
      def to_hash; end

      class << self
        # Represents an embedding vector returned by embedding endpoint.
        sig { params(embedding: T::Array[Float], index: Integer, object: Symbol).returns(T.attached_class) }
        def new(
          embedding:, # The embedding vector, which is a list of floats. The length of vector depends on
                      # the model as listed in the
                      # [embedding guide](https://platform.openai.com/docs/guides/embeddings).
          index:, # The index of the embedding in the list of embeddings.
          object: :embedding # The object type, which is always "embedding".
); end
      end

      OrHash = T.type_alias { T.any(OpenAI::Embedding, OpenAI::Internal::AnyHash) }
    end

    class EmbeddingCreateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # The number of dimensions the resulting output embeddings should have. Only
      # supported in `text-embedding-3` and later models.
      sig { returns(T.nilable(Integer)) }
      attr_reader :dimensions

      sig { params(dimensions: Integer).void }
      attr_writer :dimensions

      # The format to return the embeddings in. Can be either `float` or
      # [`base64`](https://pypi.org/project/pybase64/).
      sig { returns(T.nilable(OpenAI::EmbeddingCreateParams::EncodingFormat::OrSymbol)) }
      attr_reader :encoding_format

      sig { params(encoding_format: OpenAI::EmbeddingCreateParams::EncodingFormat::OrSymbol).void }
      attr_writer :encoding_format

      # Input text to embed, encoded as a string or array of tokens. To embed multiple
      # inputs in a single request, pass an array of strings or array of token arrays.
      # The input must not exceed the max input tokens for the model (8192 tokens for
      # all embedding models), cannot be an empty string, and any array must be 2048
      # dimensions or less.
      # [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
      # for counting tokens. In addition to the per-input token limit, all embedding
      # models enforce a maximum of 300,000 tokens summed across all inputs in a single
      # request.
      sig { returns(OpenAI::EmbeddingCreateParams::Input::Variants) }
      attr_accessor :input

      # ID of the model to use. You can use the
      # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
      # see all of your available models, or see our
      # [Model overview](https://platform.openai.com/docs/models) for descriptions of
      # them.
      sig { returns(T.any(String, OpenAI::EmbeddingModel::OrSymbol)) }
      attr_accessor :model

      # A unique identifier representing your end-user, which can help OpenAI to monitor
      # and detect abuse.
      # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
      sig { returns(T.nilable(String)) }
      attr_reader :user

      sig { params(user: String).void }
      attr_writer :user

      sig do
        override
          .returns({
            input: OpenAI::EmbeddingCreateParams::Input::Variants,
            model: T.any(String, OpenAI::EmbeddingModel::OrSymbol),
            dimensions: Integer,
            encoding_format:
              OpenAI::EmbeddingCreateParams::EncodingFormat::OrSymbol,
            user: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            input: OpenAI::EmbeddingCreateParams::Input::Variants,
            model: T.any(String, OpenAI::EmbeddingModel::OrSymbol),
            dimensions: Integer,
            encoding_format: OpenAI::EmbeddingCreateParams::EncodingFormat::OrSymbol,
            user: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          input:, # Input text to embed, encoded as a string or array of tokens. To embed multiple
                  # inputs in a single request, pass an array of strings or array of token arrays.
                  # The input must not exceed the max input tokens for the model (8192 tokens for
                  # all embedding models), cannot be an empty string, and any array must be 2048
                  # dimensions or less.
                  # [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
                  # for counting tokens. In addition to the per-input token limit, all embedding
                  # models enforce a maximum of 300,000 tokens summed across all inputs in a single
                  # request.
          model:, # ID of the model to use. You can use the
                  # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                  # see all of your available models, or see our
                  # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                  # them.
          dimensions: nil, # The number of dimensions the resulting output embeddings should have. Only
                           # supported in `text-embedding-3` and later models.
          encoding_format: nil, # The format to return the embeddings in. Can be either `float` or
                                # [`base64`](https://pypi.org/project/pybase64/).
          user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                     # and detect abuse.
                     # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
          request_options: {}
); end
      end

      # The format to return the embeddings in. Can be either `float` or
      # [`base64`](https://pypi.org/project/pybase64/).
      module EncodingFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::EmbeddingCreateParams::EncodingFormat::TaggedSymbol
            ])
          end
          def values; end
        end

        BASE64 = T.let(
            :base64,
            OpenAI::EmbeddingCreateParams::EncodingFormat::TaggedSymbol
          )

        FLOAT = T.let(
            :float,
            OpenAI::EmbeddingCreateParams::EncodingFormat::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::EmbeddingCreateParams::EncodingFormat)
          end
      end

      # Input text to embed, encoded as a string or array of tokens. To embed multiple
      # inputs in a single request, pass an array of strings or array of token arrays.
      # The input must not exceed the max input tokens for the model (8192 tokens for
      # all embedding models), cannot be an empty string, and any array must be 2048
      # dimensions or less.
      # [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
      # for counting tokens. In addition to the per-input token limit, all embedding
      # models enforce a maximum of 300,000 tokens summed across all inputs in a single
      # request.
      module Input
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::EmbeddingCreateParams::Input::Variants]) }
          def variants; end
        end

        ArrayOfToken2DArray = T.let(
            OpenAI::Internal::Type::ArrayOf[
              OpenAI::Internal::Type::ArrayOf[Integer]
            ],
            OpenAI::Internal::Type::Converter
          )

        IntegerArray = T.let(
            OpenAI::Internal::Type::ArrayOf[Integer],
            OpenAI::Internal::Type::Converter
          )

        StringArray = T.let(
            OpenAI::Internal::Type::ArrayOf[String],
            OpenAI::Internal::Type::Converter
          )

        Variants = T.type_alias do
            T.any(
              String,
              T::Array[String],
              T::Array[Integer],
              T::Array[T::Array[Integer]]
            )
          end
      end

      # ID of the model to use. You can use the
      # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
      # see all of your available models, or see our
      # [Model overview](https://platform.openai.com/docs/models) for descriptions of
      # them.
      module Model
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::EmbeddingCreateParams::Model::Variants]) }
          def variants; end
        end

        Variants = T.type_alias { T.any(String, OpenAI::EmbeddingModel::TaggedSymbol) }
      end

      OrHash = T.type_alias do
          T.any(OpenAI::EmbeddingCreateParams, OpenAI::Internal::AnyHash)
        end
    end

    module EmbeddingModel
      extend OpenAI::Internal::Type::Enum

      class << self
        sig { override.returns(T::Array[OpenAI::EmbeddingModel::TaggedSymbol]) }
        def values; end
      end

      OrSymbol = T.type_alias { T.any(Symbol, String) }

      TEXT_EMBEDDING_3_LARGE = T.let(:"text-embedding-3-large", OpenAI::EmbeddingModel::TaggedSymbol)

      TEXT_EMBEDDING_3_SMALL = T.let(:"text-embedding-3-small", OpenAI::EmbeddingModel::TaggedSymbol)

      TEXT_EMBEDDING_ADA_002 = T.let(:"text-embedding-ada-002", OpenAI::EmbeddingModel::TaggedSymbol)

      TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::EmbeddingModel) }
    end

    class ErrorObject < OpenAI::Internal::Type::BaseModel
      sig { returns(T.nilable(String)) }
      attr_accessor :code

      sig { returns(String) }
      attr_accessor :message

      sig { returns(T.nilable(String)) }
      attr_accessor :param

      sig { returns(String) }
      attr_accessor :type

      sig do
        override
          .returns({
            code: T.nilable(String),
            message: String,
            param: T.nilable(String),
            type: String
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            code: T.nilable(String),
            message: String,
            param: T.nilable(String),
            type: String
          ).returns(T.attached_class)
        end
        def new(code:, message:, param:, type:); end
      end

      OrHash = T.type_alias { T.any(OpenAI::ErrorObject, OpenAI::Internal::AnyHash) }
    end

    EvalAPIError = Evals::EvalAPIError

    class EvalCreateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # The configuration for the data source used for the evaluation runs. Dictates the
      # schema of the data used in the evaluation.
      sig do
        returns(T.any(
            OpenAI::EvalCreateParams::DataSourceConfig::Custom,
            OpenAI::EvalCreateParams::DataSourceConfig::Logs,
            OpenAI::EvalCreateParams::DataSourceConfig::StoredCompletions
          ))
      end
      attr_accessor :data_source_config

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The name of the evaluation.
      sig { returns(T.nilable(String)) }
      attr_reader :name

      sig { params(name: String).void }
      attr_writer :name

      # A list of graders for all eval runs in this group. Graders can reference
      # variables in the data source using double curly braces notation, like
      # `{{item.variable_name}}`. To reference the model's output, use the `sample`
      # namespace (ie, `{{sample.output_text}}`).
      sig do
        returns(T::Array[
            T.any(
              OpenAI::EvalCreateParams::TestingCriterion::LabelModel,
              OpenAI::Graders::StringCheckGrader,
              OpenAI::EvalCreateParams::TestingCriterion::TextSimilarity,
              OpenAI::EvalCreateParams::TestingCriterion::Python,
              OpenAI::EvalCreateParams::TestingCriterion::ScoreModel
            )
          ])
      end
      attr_accessor :testing_criteria

      sig do
        override
          .returns({
            data_source_config:
              T.any(
                OpenAI::EvalCreateParams::DataSourceConfig::Custom,
                OpenAI::EvalCreateParams::DataSourceConfig::Logs,
                OpenAI::EvalCreateParams::DataSourceConfig::StoredCompletions
              ),
            testing_criteria:
              T::Array[
                T.any(
                  OpenAI::EvalCreateParams::TestingCriterion::LabelModel,
                  OpenAI::Graders::StringCheckGrader,
                  OpenAI::EvalCreateParams::TestingCriterion::TextSimilarity,
                  OpenAI::EvalCreateParams::TestingCriterion::Python,
                  OpenAI::EvalCreateParams::TestingCriterion::ScoreModel
                )
              ],
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            data_source_config: T.any(
              OpenAI::EvalCreateParams::DataSourceConfig::Custom::OrHash,
              OpenAI::EvalCreateParams::DataSourceConfig::Logs::OrHash,
              OpenAI::EvalCreateParams::DataSourceConfig::StoredCompletions::OrHash
            ),
            testing_criteria: T::Array[
              T.any(
                OpenAI::EvalCreateParams::TestingCriterion::LabelModel::OrHash,
                OpenAI::Graders::StringCheckGrader::OrHash,
                OpenAI::EvalCreateParams::TestingCriterion::TextSimilarity::OrHash,
                OpenAI::EvalCreateParams::TestingCriterion::Python::OrHash,
                OpenAI::EvalCreateParams::TestingCriterion::ScoreModel::OrHash
              )
            ],
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          data_source_config:, # The configuration for the data source used for the evaluation runs. Dictates the
                               # schema of the data used in the evaluation.
          testing_criteria:, # A list of graders for all eval runs in this group. Graders can reference
                             # variables in the data source using double curly braces notation, like
                             # `{{item.variable_name}}`. To reference the model's output, use the `sample`
                             # namespace (ie, `{{sample.output_text}}`).
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          name: nil, # The name of the evaluation.
          request_options: {}
); end
      end

      # The configuration for the data source used for the evaluation runs. Dictates the
      # schema of the data used in the evaluation.
      module DataSourceConfig
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::EvalCreateParams::DataSourceConfig::Variants]) }
          def variants; end
        end

        class Custom < OpenAI::Internal::Type::BaseModel
          # Whether the eval should expect you to populate the sample namespace (ie, by
          # generating responses off of your data source)
          sig { returns(T.nilable(T::Boolean)) }
          attr_reader :include_sample_schema

          sig { params(include_sample_schema: T::Boolean).void }
          attr_writer :include_sample_schema

          # The json schema for each row in the data source.
          sig { returns(T::Hash[Symbol, T.anything]) }
          attr_accessor :item_schema

          # The type of data source. Always `custom`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                item_schema: T::Hash[Symbol, T.anything],
                type: Symbol,
                include_sample_schema: T::Boolean
              })
          end
          def to_hash; end

          class << self
            # A CustomDataSourceConfig object that defines the schema for the data source used
            # for the evaluation runs. This schema is used to define the shape of the data
            # that will be:
            #
            # - Used to define your testing criteria and
            # - What data is required when creating a run
            sig do
              params(
                item_schema: T::Hash[Symbol, T.anything],
                include_sample_schema: T::Boolean,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              item_schema:, # The json schema for each row in the data source.
              include_sample_schema: nil, # Whether the eval should expect you to populate the sample namespace (ie, by
                                          # generating responses off of your data source)
              type: :custom # The type of data source. Always `custom`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::EvalCreateParams::DataSourceConfig::Custom,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class Logs < OpenAI::Internal::Type::BaseModel
          # Metadata filters for the logs data source.
          sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
          attr_reader :metadata

          sig { params(metadata: T::Hash[Symbol, T.anything]).void }
          attr_writer :metadata

          # The type of data source. Always `logs`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ type: Symbol, metadata: T::Hash[Symbol, T.anything] }) }
          def to_hash; end

          class << self
            # A data source config which specifies the metadata property of your logs query.
            # This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc.
            sig { params(metadata: T::Hash[Symbol, T.anything], type: Symbol).returns(T.attached_class) }
            def new(
              metadata: nil, # Metadata filters for the logs data source.
              type: :logs # The type of data source. Always `logs`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::EvalCreateParams::DataSourceConfig::Logs,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class StoredCompletions < OpenAI::Internal::Type::BaseModel
          # Metadata filters for the stored completions data source.
          sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
          attr_reader :metadata

          sig { params(metadata: T::Hash[Symbol, T.anything]).void }
          attr_writer :metadata

          # The type of data source. Always `stored_completions`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ type: Symbol, metadata: T::Hash[Symbol, T.anything] }) }
          def to_hash; end

          class << self
            # Deprecated in favor of LogsDataSourceConfig.
            sig { params(metadata: T::Hash[Symbol, T.anything], type: Symbol).returns(T.attached_class) }
            def new(
              metadata: nil, # Metadata filters for the stored completions data source.
              type: :stored_completions # The type of data source. Always `stored_completions`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::EvalCreateParams::DataSourceConfig::StoredCompletions,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::EvalCreateParams::DataSourceConfig::Custom,
              OpenAI::EvalCreateParams::DataSourceConfig::Logs,
              OpenAI::EvalCreateParams::DataSourceConfig::StoredCompletions
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::EvalCreateParams, OpenAI::Internal::AnyHash)
        end

      # A LabelModelGrader object which uses a model to assign labels to each item in
      # the evaluation.
      module TestingCriterion
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::EvalCreateParams::TestingCriterion::Variants]) }
          def variants; end
        end

        class LabelModel < OpenAI::Internal::Type::BaseModel
          # A list of chat messages forming the prompt or context. May include variable
          # references to the `item` namespace, ie {{item.name}}.
          sig do
            returns(T::Array[
                T.any(
                  OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::SimpleInputMessage,
                  OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem
                )
              ])
          end
          attr_accessor :input

          # The labels to classify to each item in the evaluation.
          sig { returns(T::Array[String]) }
          attr_accessor :labels

          # The model to use for the evaluation. Must support structured outputs.
          sig { returns(String) }
          attr_accessor :model

          # The name of the grader.
          sig { returns(String) }
          attr_accessor :name

          # The labels that indicate a passing result. Must be a subset of labels.
          sig { returns(T::Array[String]) }
          attr_accessor :passing_labels

          # The object type, which is always `label_model`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                input:
                  T::Array[
                    T.any(
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::SimpleInputMessage,
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem
                    )
                  ],
                labels: T::Array[String],
                model: String,
                name: String,
                passing_labels: T::Array[String],
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A LabelModelGrader object which uses a model to assign labels to each item in
            # the evaluation.
            sig do
              params(
                input: T::Array[
                  T.any(
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::SimpleInputMessage::OrHash,
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::OrHash
                  )
                ],
                labels: T::Array[String],
                model: String,
                name: String,
                passing_labels: T::Array[String],
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              input:, # A list of chat messages forming the prompt or context. May include variable
                      # references to the `item` namespace, ie {{item.name}}.
              labels:, # The labels to classify to each item in the evaluation.
              model:, # The model to use for the evaluation. Must support structured outputs.
              name:, # The name of the grader.
              passing_labels:, # The labels that indicate a passing result. Must be a subset of labels.
              type: :label_model # The object type, which is always `label_model`.
); end
          end

          # A chat message that makes up the prompt or context. May include variable
          # references to the `item` namespace, ie {{item.name}}.
          module Input
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::Variants
                ])
              end
              def variants; end
            end

            class EvalItem < OpenAI::Internal::Type::BaseModel
              # Inputs to the model - can contain template strings.
              sig do
                returns(T.any(
                    String,
                    OpenAI::Responses::ResponseInputText,
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::OutputText,
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::InputImage,
                    T::Array[T.anything]
                  ))
              end
              attr_accessor :content

              # The role of the message input. One of `user`, `assistant`, `system`, or
              # `developer`.
              sig { returns(OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Role::OrSymbol) }
              attr_accessor :role

              # The type of the message input. Always `message`.
              sig do
                returns(T.nilable(
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Type::OrSymbol
                  ))
              end
              attr_reader :type

              sig do
                params(
                  type: OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Type::OrSymbol
                ).void
              end
              attr_writer :type

              sig do
                override
                  .returns({
                    content:
                      T.any(
                        String,
                        OpenAI::Responses::ResponseInputText,
                        OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::OutputText,
                        OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::InputImage,
                        T::Array[T.anything]
                      ),
                    role:
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Role::OrSymbol,
                    type:
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Type::OrSymbol
                  })
              end
              def to_hash; end

              class << self
                # A message input to the model with a role indicating instruction following
                # hierarchy. Instructions given with the `developer` or `system` role take
                # precedence over instructions given with the `user` role. Messages with the
                # `assistant` role are presumed to have been generated by the model in previous
                # interactions.
                sig do
                  params(
                    content: T.any(
                      String,
                      OpenAI::Responses::ResponseInputText::OrHash,
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::OutputText::OrHash,
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::InputImage::OrHash,
                      T::Array[T.anything]
                    ),
                    role: OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Role::OrSymbol,
                    type: OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Type::OrSymbol
                  ).returns(T.attached_class)
                end
                def new(
                  content:, # Inputs to the model - can contain template strings.
                  role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                         # `developer`.
                  type: nil # The type of the message input. Always `message`.
); end
              end

              # Inputs to the model - can contain template strings.
              module Content
                extend OpenAI::Internal::Type::Union

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::Variants
                    ])
                  end
                  def variants; end
                end

                AnArrayOfInputTextAndInputImageArray = T.let(
                    OpenAI::Internal::Type::ArrayOf[
                      OpenAI::Internal::Type::Unknown
                    ],
                    OpenAI::Internal::Type::Converter
                  )

                class InputImage < OpenAI::Internal::Type::BaseModel
                  # The detail level of the image to be sent to the model. One of `high`, `low`, or
                  # `auto`. Defaults to `auto`.
                  sig { returns(T.nilable(String)) }
                  attr_reader :detail

                  sig { params(detail: String).void }
                  attr_writer :detail

                  # The URL of the image input.
                  sig { returns(String) }
                  attr_accessor :image_url

                  # The type of the image input. Always `input_image`.
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig { override.returns({ image_url: String, type: Symbol, detail: String }) }
                  def to_hash; end

                  class << self
                    # An image input to the model.
                    sig { params(image_url: String, detail: String, type: Symbol).returns(T.attached_class) }
                    def new(
                      image_url:, # The URL of the image input.
                      detail: nil, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                                   # `auto`. Defaults to `auto`.
                      type: :input_image # The type of the image input. Always `input_image`.
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::InputImage,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                class OutputText < OpenAI::Internal::Type::BaseModel
                  # The text output from the model.
                  sig { returns(String) }
                  attr_accessor :text

                  # The type of the output text. Always `output_text`.
                  sig { returns(Symbol) }
                  attr_accessor :type

                  sig { override.returns({ text: String, type: Symbol }) }
                  def to_hash; end

                  class << self
                    # A text output from the model.
                    sig { params(text: String, type: Symbol).returns(T.attached_class) }
                    def new(
                      text:, # The text output from the model.
                      type: :output_text # The type of the output text. Always `output_text`.
); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::OutputText,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                Variants = T.type_alias do
                    T.any(
                      String,
                      OpenAI::Responses::ResponseInputText,
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::OutputText,
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Content::InputImage,
                      T::Array[T.anything]
                    )
                  end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem,
                    OpenAI::Internal::AnyHash
                  )
                end

              # The role of the message input. One of `user`, `assistant`, `system`, or
              # `developer`.
              module Role
                extend OpenAI::Internal::Type::Enum

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Role::TaggedSymbol
                    ])
                  end
                  def values; end
                end

                ASSISTANT = T.let(
                    :assistant,
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Role::TaggedSymbol
                  )

                DEVELOPER = T.let(
                    :developer,
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Role::TaggedSymbol
                  )

                OrSymbol = T.type_alias { T.any(Symbol, String) }

                SYSTEM = T.let(
                    :system,
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Role::TaggedSymbol
                  )

                TaggedSymbol = T.type_alias do
                    T.all(
                      Symbol,
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Role
                    )
                  end

                USER = T.let(
                    :user,
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Role::TaggedSymbol
                  )
              end

              # The type of the message input. Always `message`.
              module Type
                extend OpenAI::Internal::Type::Enum

                class << self
                  sig do
                    override
                      .returns(T::Array[
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Type::TaggedSymbol
                    ])
                  end
                  def values; end
                end

                MESSAGE = T.let(
                    :message,
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Type::TaggedSymbol
                  )

                OrSymbol = T.type_alias { T.any(Symbol, String) }

                TaggedSymbol = T.type_alias do
                    T.all(
                      Symbol,
                      OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem::Type
                    )
                  end
              end
            end

            class SimpleInputMessage < OpenAI::Internal::Type::BaseModel
              # The content of the message.
              sig { returns(String) }
              attr_accessor :content

              # The role of the message (e.g. "system", "assistant", "user").
              sig { returns(String) }
              attr_accessor :role

              sig { override.returns({ content: String, role: String }) }
              def to_hash; end

              class << self
                sig { params(content: String, role: String).returns(T.attached_class) }
                def new(
                  content:, # The content of the message.
                  role: # The role of the message (e.g. "system", "assistant", "user").
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::SimpleInputMessage,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            Variants = T.type_alias do
                T.any(
                  OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::SimpleInputMessage,
                  OpenAI::EvalCreateParams::TestingCriterion::LabelModel::Input::EvalItem
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::EvalCreateParams::TestingCriterion::LabelModel,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class Python < OpenAI::Models::Graders::PythonGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A PythonGrader object that runs a python script on the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::EvalCreateParams::TestingCriterion::Python,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ScoreModel < OpenAI::Models::Graders::ScoreModelGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A ScoreModelGrader object that uses a model to assign a score to the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::EvalCreateParams::TestingCriterion::ScoreModel,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class TextSimilarity < OpenAI::Models::Graders::TextSimilarityGrader
          # The threshold for the score.
          sig { returns(Float) }
          attr_accessor :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A TextSimilarityGrader object which grades text based on similarity metrics.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::EvalCreateParams::TestingCriterion::TextSimilarity,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::EvalCreateParams::TestingCriterion::LabelModel,
              OpenAI::Graders::StringCheckGrader,
              OpenAI::EvalCreateParams::TestingCriterion::TextSimilarity,
              OpenAI::EvalCreateParams::TestingCriterion::Python,
              OpenAI::EvalCreateParams::TestingCriterion::ScoreModel
            )
          end
      end
    end

    class EvalCreateResponse < OpenAI::Internal::Type::BaseModel
      # The Unix timestamp (in seconds) for when the eval was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # Configuration of data sources used in runs of the evaluation.
      sig { returns(OpenAI::Models::EvalCreateResponse::DataSourceConfig::Variants) }
      attr_accessor :data_source_config

      # Unique identifier for the evaluation.
      sig { returns(String) }
      attr_accessor :id

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The name of the evaluation.
      sig { returns(String) }
      attr_accessor :name

      # The object type.
      sig { returns(Symbol) }
      attr_accessor :object

      # A list of testing criteria.
      sig { returns(T::Array[
            OpenAI::Models::EvalCreateResponse::TestingCriterion::Variants
          ]) }
      attr_accessor :testing_criteria

      sig do
        override
          .returns({
            id: String,
            created_at: Integer,
            data_source_config:
              OpenAI::Models::EvalCreateResponse::DataSourceConfig::Variants,
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            object: Symbol,
            testing_criteria:
              T::Array[
                OpenAI::Models::EvalCreateResponse::TestingCriterion::Variants
              ]
          })
      end
      def to_hash; end

      class << self
        # An Eval object with a data source config and testing criteria. An Eval
        # represents a task to be done for your LLM integration. Like:
        #
        # - Improve the quality of my chatbot
        # - See how well my chatbot handles customer support
        # - Check if o4-mini is better at my usecase than gpt-4o
        sig do
          params(
            id: String,
            created_at: Integer,
            data_source_config: T.any(
              OpenAI::EvalCustomDataSourceConfig::OrHash,
              OpenAI::Models::EvalCreateResponse::DataSourceConfig::Logs::OrHash,
              OpenAI::EvalStoredCompletionsDataSourceConfig::OrHash
            ),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            testing_criteria: T::Array[
              T.any(
                OpenAI::Models::Graders::LabelModelGrader::OrHash,
                OpenAI::Models::Graders::StringCheckGrader::OrHash,
                OpenAI::Models::EvalCreateResponse::TestingCriterion::EvalGraderTextSimilarity::OrHash,
                OpenAI::Models::EvalCreateResponse::TestingCriterion::EvalGraderPython::OrHash,
                OpenAI::Models::EvalCreateResponse::TestingCriterion::EvalGraderScoreModel::OrHash
              )
            ],
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          id:, # Unique identifier for the evaluation.
          created_at:, # The Unix timestamp (in seconds) for when the eval was created.
          data_source_config:, # Configuration of data sources used in runs of the evaluation.
          metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                     # for storing additional information about the object in a structured format, and
                     # querying for objects via API or the dashboard.
                     # Keys are strings with a maximum length of 64 characters. Values are strings with
                     # a maximum length of 512 characters.
          name:, # The name of the evaluation.
          testing_criteria:, # A list of testing criteria.
          object: :eval # The object type.
); end
      end

      # Configuration of data sources used in runs of the evaluation.
      module DataSourceConfig
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::EvalCreateResponse::DataSourceConfig::Variants
            ])
          end
          def variants; end
        end

        class Logs < OpenAI::Internal::Type::BaseModel
          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # The json schema for the run data source items. Learn how to build JSON schemas
          # [here](https://json-schema.org/).
          sig { returns(T::Hash[Symbol, T.anything]) }
          attr_accessor :schema

          # The type of data source. Always `logs`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                schema: T::Hash[Symbol, T.anything],
                type: Symbol,
                metadata: T.nilable(T::Hash[Symbol, String])
              })
          end
          def to_hash; end

          class << self
            # A LogsDataSourceConfig which specifies the metadata property of your logs query.
            # This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc. The
            # schema returned by this data source config is used to defined what variables are
            # available in your evals. `item` and `sample` are both defined when using this
            # data source config.
            sig do
              params(
                schema: T::Hash[Symbol, T.anything],
                metadata: T.nilable(T::Hash[Symbol, String]),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              schema:, # The json schema for the run data source items. Learn how to build JSON schemas
                       # [here](https://json-schema.org/).
              metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard.
                             # Keys are strings with a maximum length of 64 characters. Values are strings with
                             # a maximum length of 512 characters.
              type: :logs # The type of data source. Always `logs`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalCreateResponse::DataSourceConfig::Logs,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::EvalCustomDataSourceConfig,
              OpenAI::Models::EvalCreateResponse::DataSourceConfig::Logs,
              OpenAI::EvalStoredCompletionsDataSourceConfig
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::Models::EvalCreateResponse, OpenAI::Internal::AnyHash)
        end

      # A LabelModelGrader object which uses a model to assign labels to each item in
      # the evaluation.
      module TestingCriterion
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::EvalCreateResponse::TestingCriterion::Variants
            ])
          end
          def variants; end
        end

        class EvalGraderPython < OpenAI::Models::Graders::PythonGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A PythonGrader object that runs a python script on the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalCreateResponse::TestingCriterion::EvalGraderPython,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class EvalGraderScoreModel < OpenAI::Models::Graders::ScoreModelGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A ScoreModelGrader object that uses a model to assign a score to the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalCreateResponse::TestingCriterion::EvalGraderScoreModel,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class EvalGraderTextSimilarity < OpenAI::Models::Graders::TextSimilarityGrader
          # The threshold for the score.
          sig { returns(Float) }
          attr_accessor :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A TextSimilarityGrader object which grades text based on similarity metrics.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalCreateResponse::TestingCriterion::EvalGraderTextSimilarity,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Models::Graders::LabelModelGrader,
              OpenAI::Models::Graders::StringCheckGrader,
              OpenAI::Models::EvalCreateResponse::TestingCriterion::EvalGraderTextSimilarity,
              OpenAI::Models::EvalCreateResponse::TestingCriterion::EvalGraderPython,
              OpenAI::Models::EvalCreateResponse::TestingCriterion::EvalGraderScoreModel
            )
          end
      end
    end

    class EvalCustomDataSourceConfig < OpenAI::Internal::Type::BaseModel
      # The json schema for the run data source items. Learn how to build JSON schemas
      # [here](https://json-schema.org/).
      sig { returns(T::Hash[Symbol, T.anything]) }
      attr_accessor :schema

      # The type of data source. Always `custom`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig { override.returns({ schema: T::Hash[Symbol, T.anything], type: Symbol }) }
      def to_hash; end

      class << self
        # A CustomDataSourceConfig which specifies the schema of your `item` and
        # optionally `sample` namespaces. The response schema defines the shape of the
        # data that will be:
        #
        # - Used to define your testing criteria and
        # - What data is required when creating a run
        sig { params(schema: T::Hash[Symbol, T.anything], type: Symbol).returns(T.attached_class) }
        def new(
          schema:, # The json schema for the run data source items. Learn how to build JSON schemas
                   # [here](https://json-schema.org/).
          type: :custom # The type of data source. Always `custom`.
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::EvalCustomDataSourceConfig, OpenAI::Internal::AnyHash)
        end
    end

    class EvalDeleteParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::EvalDeleteParams, OpenAI::Internal::AnyHash)
        end
    end

    class EvalDeleteResponse < OpenAI::Internal::Type::BaseModel
      sig { returns(T::Boolean) }
      attr_accessor :deleted

      sig { returns(String) }
      attr_accessor :eval_id

      sig { returns(String) }
      attr_accessor :object

      sig { override.returns({ deleted: T::Boolean, eval_id: String, object: String }) }
      def to_hash; end

      class << self
        sig { params(deleted: T::Boolean, eval_id: String, object: String).returns(T.attached_class) }
        def new(deleted:, eval_id:, object:); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::Models::EvalDeleteResponse, OpenAI::Internal::AnyHash)
        end
    end

    class EvalListParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # Identifier for the last eval from the previous pagination request.
      sig { returns(T.nilable(String)) }
      attr_reader :after

      sig { params(after: String).void }
      attr_writer :after

      # Number of evals to retrieve.
      sig { returns(T.nilable(Integer)) }
      attr_reader :limit

      sig { params(limit: Integer).void }
      attr_writer :limit

      # Sort order for evals by timestamp. Use `asc` for ascending order or `desc` for
      # descending order.
      sig { returns(T.nilable(OpenAI::EvalListParams::Order::OrSymbol)) }
      attr_reader :order

      sig { params(order: OpenAI::EvalListParams::Order::OrSymbol).void }
      attr_writer :order

      # Evals can be ordered by creation time or last updated time. Use `created_at` for
      # creation time or `updated_at` for last updated time.
      sig { returns(T.nilable(OpenAI::EvalListParams::OrderBy::OrSymbol)) }
      attr_reader :order_by

      sig { params(order_by: OpenAI::EvalListParams::OrderBy::OrSymbol).void }
      attr_writer :order_by

      sig do
        override
          .returns({
            after: String,
            limit: Integer,
            order: OpenAI::EvalListParams::Order::OrSymbol,
            order_by: OpenAI::EvalListParams::OrderBy::OrSymbol,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            after: String,
            limit: Integer,
            order: OpenAI::EvalListParams::Order::OrSymbol,
            order_by: OpenAI::EvalListParams::OrderBy::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          after: nil, # Identifier for the last eval from the previous pagination request.
          limit: nil, # Number of evals to retrieve.
          order: nil, # Sort order for evals by timestamp. Use `asc` for ascending order or `desc` for
                      # descending order.
          order_by: nil, # Evals can be ordered by creation time or last updated time. Use `created_at` for
                         # creation time or `updated_at` for last updated time.
          request_options: {}
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::EvalListParams, OpenAI::Internal::AnyHash)
        end

      # Sort order for evals by timestamp. Use `asc` for ascending order or `desc` for
      # descending order.
      module Order
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::EvalListParams::Order::TaggedSymbol]) }
          def values; end
        end

        ASC = T.let(:asc, OpenAI::EvalListParams::Order::TaggedSymbol)
        DESC = T.let(:desc, OpenAI::EvalListParams::Order::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::EvalListParams::Order) }
      end

      # Evals can be ordered by creation time or last updated time. Use `created_at` for
      # creation time or `updated_at` for last updated time.
      module OrderBy
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::EvalListParams::OrderBy::TaggedSymbol]) }
          def values; end
        end

        CREATED_AT = T.let(:created_at, OpenAI::EvalListParams::OrderBy::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::EvalListParams::OrderBy) }

        UPDATED_AT = T.let(:updated_at, OpenAI::EvalListParams::OrderBy::TaggedSymbol)
      end
    end

    class EvalListResponse < OpenAI::Internal::Type::BaseModel
      # The Unix timestamp (in seconds) for when the eval was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # Configuration of data sources used in runs of the evaluation.
      sig { returns(OpenAI::Models::EvalListResponse::DataSourceConfig::Variants) }
      attr_accessor :data_source_config

      # Unique identifier for the evaluation.
      sig { returns(String) }
      attr_accessor :id

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The name of the evaluation.
      sig { returns(String) }
      attr_accessor :name

      # The object type.
      sig { returns(Symbol) }
      attr_accessor :object

      # A list of testing criteria.
      sig { returns(T::Array[OpenAI::Models::EvalListResponse::TestingCriterion::Variants]) }
      attr_accessor :testing_criteria

      sig do
        override
          .returns({
            id: String,
            created_at: Integer,
            data_source_config:
              OpenAI::Models::EvalListResponse::DataSourceConfig::Variants,
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            object: Symbol,
            testing_criteria:
              T::Array[
                OpenAI::Models::EvalListResponse::TestingCriterion::Variants
              ]
          })
      end
      def to_hash; end

      class << self
        # An Eval object with a data source config and testing criteria. An Eval
        # represents a task to be done for your LLM integration. Like:
        #
        # - Improve the quality of my chatbot
        # - See how well my chatbot handles customer support
        # - Check if o4-mini is better at my usecase than gpt-4o
        sig do
          params(
            id: String,
            created_at: Integer,
            data_source_config: T.any(
              OpenAI::EvalCustomDataSourceConfig::OrHash,
              OpenAI::Models::EvalListResponse::DataSourceConfig::Logs::OrHash,
              OpenAI::EvalStoredCompletionsDataSourceConfig::OrHash
            ),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            testing_criteria: T::Array[
              T.any(
                OpenAI::Models::Graders::LabelModelGrader::OrHash,
                OpenAI::Models::Graders::StringCheckGrader::OrHash,
                OpenAI::Models::EvalListResponse::TestingCriterion::EvalGraderTextSimilarity::OrHash,
                OpenAI::Models::EvalListResponse::TestingCriterion::EvalGraderPython::OrHash,
                OpenAI::Models::EvalListResponse::TestingCriterion::EvalGraderScoreModel::OrHash
              )
            ],
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          id:, # Unique identifier for the evaluation.
          created_at:, # The Unix timestamp (in seconds) for when the eval was created.
          data_source_config:, # Configuration of data sources used in runs of the evaluation.
          metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                     # for storing additional information about the object in a structured format, and
                     # querying for objects via API or the dashboard.
                     # Keys are strings with a maximum length of 64 characters. Values are strings with
                     # a maximum length of 512 characters.
          name:, # The name of the evaluation.
          testing_criteria:, # A list of testing criteria.
          object: :eval # The object type.
); end
      end

      # Configuration of data sources used in runs of the evaluation.
      module DataSourceConfig
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::EvalListResponse::DataSourceConfig::Variants
            ])
          end
          def variants; end
        end

        class Logs < OpenAI::Internal::Type::BaseModel
          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # The json schema for the run data source items. Learn how to build JSON schemas
          # [here](https://json-schema.org/).
          sig { returns(T::Hash[Symbol, T.anything]) }
          attr_accessor :schema

          # The type of data source. Always `logs`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                schema: T::Hash[Symbol, T.anything],
                type: Symbol,
                metadata: T.nilable(T::Hash[Symbol, String])
              })
          end
          def to_hash; end

          class << self
            # A LogsDataSourceConfig which specifies the metadata property of your logs query.
            # This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc. The
            # schema returned by this data source config is used to defined what variables are
            # available in your evals. `item` and `sample` are both defined when using this
            # data source config.
            sig do
              params(
                schema: T::Hash[Symbol, T.anything],
                metadata: T.nilable(T::Hash[Symbol, String]),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              schema:, # The json schema for the run data source items. Learn how to build JSON schemas
                       # [here](https://json-schema.org/).
              metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard.
                             # Keys are strings with a maximum length of 64 characters. Values are strings with
                             # a maximum length of 512 characters.
              type: :logs # The type of data source. Always `logs`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalListResponse::DataSourceConfig::Logs,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::EvalCustomDataSourceConfig,
              OpenAI::Models::EvalListResponse::DataSourceConfig::Logs,
              OpenAI::EvalStoredCompletionsDataSourceConfig
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::Models::EvalListResponse, OpenAI::Internal::AnyHash)
        end

      # A LabelModelGrader object which uses a model to assign labels to each item in
      # the evaluation.
      module TestingCriterion
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::EvalListResponse::TestingCriterion::Variants
            ])
          end
          def variants; end
        end

        class EvalGraderPython < OpenAI::Models::Graders::PythonGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A PythonGrader object that runs a python script on the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalListResponse::TestingCriterion::EvalGraderPython,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class EvalGraderScoreModel < OpenAI::Models::Graders::ScoreModelGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A ScoreModelGrader object that uses a model to assign a score to the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalListResponse::TestingCriterion::EvalGraderScoreModel,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class EvalGraderTextSimilarity < OpenAI::Models::Graders::TextSimilarityGrader
          # The threshold for the score.
          sig { returns(Float) }
          attr_accessor :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A TextSimilarityGrader object which grades text based on similarity metrics.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalListResponse::TestingCriterion::EvalGraderTextSimilarity,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Models::Graders::LabelModelGrader,
              OpenAI::Models::Graders::StringCheckGrader,
              OpenAI::Models::EvalListResponse::TestingCriterion::EvalGraderTextSimilarity,
              OpenAI::Models::EvalListResponse::TestingCriterion::EvalGraderPython,
              OpenAI::Models::EvalListResponse::TestingCriterion::EvalGraderScoreModel
            )
          end
      end
    end

    class EvalRetrieveParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::EvalRetrieveParams, OpenAI::Internal::AnyHash)
        end
    end

    class EvalRetrieveResponse < OpenAI::Internal::Type::BaseModel
      # The Unix timestamp (in seconds) for when the eval was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # Configuration of data sources used in runs of the evaluation.
      sig { returns(OpenAI::Models::EvalRetrieveResponse::DataSourceConfig::Variants) }
      attr_accessor :data_source_config

      # Unique identifier for the evaluation.
      sig { returns(String) }
      attr_accessor :id

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The name of the evaluation.
      sig { returns(String) }
      attr_accessor :name

      # The object type.
      sig { returns(Symbol) }
      attr_accessor :object

      # A list of testing criteria.
      sig do
        returns(T::Array[
            OpenAI::Models::EvalRetrieveResponse::TestingCriterion::Variants
          ])
      end
      attr_accessor :testing_criteria

      sig do
        override
          .returns({
            id: String,
            created_at: Integer,
            data_source_config:
              OpenAI::Models::EvalRetrieveResponse::DataSourceConfig::Variants,
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            object: Symbol,
            testing_criteria:
              T::Array[
                OpenAI::Models::EvalRetrieveResponse::TestingCriterion::Variants
              ]
          })
      end
      def to_hash; end

      class << self
        # An Eval object with a data source config and testing criteria. An Eval
        # represents a task to be done for your LLM integration. Like:
        #
        # - Improve the quality of my chatbot
        # - See how well my chatbot handles customer support
        # - Check if o4-mini is better at my usecase than gpt-4o
        sig do
          params(
            id: String,
            created_at: Integer,
            data_source_config: T.any(
              OpenAI::EvalCustomDataSourceConfig::OrHash,
              OpenAI::Models::EvalRetrieveResponse::DataSourceConfig::Logs::OrHash,
              OpenAI::EvalStoredCompletionsDataSourceConfig::OrHash
            ),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            testing_criteria: T::Array[
              T.any(
                OpenAI::Models::Graders::LabelModelGrader::OrHash,
                OpenAI::Models::Graders::StringCheckGrader::OrHash,
                OpenAI::Models::EvalRetrieveResponse::TestingCriterion::EvalGraderTextSimilarity::OrHash,
                OpenAI::Models::EvalRetrieveResponse::TestingCriterion::EvalGraderPython::OrHash,
                OpenAI::Models::EvalRetrieveResponse::TestingCriterion::EvalGraderScoreModel::OrHash
              )
            ],
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          id:, # Unique identifier for the evaluation.
          created_at:, # The Unix timestamp (in seconds) for when the eval was created.
          data_source_config:, # Configuration of data sources used in runs of the evaluation.
          metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                     # for storing additional information about the object in a structured format, and
                     # querying for objects via API or the dashboard.
                     # Keys are strings with a maximum length of 64 characters. Values are strings with
                     # a maximum length of 512 characters.
          name:, # The name of the evaluation.
          testing_criteria:, # A list of testing criteria.
          object: :eval # The object type.
); end
      end

      # Configuration of data sources used in runs of the evaluation.
      module DataSourceConfig
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::EvalRetrieveResponse::DataSourceConfig::Variants
            ])
          end
          def variants; end
        end

        class Logs < OpenAI::Internal::Type::BaseModel
          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # The json schema for the run data source items. Learn how to build JSON schemas
          # [here](https://json-schema.org/).
          sig { returns(T::Hash[Symbol, T.anything]) }
          attr_accessor :schema

          # The type of data source. Always `logs`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                schema: T::Hash[Symbol, T.anything],
                type: Symbol,
                metadata: T.nilable(T::Hash[Symbol, String])
              })
          end
          def to_hash; end

          class << self
            # A LogsDataSourceConfig which specifies the metadata property of your logs query.
            # This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc. The
            # schema returned by this data source config is used to defined what variables are
            # available in your evals. `item` and `sample` are both defined when using this
            # data source config.
            sig do
              params(
                schema: T::Hash[Symbol, T.anything],
                metadata: T.nilable(T::Hash[Symbol, String]),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              schema:, # The json schema for the run data source items. Learn how to build JSON schemas
                       # [here](https://json-schema.org/).
              metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard.
                             # Keys are strings with a maximum length of 64 characters. Values are strings with
                             # a maximum length of 512 characters.
              type: :logs # The type of data source. Always `logs`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalRetrieveResponse::DataSourceConfig::Logs,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::EvalCustomDataSourceConfig,
              OpenAI::Models::EvalRetrieveResponse::DataSourceConfig::Logs,
              OpenAI::EvalStoredCompletionsDataSourceConfig
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::Models::EvalRetrieveResponse, OpenAI::Internal::AnyHash)
        end

      # A LabelModelGrader object which uses a model to assign labels to each item in
      # the evaluation.
      module TestingCriterion
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::EvalRetrieveResponse::TestingCriterion::Variants
            ])
          end
          def variants; end
        end

        class EvalGraderPython < OpenAI::Models::Graders::PythonGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A PythonGrader object that runs a python script on the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalRetrieveResponse::TestingCriterion::EvalGraderPython,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class EvalGraderScoreModel < OpenAI::Models::Graders::ScoreModelGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A ScoreModelGrader object that uses a model to assign a score to the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalRetrieveResponse::TestingCriterion::EvalGraderScoreModel,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class EvalGraderTextSimilarity < OpenAI::Models::Graders::TextSimilarityGrader
          # The threshold for the score.
          sig { returns(Float) }
          attr_accessor :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A TextSimilarityGrader object which grades text based on similarity metrics.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalRetrieveResponse::TestingCriterion::EvalGraderTextSimilarity,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Models::Graders::LabelModelGrader,
              OpenAI::Models::Graders::StringCheckGrader,
              OpenAI::Models::EvalRetrieveResponse::TestingCriterion::EvalGraderTextSimilarity,
              OpenAI::Models::EvalRetrieveResponse::TestingCriterion::EvalGraderPython,
              OpenAI::Models::EvalRetrieveResponse::TestingCriterion::EvalGraderScoreModel
            )
          end
      end
    end

    class EvalStoredCompletionsDataSourceConfig < OpenAI::Internal::Type::BaseModel
      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The json schema for the run data source items. Learn how to build JSON schemas
      # [here](https://json-schema.org/).
      sig { returns(T::Hash[Symbol, T.anything]) }
      attr_accessor :schema

      # The type of data source. Always `stored_completions`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig do
        override
          .returns({
            schema: T::Hash[Symbol, T.anything],
            type: Symbol,
            metadata: T.nilable(T::Hash[Symbol, String])
          })
      end
      def to_hash; end

      class << self
        # Deprecated in favor of LogsDataSourceConfig.
        sig do
          params(
            schema: T::Hash[Symbol, T.anything],
            metadata: T.nilable(T::Hash[Symbol, String]),
            type: Symbol
          ).returns(T.attached_class)
        end
        def new(
          schema:, # The json schema for the run data source items. Learn how to build JSON schemas
                   # [here](https://json-schema.org/).
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          type: :stored_completions # The type of data source. Always `stored_completions`.
); end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::EvalStoredCompletionsDataSourceConfig,
            OpenAI::Internal::AnyHash
          )
        end
    end

    class EvalUpdateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # Rename the evaluation.
      sig { returns(T.nilable(String)) }
      attr_reader :name

      sig { params(name: String).void }
      attr_writer :name

      sig do
        override
          .returns({
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          name: nil, # Rename the evaluation.
          request_options: {}
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::EvalUpdateParams, OpenAI::Internal::AnyHash)
        end
    end

    class EvalUpdateResponse < OpenAI::Internal::Type::BaseModel
      # The Unix timestamp (in seconds) for when the eval was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # Configuration of data sources used in runs of the evaluation.
      sig { returns(OpenAI::Models::EvalUpdateResponse::DataSourceConfig::Variants) }
      attr_accessor :data_source_config

      # Unique identifier for the evaluation.
      sig { returns(String) }
      attr_accessor :id

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The name of the evaluation.
      sig { returns(String) }
      attr_accessor :name

      # The object type.
      sig { returns(Symbol) }
      attr_accessor :object

      # A list of testing criteria.
      sig { returns(T::Array[
            OpenAI::Models::EvalUpdateResponse::TestingCriterion::Variants
          ]) }
      attr_accessor :testing_criteria

      sig do
        override
          .returns({
            id: String,
            created_at: Integer,
            data_source_config:
              OpenAI::Models::EvalUpdateResponse::DataSourceConfig::Variants,
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            object: Symbol,
            testing_criteria:
              T::Array[
                OpenAI::Models::EvalUpdateResponse::TestingCriterion::Variants
              ]
          })
      end
      def to_hash; end

      class << self
        # An Eval object with a data source config and testing criteria. An Eval
        # represents a task to be done for your LLM integration. Like:
        #
        # - Improve the quality of my chatbot
        # - See how well my chatbot handles customer support
        # - Check if o4-mini is better at my usecase than gpt-4o
        sig do
          params(
            id: String,
            created_at: Integer,
            data_source_config: T.any(
              OpenAI::EvalCustomDataSourceConfig::OrHash,
              OpenAI::Models::EvalUpdateResponse::DataSourceConfig::Logs::OrHash,
              OpenAI::EvalStoredCompletionsDataSourceConfig::OrHash
            ),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            testing_criteria: T::Array[
              T.any(
                OpenAI::Models::Graders::LabelModelGrader::OrHash,
                OpenAI::Models::Graders::StringCheckGrader::OrHash,
                OpenAI::Models::EvalUpdateResponse::TestingCriterion::EvalGraderTextSimilarity::OrHash,
                OpenAI::Models::EvalUpdateResponse::TestingCriterion::EvalGraderPython::OrHash,
                OpenAI::Models::EvalUpdateResponse::TestingCriterion::EvalGraderScoreModel::OrHash
              )
            ],
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          id:, # Unique identifier for the evaluation.
          created_at:, # The Unix timestamp (in seconds) for when the eval was created.
          data_source_config:, # Configuration of data sources used in runs of the evaluation.
          metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                     # for storing additional information about the object in a structured format, and
                     # querying for objects via API or the dashboard.
                     # Keys are strings with a maximum length of 64 characters. Values are strings with
                     # a maximum length of 512 characters.
          name:, # The name of the evaluation.
          testing_criteria:, # A list of testing criteria.
          object: :eval # The object type.
); end
      end

      # Configuration of data sources used in runs of the evaluation.
      module DataSourceConfig
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::EvalUpdateResponse::DataSourceConfig::Variants
            ])
          end
          def variants; end
        end

        class Logs < OpenAI::Internal::Type::BaseModel
          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :metadata

          # The json schema for the run data source items. Learn how to build JSON schemas
          # [here](https://json-schema.org/).
          sig { returns(T::Hash[Symbol, T.anything]) }
          attr_accessor :schema

          # The type of data source. Always `logs`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                schema: T::Hash[Symbol, T.anything],
                type: Symbol,
                metadata: T.nilable(T::Hash[Symbol, String])
              })
          end
          def to_hash; end

          class << self
            # A LogsDataSourceConfig which specifies the metadata property of your logs query.
            # This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc. The
            # schema returned by this data source config is used to defined what variables are
            # available in your evals. `item` and `sample` are both defined when using this
            # data source config.
            sig do
              params(
                schema: T::Hash[Symbol, T.anything],
                metadata: T.nilable(T::Hash[Symbol, String]),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              schema:, # The json schema for the run data source items. Learn how to build JSON schemas
                       # [here](https://json-schema.org/).
              metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard.
                             # Keys are strings with a maximum length of 64 characters. Values are strings with
                             # a maximum length of 512 characters.
              type: :logs # The type of data source. Always `logs`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalUpdateResponse::DataSourceConfig::Logs,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::EvalCustomDataSourceConfig,
              OpenAI::Models::EvalUpdateResponse::DataSourceConfig::Logs,
              OpenAI::EvalStoredCompletionsDataSourceConfig
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::Models::EvalUpdateResponse, OpenAI::Internal::AnyHash)
        end

      # A LabelModelGrader object which uses a model to assign labels to each item in
      # the evaluation.
      module TestingCriterion
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::EvalUpdateResponse::TestingCriterion::Variants
            ])
          end
          def variants; end
        end

        class EvalGraderPython < OpenAI::Models::Graders::PythonGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A PythonGrader object that runs a python script on the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalUpdateResponse::TestingCriterion::EvalGraderPython,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class EvalGraderScoreModel < OpenAI::Models::Graders::ScoreModelGrader
          # The threshold for the score.
          sig { returns(T.nilable(Float)) }
          attr_reader :pass_threshold

          sig { params(pass_threshold: Float).void }
          attr_writer :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A ScoreModelGrader object that uses a model to assign a score to the input.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: nil # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalUpdateResponse::TestingCriterion::EvalGraderScoreModel,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class EvalGraderTextSimilarity < OpenAI::Models::Graders::TextSimilarityGrader
          # The threshold for the score.
          sig { returns(Float) }
          attr_accessor :pass_threshold

          sig { override.returns({ pass_threshold: Float }) }
          def to_hash; end

          class << self
            # A TextSimilarityGrader object which grades text based on similarity metrics.
            sig { params(pass_threshold: Float).returns(T.attached_class) }
            def new(
              pass_threshold: # The threshold for the score.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::EvalUpdateResponse::TestingCriterion::EvalGraderTextSimilarity,
                OpenAI::Internal::AnyHash
              )
            end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Models::Graders::LabelModelGrader,
              OpenAI::Models::Graders::StringCheckGrader,
              OpenAI::Models::EvalUpdateResponse::TestingCriterion::EvalGraderTextSimilarity,
              OpenAI::Models::EvalUpdateResponse::TestingCriterion::EvalGraderPython,
              OpenAI::Models::EvalUpdateResponse::TestingCriterion::EvalGraderScoreModel
            )
          end
      end
    end

    module Evals
      class CreateEvalCompletionsRunDataSource < OpenAI::Internal::Type::BaseModel
        # Used when sampling from a model. Dictates the structure of the messages passed
        # into the model. Can either be a reference to a prebuilt trajectory (ie,
        # `item.input_trajectory`), or a template with variable references to the `item`
        # namespace.
        sig do
          returns(T.nilable(
              T.any(
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::ItemReference
              )
            ))
        end
        attr_reader :input_messages

        sig do
          params(
            input_messages: T.any(
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::ItemReference::OrHash
              )
          ).void
        end
        attr_writer :input_messages

        # The name of the model to use for generating completions (e.g. "o3-mini").
        sig { returns(T.nilable(String)) }
        attr_reader :model

        sig { params(model: String).void }
        attr_writer :model

        sig do
          returns(T.nilable(
              OpenAI::Evals::CreateEvalCompletionsRunDataSource::SamplingParams
            ))
        end
        attr_reader :sampling_params

        sig { params(sampling_params: OpenAI::Evals::CreateEvalCompletionsRunDataSource::SamplingParams::OrHash).void }
        attr_writer :sampling_params

        # Determines what populates the `item` namespace in this run's data source.
        sig do
          returns(T.any(
              OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileContent,
              OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileID,
              OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::StoredCompletions
            ))
        end
        attr_accessor :source

        # The type of run data source. Always `completions`.
        sig { returns(OpenAI::Evals::CreateEvalCompletionsRunDataSource::Type::OrSymbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              source:
                T.any(
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileContent,
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileID,
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::StoredCompletions
                ),
              type:
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Type::OrSymbol,
              input_messages:
                T.any(
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template,
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::ItemReference
                ),
              model: String,
              sampling_params:
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::SamplingParams
            })
        end
        def to_hash; end

        class << self
          # A CompletionsRunDataSource object describing a model sampling configuration.
          sig do
            params(
              source: T.any(
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileContent::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileID::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::StoredCompletions::OrHash
              ),
              type: OpenAI::Evals::CreateEvalCompletionsRunDataSource::Type::OrSymbol,
              input_messages: T.any(
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::ItemReference::OrHash
              ),
              model: String,
              sampling_params: OpenAI::Evals::CreateEvalCompletionsRunDataSource::SamplingParams::OrHash
            ).returns(T.attached_class)
          end
          def new(
            source:, # Determines what populates the `item` namespace in this run's data source.
            type:, # The type of run data source. Always `completions`.
            input_messages: nil, # Used when sampling from a model. Dictates the structure of the messages passed
                                 # into the model. Can either be a reference to a prebuilt trajectory (ie,
                                 # `item.input_trajectory`), or a template with variable references to the `item`
                                 # namespace.
            model: nil, # The name of the model to use for generating completions (e.g. "o3-mini").
            sampling_params: nil
); end
        end

        # Used when sampling from a model. Dictates the structure of the messages passed
        # into the model. Can either be a reference to a prebuilt trajectory (ie,
        # `item.input_trajectory`), or a template with variable references to the `item`
        # namespace.
        module InputMessages
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Variants
              ])
            end
            def variants; end
          end

          class ItemReference < OpenAI::Internal::Type::BaseModel
            # A reference to a variable in the `item` namespace. Ie, "item.input_trajectory"
            sig { returns(String) }
            attr_accessor :item_reference

            # The type of input messages. Always `item_reference`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ item_reference: String, type: Symbol }) }
            def to_hash; end

            class << self
              sig { params(item_reference: String, type: Symbol).returns(T.attached_class) }
              def new(
                item_reference:, # A reference to a variable in the `item` namespace. Ie, "item.input_trajectory"
                type: :item_reference # The type of input messages. Always `item_reference`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::ItemReference,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class Template < OpenAI::Internal::Type::BaseModel
            # A list of chat messages forming the prompt or context. May include variable
            # references to the `item` namespace, ie {{item.name}}.
            sig do
              returns(T::Array[
                  T.any(
                    OpenAI::Responses::EasyInputMessage,
                    OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message
                  )
                ])
            end
            attr_accessor :template

            # The type of input messages. Always `template`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  template:
                    T::Array[
                      T.any(
                        OpenAI::Responses::EasyInputMessage,
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message
                      )
                    ],
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  template: T::Array[
                    T.any(
                      OpenAI::Responses::EasyInputMessage::OrHash,
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::OrHash
                    )
                  ],
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                template:, # A list of chat messages forming the prompt or context. May include variable
                           # references to the `item` namespace, ie {{item.name}}.
                type: :template # The type of input messages. Always `template`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template,
                  OpenAI::Internal::AnyHash
                )
              end

            # A message input to the model with a role indicating instruction following
            # hierarchy. Instructions given with the `developer` or `system` role take
            # precedence over instructions given with the `user` role. Messages with the
            # `assistant` role are presumed to have been generated by the model in previous
            # interactions.
            module Template
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Variants
                  ])
                end
                def variants; end
              end

              class Message < OpenAI::Internal::Type::BaseModel
                # Inputs to the model - can contain template strings.
                sig do
                  returns(T.any(
                      String,
                      OpenAI::Responses::ResponseInputText,
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::OutputText,
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::InputImage,
                      T::Array[T.anything]
                    ))
                end
                attr_accessor :content

                # The role of the message input. One of `user`, `assistant`, `system`, or
                # `developer`.
                sig do
                  returns(OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Role::OrSymbol)
                end
                attr_accessor :role

                # The type of the message input. Always `message`.
                sig do
                  returns(T.nilable(
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Type::OrSymbol
                    ))
                end
                attr_reader :type

                sig do
                  params(
                    type: OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Type::OrSymbol
                  ).void
                end
                attr_writer :type

                sig do
                  override
                    .returns({
                      content:
                        T.any(
                          String,
                          OpenAI::Responses::ResponseInputText,
                          OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::OutputText,
                          OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::InputImage,
                          T::Array[T.anything]
                        ),
                      role:
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Role::OrSymbol,
                      type:
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Type::OrSymbol
                    })
                end
                def to_hash; end

                class << self
                  # A message input to the model with a role indicating instruction following
                  # hierarchy. Instructions given with the `developer` or `system` role take
                  # precedence over instructions given with the `user` role. Messages with the
                  # `assistant` role are presumed to have been generated by the model in previous
                  # interactions.
                  sig do
                    params(
                      content: T.any(
                        String,
                        OpenAI::Responses::ResponseInputText::OrHash,
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::OutputText::OrHash,
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::InputImage::OrHash,
                        T::Array[T.anything]
                      ),
                      role: OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Role::OrSymbol,
                      type: OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Type::OrSymbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    content:, # Inputs to the model - can contain template strings.
                    role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                           # `developer`.
                    type: nil # The type of the message input. Always `message`.
); end
                end

                # Inputs to the model - can contain template strings.
                module Content
                  extend OpenAI::Internal::Type::Union

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::Variants
                      ])
                    end
                    def variants; end
                  end

                  AnArrayOfInputTextAndInputImageArray = T.let(
                      OpenAI::Internal::Type::ArrayOf[
                        OpenAI::Internal::Type::Unknown
                      ],
                      OpenAI::Internal::Type::Converter
                    )

                  class InputImage < OpenAI::Internal::Type::BaseModel
                    # The detail level of the image to be sent to the model. One of `high`, `low`, or
                    # `auto`. Defaults to `auto`.
                    sig { returns(T.nilable(String)) }
                    attr_reader :detail

                    sig { params(detail: String).void }
                    attr_writer :detail

                    # The URL of the image input.
                    sig { returns(String) }
                    attr_accessor :image_url

                    # The type of the image input. Always `input_image`.
                    sig { returns(Symbol) }
                    attr_accessor :type

                    sig { override.returns({ image_url: String, type: Symbol, detail: String }) }
                    def to_hash; end

                    class << self
                      # An image input to the model.
                      sig { params(image_url: String, detail: String, type: Symbol).returns(T.attached_class) }
                      def new(
                        image_url:, # The URL of the image input.
                        detail: nil, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                                     # `auto`. Defaults to `auto`.
                        type: :input_image # The type of the image input. Always `input_image`.
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::InputImage,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end

                  class OutputText < OpenAI::Internal::Type::BaseModel
                    # The text output from the model.
                    sig { returns(String) }
                    attr_accessor :text

                    # The type of the output text. Always `output_text`.
                    sig { returns(Symbol) }
                    attr_accessor :type

                    sig { override.returns({ text: String, type: Symbol }) }
                    def to_hash; end

                    class << self
                      # A text output from the model.
                      sig { params(text: String, type: Symbol).returns(T.attached_class) }
                      def new(
                        text:, # The text output from the model.
                        type: :output_text # The type of the output text. Always `output_text`.
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::OutputText,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end

                  Variants = T.type_alias do
                      T.any(
                        String,
                        OpenAI::Responses::ResponseInputText,
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::OutputText,
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Content::InputImage,
                        T::Array[T.anything]
                      )
                    end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message,
                      OpenAI::Internal::AnyHash
                    )
                  end

                # The role of the message input. One of `user`, `assistant`, `system`, or
                # `developer`.
                module Role
                  extend OpenAI::Internal::Type::Enum

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Role::TaggedSymbol
                      ])
                    end
                    def values; end
                  end

                  ASSISTANT = T.let(
                      :assistant,
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Role::TaggedSymbol
                    )

                  DEVELOPER = T.let(
                      :developer,
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Role::TaggedSymbol
                    )

                  OrSymbol = T.type_alias { T.any(Symbol, String) }

                  SYSTEM = T.let(
                      :system,
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Role::TaggedSymbol
                    )

                  TaggedSymbol = T.type_alias do
                      T.all(
                        Symbol,
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Role
                      )
                    end

                  USER = T.let(
                      :user,
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Role::TaggedSymbol
                    )
                end

                # The type of the message input. Always `message`.
                module Type
                  extend OpenAI::Internal::Type::Enum

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Type::TaggedSymbol
                      ])
                    end
                    def values; end
                  end

                  MESSAGE = T.let(
                      :message,
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Type::TaggedSymbol
                    )

                  OrSymbol = T.type_alias { T.any(Symbol, String) }

                  TaggedSymbol = T.type_alias do
                      T.all(
                        Symbol,
                        OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message::Type
                      )
                    end
                end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Responses::EasyInputMessage,
                    OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template::Template::Message
                  )
                end
            end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::Template,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::InputMessages::ItemReference
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Evals::CreateEvalCompletionsRunDataSource,
              OpenAI::Internal::AnyHash
            )
          end

        class SamplingParams < OpenAI::Internal::Type::BaseModel
          # The maximum number of tokens in the generated output.
          sig { returns(T.nilable(Integer)) }
          attr_reader :max_completion_tokens

          sig { params(max_completion_tokens: Integer).void }
          attr_writer :max_completion_tokens

          # An object specifying the format that the model must output.
          #
          # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
          # Outputs which ensures the model will match your supplied JSON schema. Learn more
          # in the
          # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
          #
          # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
          # ensures the message the model generates is valid JSON. Using `json_schema` is
          # preferred for models that support it.
          sig do
            returns(T.nilable(
                T.any(
                  OpenAI::ResponseFormatText,
                  OpenAI::ResponseFormatJSONSchema,
                  OpenAI::ResponseFormatJSONObject
                )
              ))
          end
          attr_reader :response_format

          sig do
            params(
              response_format: T.any(
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash
                )
            ).void
          end
          attr_writer :response_format

          # A seed value to initialize the randomness, during sampling.
          sig { returns(T.nilable(Integer)) }
          attr_reader :seed

          sig { params(seed: Integer).void }
          attr_writer :seed

          # A higher temperature increases randomness in the outputs.
          sig { returns(T.nilable(Float)) }
          attr_reader :temperature

          sig { params(temperature: Float).void }
          attr_writer :temperature

          # A list of tools the model may call. Currently, only functions are supported as a
          # tool. Use this to provide a list of functions the model may generate JSON inputs
          # for. A max of 128 functions are supported.
          sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionTool])) }
          attr_reader :tools

          sig { params(tools: T::Array[OpenAI::Chat::ChatCompletionTool::OrHash]).void }
          attr_writer :tools

          # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
          sig { returns(T.nilable(Float)) }
          attr_reader :top_p

          sig { params(top_p: Float).void }
          attr_writer :top_p

          sig do
            override
              .returns({
                max_completion_tokens: Integer,
                response_format:
                  T.any(
                    OpenAI::ResponseFormatText,
                    OpenAI::ResponseFormatJSONSchema,
                    OpenAI::ResponseFormatJSONObject
                  ),
                seed: Integer,
                temperature: Float,
                tools: T::Array[OpenAI::Chat::ChatCompletionTool],
                top_p: Float
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                max_completion_tokens: Integer,
                response_format: T.any(
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash
                ),
                seed: Integer,
                temperature: Float,
                tools: T::Array[OpenAI::Chat::ChatCompletionTool::OrHash],
                top_p: Float
              ).returns(T.attached_class)
            end
            def new(
              max_completion_tokens: nil, # The maximum number of tokens in the generated output.
              response_format: nil, # An object specifying the format that the model must output.
                                    # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                    # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                    # in the
                                    # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                    # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                                    # ensures the message the model generates is valid JSON. Using `json_schema` is
                                    # preferred for models that support it.
              seed: nil, # A seed value to initialize the randomness, during sampling.
              temperature: nil, # A higher temperature increases randomness in the outputs.
              tools: nil, # A list of tools the model may call. Currently, only functions are supported as a
                          # tool. Use this to provide a list of functions the model may generate JSON inputs
                          # for. A max of 128 functions are supported.
              top_p: nil # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::SamplingParams,
                OpenAI::Internal::AnyHash
              )
            end

          # An object specifying the format that the model must output.
          #
          # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
          # Outputs which ensures the model will match your supplied JSON schema. Learn more
          # in the
          # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
          #
          # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
          # ensures the message the model generates is valid JSON. Using `json_schema` is
          # preferred for models that support it.
          module ResponseFormat
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::SamplingParams::ResponseFormat::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias do
                T.any(
                  OpenAI::ResponseFormatText,
                  OpenAI::ResponseFormatJSONSchema,
                  OpenAI::ResponseFormatJSONObject
                )
              end
          end
        end

        # Determines what populates the `item` namespace in this run's data source.
        module Source
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::Variants
              ])
            end
            def variants; end
          end

          class FileContent < OpenAI::Internal::Type::BaseModel
            # The content of the jsonl file.
            sig do
              returns(T::Array[
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileContent::Content
                ])
            end
            attr_accessor :content

            # The type of jsonl source. Always `file_content`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  content:
                    T::Array[
                      OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileContent::Content
                    ],
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  content: T::Array[
                    OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileContent::Content::OrHash
                  ],
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                content:, # The content of the jsonl file.
                type: :file_content # The type of jsonl source. Always `file_content`.
); end
            end

            class Content < OpenAI::Internal::Type::BaseModel
              sig { returns(T::Hash[Symbol, T.anything]) }
              attr_accessor :item

              sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
              attr_reader :sample

              sig { params(sample: T::Hash[Symbol, T.anything]).void }
              attr_writer :sample

              sig do
                override
                  .returns({
                    item: T::Hash[Symbol, T.anything],
                    sample: T::Hash[Symbol, T.anything]
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    item: T::Hash[Symbol, T.anything],
                    sample: T::Hash[Symbol, T.anything]
                  ).returns(T.attached_class)
                end
                def new(item:, sample: nil); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileContent::Content,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileContent,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileID < OpenAI::Internal::Type::BaseModel
            # The identifier of the file.
            sig { returns(String) }
            attr_accessor :id

            # The type of jsonl source. Always `file_id`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ id: String, type: Symbol }) }
            def to_hash; end

            class << self
              sig { params(id: String, type: Symbol).returns(T.attached_class) }
              def new(
                id:, # The identifier of the file.
                type: :file_id # The type of jsonl source. Always `file_id`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileID,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class StoredCompletions < OpenAI::Internal::Type::BaseModel
            # An optional Unix timestamp to filter items created after this time.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :created_after

            # An optional Unix timestamp to filter items created before this time.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :created_before

            # An optional maximum number of items to return.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :limit

            # Set of 16 key-value pairs that can be attached to an object. This can be useful
            # for storing additional information about the object in a structured format, and
            # querying for objects via API or the dashboard.
            #
            # Keys are strings with a maximum length of 64 characters. Values are strings with
            # a maximum length of 512 characters.
            sig { returns(T.nilable(T::Hash[Symbol, String])) }
            attr_accessor :metadata

            # An optional model to filter by (e.g., 'gpt-4o').
            sig { returns(T.nilable(String)) }
            attr_accessor :model

            # The type of source. Always `stored_completions`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  type: Symbol,
                  created_after: T.nilable(Integer),
                  created_before: T.nilable(Integer),
                  limit: T.nilable(Integer),
                  metadata: T.nilable(T::Hash[Symbol, String]),
                  model: T.nilable(String)
                })
            end
            def to_hash; end

            class << self
              # A StoredCompletionsRunDataSource configuration describing a set of filters
              sig do
                params(
                  created_after: T.nilable(Integer),
                  created_before: T.nilable(Integer),
                  limit: T.nilable(Integer),
                  metadata: T.nilable(T::Hash[Symbol, String]),
                  model: T.nilable(String),
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                created_after: nil, # An optional Unix timestamp to filter items created after this time.
                created_before: nil, # An optional Unix timestamp to filter items created before this time.
                limit: nil, # An optional maximum number of items to return.
                metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                               # for storing additional information about the object in a structured format, and
                               # querying for objects via API or the dashboard.
                               # Keys are strings with a maximum length of 64 characters. Values are strings with
                               # a maximum length of 512 characters.
                model: nil, # An optional model to filter by (e.g., 'gpt-4o').
                type: :stored_completions # The type of source. Always `stored_completions`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::StoredCompletions,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileContent,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::FileID,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Source::StoredCompletions
              )
            end
        end

        # The type of run data source. Always `completions`.
        module Type
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Type::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETIONS = T.let(
              :completions,
              OpenAI::Evals::CreateEvalCompletionsRunDataSource::Type::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::Type
              )
            end
        end
      end

      class CreateEvalJSONLRunDataSource < OpenAI::Internal::Type::BaseModel
        # Determines what populates the `item` namespace in the data source.
        sig do
          returns(T.any(
              OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileContent,
              OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileID
            ))
        end
        attr_accessor :source

        # The type of data source. Always `jsonl`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              source:
                T.any(
                  OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileContent,
                  OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileID
                ),
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # A JsonlRunDataSource object with that specifies a JSONL file that matches the
          # eval
          sig do
            params(
              source: T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileContent::OrHash,
                OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileID::OrHash
              ),
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            source:, # Determines what populates the `item` namespace in the data source.
            type: :jsonl # The type of data source. Always `jsonl`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Evals::CreateEvalJSONLRunDataSource,
              OpenAI::Internal::AnyHash
            )
          end

        # Determines what populates the `item` namespace in the data source.
        module Source
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::Variants
              ])
            end
            def variants; end
          end

          class FileContent < OpenAI::Internal::Type::BaseModel
            # The content of the jsonl file.
            sig do
              returns(T::Array[
                  OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileContent::Content
                ])
            end
            attr_accessor :content

            # The type of jsonl source. Always `file_content`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  content:
                    T::Array[
                      OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileContent::Content
                    ],
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  content: T::Array[
                    OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileContent::Content::OrHash
                  ],
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                content:, # The content of the jsonl file.
                type: :file_content # The type of jsonl source. Always `file_content`.
); end
            end

            class Content < OpenAI::Internal::Type::BaseModel
              sig { returns(T::Hash[Symbol, T.anything]) }
              attr_accessor :item

              sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
              attr_reader :sample

              sig { params(sample: T::Hash[Symbol, T.anything]).void }
              attr_writer :sample

              sig do
                override
                  .returns({
                    item: T::Hash[Symbol, T.anything],
                    sample: T::Hash[Symbol, T.anything]
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    item: T::Hash[Symbol, T.anything],
                    sample: T::Hash[Symbol, T.anything]
                  ).returns(T.attached_class)
                end
                def new(item:, sample: nil); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileContent::Content,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileContent,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileID < OpenAI::Internal::Type::BaseModel
            # The identifier of the file.
            sig { returns(String) }
            attr_accessor :id

            # The type of jsonl source. Always `file_id`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ id: String, type: Symbol }) }
            def to_hash; end

            class << self
              sig { params(id: String, type: Symbol).returns(T.attached_class) }
              def new(
                id:, # The identifier of the file.
                type: :file_id # The type of jsonl source. Always `file_id`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileID,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileContent,
                OpenAI::Evals::CreateEvalJSONLRunDataSource::Source::FileID
              )
            end
        end
      end

      class EvalAPIError < OpenAI::Internal::Type::BaseModel
        # The error code.
        sig { returns(String) }
        attr_accessor :code

        # The error message.
        sig { returns(String) }
        attr_accessor :message

        sig { override.returns({ code: String, message: String }) }
        def to_hash; end

        class << self
          # An object representing an error response from the Eval API.
          sig { params(code: String, message: String).returns(T.attached_class) }
          def new(
            code:, # The error code.
            message: # The error message.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Evals::EvalAPIError, OpenAI::Internal::AnyHash)
          end
      end

      class RunCancelParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :eval_id

        sig { override.returns({ eval_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(eval_id: String, request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(eval_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Evals::RunCancelParams, OpenAI::Internal::AnyHash)
          end
      end

      class RunCancelResponse < OpenAI::Internal::Type::BaseModel
        # Unix timestamp (in seconds) when the evaluation run was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Information about the run's data source.
        sig { returns(OpenAI::Models::Evals::RunCancelResponse::DataSource::Variants) }
        attr_accessor :data_source

        # An object representing an error response from the Eval API.
        sig { returns(OpenAI::Evals::EvalAPIError) }
        attr_reader :error

        sig { params(error: OpenAI::Evals::EvalAPIError::OrHash).void }
        attr_writer :error

        # The identifier of the associated evaluation.
        sig { returns(String) }
        attr_accessor :eval_id

        # Unique identifier for the evaluation run.
        sig { returns(String) }
        attr_accessor :id

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The model that is evaluated, if applicable.
        sig { returns(String) }
        attr_accessor :model

        # The name of the evaluation run.
        sig { returns(String) }
        attr_accessor :name

        # The type of the object. Always "eval.run".
        sig { returns(Symbol) }
        attr_accessor :object

        # Usage statistics for each model during the evaluation run.
        sig { returns(T::Array[OpenAI::Models::Evals::RunCancelResponse::PerModelUsage]) }
        attr_accessor :per_model_usage

        # Results per testing criteria applied during the evaluation run.
        sig do
          returns(T::Array[
              OpenAI::Models::Evals::RunCancelResponse::PerTestingCriteriaResult
            ])
        end
        attr_accessor :per_testing_criteria_results

        # The URL to the rendered evaluation run report on the UI dashboard.
        sig { returns(String) }
        attr_accessor :report_url

        # Counters summarizing the outcomes of the evaluation run.
        sig { returns(OpenAI::Models::Evals::RunCancelResponse::ResultCounts) }
        attr_reader :result_counts

        sig { params(result_counts: OpenAI::Models::Evals::RunCancelResponse::ResultCounts::OrHash).void }
        attr_writer :result_counts

        # The status of the evaluation run.
        sig { returns(String) }
        attr_accessor :status

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data_source:
                OpenAI::Models::Evals::RunCancelResponse::DataSource::Variants,
              error: OpenAI::Evals::EvalAPIError,
              eval_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: String,
              object: Symbol,
              per_model_usage:
                T::Array[
                  OpenAI::Models::Evals::RunCancelResponse::PerModelUsage
                ],
              per_testing_criteria_results:
                T::Array[
                  OpenAI::Models::Evals::RunCancelResponse::PerTestingCriteriaResult
                ],
              report_url: String,
              result_counts:
                OpenAI::Models::Evals::RunCancelResponse::ResultCounts,
              status: String
            })
        end
        def to_hash; end

        class << self
          # A schema representing an evaluation run.
          sig do
            params(
              id: String,
              created_at: Integer,
              data_source: T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::OrHash,
                OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::OrHash
              ),
              error: OpenAI::Evals::EvalAPIError::OrHash,
              eval_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: String,
              per_model_usage: T::Array[
                OpenAI::Models::Evals::RunCancelResponse::PerModelUsage::OrHash
              ],
              per_testing_criteria_results: T::Array[
                OpenAI::Models::Evals::RunCancelResponse::PerTestingCriteriaResult::OrHash
              ],
              report_url: String,
              result_counts: OpenAI::Models::Evals::RunCancelResponse::ResultCounts::OrHash,
              status: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # Unique identifier for the evaluation run.
            created_at:, # Unix timestamp (in seconds) when the evaluation run was created.
            data_source:, # Information about the run's data source.
            error:, # An object representing an error response from the Eval API.
            eval_id:, # The identifier of the associated evaluation.
            metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
            model:, # The model that is evaluated, if applicable.
            name:, # The name of the evaluation run.
            per_model_usage:, # Usage statistics for each model during the evaluation run.
            per_testing_criteria_results:, # Results per testing criteria applied during the evaluation run.
            report_url:, # The URL to the rendered evaluation run report on the UI dashboard.
            result_counts:, # Counters summarizing the outcomes of the evaluation run.
            status:, # The status of the evaluation run.
            object: :"eval.run" # The type of the object. Always "eval.run".
); end
        end

        # Information about the run's data source.
        module DataSource
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Models::Evals::RunCancelResponse::DataSource::Variants
              ])
            end
            def variants; end
          end

          class Responses < OpenAI::Internal::Type::BaseModel
            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            sig do
              returns(T.nilable(
                  OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Variants
                ))
            end
            attr_reader :input_messages

            sig do
              params(
                input_messages: T.any(
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::OrHash,
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::ItemReference::OrHash
                  )
              ).void
            end
            attr_writer :input_messages

            # The name of the model to use for generating completions (e.g. "o3-mini").
            sig { returns(T.nilable(String)) }
            attr_reader :model

            sig { params(model: String).void }
            attr_writer :model

            sig do
              returns(T.nilable(
                  OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams
                ))
            end
            attr_reader :sampling_params

            sig do
              params(
                sampling_params: OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams::OrHash
              ).void
            end
            attr_writer :sampling_params

            # Determines what populates the `item` namespace in this run's data source.
            sig { returns(OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::Variants) }
            attr_accessor :source

            # The type of run data source. Always `responses`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  source:
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::Variants,
                  type: Symbol,
                  input_messages:
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Variants,
                  model: String,
                  sampling_params:
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams
                })
            end
            def to_hash; end

            class << self
              # A ResponsesRunDataSource object describing a model sampling configuration.
              sig do
                params(
                  source: T.any(
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileContent::OrHash,
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileID::OrHash,
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::Responses::OrHash
                  ),
                  input_messages: T.any(
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::OrHash,
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::ItemReference::OrHash
                  ),
                  model: String,
                  sampling_params: OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                source:, # Determines what populates the `item` namespace in this run's data source.
                input_messages: nil, # Used when sampling from a model. Dictates the structure of the messages passed
                                     # into the model. Can either be a reference to a prebuilt trajectory (ie,
                                     # `item.input_trajectory`), or a template with variable references to the `item`
                                     # namespace.
                model: nil, # The name of the model to use for generating completions (e.g. "o3-mini").
                sampling_params: nil,
                type: :responses # The type of run data source. Always `responses`.
); end
            end

            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            module InputMessages
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Variants
                  ])
                end
                def variants; end
              end

              class ItemReference < OpenAI::Internal::Type::BaseModel
                # A reference to a variable in the `item` namespace. Ie, "item.name"
                sig { returns(String) }
                attr_accessor :item_reference

                # The type of input messages. Always `item_reference`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ item_reference: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(item_reference: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    item_reference:, # A reference to a variable in the `item` namespace. Ie, "item.name"
                    type: :item_reference # The type of input messages. Always `item_reference`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::ItemReference,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Template < OpenAI::Internal::Type::BaseModel
                # A list of chat messages forming the prompt or context. May include variable
                # references to the `item` namespace, ie {{item.name}}.
                sig do
                  returns(T::Array[
                      OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                    ])
                end
                attr_accessor :template

                # The type of input messages. Always `template`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      template:
                        T::Array[
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      template: T::Array[
                        T.any(
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage::OrHash,
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::OrHash
                        )
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    template:, # A list of chat messages forming the prompt or context. May include variable
                               # references to the `item` namespace, ie {{item.name}}.
                    type: :template # The type of input messages. Always `template`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template,
                      OpenAI::Internal::AnyHash
                    )
                  end

                # A message input to the model with a role indicating instruction following
                # hierarchy. Instructions given with the `developer` or `system` role take
                # precedence over instructions given with the `user` role. Messages with the
                # `assistant` role are presumed to have been generated by the model in previous
                # interactions.
                module Template
                  extend OpenAI::Internal::Type::Union

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                      ])
                    end
                    def variants; end
                  end

                  class ChatMessage < OpenAI::Internal::Type::BaseModel
                    # The content of the message.
                    sig { returns(String) }
                    attr_accessor :content

                    # The role of the message (e.g. "system", "assistant", "user").
                    sig { returns(String) }
                    attr_accessor :role

                    sig { override.returns({ content: String, role: String }) }
                    def to_hash; end

                    class << self
                      sig { params(content: String, role: String).returns(T.attached_class) }
                      def new(
                        content:, # The content of the message.
                        role: # The role of the message (e.g. "system", "assistant", "user").
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end

                  class EvalItem < OpenAI::Internal::Type::BaseModel
                    # Inputs to the model - can contain template strings.
                    sig do
                      returns(OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants)
                    end
                    attr_accessor :content

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    sig do
                      returns(OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol)
                    end
                    attr_accessor :role

                    # The type of the message input. Always `message`.
                    sig do
                      returns(T.nilable(
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        ))
                    end
                    attr_reader :type

                    sig do
                      params(
                        type: OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                      ).void
                    end
                    attr_writer :type

                    sig do
                      override
                        .returns({
                          content:
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants,
                          role:
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol,
                          type:
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        })
                    end
                    def to_hash; end

                    class << self
                      # A message input to the model with a role indicating instruction following
                      # hierarchy. Instructions given with the `developer` or `system` role take
                      # precedence over instructions given with the `user` role. Messages with the
                      # `assistant` role are presumed to have been generated by the model in previous
                      # interactions.
                      sig do
                        params(
                          content: T.any(
                            String,
                            OpenAI::Responses::ResponseInputText::OrHash,
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText::OrHash,
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage::OrHash,
                            T::Array[T.anything]
                          ),
                          role: OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::OrSymbol,
                          type: OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                        ).returns(T.attached_class)
                      end
                      def new(
                        content:, # Inputs to the model - can contain template strings.
                        role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                               # `developer`.
                        type: nil # The type of the message input. Always `message`.
); end
                    end

                    # Inputs to the model - can contain template strings.
                    module Content
                      extend OpenAI::Internal::Type::Union

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants
                          ])
                        end
                        def variants; end
                      end

                      AnArrayOfInputTextAndInputImageArray = T.let(
                          OpenAI::Internal::Type::ArrayOf[
                            OpenAI::Internal::Type::Unknown
                          ],
                          OpenAI::Internal::Type::Converter
                        )

                      class InputImage < OpenAI::Internal::Type::BaseModel
                        # The detail level of the image to be sent to the model. One of `high`, `low`, or
                        # `auto`. Defaults to `auto`.
                        sig { returns(T.nilable(String)) }
                        attr_reader :detail

                        sig { params(detail: String).void }
                        attr_writer :detail

                        # The URL of the image input.
                        sig { returns(String) }
                        attr_accessor :image_url

                        # The type of the image input. Always `input_image`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ image_url: String, type: Symbol, detail: String }) }
                        def to_hash; end

                        class << self
                          # An image input to the model.
                          sig { params(image_url: String, detail: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            image_url:, # The URL of the image input.
                            detail: nil, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                                         # `auto`. Defaults to `auto`.
                            type: :input_image # The type of the image input. Always `input_image`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      class OutputText < OpenAI::Internal::Type::BaseModel
                        # The text output from the model.
                        sig { returns(String) }
                        attr_accessor :text

                        # The type of the output text. Always `output_text`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ text: String, type: Symbol }) }
                        def to_hash; end

                        class << self
                          # A text output from the model.
                          sig { params(text: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            text:, # The text output from the model.
                            type: :output_text # The type of the output text. Always `output_text`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      Variants = T.type_alias do
                          T.any(
                            String,
                            OpenAI::Responses::ResponseInputText,
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText,
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage,
                            T::Array[T.anything]
                          )
                        end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem,
                          OpenAI::Internal::AnyHash
                        )
                      end

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    module Role
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      ASSISTANT = T.let(
                          :assistant,
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      DEVELOPER = T.let(
                          :developer,
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      SYSTEM = T.let(
                          :system,
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role
                          )
                        end

                      USER = T.let(
                          :user,
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )
                    end

                    # The type of the message input. Always `message`.
                    module Type
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      MESSAGE = T.let(
                          :message,
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type
                          )
                        end
                    end
                  end

                  Variants = T.type_alias do
                      T.any(
                        OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage,
                        OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem
                      )
                    end
                end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::Template,
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::InputMessages::ItemReference
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses,
                  OpenAI::Internal::AnyHash
                )
              end

            class SamplingParams < OpenAI::Internal::Type::BaseModel
              # The maximum number of tokens in the generated output.
              sig { returns(T.nilable(Integer)) }
              attr_reader :max_completion_tokens

              sig { params(max_completion_tokens: Integer).void }
              attr_writer :max_completion_tokens

              # A seed value to initialize the randomness, during sampling.
              sig { returns(T.nilable(Integer)) }
              attr_reader :seed

              sig { params(seed: Integer).void }
              attr_writer :seed

              # A higher temperature increases randomness in the outputs.
              sig { returns(T.nilable(Float)) }
              attr_reader :temperature

              sig { params(temperature: Float).void }
              attr_writer :temperature

              # Configuration options for a text response from the model. Can be plain text or
              # structured JSON data. Learn more:
              #
              # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
              # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
              sig do
                returns(T.nilable(
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams::Text
                  ))
              end
              attr_reader :text

              sig do
                params(
                  text: OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams::Text::OrHash
                ).void
              end
              attr_writer :text

              # An array of tools the model may call while generating a response. You can
              # specify which tool to use by setting the `tool_choice` parameter.
              #
              # The two categories of tools you can provide the model are:
              #
              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
              #   capabilities, like
              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
              #   Learn more about
              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
              #   the model to call your own code. Learn more about
              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
              sig { returns(T.nilable(T::Array[OpenAI::Responses::Tool::Variants])) }
              attr_reader :tools

              sig do
                params(
                  tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ]
                ).void
              end
              attr_writer :tools

              # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
              sig { returns(T.nilable(Float)) }
              attr_reader :top_p

              sig { params(top_p: Float).void }
              attr_writer :top_p

              sig do
                override
                  .returns({
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text:
                      OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams::Text,
                    tools: T::Array[OpenAI::Responses::Tool::Variants],
                    top_p: Float
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text: OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams::Text::OrHash,
                    tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ],
                    top_p: Float
                  ).returns(T.attached_class)
                end
                def new(
                  max_completion_tokens: nil, # The maximum number of tokens in the generated output.
                  seed: nil, # A seed value to initialize the randomness, during sampling.
                  temperature: nil, # A higher temperature increases randomness in the outputs.
                  text: nil, # Configuration options for a text response from the model. Can be plain text or
                             # structured JSON data. Learn more:
                             # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                             # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  tools: nil, # An array of tools the model may call while generating a response. You can
                              # specify which tool to use by setting the `tool_choice` parameter.
                              # The two categories of tools you can provide the model are:
                              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
                              #   capabilities, like
                              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
                              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
                              #   Learn more about
                              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
                              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
                              #   the model to call your own code. Learn more about
                              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
                  top_p: nil # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams,
                    OpenAI::Internal::AnyHash
                  )
                end

              class Text < OpenAI::Internal::Type::BaseModel
                # An object specifying the format that the model must output.
                #
                # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                # ensures the model will match your supplied JSON schema. Learn more in the
                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                #
                # The default format is `{ "type": "text" }` with no additional options.
                #
                # **Not recommended for gpt-4o and newer models:**
                #
                # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                # ensures the message the model generates is valid JSON. Using `json_schema` is
                # preferred for models that support it.
                sig do
                  returns(T.nilable(
                      OpenAI::Responses::ResponseFormatTextConfig::Variants
                    ))
                end
                attr_reader :format_

                sig do
                  params(
                    format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                  ).void
                end
                attr_writer :format_

                sig do
                  override
                    .returns({
                      format_:
                        OpenAI::Responses::ResponseFormatTextConfig::Variants
                    })
                end
                def to_hash; end

                class << self
                  # Configuration options for a text response from the model. Can be plain text or
                  # structured JSON data. Learn more:
                  #
                  # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                  # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  sig do
                    params(
                      format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                    ).returns(T.attached_class)
                  end
                  def new(
                    format_: nil # An object specifying the format that the model must output.
                                 # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                                 # ensures the model will match your supplied JSON schema. Learn more in the
                                 # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                 # The default format is `{ "type": "text" }` with no additional options.
                                 # **Not recommended for gpt-4o and newer models:**
                                 # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                                 # ensures the message the model generates is valid JSON. Using `json_schema` is
                                 # preferred for models that support it.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::SamplingParams::Text,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end
            end

            # Determines what populates the `item` namespace in this run's data source.
            module Source
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::Variants
                  ])
                end
                def variants; end
              end

              class FileContent < OpenAI::Internal::Type::BaseModel
                # The content of the jsonl file.
                sig do
                  returns(T::Array[
                      OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileContent::Content
                    ])
                end
                attr_accessor :content

                # The type of jsonl source. Always `file_content`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      content:
                        T::Array[
                          OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileContent::Content
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      content: T::Array[
                        OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileContent::Content::OrHash
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    content:, # The content of the jsonl file.
                    type: :file_content # The type of jsonl source. Always `file_content`.
); end
                end

                class Content < OpenAI::Internal::Type::BaseModel
                  sig { returns(T::Hash[Symbol, T.anything]) }
                  attr_accessor :item

                  sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
                  attr_reader :sample

                  sig { params(sample: T::Hash[Symbol, T.anything]).void }
                  attr_writer :sample

                  sig do
                    override
                      .returns({
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      })
                  end
                  def to_hash; end

                  class << self
                    sig do
                      params(
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      ).returns(T.attached_class)
                    end
                    def new(item:, sample: nil); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileContent::Content,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileContent,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class FileID < OpenAI::Internal::Type::BaseModel
                # The identifier of the file.
                sig { returns(String) }
                attr_accessor :id

                # The type of jsonl source. Always `file_id`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ id: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(id: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    id:, # The identifier of the file.
                    type: :file_id # The type of jsonl source. Always `file_id`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileID,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Responses < OpenAI::Internal::Type::BaseModel
                # Only include items created after this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_after

                # Only include items created before this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_before

                # Optional string to search the 'instructions' field. This is a query parameter
                # used to select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :instructions_search

                # Metadata filter for the responses. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(T.anything)) }
                attr_accessor :metadata

                # The name of the model to find responses for. This is a query parameter used to
                # select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :model

                # Optional reasoning effort parameter. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(OpenAI::ReasoningEffort::TaggedSymbol)) }
                attr_accessor :reasoning_effort

                # Sampling temperature. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :temperature

                # List of tool names. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :tools

                # Nucleus sampling parameter. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :top_p

                # The type of run data source. Always `responses`.
                sig { returns(Symbol) }
                attr_accessor :type

                # List of user identifiers. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :users

                sig do
                  override
                    .returns({
                      type: Symbol,
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort:
                        T.nilable(OpenAI::ReasoningEffort::TaggedSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String])
                    })
                end
                def to_hash; end

                class << self
                  # A EvalResponsesSource object describing a run data source configuration.
                  sig do
                    params(
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String]),
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    created_after: nil, # Only include items created after this timestamp (inclusive). This is a query
                                        # parameter used to select responses.
                    created_before: nil, # Only include items created before this timestamp (inclusive). This is a query
                                         # parameter used to select responses.
                    instructions_search: nil, # Optional string to search the 'instructions' field. This is a query parameter
                                              # used to select responses.
                    metadata: nil, # Metadata filter for the responses. This is a query parameter used to select
                                   # responses.
                    model: nil, # The name of the model to find responses for. This is a query parameter used to
                                # select responses.
                    reasoning_effort: nil, # Optional reasoning effort parameter. This is a query parameter used to select
                                           # responses.
                    temperature: nil, # Sampling temperature. This is a query parameter used to select responses.
                    tools: nil, # List of tool names. This is a query parameter used to select responses.
                    top_p: nil, # Nucleus sampling parameter. This is a query parameter used to select responses.
                    users: nil, # List of user identifiers. This is a query parameter used to select responses.
                    type: :responses # The type of run data source. Always `responses`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::Responses,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileContent,
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::FileID,
                    OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses::Source::Responses
                  )
                end
            end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource,
                OpenAI::Models::Evals::RunCancelResponse::DataSource::Responses
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::Evals::RunCancelResponse,
              OpenAI::Internal::AnyHash
            )
          end

        class PerModelUsage < OpenAI::Internal::Type::BaseModel
          # The number of tokens retrieved from cache.
          sig { returns(Integer) }
          attr_accessor :cached_tokens

          # The number of completion tokens generated.
          sig { returns(Integer) }
          attr_accessor :completion_tokens

          # The number of invocations.
          sig { returns(Integer) }
          attr_accessor :invocation_count

          # The name of the model.
          sig { returns(String) }
          attr_accessor :model_name

          # The number of prompt tokens used.
          sig { returns(Integer) }
          attr_accessor :prompt_tokens

          # The total number of tokens used.
          sig { returns(Integer) }
          attr_accessor :total_tokens

          sig do
            override
              .returns({
                cached_tokens: Integer,
                completion_tokens: Integer,
                invocation_count: Integer,
                model_name: String,
                prompt_tokens: Integer,
                total_tokens: Integer
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                cached_tokens: Integer,
                completion_tokens: Integer,
                invocation_count: Integer,
                model_name: String,
                prompt_tokens: Integer,
                total_tokens: Integer
              ).returns(T.attached_class)
            end
            def new(
              cached_tokens:, # The number of tokens retrieved from cache.
              completion_tokens:, # The number of completion tokens generated.
              invocation_count:, # The number of invocations.
              model_name:, # The name of the model.
              prompt_tokens:, # The number of prompt tokens used.
              total_tokens: # The total number of tokens used.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunCancelResponse::PerModelUsage,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class PerTestingCriteriaResult < OpenAI::Internal::Type::BaseModel
          # Number of tests failed for this criteria.
          sig { returns(Integer) }
          attr_accessor :failed

          # Number of tests passed for this criteria.
          sig { returns(Integer) }
          attr_accessor :passed

          # A description of the testing criteria.
          sig { returns(String) }
          attr_accessor :testing_criteria

          sig { override.returns({ failed: Integer, passed: Integer, testing_criteria: String }) }
          def to_hash; end

          class << self
            sig { params(failed: Integer, passed: Integer, testing_criteria: String).returns(T.attached_class) }
            def new(
              failed:, # Number of tests failed for this criteria.
              passed:, # Number of tests passed for this criteria.
              testing_criteria: # A description of the testing criteria.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunCancelResponse::PerTestingCriteriaResult,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ResultCounts < OpenAI::Internal::Type::BaseModel
          # Number of output items that resulted in an error.
          sig { returns(Integer) }
          attr_accessor :errored

          # Number of output items that failed to pass the evaluation.
          sig { returns(Integer) }
          attr_accessor :failed

          # Number of output items that passed the evaluation.
          sig { returns(Integer) }
          attr_accessor :passed

          # Total number of executed output items.
          sig { returns(Integer) }
          attr_accessor :total

          sig do
            override
              .returns({
                errored: Integer,
                failed: Integer,
                passed: Integer,
                total: Integer
              })
          end
          def to_hash; end

          class << self
            # Counters summarizing the outcomes of the evaluation run.
            sig do
              params(
                errored: Integer,
                failed: Integer,
                passed: Integer,
                total: Integer
              ).returns(T.attached_class)
            end
            def new(
              errored:, # Number of output items that resulted in an error.
              failed:, # Number of output items that failed to pass the evaluation.
              passed:, # Number of output items that passed the evaluation.
              total: # Total number of executed output items.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunCancelResponse::ResultCounts,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class RunCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Details about the run's data source.
        sig do
          returns(T.any(
              OpenAI::Evals::CreateEvalJSONLRunDataSource,
              OpenAI::Evals::CreateEvalCompletionsRunDataSource,
              OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource
            ))
        end
        attr_accessor :data_source

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The name of the run.
        sig { returns(T.nilable(String)) }
        attr_reader :name

        sig { params(name: String).void }
        attr_writer :name

        sig do
          override
            .returns({
              data_source:
                T.any(
                  OpenAI::Evals::CreateEvalJSONLRunDataSource,
                  OpenAI::Evals::CreateEvalCompletionsRunDataSource,
                  OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource
                ),
              metadata: T.nilable(T::Hash[Symbol, String]),
              name: String,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              data_source: T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::OrHash,
                OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::OrHash
              ),
              metadata: T.nilable(T::Hash[Symbol, String]),
              name: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            data_source:, # Details about the run's data source.
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            name: nil, # The name of the run.
            request_options: {}
); end
        end

        # Details about the run's data source.
        module DataSource
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Evals::RunCreateParams::DataSource::Variants]) }
            def variants; end
          end

          class CreateEvalResponsesRunDataSource < OpenAI::Internal::Type::BaseModel
            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            sig do
              returns(T.nilable(
                  T.any(
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template,
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::ItemReference
                  )
                ))
            end
            attr_reader :input_messages

            sig do
              params(
                input_messages: T.any(
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::OrHash,
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::ItemReference::OrHash
                  )
              ).void
            end
            attr_writer :input_messages

            # The name of the model to use for generating completions (e.g. "o3-mini").
            sig { returns(T.nilable(String)) }
            attr_reader :model

            sig { params(model: String).void }
            attr_writer :model

            sig do
              returns(T.nilable(
                  OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams
                ))
            end
            attr_reader :sampling_params

            sig do
              params(
                sampling_params: OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams::OrHash
              ).void
            end
            attr_writer :sampling_params

            # Determines what populates the `item` namespace in this run's data source.
            sig do
              returns(T.any(
                  OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileContent,
                  OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileID,
                  OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::Responses
                ))
            end
            attr_accessor :source

            # The type of run data source. Always `responses`.
            sig do
              returns(OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Type::OrSymbol)
            end
            attr_accessor :type

            sig do
              override
                .returns({
                  source:
                    T.any(
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileContent,
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileID,
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::Responses
                    ),
                  type:
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Type::OrSymbol,
                  input_messages:
                    T.any(
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template,
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::ItemReference
                    ),
                  model: String,
                  sampling_params:
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams
                })
            end
            def to_hash; end

            class << self
              # A ResponsesRunDataSource object describing a model sampling configuration.
              sig do
                params(
                  source: T.any(
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileContent::OrHash,
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileID::OrHash,
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::Responses::OrHash
                  ),
                  type: OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Type::OrSymbol,
                  input_messages: T.any(
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::OrHash,
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::ItemReference::OrHash
                  ),
                  model: String,
                  sampling_params: OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams::OrHash
                ).returns(T.attached_class)
              end
              def new(
                source:, # Determines what populates the `item` namespace in this run's data source.
                type:, # The type of run data source. Always `responses`.
                input_messages: nil, # Used when sampling from a model. Dictates the structure of the messages passed
                                     # into the model. Can either be a reference to a prebuilt trajectory (ie,
                                     # `item.input_trajectory`), or a template with variable references to the `item`
                                     # namespace.
                model: nil, # The name of the model to use for generating completions (e.g. "o3-mini").
                sampling_params: nil
); end
            end

            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            module InputMessages
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Variants
                  ])
                end
                def variants; end
              end

              class ItemReference < OpenAI::Internal::Type::BaseModel
                # A reference to a variable in the `item` namespace. Ie, "item.name"
                sig { returns(String) }
                attr_accessor :item_reference

                # The type of input messages. Always `item_reference`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ item_reference: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(item_reference: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    item_reference:, # A reference to a variable in the `item` namespace. Ie, "item.name"
                    type: :item_reference # The type of input messages. Always `item_reference`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::ItemReference,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Template < OpenAI::Internal::Type::BaseModel
                # A list of chat messages forming the prompt or context. May include variable
                # references to the `item` namespace, ie {{item.name}}.
                sig do
                  returns(T::Array[
                      T.any(
                        OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::ChatMessage,
                        OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem
                      )
                    ])
                end
                attr_accessor :template

                # The type of input messages. Always `template`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      template:
                        T::Array[
                          T.any(
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::ChatMessage,
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem
                          )
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      template: T::Array[
                        T.any(
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::ChatMessage::OrHash,
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::OrHash
                        )
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    template:, # A list of chat messages forming the prompt or context. May include variable
                               # references to the `item` namespace, ie {{item.name}}.
                    type: :template # The type of input messages. Always `template`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template,
                      OpenAI::Internal::AnyHash
                    )
                  end

                # A message input to the model with a role indicating instruction following
                # hierarchy. Instructions given with the `developer` or `system` role take
                # precedence over instructions given with the `user` role. Messages with the
                # `assistant` role are presumed to have been generated by the model in previous
                # interactions.
                module Template
                  extend OpenAI::Internal::Type::Union

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::Variants
                      ])
                    end
                    def variants; end
                  end

                  class ChatMessage < OpenAI::Internal::Type::BaseModel
                    # The content of the message.
                    sig { returns(String) }
                    attr_accessor :content

                    # The role of the message (e.g. "system", "assistant", "user").
                    sig { returns(String) }
                    attr_accessor :role

                    sig { override.returns({ content: String, role: String }) }
                    def to_hash; end

                    class << self
                      sig { params(content: String, role: String).returns(T.attached_class) }
                      def new(
                        content:, # The content of the message.
                        role: # The role of the message (e.g. "system", "assistant", "user").
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::ChatMessage,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end

                  class EvalItem < OpenAI::Internal::Type::BaseModel
                    # Inputs to the model - can contain template strings.
                    sig do
                      returns(T.any(
                          String,
                          OpenAI::Responses::ResponseInputText,
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::OutputText,
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::InputImage,
                          T::Array[T.anything]
                        ))
                    end
                    attr_accessor :content

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    sig do
                      returns(OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Role::OrSymbol)
                    end
                    attr_accessor :role

                    # The type of the message input. Always `message`.
                    sig do
                      returns(T.nilable(
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                        ))
                    end
                    attr_reader :type

                    sig do
                      params(
                        type: OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                      ).void
                    end
                    attr_writer :type

                    sig do
                      override
                        .returns({
                          content:
                            T.any(
                              String,
                              OpenAI::Responses::ResponseInputText,
                              OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::OutputText,
                              OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::InputImage,
                              T::Array[T.anything]
                            ),
                          role:
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Role::OrSymbol,
                          type:
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                        })
                    end
                    def to_hash; end

                    class << self
                      # A message input to the model with a role indicating instruction following
                      # hierarchy. Instructions given with the `developer` or `system` role take
                      # precedence over instructions given with the `user` role. Messages with the
                      # `assistant` role are presumed to have been generated by the model in previous
                      # interactions.
                      sig do
                        params(
                          content: T.any(
                            String,
                            OpenAI::Responses::ResponseInputText::OrHash,
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::OutputText::OrHash,
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::InputImage::OrHash,
                            T::Array[T.anything]
                          ),
                          role: OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Role::OrSymbol,
                          type: OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                        ).returns(T.attached_class)
                      end
                      def new(
                        content:, # Inputs to the model - can contain template strings.
                        role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                               # `developer`.
                        type: nil # The type of the message input. Always `message`.
); end
                    end

                    # Inputs to the model - can contain template strings.
                    module Content
                      extend OpenAI::Internal::Type::Union

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::Variants
                          ])
                        end
                        def variants; end
                      end

                      AnArrayOfInputTextAndInputImageArray = T.let(
                          OpenAI::Internal::Type::ArrayOf[
                            OpenAI::Internal::Type::Unknown
                          ],
                          OpenAI::Internal::Type::Converter
                        )

                      class InputImage < OpenAI::Internal::Type::BaseModel
                        # The detail level of the image to be sent to the model. One of `high`, `low`, or
                        # `auto`. Defaults to `auto`.
                        sig { returns(T.nilable(String)) }
                        attr_reader :detail

                        sig { params(detail: String).void }
                        attr_writer :detail

                        # The URL of the image input.
                        sig { returns(String) }
                        attr_accessor :image_url

                        # The type of the image input. Always `input_image`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ image_url: String, type: Symbol, detail: String }) }
                        def to_hash; end

                        class << self
                          # An image input to the model.
                          sig { params(image_url: String, detail: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            image_url:, # The URL of the image input.
                            detail: nil, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                                         # `auto`. Defaults to `auto`.
                            type: :input_image # The type of the image input. Always `input_image`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::InputImage,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      class OutputText < OpenAI::Internal::Type::BaseModel
                        # The text output from the model.
                        sig { returns(String) }
                        attr_accessor :text

                        # The type of the output text. Always `output_text`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ text: String, type: Symbol }) }
                        def to_hash; end

                        class << self
                          # A text output from the model.
                          sig { params(text: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            text:, # The text output from the model.
                            type: :output_text # The type of the output text. Always `output_text`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::OutputText,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      Variants = T.type_alias do
                          T.any(
                            String,
                            OpenAI::Responses::ResponseInputText,
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::OutputText,
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Content::InputImage,
                            T::Array[T.anything]
                          )
                        end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem,
                          OpenAI::Internal::AnyHash
                        )
                      end

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    module Role
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      ASSISTANT = T.let(
                          :assistant,
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      DEVELOPER = T.let(
                          :developer,
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      SYSTEM = T.let(
                          :system,
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Role
                          )
                        end

                      USER = T.let(
                          :user,
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )
                    end

                    # The type of the message input. Always `message`.
                    module Type
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      MESSAGE = T.let(
                          :message,
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem::Type
                          )
                        end
                    end
                  end

                  Variants = T.type_alias do
                      T.any(
                        OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::ChatMessage,
                        OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template::Template::EvalItem
                      )
                    end
                end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::Template,
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::InputMessages::ItemReference
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource,
                  OpenAI::Internal::AnyHash
                )
              end

            class SamplingParams < OpenAI::Internal::Type::BaseModel
              # The maximum number of tokens in the generated output.
              sig { returns(T.nilable(Integer)) }
              attr_reader :max_completion_tokens

              sig { params(max_completion_tokens: Integer).void }
              attr_writer :max_completion_tokens

              # A seed value to initialize the randomness, during sampling.
              sig { returns(T.nilable(Integer)) }
              attr_reader :seed

              sig { params(seed: Integer).void }
              attr_writer :seed

              # A higher temperature increases randomness in the outputs.
              sig { returns(T.nilable(Float)) }
              attr_reader :temperature

              sig { params(temperature: Float).void }
              attr_writer :temperature

              # Configuration options for a text response from the model. Can be plain text or
              # structured JSON data. Learn more:
              #
              # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
              # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
              sig do
                returns(T.nilable(
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams::Text
                  ))
              end
              attr_reader :text

              sig do
                params(
                  text: OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams::Text::OrHash
                ).void
              end
              attr_writer :text

              # An array of tools the model may call while generating a response. You can
              # specify which tool to use by setting the `tool_choice` parameter.
              #
              # The two categories of tools you can provide the model are:
              #
              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
              #   capabilities, like
              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
              #   Learn more about
              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
              #   the model to call your own code. Learn more about
              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
              sig do
                returns(T.nilable(
                    T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool,
                        OpenAI::Responses::FileSearchTool,
                        OpenAI::Responses::ComputerTool,
                        OpenAI::Responses::Tool::Mcp,
                        OpenAI::Responses::Tool::CodeInterpreter,
                        OpenAI::Responses::Tool::ImageGeneration,
                        OpenAI::Responses::Tool::LocalShell,
                        OpenAI::Responses::WebSearchTool
                      )
                    ]
                  ))
              end
              attr_reader :tools

              sig do
                params(
                  tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ]
                ).void
              end
              attr_writer :tools

              # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
              sig { returns(T.nilable(Float)) }
              attr_reader :top_p

              sig { params(top_p: Float).void }
              attr_writer :top_p

              sig do
                override
                  .returns({
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text:
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams::Text,
                    tools:
                      T::Array[
                        T.any(
                          OpenAI::Responses::FunctionTool,
                          OpenAI::Responses::FileSearchTool,
                          OpenAI::Responses::ComputerTool,
                          OpenAI::Responses::Tool::Mcp,
                          OpenAI::Responses::Tool::CodeInterpreter,
                          OpenAI::Responses::Tool::ImageGeneration,
                          OpenAI::Responses::Tool::LocalShell,
                          OpenAI::Responses::WebSearchTool
                        )
                      ],
                    top_p: Float
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text: OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams::Text::OrHash,
                    tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ],
                    top_p: Float
                  ).returns(T.attached_class)
                end
                def new(
                  max_completion_tokens: nil, # The maximum number of tokens in the generated output.
                  seed: nil, # A seed value to initialize the randomness, during sampling.
                  temperature: nil, # A higher temperature increases randomness in the outputs.
                  text: nil, # Configuration options for a text response from the model. Can be plain text or
                             # structured JSON data. Learn more:
                             # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                             # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  tools: nil, # An array of tools the model may call while generating a response. You can
                              # specify which tool to use by setting the `tool_choice` parameter.
                              # The two categories of tools you can provide the model are:
                              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
                              #   capabilities, like
                              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
                              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
                              #   Learn more about
                              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
                              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
                              #   the model to call your own code. Learn more about
                              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
                  top_p: nil # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams,
                    OpenAI::Internal::AnyHash
                  )
                end

              class Text < OpenAI::Internal::Type::BaseModel
                # An object specifying the format that the model must output.
                #
                # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                # ensures the model will match your supplied JSON schema. Learn more in the
                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                #
                # The default format is `{ "type": "text" }` with no additional options.
                #
                # **Not recommended for gpt-4o and newer models:**
                #
                # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                # ensures the message the model generates is valid JSON. Using `json_schema` is
                # preferred for models that support it.
                sig do
                  returns(T.nilable(
                      T.any(
                        OpenAI::ResponseFormatText,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig,
                        OpenAI::ResponseFormatJSONObject
                      )
                    ))
                end
                attr_reader :format_

                sig do
                  params(
                    format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                  ).void
                end
                attr_writer :format_

                sig do
                  override
                    .returns({
                      format_:
                        T.any(
                          OpenAI::ResponseFormatText,
                          OpenAI::Responses::ResponseFormatTextJSONSchemaConfig,
                          OpenAI::ResponseFormatJSONObject
                        )
                    })
                end
                def to_hash; end

                class << self
                  # Configuration options for a text response from the model. Can be plain text or
                  # structured JSON data. Learn more:
                  #
                  # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                  # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  sig do
                    params(
                      format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                    ).returns(T.attached_class)
                  end
                  def new(
                    format_: nil # An object specifying the format that the model must output.
                                 # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                                 # ensures the model will match your supplied JSON schema. Learn more in the
                                 # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                 # The default format is `{ "type": "text" }` with no additional options.
                                 # **Not recommended for gpt-4o and newer models:**
                                 # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                                 # ensures the message the model generates is valid JSON. Using `json_schema` is
                                 # preferred for models that support it.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::SamplingParams::Text,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end
            end

            # Determines what populates the `item` namespace in this run's data source.
            module Source
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::Variants
                  ])
                end
                def variants; end
              end

              class FileContent < OpenAI::Internal::Type::BaseModel
                # The content of the jsonl file.
                sig do
                  returns(T::Array[
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileContent::Content
                    ])
                end
                attr_accessor :content

                # The type of jsonl source. Always `file_content`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      content:
                        T::Array[
                          OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileContent::Content
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      content: T::Array[
                        OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileContent::Content::OrHash
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    content:, # The content of the jsonl file.
                    type: :file_content # The type of jsonl source. Always `file_content`.
); end
                end

                class Content < OpenAI::Internal::Type::BaseModel
                  sig { returns(T::Hash[Symbol, T.anything]) }
                  attr_accessor :item

                  sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
                  attr_reader :sample

                  sig { params(sample: T::Hash[Symbol, T.anything]).void }
                  attr_writer :sample

                  sig do
                    override
                      .returns({
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      })
                  end
                  def to_hash; end

                  class << self
                    sig do
                      params(
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      ).returns(T.attached_class)
                    end
                    def new(item:, sample: nil); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileContent::Content,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileContent,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class FileID < OpenAI::Internal::Type::BaseModel
                # The identifier of the file.
                sig { returns(String) }
                attr_accessor :id

                # The type of jsonl source. Always `file_id`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ id: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(id: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    id:, # The identifier of the file.
                    type: :file_id # The type of jsonl source. Always `file_id`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileID,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Responses < OpenAI::Internal::Type::BaseModel
                # Only include items created after this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_after

                # Only include items created before this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_before

                # Optional string to search the 'instructions' field. This is a query parameter
                # used to select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :instructions_search

                # Metadata filter for the responses. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(T.anything)) }
                attr_accessor :metadata

                # The name of the model to find responses for. This is a query parameter used to
                # select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :model

                # Optional reasoning effort parameter. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(OpenAI::ReasoningEffort::OrSymbol)) }
                attr_accessor :reasoning_effort

                # Sampling temperature. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :temperature

                # List of tool names. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :tools

                # Nucleus sampling parameter. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :top_p

                # The type of run data source. Always `responses`.
                sig { returns(Symbol) }
                attr_accessor :type

                # List of user identifiers. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :users

                sig do
                  override
                    .returns({
                      type: Symbol,
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort:
                        T.nilable(OpenAI::ReasoningEffort::OrSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String])
                    })
                end
                def to_hash; end

                class << self
                  # A EvalResponsesSource object describing a run data source configuration.
                  sig do
                    params(
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String]),
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    created_after: nil, # Only include items created after this timestamp (inclusive). This is a query
                                        # parameter used to select responses.
                    created_before: nil, # Only include items created before this timestamp (inclusive). This is a query
                                         # parameter used to select responses.
                    instructions_search: nil, # Optional string to search the 'instructions' field. This is a query parameter
                                              # used to select responses.
                    metadata: nil, # Metadata filter for the responses. This is a query parameter used to select
                                   # responses.
                    model: nil, # The name of the model to find responses for. This is a query parameter used to
                                # select responses.
                    reasoning_effort: nil, # Optional reasoning effort parameter. This is a query parameter used to select
                                           # responses.
                    temperature: nil, # Sampling temperature. This is a query parameter used to select responses.
                    tools: nil, # List of tool names. This is a query parameter used to select responses.
                    top_p: nil, # Nucleus sampling parameter. This is a query parameter used to select responses.
                    users: nil, # List of user identifiers. This is a query parameter used to select responses.
                    type: :responses # The type of run data source. Always `responses`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::Responses,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileContent,
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::FileID,
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Source::Responses
                  )
                end
            end

            # The type of run data source. Always `responses`.
            module Type
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Type::TaggedSymbol
                  ])
                end
                def values; end
              end

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              RESPONSES = T.let(
                  :responses,
                  OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Type::TaggedSymbol
                )

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::Type
                  )
                end
            end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource,
                OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource
              )
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Evals::RunCreateParams, OpenAI::Internal::AnyHash)
          end
      end

      class RunCreateResponse < OpenAI::Internal::Type::BaseModel
        # Unix timestamp (in seconds) when the evaluation run was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Information about the run's data source.
        sig { returns(OpenAI::Models::Evals::RunCreateResponse::DataSource::Variants) }
        attr_accessor :data_source

        # An object representing an error response from the Eval API.
        sig { returns(OpenAI::Evals::EvalAPIError) }
        attr_reader :error

        sig { params(error: OpenAI::Evals::EvalAPIError::OrHash).void }
        attr_writer :error

        # The identifier of the associated evaluation.
        sig { returns(String) }
        attr_accessor :eval_id

        # Unique identifier for the evaluation run.
        sig { returns(String) }
        attr_accessor :id

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The model that is evaluated, if applicable.
        sig { returns(String) }
        attr_accessor :model

        # The name of the evaluation run.
        sig { returns(String) }
        attr_accessor :name

        # The type of the object. Always "eval.run".
        sig { returns(Symbol) }
        attr_accessor :object

        # Usage statistics for each model during the evaluation run.
        sig { returns(T::Array[OpenAI::Models::Evals::RunCreateResponse::PerModelUsage]) }
        attr_accessor :per_model_usage

        # Results per testing criteria applied during the evaluation run.
        sig do
          returns(T::Array[
              OpenAI::Models::Evals::RunCreateResponse::PerTestingCriteriaResult
            ])
        end
        attr_accessor :per_testing_criteria_results

        # The URL to the rendered evaluation run report on the UI dashboard.
        sig { returns(String) }
        attr_accessor :report_url

        # Counters summarizing the outcomes of the evaluation run.
        sig { returns(OpenAI::Models::Evals::RunCreateResponse::ResultCounts) }
        attr_reader :result_counts

        sig { params(result_counts: OpenAI::Models::Evals::RunCreateResponse::ResultCounts::OrHash).void }
        attr_writer :result_counts

        # The status of the evaluation run.
        sig { returns(String) }
        attr_accessor :status

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data_source:
                OpenAI::Models::Evals::RunCreateResponse::DataSource::Variants,
              error: OpenAI::Evals::EvalAPIError,
              eval_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: String,
              object: Symbol,
              per_model_usage:
                T::Array[
                  OpenAI::Models::Evals::RunCreateResponse::PerModelUsage
                ],
              per_testing_criteria_results:
                T::Array[
                  OpenAI::Models::Evals::RunCreateResponse::PerTestingCriteriaResult
                ],
              report_url: String,
              result_counts:
                OpenAI::Models::Evals::RunCreateResponse::ResultCounts,
              status: String
            })
        end
        def to_hash; end

        class << self
          # A schema representing an evaluation run.
          sig do
            params(
              id: String,
              created_at: Integer,
              data_source: T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::OrHash,
                OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::OrHash
              ),
              error: OpenAI::Evals::EvalAPIError::OrHash,
              eval_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: String,
              per_model_usage: T::Array[
                OpenAI::Models::Evals::RunCreateResponse::PerModelUsage::OrHash
              ],
              per_testing_criteria_results: T::Array[
                OpenAI::Models::Evals::RunCreateResponse::PerTestingCriteriaResult::OrHash
              ],
              report_url: String,
              result_counts: OpenAI::Models::Evals::RunCreateResponse::ResultCounts::OrHash,
              status: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # Unique identifier for the evaluation run.
            created_at:, # Unix timestamp (in seconds) when the evaluation run was created.
            data_source:, # Information about the run's data source.
            error:, # An object representing an error response from the Eval API.
            eval_id:, # The identifier of the associated evaluation.
            metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
            model:, # The model that is evaluated, if applicable.
            name:, # The name of the evaluation run.
            per_model_usage:, # Usage statistics for each model during the evaluation run.
            per_testing_criteria_results:, # Results per testing criteria applied during the evaluation run.
            report_url:, # The URL to the rendered evaluation run report on the UI dashboard.
            result_counts:, # Counters summarizing the outcomes of the evaluation run.
            status:, # The status of the evaluation run.
            object: :"eval.run" # The type of the object. Always "eval.run".
); end
        end

        # Information about the run's data source.
        module DataSource
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Models::Evals::RunCreateResponse::DataSource::Variants
              ])
            end
            def variants; end
          end

          class Responses < OpenAI::Internal::Type::BaseModel
            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            sig do
              returns(T.nilable(
                  OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Variants
                ))
            end
            attr_reader :input_messages

            sig do
              params(
                input_messages: T.any(
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::OrHash,
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::ItemReference::OrHash
                  )
              ).void
            end
            attr_writer :input_messages

            # The name of the model to use for generating completions (e.g. "o3-mini").
            sig { returns(T.nilable(String)) }
            attr_reader :model

            sig { params(model: String).void }
            attr_writer :model

            sig do
              returns(T.nilable(
                  OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams
                ))
            end
            attr_reader :sampling_params

            sig do
              params(
                sampling_params: OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams::OrHash
              ).void
            end
            attr_writer :sampling_params

            # Determines what populates the `item` namespace in this run's data source.
            sig { returns(OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::Variants) }
            attr_accessor :source

            # The type of run data source. Always `responses`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  source:
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::Variants,
                  type: Symbol,
                  input_messages:
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Variants,
                  model: String,
                  sampling_params:
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams
                })
            end
            def to_hash; end

            class << self
              # A ResponsesRunDataSource object describing a model sampling configuration.
              sig do
                params(
                  source: T.any(
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileContent::OrHash,
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileID::OrHash,
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::Responses::OrHash
                  ),
                  input_messages: T.any(
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::OrHash,
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::ItemReference::OrHash
                  ),
                  model: String,
                  sampling_params: OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                source:, # Determines what populates the `item` namespace in this run's data source.
                input_messages: nil, # Used when sampling from a model. Dictates the structure of the messages passed
                                     # into the model. Can either be a reference to a prebuilt trajectory (ie,
                                     # `item.input_trajectory`), or a template with variable references to the `item`
                                     # namespace.
                model: nil, # The name of the model to use for generating completions (e.g. "o3-mini").
                sampling_params: nil,
                type: :responses # The type of run data source. Always `responses`.
); end
            end

            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            module InputMessages
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Variants
                  ])
                end
                def variants; end
              end

              class ItemReference < OpenAI::Internal::Type::BaseModel
                # A reference to a variable in the `item` namespace. Ie, "item.name"
                sig { returns(String) }
                attr_accessor :item_reference

                # The type of input messages. Always `item_reference`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ item_reference: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(item_reference: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    item_reference:, # A reference to a variable in the `item` namespace. Ie, "item.name"
                    type: :item_reference # The type of input messages. Always `item_reference`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::ItemReference,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Template < OpenAI::Internal::Type::BaseModel
                # A list of chat messages forming the prompt or context. May include variable
                # references to the `item` namespace, ie {{item.name}}.
                sig do
                  returns(T::Array[
                      OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                    ])
                end
                attr_accessor :template

                # The type of input messages. Always `template`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      template:
                        T::Array[
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      template: T::Array[
                        T.any(
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage::OrHash,
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::OrHash
                        )
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    template:, # A list of chat messages forming the prompt or context. May include variable
                               # references to the `item` namespace, ie {{item.name}}.
                    type: :template # The type of input messages. Always `template`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template,
                      OpenAI::Internal::AnyHash
                    )
                  end

                # A message input to the model with a role indicating instruction following
                # hierarchy. Instructions given with the `developer` or `system` role take
                # precedence over instructions given with the `user` role. Messages with the
                # `assistant` role are presumed to have been generated by the model in previous
                # interactions.
                module Template
                  extend OpenAI::Internal::Type::Union

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                      ])
                    end
                    def variants; end
                  end

                  class ChatMessage < OpenAI::Internal::Type::BaseModel
                    # The content of the message.
                    sig { returns(String) }
                    attr_accessor :content

                    # The role of the message (e.g. "system", "assistant", "user").
                    sig { returns(String) }
                    attr_accessor :role

                    sig { override.returns({ content: String, role: String }) }
                    def to_hash; end

                    class << self
                      sig { params(content: String, role: String).returns(T.attached_class) }
                      def new(
                        content:, # The content of the message.
                        role: # The role of the message (e.g. "system", "assistant", "user").
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end

                  class EvalItem < OpenAI::Internal::Type::BaseModel
                    # Inputs to the model - can contain template strings.
                    sig do
                      returns(OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants)
                    end
                    attr_accessor :content

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    sig do
                      returns(OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol)
                    end
                    attr_accessor :role

                    # The type of the message input. Always `message`.
                    sig do
                      returns(T.nilable(
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        ))
                    end
                    attr_reader :type

                    sig do
                      params(
                        type: OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                      ).void
                    end
                    attr_writer :type

                    sig do
                      override
                        .returns({
                          content:
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants,
                          role:
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol,
                          type:
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        })
                    end
                    def to_hash; end

                    class << self
                      # A message input to the model with a role indicating instruction following
                      # hierarchy. Instructions given with the `developer` or `system` role take
                      # precedence over instructions given with the `user` role. Messages with the
                      # `assistant` role are presumed to have been generated by the model in previous
                      # interactions.
                      sig do
                        params(
                          content: T.any(
                            String,
                            OpenAI::Responses::ResponseInputText::OrHash,
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText::OrHash,
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage::OrHash,
                            T::Array[T.anything]
                          ),
                          role: OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::OrSymbol,
                          type: OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                        ).returns(T.attached_class)
                      end
                      def new(
                        content:, # Inputs to the model - can contain template strings.
                        role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                               # `developer`.
                        type: nil # The type of the message input. Always `message`.
); end
                    end

                    # Inputs to the model - can contain template strings.
                    module Content
                      extend OpenAI::Internal::Type::Union

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants
                          ])
                        end
                        def variants; end
                      end

                      AnArrayOfInputTextAndInputImageArray = T.let(
                          OpenAI::Internal::Type::ArrayOf[
                            OpenAI::Internal::Type::Unknown
                          ],
                          OpenAI::Internal::Type::Converter
                        )

                      class InputImage < OpenAI::Internal::Type::BaseModel
                        # The detail level of the image to be sent to the model. One of `high`, `low`, or
                        # `auto`. Defaults to `auto`.
                        sig { returns(T.nilable(String)) }
                        attr_reader :detail

                        sig { params(detail: String).void }
                        attr_writer :detail

                        # The URL of the image input.
                        sig { returns(String) }
                        attr_accessor :image_url

                        # The type of the image input. Always `input_image`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ image_url: String, type: Symbol, detail: String }) }
                        def to_hash; end

                        class << self
                          # An image input to the model.
                          sig { params(image_url: String, detail: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            image_url:, # The URL of the image input.
                            detail: nil, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                                         # `auto`. Defaults to `auto`.
                            type: :input_image # The type of the image input. Always `input_image`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      class OutputText < OpenAI::Internal::Type::BaseModel
                        # The text output from the model.
                        sig { returns(String) }
                        attr_accessor :text

                        # The type of the output text. Always `output_text`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ text: String, type: Symbol }) }
                        def to_hash; end

                        class << self
                          # A text output from the model.
                          sig { params(text: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            text:, # The text output from the model.
                            type: :output_text # The type of the output text. Always `output_text`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      Variants = T.type_alias do
                          T.any(
                            String,
                            OpenAI::Responses::ResponseInputText,
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText,
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage,
                            T::Array[T.anything]
                          )
                        end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem,
                          OpenAI::Internal::AnyHash
                        )
                      end

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    module Role
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      ASSISTANT = T.let(
                          :assistant,
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      DEVELOPER = T.let(
                          :developer,
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      SYSTEM = T.let(
                          :system,
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role
                          )
                        end

                      USER = T.let(
                          :user,
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )
                    end

                    # The type of the message input. Always `message`.
                    module Type
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      MESSAGE = T.let(
                          :message,
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type
                          )
                        end
                    end
                  end

                  Variants = T.type_alias do
                      T.any(
                        OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage,
                        OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem
                      )
                    end
                end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::Template,
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::InputMessages::ItemReference
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses,
                  OpenAI::Internal::AnyHash
                )
              end

            class SamplingParams < OpenAI::Internal::Type::BaseModel
              # The maximum number of tokens in the generated output.
              sig { returns(T.nilable(Integer)) }
              attr_reader :max_completion_tokens

              sig { params(max_completion_tokens: Integer).void }
              attr_writer :max_completion_tokens

              # A seed value to initialize the randomness, during sampling.
              sig { returns(T.nilable(Integer)) }
              attr_reader :seed

              sig { params(seed: Integer).void }
              attr_writer :seed

              # A higher temperature increases randomness in the outputs.
              sig { returns(T.nilable(Float)) }
              attr_reader :temperature

              sig { params(temperature: Float).void }
              attr_writer :temperature

              # Configuration options for a text response from the model. Can be plain text or
              # structured JSON data. Learn more:
              #
              # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
              # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
              sig do
                returns(T.nilable(
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams::Text
                  ))
              end
              attr_reader :text

              sig do
                params(
                  text: OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams::Text::OrHash
                ).void
              end
              attr_writer :text

              # An array of tools the model may call while generating a response. You can
              # specify which tool to use by setting the `tool_choice` parameter.
              #
              # The two categories of tools you can provide the model are:
              #
              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
              #   capabilities, like
              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
              #   Learn more about
              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
              #   the model to call your own code. Learn more about
              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
              sig { returns(T.nilable(T::Array[OpenAI::Responses::Tool::Variants])) }
              attr_reader :tools

              sig do
                params(
                  tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ]
                ).void
              end
              attr_writer :tools

              # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
              sig { returns(T.nilable(Float)) }
              attr_reader :top_p

              sig { params(top_p: Float).void }
              attr_writer :top_p

              sig do
                override
                  .returns({
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text:
                      OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams::Text,
                    tools: T::Array[OpenAI::Responses::Tool::Variants],
                    top_p: Float
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text: OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams::Text::OrHash,
                    tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ],
                    top_p: Float
                  ).returns(T.attached_class)
                end
                def new(
                  max_completion_tokens: nil, # The maximum number of tokens in the generated output.
                  seed: nil, # A seed value to initialize the randomness, during sampling.
                  temperature: nil, # A higher temperature increases randomness in the outputs.
                  text: nil, # Configuration options for a text response from the model. Can be plain text or
                             # structured JSON data. Learn more:
                             # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                             # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  tools: nil, # An array of tools the model may call while generating a response. You can
                              # specify which tool to use by setting the `tool_choice` parameter.
                              # The two categories of tools you can provide the model are:
                              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
                              #   capabilities, like
                              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
                              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
                              #   Learn more about
                              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
                              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
                              #   the model to call your own code. Learn more about
                              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
                  top_p: nil # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams,
                    OpenAI::Internal::AnyHash
                  )
                end

              class Text < OpenAI::Internal::Type::BaseModel
                # An object specifying the format that the model must output.
                #
                # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                # ensures the model will match your supplied JSON schema. Learn more in the
                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                #
                # The default format is `{ "type": "text" }` with no additional options.
                #
                # **Not recommended for gpt-4o and newer models:**
                #
                # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                # ensures the message the model generates is valid JSON. Using `json_schema` is
                # preferred for models that support it.
                sig do
                  returns(T.nilable(
                      OpenAI::Responses::ResponseFormatTextConfig::Variants
                    ))
                end
                attr_reader :format_

                sig do
                  params(
                    format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                  ).void
                end
                attr_writer :format_

                sig do
                  override
                    .returns({
                      format_:
                        OpenAI::Responses::ResponseFormatTextConfig::Variants
                    })
                end
                def to_hash; end

                class << self
                  # Configuration options for a text response from the model. Can be plain text or
                  # structured JSON data. Learn more:
                  #
                  # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                  # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  sig do
                    params(
                      format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                    ).returns(T.attached_class)
                  end
                  def new(
                    format_: nil # An object specifying the format that the model must output.
                                 # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                                 # ensures the model will match your supplied JSON schema. Learn more in the
                                 # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                 # The default format is `{ "type": "text" }` with no additional options.
                                 # **Not recommended for gpt-4o and newer models:**
                                 # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                                 # ensures the message the model generates is valid JSON. Using `json_schema` is
                                 # preferred for models that support it.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::SamplingParams::Text,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end
            end

            # Determines what populates the `item` namespace in this run's data source.
            module Source
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::Variants
                  ])
                end
                def variants; end
              end

              class FileContent < OpenAI::Internal::Type::BaseModel
                # The content of the jsonl file.
                sig do
                  returns(T::Array[
                      OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileContent::Content
                    ])
                end
                attr_accessor :content

                # The type of jsonl source. Always `file_content`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      content:
                        T::Array[
                          OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileContent::Content
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      content: T::Array[
                        OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileContent::Content::OrHash
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    content:, # The content of the jsonl file.
                    type: :file_content # The type of jsonl source. Always `file_content`.
); end
                end

                class Content < OpenAI::Internal::Type::BaseModel
                  sig { returns(T::Hash[Symbol, T.anything]) }
                  attr_accessor :item

                  sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
                  attr_reader :sample

                  sig { params(sample: T::Hash[Symbol, T.anything]).void }
                  attr_writer :sample

                  sig do
                    override
                      .returns({
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      })
                  end
                  def to_hash; end

                  class << self
                    sig do
                      params(
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      ).returns(T.attached_class)
                    end
                    def new(item:, sample: nil); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileContent::Content,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileContent,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class FileID < OpenAI::Internal::Type::BaseModel
                # The identifier of the file.
                sig { returns(String) }
                attr_accessor :id

                # The type of jsonl source. Always `file_id`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ id: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(id: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    id:, # The identifier of the file.
                    type: :file_id # The type of jsonl source. Always `file_id`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileID,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Responses < OpenAI::Internal::Type::BaseModel
                # Only include items created after this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_after

                # Only include items created before this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_before

                # Optional string to search the 'instructions' field. This is a query parameter
                # used to select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :instructions_search

                # Metadata filter for the responses. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(T.anything)) }
                attr_accessor :metadata

                # The name of the model to find responses for. This is a query parameter used to
                # select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :model

                # Optional reasoning effort parameter. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(OpenAI::ReasoningEffort::TaggedSymbol)) }
                attr_accessor :reasoning_effort

                # Sampling temperature. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :temperature

                # List of tool names. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :tools

                # Nucleus sampling parameter. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :top_p

                # The type of run data source. Always `responses`.
                sig { returns(Symbol) }
                attr_accessor :type

                # List of user identifiers. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :users

                sig do
                  override
                    .returns({
                      type: Symbol,
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort:
                        T.nilable(OpenAI::ReasoningEffort::TaggedSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String])
                    })
                end
                def to_hash; end

                class << self
                  # A EvalResponsesSource object describing a run data source configuration.
                  sig do
                    params(
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String]),
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    created_after: nil, # Only include items created after this timestamp (inclusive). This is a query
                                        # parameter used to select responses.
                    created_before: nil, # Only include items created before this timestamp (inclusive). This is a query
                                         # parameter used to select responses.
                    instructions_search: nil, # Optional string to search the 'instructions' field. This is a query parameter
                                              # used to select responses.
                    metadata: nil, # Metadata filter for the responses. This is a query parameter used to select
                                   # responses.
                    model: nil, # The name of the model to find responses for. This is a query parameter used to
                                # select responses.
                    reasoning_effort: nil, # Optional reasoning effort parameter. This is a query parameter used to select
                                           # responses.
                    temperature: nil, # Sampling temperature. This is a query parameter used to select responses.
                    tools: nil, # List of tool names. This is a query parameter used to select responses.
                    top_p: nil, # Nucleus sampling parameter. This is a query parameter used to select responses.
                    users: nil, # List of user identifiers. This is a query parameter used to select responses.
                    type: :responses # The type of run data source. Always `responses`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::Responses,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileContent,
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::FileID,
                    OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses::Source::Responses
                  )
                end
            end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource,
                OpenAI::Models::Evals::RunCreateResponse::DataSource::Responses
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::Evals::RunCreateResponse,
              OpenAI::Internal::AnyHash
            )
          end

        class PerModelUsage < OpenAI::Internal::Type::BaseModel
          # The number of tokens retrieved from cache.
          sig { returns(Integer) }
          attr_accessor :cached_tokens

          # The number of completion tokens generated.
          sig { returns(Integer) }
          attr_accessor :completion_tokens

          # The number of invocations.
          sig { returns(Integer) }
          attr_accessor :invocation_count

          # The name of the model.
          sig { returns(String) }
          attr_accessor :model_name

          # The number of prompt tokens used.
          sig { returns(Integer) }
          attr_accessor :prompt_tokens

          # The total number of tokens used.
          sig { returns(Integer) }
          attr_accessor :total_tokens

          sig do
            override
              .returns({
                cached_tokens: Integer,
                completion_tokens: Integer,
                invocation_count: Integer,
                model_name: String,
                prompt_tokens: Integer,
                total_tokens: Integer
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                cached_tokens: Integer,
                completion_tokens: Integer,
                invocation_count: Integer,
                model_name: String,
                prompt_tokens: Integer,
                total_tokens: Integer
              ).returns(T.attached_class)
            end
            def new(
              cached_tokens:, # The number of tokens retrieved from cache.
              completion_tokens:, # The number of completion tokens generated.
              invocation_count:, # The number of invocations.
              model_name:, # The name of the model.
              prompt_tokens:, # The number of prompt tokens used.
              total_tokens: # The total number of tokens used.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunCreateResponse::PerModelUsage,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class PerTestingCriteriaResult < OpenAI::Internal::Type::BaseModel
          # Number of tests failed for this criteria.
          sig { returns(Integer) }
          attr_accessor :failed

          # Number of tests passed for this criteria.
          sig { returns(Integer) }
          attr_accessor :passed

          # A description of the testing criteria.
          sig { returns(String) }
          attr_accessor :testing_criteria

          sig { override.returns({ failed: Integer, passed: Integer, testing_criteria: String }) }
          def to_hash; end

          class << self
            sig { params(failed: Integer, passed: Integer, testing_criteria: String).returns(T.attached_class) }
            def new(
              failed:, # Number of tests failed for this criteria.
              passed:, # Number of tests passed for this criteria.
              testing_criteria: # A description of the testing criteria.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunCreateResponse::PerTestingCriteriaResult,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ResultCounts < OpenAI::Internal::Type::BaseModel
          # Number of output items that resulted in an error.
          sig { returns(Integer) }
          attr_accessor :errored

          # Number of output items that failed to pass the evaluation.
          sig { returns(Integer) }
          attr_accessor :failed

          # Number of output items that passed the evaluation.
          sig { returns(Integer) }
          attr_accessor :passed

          # Total number of executed output items.
          sig { returns(Integer) }
          attr_accessor :total

          sig do
            override
              .returns({
                errored: Integer,
                failed: Integer,
                passed: Integer,
                total: Integer
              })
          end
          def to_hash; end

          class << self
            # Counters summarizing the outcomes of the evaluation run.
            sig do
              params(
                errored: Integer,
                failed: Integer,
                passed: Integer,
                total: Integer
              ).returns(T.attached_class)
            end
            def new(
              errored:, # Number of output items that resulted in an error.
              failed:, # Number of output items that failed to pass the evaluation.
              passed:, # Number of output items that passed the evaluation.
              total: # Total number of executed output items.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunCreateResponse::ResultCounts,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class RunDeleteParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :eval_id

        sig { override.returns({ eval_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(eval_id: String, request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(eval_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Evals::RunDeleteParams, OpenAI::Internal::AnyHash)
          end
      end

      class RunDeleteResponse < OpenAI::Internal::Type::BaseModel
        sig { returns(T.nilable(T::Boolean)) }
        attr_reader :deleted

        sig { params(deleted: T::Boolean).void }
        attr_writer :deleted

        sig { returns(T.nilable(String)) }
        attr_reader :object

        sig { params(object: String).void }
        attr_writer :object

        sig { returns(T.nilable(String)) }
        attr_reader :run_id

        sig { params(run_id: String).void }
        attr_writer :run_id

        sig { override.returns({ deleted: T::Boolean, object: String, run_id: String }) }
        def to_hash; end

        class << self
          sig { params(deleted: T::Boolean, object: String, run_id: String).returns(T.attached_class) }
          def new(deleted: nil, object: nil, run_id: nil); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::Evals::RunDeleteResponse,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class RunListParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Identifier for the last run from the previous pagination request.
        sig { returns(T.nilable(String)) }
        attr_reader :after

        sig { params(after: String).void }
        attr_writer :after

        # Number of runs to retrieve.
        sig { returns(T.nilable(Integer)) }
        attr_reader :limit

        sig { params(limit: Integer).void }
        attr_writer :limit

        # Sort order for runs by timestamp. Use `asc` for ascending order or `desc` for
        # descending order. Defaults to `asc`.
        sig { returns(T.nilable(OpenAI::Evals::RunListParams::Order::OrSymbol)) }
        attr_reader :order

        sig { params(order: OpenAI::Evals::RunListParams::Order::OrSymbol).void }
        attr_writer :order

        # Filter runs by status. One of `queued` | `in_progress` | `failed` | `completed`
        # | `canceled`.
        sig { returns(T.nilable(OpenAI::Evals::RunListParams::Status::OrSymbol)) }
        attr_reader :status

        sig { params(status: OpenAI::Evals::RunListParams::Status::OrSymbol).void }
        attr_writer :status

        sig do
          override
            .returns({
              after: String,
              limit: Integer,
              order: OpenAI::Evals::RunListParams::Order::OrSymbol,
              status: OpenAI::Evals::RunListParams::Status::OrSymbol,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              after: String,
              limit: Integer,
              order: OpenAI::Evals::RunListParams::Order::OrSymbol,
              status: OpenAI::Evals::RunListParams::Status::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            after: nil, # Identifier for the last run from the previous pagination request.
            limit: nil, # Number of runs to retrieve.
            order: nil, # Sort order for runs by timestamp. Use `asc` for ascending order or `desc` for
                        # descending order. Defaults to `asc`.
            status: nil, # Filter runs by status. One of `queued` | `in_progress` | `failed` | `completed`
                         # | `canceled`.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Evals::RunListParams, OpenAI::Internal::AnyHash)
          end

        # Sort order for runs by timestamp. Use `asc` for ascending order or `desc` for
        # descending order. Defaults to `asc`.
        module Order
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Evals::RunListParams::Order::TaggedSymbol]) }
            def values; end
          end

          ASC = T.let(:asc, OpenAI::Evals::RunListParams::Order::TaggedSymbol)
          DESC = T.let(:desc, OpenAI::Evals::RunListParams::Order::TaggedSymbol)
          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Evals::RunListParams::Order) }
        end

        # Filter runs by status. One of `queued` | `in_progress` | `failed` | `completed`
        # | `canceled`.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Evals::RunListParams::Status::TaggedSymbol]) }
            def values; end
          end

          CANCELED = T.let(:canceled, OpenAI::Evals::RunListParams::Status::TaggedSymbol)

          COMPLETED = T.let(
              :completed,
              OpenAI::Evals::RunListParams::Status::TaggedSymbol
            )

          FAILED = T.let(:failed, OpenAI::Evals::RunListParams::Status::TaggedSymbol)

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Evals::RunListParams::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          QUEUED = T.let(:queued, OpenAI::Evals::RunListParams::Status::TaggedSymbol)

          TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Evals::RunListParams::Status) }
        end
      end

      class RunListResponse < OpenAI::Internal::Type::BaseModel
        # Unix timestamp (in seconds) when the evaluation run was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Information about the run's data source.
        sig { returns(OpenAI::Models::Evals::RunListResponse::DataSource::Variants) }
        attr_accessor :data_source

        # An object representing an error response from the Eval API.
        sig { returns(OpenAI::Evals::EvalAPIError) }
        attr_reader :error

        sig { params(error: OpenAI::Evals::EvalAPIError::OrHash).void }
        attr_writer :error

        # The identifier of the associated evaluation.
        sig { returns(String) }
        attr_accessor :eval_id

        # Unique identifier for the evaluation run.
        sig { returns(String) }
        attr_accessor :id

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The model that is evaluated, if applicable.
        sig { returns(String) }
        attr_accessor :model

        # The name of the evaluation run.
        sig { returns(String) }
        attr_accessor :name

        # The type of the object. Always "eval.run".
        sig { returns(Symbol) }
        attr_accessor :object

        # Usage statistics for each model during the evaluation run.
        sig { returns(T::Array[OpenAI::Models::Evals::RunListResponse::PerModelUsage]) }
        attr_accessor :per_model_usage

        # Results per testing criteria applied during the evaluation run.
        sig do
          returns(T::Array[
              OpenAI::Models::Evals::RunListResponse::PerTestingCriteriaResult
            ])
        end
        attr_accessor :per_testing_criteria_results

        # The URL to the rendered evaluation run report on the UI dashboard.
        sig { returns(String) }
        attr_accessor :report_url

        # Counters summarizing the outcomes of the evaluation run.
        sig { returns(OpenAI::Models::Evals::RunListResponse::ResultCounts) }
        attr_reader :result_counts

        sig { params(result_counts: OpenAI::Models::Evals::RunListResponse::ResultCounts::OrHash).void }
        attr_writer :result_counts

        # The status of the evaluation run.
        sig { returns(String) }
        attr_accessor :status

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data_source:
                OpenAI::Models::Evals::RunListResponse::DataSource::Variants,
              error: OpenAI::Evals::EvalAPIError,
              eval_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: String,
              object: Symbol,
              per_model_usage:
                T::Array[OpenAI::Models::Evals::RunListResponse::PerModelUsage],
              per_testing_criteria_results:
                T::Array[
                  OpenAI::Models::Evals::RunListResponse::PerTestingCriteriaResult
                ],
              report_url: String,
              result_counts:
                OpenAI::Models::Evals::RunListResponse::ResultCounts,
              status: String
            })
        end
        def to_hash; end

        class << self
          # A schema representing an evaluation run.
          sig do
            params(
              id: String,
              created_at: Integer,
              data_source: T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::OrHash,
                OpenAI::Models::Evals::RunListResponse::DataSource::Responses::OrHash
              ),
              error: OpenAI::Evals::EvalAPIError::OrHash,
              eval_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: String,
              per_model_usage: T::Array[
                OpenAI::Models::Evals::RunListResponse::PerModelUsage::OrHash
              ],
              per_testing_criteria_results: T::Array[
                OpenAI::Models::Evals::RunListResponse::PerTestingCriteriaResult::OrHash
              ],
              report_url: String,
              result_counts: OpenAI::Models::Evals::RunListResponse::ResultCounts::OrHash,
              status: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # Unique identifier for the evaluation run.
            created_at:, # Unix timestamp (in seconds) when the evaluation run was created.
            data_source:, # Information about the run's data source.
            error:, # An object representing an error response from the Eval API.
            eval_id:, # The identifier of the associated evaluation.
            metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
            model:, # The model that is evaluated, if applicable.
            name:, # The name of the evaluation run.
            per_model_usage:, # Usage statistics for each model during the evaluation run.
            per_testing_criteria_results:, # Results per testing criteria applied during the evaluation run.
            report_url:, # The URL to the rendered evaluation run report on the UI dashboard.
            result_counts:, # Counters summarizing the outcomes of the evaluation run.
            status:, # The status of the evaluation run.
            object: :"eval.run" # The type of the object. Always "eval.run".
); end
        end

        # Information about the run's data source.
        module DataSource
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Models::Evals::RunListResponse::DataSource::Variants
              ])
            end
            def variants; end
          end

          class Responses < OpenAI::Internal::Type::BaseModel
            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            sig do
              returns(T.nilable(
                  OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Variants
                ))
            end
            attr_reader :input_messages

            sig do
              params(
                input_messages: T.any(
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::OrHash,
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::ItemReference::OrHash
                  )
              ).void
            end
            attr_writer :input_messages

            # The name of the model to use for generating completions (e.g. "o3-mini").
            sig { returns(T.nilable(String)) }
            attr_reader :model

            sig { params(model: String).void }
            attr_writer :model

            sig do
              returns(T.nilable(
                  OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams
                ))
            end
            attr_reader :sampling_params

            sig do
              params(
                sampling_params: OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams::OrHash
              ).void
            end
            attr_writer :sampling_params

            # Determines what populates the `item` namespace in this run's data source.
            sig { returns(OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::Variants) }
            attr_accessor :source

            # The type of run data source. Always `responses`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  source:
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::Variants,
                  type: Symbol,
                  input_messages:
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Variants,
                  model: String,
                  sampling_params:
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams
                })
            end
            def to_hash; end

            class << self
              # A ResponsesRunDataSource object describing a model sampling configuration.
              sig do
                params(
                  source: T.any(
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileContent::OrHash,
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileID::OrHash,
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::Responses::OrHash
                  ),
                  input_messages: T.any(
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::OrHash,
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::ItemReference::OrHash
                  ),
                  model: String,
                  sampling_params: OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                source:, # Determines what populates the `item` namespace in this run's data source.
                input_messages: nil, # Used when sampling from a model. Dictates the structure of the messages passed
                                     # into the model. Can either be a reference to a prebuilt trajectory (ie,
                                     # `item.input_trajectory`), or a template with variable references to the `item`
                                     # namespace.
                model: nil, # The name of the model to use for generating completions (e.g. "o3-mini").
                sampling_params: nil,
                type: :responses # The type of run data source. Always `responses`.
); end
            end

            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            module InputMessages
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Variants
                  ])
                end
                def variants; end
              end

              class ItemReference < OpenAI::Internal::Type::BaseModel
                # A reference to a variable in the `item` namespace. Ie, "item.name"
                sig { returns(String) }
                attr_accessor :item_reference

                # The type of input messages. Always `item_reference`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ item_reference: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(item_reference: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    item_reference:, # A reference to a variable in the `item` namespace. Ie, "item.name"
                    type: :item_reference # The type of input messages. Always `item_reference`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::ItemReference,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Template < OpenAI::Internal::Type::BaseModel
                # A list of chat messages forming the prompt or context. May include variable
                # references to the `item` namespace, ie {{item.name}}.
                sig do
                  returns(T::Array[
                      OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                    ])
                end
                attr_accessor :template

                # The type of input messages. Always `template`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      template:
                        T::Array[
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      template: T::Array[
                        T.any(
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage::OrHash,
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::OrHash
                        )
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    template:, # A list of chat messages forming the prompt or context. May include variable
                               # references to the `item` namespace, ie {{item.name}}.
                    type: :template # The type of input messages. Always `template`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template,
                      OpenAI::Internal::AnyHash
                    )
                  end

                # A message input to the model with a role indicating instruction following
                # hierarchy. Instructions given with the `developer` or `system` role take
                # precedence over instructions given with the `user` role. Messages with the
                # `assistant` role are presumed to have been generated by the model in previous
                # interactions.
                module Template
                  extend OpenAI::Internal::Type::Union

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                      ])
                    end
                    def variants; end
                  end

                  class ChatMessage < OpenAI::Internal::Type::BaseModel
                    # The content of the message.
                    sig { returns(String) }
                    attr_accessor :content

                    # The role of the message (e.g. "system", "assistant", "user").
                    sig { returns(String) }
                    attr_accessor :role

                    sig { override.returns({ content: String, role: String }) }
                    def to_hash; end

                    class << self
                      sig { params(content: String, role: String).returns(T.attached_class) }
                      def new(
                        content:, # The content of the message.
                        role: # The role of the message (e.g. "system", "assistant", "user").
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end

                  class EvalItem < OpenAI::Internal::Type::BaseModel
                    # Inputs to the model - can contain template strings.
                    sig do
                      returns(OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants)
                    end
                    attr_accessor :content

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    sig do
                      returns(OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol)
                    end
                    attr_accessor :role

                    # The type of the message input. Always `message`.
                    sig do
                      returns(T.nilable(
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        ))
                    end
                    attr_reader :type

                    sig do
                      params(
                        type: OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                      ).void
                    end
                    attr_writer :type

                    sig do
                      override
                        .returns({
                          content:
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants,
                          role:
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol,
                          type:
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        })
                    end
                    def to_hash; end

                    class << self
                      # A message input to the model with a role indicating instruction following
                      # hierarchy. Instructions given with the `developer` or `system` role take
                      # precedence over instructions given with the `user` role. Messages with the
                      # `assistant` role are presumed to have been generated by the model in previous
                      # interactions.
                      sig do
                        params(
                          content: T.any(
                            String,
                            OpenAI::Responses::ResponseInputText::OrHash,
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText::OrHash,
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage::OrHash,
                            T::Array[T.anything]
                          ),
                          role: OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::OrSymbol,
                          type: OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                        ).returns(T.attached_class)
                      end
                      def new(
                        content:, # Inputs to the model - can contain template strings.
                        role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                               # `developer`.
                        type: nil # The type of the message input. Always `message`.
); end
                    end

                    # Inputs to the model - can contain template strings.
                    module Content
                      extend OpenAI::Internal::Type::Union

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants
                          ])
                        end
                        def variants; end
                      end

                      AnArrayOfInputTextAndInputImageArray = T.let(
                          OpenAI::Internal::Type::ArrayOf[
                            OpenAI::Internal::Type::Unknown
                          ],
                          OpenAI::Internal::Type::Converter
                        )

                      class InputImage < OpenAI::Internal::Type::BaseModel
                        # The detail level of the image to be sent to the model. One of `high`, `low`, or
                        # `auto`. Defaults to `auto`.
                        sig { returns(T.nilable(String)) }
                        attr_reader :detail

                        sig { params(detail: String).void }
                        attr_writer :detail

                        # The URL of the image input.
                        sig { returns(String) }
                        attr_accessor :image_url

                        # The type of the image input. Always `input_image`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ image_url: String, type: Symbol, detail: String }) }
                        def to_hash; end

                        class << self
                          # An image input to the model.
                          sig { params(image_url: String, detail: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            image_url:, # The URL of the image input.
                            detail: nil, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                                         # `auto`. Defaults to `auto`.
                            type: :input_image # The type of the image input. Always `input_image`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      class OutputText < OpenAI::Internal::Type::BaseModel
                        # The text output from the model.
                        sig { returns(String) }
                        attr_accessor :text

                        # The type of the output text. Always `output_text`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ text: String, type: Symbol }) }
                        def to_hash; end

                        class << self
                          # A text output from the model.
                          sig { params(text: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            text:, # The text output from the model.
                            type: :output_text # The type of the output text. Always `output_text`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      Variants = T.type_alias do
                          T.any(
                            String,
                            OpenAI::Responses::ResponseInputText,
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText,
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage,
                            T::Array[T.anything]
                          )
                        end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem,
                          OpenAI::Internal::AnyHash
                        )
                      end

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    module Role
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      ASSISTANT = T.let(
                          :assistant,
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      DEVELOPER = T.let(
                          :developer,
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      SYSTEM = T.let(
                          :system,
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role
                          )
                        end

                      USER = T.let(
                          :user,
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )
                    end

                    # The type of the message input. Always `message`.
                    module Type
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      MESSAGE = T.let(
                          :message,
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type
                          )
                        end
                    end
                  end

                  Variants = T.type_alias do
                      T.any(
                        OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage,
                        OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem
                      )
                    end
                end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::Template,
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::InputMessages::ItemReference
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Models::Evals::RunListResponse::DataSource::Responses,
                  OpenAI::Internal::AnyHash
                )
              end

            class SamplingParams < OpenAI::Internal::Type::BaseModel
              # The maximum number of tokens in the generated output.
              sig { returns(T.nilable(Integer)) }
              attr_reader :max_completion_tokens

              sig { params(max_completion_tokens: Integer).void }
              attr_writer :max_completion_tokens

              # A seed value to initialize the randomness, during sampling.
              sig { returns(T.nilable(Integer)) }
              attr_reader :seed

              sig { params(seed: Integer).void }
              attr_writer :seed

              # A higher temperature increases randomness in the outputs.
              sig { returns(T.nilable(Float)) }
              attr_reader :temperature

              sig { params(temperature: Float).void }
              attr_writer :temperature

              # Configuration options for a text response from the model. Can be plain text or
              # structured JSON data. Learn more:
              #
              # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
              # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
              sig do
                returns(T.nilable(
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams::Text
                  ))
              end
              attr_reader :text

              sig do
                params(
                  text: OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams::Text::OrHash
                ).void
              end
              attr_writer :text

              # An array of tools the model may call while generating a response. You can
              # specify which tool to use by setting the `tool_choice` parameter.
              #
              # The two categories of tools you can provide the model are:
              #
              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
              #   capabilities, like
              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
              #   Learn more about
              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
              #   the model to call your own code. Learn more about
              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
              sig { returns(T.nilable(T::Array[OpenAI::Responses::Tool::Variants])) }
              attr_reader :tools

              sig do
                params(
                  tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ]
                ).void
              end
              attr_writer :tools

              # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
              sig { returns(T.nilable(Float)) }
              attr_reader :top_p

              sig { params(top_p: Float).void }
              attr_writer :top_p

              sig do
                override
                  .returns({
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text:
                      OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams::Text,
                    tools: T::Array[OpenAI::Responses::Tool::Variants],
                    top_p: Float
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text: OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams::Text::OrHash,
                    tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ],
                    top_p: Float
                  ).returns(T.attached_class)
                end
                def new(
                  max_completion_tokens: nil, # The maximum number of tokens in the generated output.
                  seed: nil, # A seed value to initialize the randomness, during sampling.
                  temperature: nil, # A higher temperature increases randomness in the outputs.
                  text: nil, # Configuration options for a text response from the model. Can be plain text or
                             # structured JSON data. Learn more:
                             # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                             # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  tools: nil, # An array of tools the model may call while generating a response. You can
                              # specify which tool to use by setting the `tool_choice` parameter.
                              # The two categories of tools you can provide the model are:
                              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
                              #   capabilities, like
                              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
                              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
                              #   Learn more about
                              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
                              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
                              #   the model to call your own code. Learn more about
                              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
                  top_p: nil # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams,
                    OpenAI::Internal::AnyHash
                  )
                end

              class Text < OpenAI::Internal::Type::BaseModel
                # An object specifying the format that the model must output.
                #
                # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                # ensures the model will match your supplied JSON schema. Learn more in the
                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                #
                # The default format is `{ "type": "text" }` with no additional options.
                #
                # **Not recommended for gpt-4o and newer models:**
                #
                # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                # ensures the message the model generates is valid JSON. Using `json_schema` is
                # preferred for models that support it.
                sig do
                  returns(T.nilable(
                      OpenAI::Responses::ResponseFormatTextConfig::Variants
                    ))
                end
                attr_reader :format_

                sig do
                  params(
                    format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                  ).void
                end
                attr_writer :format_

                sig do
                  override
                    .returns({
                      format_:
                        OpenAI::Responses::ResponseFormatTextConfig::Variants
                    })
                end
                def to_hash; end

                class << self
                  # Configuration options for a text response from the model. Can be plain text or
                  # structured JSON data. Learn more:
                  #
                  # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                  # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  sig do
                    params(
                      format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                    ).returns(T.attached_class)
                  end
                  def new(
                    format_: nil # An object specifying the format that the model must output.
                                 # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                                 # ensures the model will match your supplied JSON schema. Learn more in the
                                 # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                 # The default format is `{ "type": "text" }` with no additional options.
                                 # **Not recommended for gpt-4o and newer models:**
                                 # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                                 # ensures the message the model generates is valid JSON. Using `json_schema` is
                                 # preferred for models that support it.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunListResponse::DataSource::Responses::SamplingParams::Text,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end
            end

            # Determines what populates the `item` namespace in this run's data source.
            module Source
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::Variants
                  ])
                end
                def variants; end
              end

              class FileContent < OpenAI::Internal::Type::BaseModel
                # The content of the jsonl file.
                sig do
                  returns(T::Array[
                      OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileContent::Content
                    ])
                end
                attr_accessor :content

                # The type of jsonl source. Always `file_content`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      content:
                        T::Array[
                          OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileContent::Content
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      content: T::Array[
                        OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileContent::Content::OrHash
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    content:, # The content of the jsonl file.
                    type: :file_content # The type of jsonl source. Always `file_content`.
); end
                end

                class Content < OpenAI::Internal::Type::BaseModel
                  sig { returns(T::Hash[Symbol, T.anything]) }
                  attr_accessor :item

                  sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
                  attr_reader :sample

                  sig { params(sample: T::Hash[Symbol, T.anything]).void }
                  attr_writer :sample

                  sig do
                    override
                      .returns({
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      })
                  end
                  def to_hash; end

                  class << self
                    sig do
                      params(
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      ).returns(T.attached_class)
                    end
                    def new(item:, sample: nil); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileContent::Content,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileContent,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class FileID < OpenAI::Internal::Type::BaseModel
                # The identifier of the file.
                sig { returns(String) }
                attr_accessor :id

                # The type of jsonl source. Always `file_id`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ id: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(id: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    id:, # The identifier of the file.
                    type: :file_id # The type of jsonl source. Always `file_id`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileID,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Responses < OpenAI::Internal::Type::BaseModel
                # Only include items created after this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_after

                # Only include items created before this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_before

                # Optional string to search the 'instructions' field. This is a query parameter
                # used to select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :instructions_search

                # Metadata filter for the responses. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(T.anything)) }
                attr_accessor :metadata

                # The name of the model to find responses for. This is a query parameter used to
                # select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :model

                # Optional reasoning effort parameter. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(OpenAI::ReasoningEffort::TaggedSymbol)) }
                attr_accessor :reasoning_effort

                # Sampling temperature. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :temperature

                # List of tool names. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :tools

                # Nucleus sampling parameter. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :top_p

                # The type of run data source. Always `responses`.
                sig { returns(Symbol) }
                attr_accessor :type

                # List of user identifiers. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :users

                sig do
                  override
                    .returns({
                      type: Symbol,
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort:
                        T.nilable(OpenAI::ReasoningEffort::TaggedSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String])
                    })
                end
                def to_hash; end

                class << self
                  # A EvalResponsesSource object describing a run data source configuration.
                  sig do
                    params(
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String]),
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    created_after: nil, # Only include items created after this timestamp (inclusive). This is a query
                                        # parameter used to select responses.
                    created_before: nil, # Only include items created before this timestamp (inclusive). This is a query
                                         # parameter used to select responses.
                    instructions_search: nil, # Optional string to search the 'instructions' field. This is a query parameter
                                              # used to select responses.
                    metadata: nil, # Metadata filter for the responses. This is a query parameter used to select
                                   # responses.
                    model: nil, # The name of the model to find responses for. This is a query parameter used to
                                # select responses.
                    reasoning_effort: nil, # Optional reasoning effort parameter. This is a query parameter used to select
                                           # responses.
                    temperature: nil, # Sampling temperature. This is a query parameter used to select responses.
                    tools: nil, # List of tool names. This is a query parameter used to select responses.
                    top_p: nil, # Nucleus sampling parameter. This is a query parameter used to select responses.
                    users: nil, # List of user identifiers. This is a query parameter used to select responses.
                    type: :responses # The type of run data source. Always `responses`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::Responses,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileContent,
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::FileID,
                    OpenAI::Models::Evals::RunListResponse::DataSource::Responses::Source::Responses
                  )
                end
            end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource,
                OpenAI::Models::Evals::RunListResponse::DataSource::Responses
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::Evals::RunListResponse,
              OpenAI::Internal::AnyHash
            )
          end

        class PerModelUsage < OpenAI::Internal::Type::BaseModel
          # The number of tokens retrieved from cache.
          sig { returns(Integer) }
          attr_accessor :cached_tokens

          # The number of completion tokens generated.
          sig { returns(Integer) }
          attr_accessor :completion_tokens

          # The number of invocations.
          sig { returns(Integer) }
          attr_accessor :invocation_count

          # The name of the model.
          sig { returns(String) }
          attr_accessor :model_name

          # The number of prompt tokens used.
          sig { returns(Integer) }
          attr_accessor :prompt_tokens

          # The total number of tokens used.
          sig { returns(Integer) }
          attr_accessor :total_tokens

          sig do
            override
              .returns({
                cached_tokens: Integer,
                completion_tokens: Integer,
                invocation_count: Integer,
                model_name: String,
                prompt_tokens: Integer,
                total_tokens: Integer
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                cached_tokens: Integer,
                completion_tokens: Integer,
                invocation_count: Integer,
                model_name: String,
                prompt_tokens: Integer,
                total_tokens: Integer
              ).returns(T.attached_class)
            end
            def new(
              cached_tokens:, # The number of tokens retrieved from cache.
              completion_tokens:, # The number of completion tokens generated.
              invocation_count:, # The number of invocations.
              model_name:, # The name of the model.
              prompt_tokens:, # The number of prompt tokens used.
              total_tokens: # The total number of tokens used.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunListResponse::PerModelUsage,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class PerTestingCriteriaResult < OpenAI::Internal::Type::BaseModel
          # Number of tests failed for this criteria.
          sig { returns(Integer) }
          attr_accessor :failed

          # Number of tests passed for this criteria.
          sig { returns(Integer) }
          attr_accessor :passed

          # A description of the testing criteria.
          sig { returns(String) }
          attr_accessor :testing_criteria

          sig { override.returns({ failed: Integer, passed: Integer, testing_criteria: String }) }
          def to_hash; end

          class << self
            sig { params(failed: Integer, passed: Integer, testing_criteria: String).returns(T.attached_class) }
            def new(
              failed:, # Number of tests failed for this criteria.
              passed:, # Number of tests passed for this criteria.
              testing_criteria: # A description of the testing criteria.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunListResponse::PerTestingCriteriaResult,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ResultCounts < OpenAI::Internal::Type::BaseModel
          # Number of output items that resulted in an error.
          sig { returns(Integer) }
          attr_accessor :errored

          # Number of output items that failed to pass the evaluation.
          sig { returns(Integer) }
          attr_accessor :failed

          # Number of output items that passed the evaluation.
          sig { returns(Integer) }
          attr_accessor :passed

          # Total number of executed output items.
          sig { returns(Integer) }
          attr_accessor :total

          sig do
            override
              .returns({
                errored: Integer,
                failed: Integer,
                passed: Integer,
                total: Integer
              })
          end
          def to_hash; end

          class << self
            # Counters summarizing the outcomes of the evaluation run.
            sig do
              params(
                errored: Integer,
                failed: Integer,
                passed: Integer,
                total: Integer
              ).returns(T.attached_class)
            end
            def new(
              errored:, # Number of output items that resulted in an error.
              failed:, # Number of output items that failed to pass the evaluation.
              passed:, # Number of output items that passed the evaluation.
              total: # Total number of executed output items.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunListResponse::ResultCounts,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class RunRetrieveParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :eval_id

        sig { override.returns({ eval_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(eval_id: String, request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(eval_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Evals::RunRetrieveParams, OpenAI::Internal::AnyHash)
          end
      end

      class RunRetrieveResponse < OpenAI::Internal::Type::BaseModel
        # Unix timestamp (in seconds) when the evaluation run was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Information about the run's data source.
        sig { returns(OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Variants) }
        attr_accessor :data_source

        # An object representing an error response from the Eval API.
        sig { returns(OpenAI::Evals::EvalAPIError) }
        attr_reader :error

        sig { params(error: OpenAI::Evals::EvalAPIError::OrHash).void }
        attr_writer :error

        # The identifier of the associated evaluation.
        sig { returns(String) }
        attr_accessor :eval_id

        # Unique identifier for the evaluation run.
        sig { returns(String) }
        attr_accessor :id

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The model that is evaluated, if applicable.
        sig { returns(String) }
        attr_accessor :model

        # The name of the evaluation run.
        sig { returns(String) }
        attr_accessor :name

        # The type of the object. Always "eval.run".
        sig { returns(Symbol) }
        attr_accessor :object

        # Usage statistics for each model during the evaluation run.
        sig { returns(T::Array[OpenAI::Models::Evals::RunRetrieveResponse::PerModelUsage]) }
        attr_accessor :per_model_usage

        # Results per testing criteria applied during the evaluation run.
        sig do
          returns(T::Array[
              OpenAI::Models::Evals::RunRetrieveResponse::PerTestingCriteriaResult
            ])
        end
        attr_accessor :per_testing_criteria_results

        # The URL to the rendered evaluation run report on the UI dashboard.
        sig { returns(String) }
        attr_accessor :report_url

        # Counters summarizing the outcomes of the evaluation run.
        sig { returns(OpenAI::Models::Evals::RunRetrieveResponse::ResultCounts) }
        attr_reader :result_counts

        sig { params(result_counts: OpenAI::Models::Evals::RunRetrieveResponse::ResultCounts::OrHash).void }
        attr_writer :result_counts

        # The status of the evaluation run.
        sig { returns(String) }
        attr_accessor :status

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data_source:
                OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Variants,
              error: OpenAI::Evals::EvalAPIError,
              eval_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: String,
              object: Symbol,
              per_model_usage:
                T::Array[
                  OpenAI::Models::Evals::RunRetrieveResponse::PerModelUsage
                ],
              per_testing_criteria_results:
                T::Array[
                  OpenAI::Models::Evals::RunRetrieveResponse::PerTestingCriteriaResult
                ],
              report_url: String,
              result_counts:
                OpenAI::Models::Evals::RunRetrieveResponse::ResultCounts,
              status: String
            })
        end
        def to_hash; end

        class << self
          # A schema representing an evaluation run.
          sig do
            params(
              id: String,
              created_at: Integer,
              data_source: T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::OrHash,
                OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::OrHash
              ),
              error: OpenAI::Evals::EvalAPIError::OrHash,
              eval_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: String,
              name: String,
              per_model_usage: T::Array[
                OpenAI::Models::Evals::RunRetrieveResponse::PerModelUsage::OrHash
              ],
              per_testing_criteria_results: T::Array[
                OpenAI::Models::Evals::RunRetrieveResponse::PerTestingCriteriaResult::OrHash
              ],
              report_url: String,
              result_counts: OpenAI::Models::Evals::RunRetrieveResponse::ResultCounts::OrHash,
              status: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # Unique identifier for the evaluation run.
            created_at:, # Unix timestamp (in seconds) when the evaluation run was created.
            data_source:, # Information about the run's data source.
            error:, # An object representing an error response from the Eval API.
            eval_id:, # The identifier of the associated evaluation.
            metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
            model:, # The model that is evaluated, if applicable.
            name:, # The name of the evaluation run.
            per_model_usage:, # Usage statistics for each model during the evaluation run.
            per_testing_criteria_results:, # Results per testing criteria applied during the evaluation run.
            report_url:, # The URL to the rendered evaluation run report on the UI dashboard.
            result_counts:, # Counters summarizing the outcomes of the evaluation run.
            status:, # The status of the evaluation run.
            object: :"eval.run" # The type of the object. Always "eval.run".
); end
        end

        # Information about the run's data source.
        module DataSource
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Variants
              ])
            end
            def variants; end
          end

          class Responses < OpenAI::Internal::Type::BaseModel
            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            sig do
              returns(T.nilable(
                  OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Variants
                ))
            end
            attr_reader :input_messages

            sig do
              params(
                input_messages: T.any(
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::OrHash,
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::ItemReference::OrHash
                  )
              ).void
            end
            attr_writer :input_messages

            # The name of the model to use for generating completions (e.g. "o3-mini").
            sig { returns(T.nilable(String)) }
            attr_reader :model

            sig { params(model: String).void }
            attr_writer :model

            sig do
              returns(T.nilable(
                  OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams
                ))
            end
            attr_reader :sampling_params

            sig do
              params(
                sampling_params: OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams::OrHash
              ).void
            end
            attr_writer :sampling_params

            # Determines what populates the `item` namespace in this run's data source.
            sig { returns(OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::Variants) }
            attr_accessor :source

            # The type of run data source. Always `responses`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  source:
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::Variants,
                  type: Symbol,
                  input_messages:
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Variants,
                  model: String,
                  sampling_params:
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams
                })
            end
            def to_hash; end

            class << self
              # A ResponsesRunDataSource object describing a model sampling configuration.
              sig do
                params(
                  source: T.any(
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileContent::OrHash,
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileID::OrHash,
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::Responses::OrHash
                  ),
                  input_messages: T.any(
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::OrHash,
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::ItemReference::OrHash
                  ),
                  model: String,
                  sampling_params: OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams::OrHash,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                source:, # Determines what populates the `item` namespace in this run's data source.
                input_messages: nil, # Used when sampling from a model. Dictates the structure of the messages passed
                                     # into the model. Can either be a reference to a prebuilt trajectory (ie,
                                     # `item.input_trajectory`), or a template with variable references to the `item`
                                     # namespace.
                model: nil, # The name of the model to use for generating completions (e.g. "o3-mini").
                sampling_params: nil,
                type: :responses # The type of run data source. Always `responses`.
); end
            end

            # Used when sampling from a model. Dictates the structure of the messages passed
            # into the model. Can either be a reference to a prebuilt trajectory (ie,
            # `item.input_trajectory`), or a template with variable references to the `item`
            # namespace.
            module InputMessages
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Variants
                  ])
                end
                def variants; end
              end

              class ItemReference < OpenAI::Internal::Type::BaseModel
                # A reference to a variable in the `item` namespace. Ie, "item.name"
                sig { returns(String) }
                attr_accessor :item_reference

                # The type of input messages. Always `item_reference`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ item_reference: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(item_reference: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    item_reference:, # A reference to a variable in the `item` namespace. Ie, "item.name"
                    type: :item_reference # The type of input messages. Always `item_reference`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::ItemReference,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Template < OpenAI::Internal::Type::BaseModel
                # A list of chat messages forming the prompt or context. May include variable
                # references to the `item` namespace, ie {{item.name}}.
                sig do
                  returns(T::Array[
                      OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                    ])
                end
                attr_accessor :template

                # The type of input messages. Always `template`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      template:
                        T::Array[
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      template: T::Array[
                        T.any(
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage::OrHash,
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::OrHash
                        )
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    template:, # A list of chat messages forming the prompt or context. May include variable
                               # references to the `item` namespace, ie {{item.name}}.
                    type: :template # The type of input messages. Always `template`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template,
                      OpenAI::Internal::AnyHash
                    )
                  end

                # A message input to the model with a role indicating instruction following
                # hierarchy. Instructions given with the `developer` or `system` role take
                # precedence over instructions given with the `user` role. Messages with the
                # `assistant` role are presumed to have been generated by the model in previous
                # interactions.
                module Template
                  extend OpenAI::Internal::Type::Union

                  class << self
                    sig do
                      override
                        .returns(T::Array[
                        OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::Variants
                      ])
                    end
                    def variants; end
                  end

                  class ChatMessage < OpenAI::Internal::Type::BaseModel
                    # The content of the message.
                    sig { returns(String) }
                    attr_accessor :content

                    # The role of the message (e.g. "system", "assistant", "user").
                    sig { returns(String) }
                    attr_accessor :role

                    sig { override.returns({ content: String, role: String }) }
                    def to_hash; end

                    class << self
                      sig { params(content: String, role: String).returns(T.attached_class) }
                      def new(
                        content:, # The content of the message.
                        role: # The role of the message (e.g. "system", "assistant", "user").
); end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage,
                          OpenAI::Internal::AnyHash
                        )
                      end
                  end

                  class EvalItem < OpenAI::Internal::Type::BaseModel
                    # Inputs to the model - can contain template strings.
                    sig do
                      returns(OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants)
                    end
                    attr_accessor :content

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    sig do
                      returns(OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol)
                    end
                    attr_accessor :role

                    # The type of the message input. Always `message`.
                    sig do
                      returns(T.nilable(
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        ))
                    end
                    attr_reader :type

                    sig do
                      params(
                        type: OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                      ).void
                    end
                    attr_writer :type

                    sig do
                      override
                        .returns({
                          content:
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants,
                          role:
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol,
                          type:
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        })
                    end
                    def to_hash; end

                    class << self
                      # A message input to the model with a role indicating instruction following
                      # hierarchy. Instructions given with the `developer` or `system` role take
                      # precedence over instructions given with the `user` role. Messages with the
                      # `assistant` role are presumed to have been generated by the model in previous
                      # interactions.
                      sig do
                        params(
                          content: T.any(
                            String,
                            OpenAI::Responses::ResponseInputText::OrHash,
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText::OrHash,
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage::OrHash,
                            T::Array[T.anything]
                          ),
                          role: OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::OrSymbol,
                          type: OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::OrSymbol
                        ).returns(T.attached_class)
                      end
                      def new(
                        content:, # Inputs to the model - can contain template strings.
                        role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                               # `developer`.
                        type: nil # The type of the message input. Always `message`.
); end
                    end

                    # Inputs to the model - can contain template strings.
                    module Content
                      extend OpenAI::Internal::Type::Union

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::Variants
                          ])
                        end
                        def variants; end
                      end

                      AnArrayOfInputTextAndInputImageArray = T.let(
                          OpenAI::Internal::Type::ArrayOf[
                            OpenAI::Internal::Type::Unknown
                          ],
                          OpenAI::Internal::Type::Converter
                        )

                      class InputImage < OpenAI::Internal::Type::BaseModel
                        # The detail level of the image to be sent to the model. One of `high`, `low`, or
                        # `auto`. Defaults to `auto`.
                        sig { returns(T.nilable(String)) }
                        attr_reader :detail

                        sig { params(detail: String).void }
                        attr_writer :detail

                        # The URL of the image input.
                        sig { returns(String) }
                        attr_accessor :image_url

                        # The type of the image input. Always `input_image`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ image_url: String, type: Symbol, detail: String }) }
                        def to_hash; end

                        class << self
                          # An image input to the model.
                          sig { params(image_url: String, detail: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            image_url:, # The URL of the image input.
                            detail: nil, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                                         # `auto`. Defaults to `auto`.
                            type: :input_image # The type of the image input. Always `input_image`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      class OutputText < OpenAI::Internal::Type::BaseModel
                        # The text output from the model.
                        sig { returns(String) }
                        attr_accessor :text

                        # The type of the output text. Always `output_text`.
                        sig { returns(Symbol) }
                        attr_accessor :type

                        sig { override.returns({ text: String, type: Symbol }) }
                        def to_hash; end

                        class << self
                          # A text output from the model.
                          sig { params(text: String, type: Symbol).returns(T.attached_class) }
                          def new(
                            text:, # The text output from the model.
                            type: :output_text # The type of the output text. Always `output_text`.
); end
                        end

                        OrHash = T.type_alias do
                            T.any(
                              OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText,
                              OpenAI::Internal::AnyHash
                            )
                          end
                      end

                      Variants = T.type_alias do
                          T.any(
                            String,
                            OpenAI::Responses::ResponseInputText,
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::OutputText,
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Content::InputImage,
                            T::Array[T.anything]
                          )
                        end
                    end

                    OrHash = T.type_alias do
                        T.any(
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem,
                          OpenAI::Internal::AnyHash
                        )
                      end

                    # The role of the message input. One of `user`, `assistant`, `system`, or
                    # `developer`.
                    module Role
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      ASSISTANT = T.let(
                          :assistant,
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      DEVELOPER = T.let(
                          :developer,
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      SYSTEM = T.let(
                          :system,
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role
                          )
                        end

                      USER = T.let(
                          :user,
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Role::TaggedSymbol
                        )
                    end

                    # The type of the message input. Always `message`.
                    module Type
                      extend OpenAI::Internal::Type::Enum

                      class << self
                        sig do
                          override
                            .returns(T::Array[
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                          ])
                        end
                        def values; end
                      end

                      MESSAGE = T.let(
                          :message,
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type::TaggedSymbol
                        )

                      OrSymbol = T.type_alias { T.any(Symbol, String) }

                      TaggedSymbol = T.type_alias do
                          T.all(
                            Symbol,
                            OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem::Type
                          )
                        end
                    end
                  end

                  Variants = T.type_alias do
                      T.any(
                        OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::ChatMessage,
                        OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template::Template::EvalItem
                      )
                    end
                end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::Template,
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::InputMessages::ItemReference
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses,
                  OpenAI::Internal::AnyHash
                )
              end

            class SamplingParams < OpenAI::Internal::Type::BaseModel
              # The maximum number of tokens in the generated output.
              sig { returns(T.nilable(Integer)) }
              attr_reader :max_completion_tokens

              sig { params(max_completion_tokens: Integer).void }
              attr_writer :max_completion_tokens

              # A seed value to initialize the randomness, during sampling.
              sig { returns(T.nilable(Integer)) }
              attr_reader :seed

              sig { params(seed: Integer).void }
              attr_writer :seed

              # A higher temperature increases randomness in the outputs.
              sig { returns(T.nilable(Float)) }
              attr_reader :temperature

              sig { params(temperature: Float).void }
              attr_writer :temperature

              # Configuration options for a text response from the model. Can be plain text or
              # structured JSON data. Learn more:
              #
              # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
              # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
              sig do
                returns(T.nilable(
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams::Text
                  ))
              end
              attr_reader :text

              sig do
                params(
                  text: OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams::Text::OrHash
                ).void
              end
              attr_writer :text

              # An array of tools the model may call while generating a response. You can
              # specify which tool to use by setting the `tool_choice` parameter.
              #
              # The two categories of tools you can provide the model are:
              #
              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
              #   capabilities, like
              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
              #   Learn more about
              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
              #   the model to call your own code. Learn more about
              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
              sig { returns(T.nilable(T::Array[OpenAI::Responses::Tool::Variants])) }
              attr_reader :tools

              sig do
                params(
                  tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ]
                ).void
              end
              attr_writer :tools

              # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
              sig { returns(T.nilable(Float)) }
              attr_reader :top_p

              sig { params(top_p: Float).void }
              attr_writer :top_p

              sig do
                override
                  .returns({
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text:
                      OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams::Text,
                    tools: T::Array[OpenAI::Responses::Tool::Variants],
                    top_p: Float
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    max_completion_tokens: Integer,
                    seed: Integer,
                    temperature: Float,
                    text: OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams::Text::OrHash,
                    tools: T::Array[
                      T.any(
                        OpenAI::Responses::FunctionTool::OrHash,
                        OpenAI::Responses::FileSearchTool::OrHash,
                        OpenAI::Responses::ComputerTool::OrHash,
                        OpenAI::Responses::Tool::Mcp::OrHash,
                        OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                        OpenAI::Responses::Tool::ImageGeneration::OrHash,
                        OpenAI::Responses::Tool::LocalShell::OrHash,
                        OpenAI::Responses::WebSearchTool::OrHash
                      )
                    ],
                    top_p: Float
                  ).returns(T.attached_class)
                end
                def new(
                  max_completion_tokens: nil, # The maximum number of tokens in the generated output.
                  seed: nil, # A seed value to initialize the randomness, during sampling.
                  temperature: nil, # A higher temperature increases randomness in the outputs.
                  text: nil, # Configuration options for a text response from the model. Can be plain text or
                             # structured JSON data. Learn more:
                             # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                             # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  tools: nil, # An array of tools the model may call while generating a response. You can
                              # specify which tool to use by setting the `tool_choice` parameter.
                              # The two categories of tools you can provide the model are:
                              # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
                              #   capabilities, like
                              #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
                              #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
                              #   Learn more about
                              #   [built-in tools](https://platform.openai.com/docs/guides/tools).
                              # - **Function calls (custom tools)**: Functions that are defined by you, enabling
                              #   the model to call your own code. Learn more about
                              #   [function calling](https://platform.openai.com/docs/guides/function-calling).
                  top_p: nil # An alternative to temperature for nucleus sampling; 1.0 includes all tokens.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams,
                    OpenAI::Internal::AnyHash
                  )
                end

              class Text < OpenAI::Internal::Type::BaseModel
                # An object specifying the format that the model must output.
                #
                # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                # ensures the model will match your supplied JSON schema. Learn more in the
                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                #
                # The default format is `{ "type": "text" }` with no additional options.
                #
                # **Not recommended for gpt-4o and newer models:**
                #
                # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                # ensures the message the model generates is valid JSON. Using `json_schema` is
                # preferred for models that support it.
                sig do
                  returns(T.nilable(
                      OpenAI::Responses::ResponseFormatTextConfig::Variants
                    ))
                end
                attr_reader :format_

                sig do
                  params(
                    format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                  ).void
                end
                attr_writer :format_

                sig do
                  override
                    .returns({
                      format_:
                        OpenAI::Responses::ResponseFormatTextConfig::Variants
                    })
                end
                def to_hash; end

                class << self
                  # Configuration options for a text response from the model. Can be plain text or
                  # structured JSON data. Learn more:
                  #
                  # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                  # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
                  sig do
                    params(
                      format_: T.any(
                        OpenAI::ResponseFormatText::OrHash,
                        OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                        OpenAI::ResponseFormatJSONObject::OrHash
                      )
                    ).returns(T.attached_class)
                  end
                  def new(
                    format_: nil # An object specifying the format that the model must output.
                                 # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                                 # ensures the model will match your supplied JSON schema. Learn more in the
                                 # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                 # The default format is `{ "type": "text" }` with no additional options.
                                 # **Not recommended for gpt-4o and newer models:**
                                 # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                                 # ensures the message the model generates is valid JSON. Using `json_schema` is
                                 # preferred for models that support it.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::SamplingParams::Text,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end
            end

            # Determines what populates the `item` namespace in this run's data source.
            module Source
              extend OpenAI::Internal::Type::Union

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::Variants
                  ])
                end
                def variants; end
              end

              class FileContent < OpenAI::Internal::Type::BaseModel
                # The content of the jsonl file.
                sig do
                  returns(T::Array[
                      OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileContent::Content
                    ])
                end
                attr_accessor :content

                # The type of jsonl source. Always `file_content`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig do
                  override
                    .returns({
                      content:
                        T::Array[
                          OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileContent::Content
                        ],
                      type: Symbol
                    })
                end
                def to_hash; end

                class << self
                  sig do
                    params(
                      content: T::Array[
                        OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileContent::Content::OrHash
                      ],
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    content:, # The content of the jsonl file.
                    type: :file_content # The type of jsonl source. Always `file_content`.
); end
                end

                class Content < OpenAI::Internal::Type::BaseModel
                  sig { returns(T::Hash[Symbol, T.anything]) }
                  attr_accessor :item

                  sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
                  attr_reader :sample

                  sig { params(sample: T::Hash[Symbol, T.anything]).void }
                  attr_writer :sample

                  sig do
                    override
                      .returns({
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      })
                  end
                  def to_hash; end

                  class << self
                    sig do
                      params(
                        item: T::Hash[Symbol, T.anything],
                        sample: T::Hash[Symbol, T.anything]
                      ).returns(T.attached_class)
                    end
                    def new(item:, sample: nil); end
                  end

                  OrHash = T.type_alias do
                      T.any(
                        OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileContent::Content,
                        OpenAI::Internal::AnyHash
                      )
                    end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileContent,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class FileID < OpenAI::Internal::Type::BaseModel
                # The identifier of the file.
                sig { returns(String) }
                attr_accessor :id

                # The type of jsonl source. Always `file_id`.
                sig { returns(Symbol) }
                attr_accessor :type

                sig { override.returns({ id: String, type: Symbol }) }
                def to_hash; end

                class << self
                  sig { params(id: String, type: Symbol).returns(T.attached_class) }
                  def new(
                    id:, # The identifier of the file.
                    type: :file_id # The type of jsonl source. Always `file_id`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileID,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Responses < OpenAI::Internal::Type::BaseModel
                # Only include items created after this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_after

                # Only include items created before this timestamp (inclusive). This is a query
                # parameter used to select responses.
                sig { returns(T.nilable(Integer)) }
                attr_accessor :created_before

                # Optional string to search the 'instructions' field. This is a query parameter
                # used to select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :instructions_search

                # Metadata filter for the responses. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(T.anything)) }
                attr_accessor :metadata

                # The name of the model to find responses for. This is a query parameter used to
                # select responses.
                sig { returns(T.nilable(String)) }
                attr_accessor :model

                # Optional reasoning effort parameter. This is a query parameter used to select
                # responses.
                sig { returns(T.nilable(OpenAI::ReasoningEffort::TaggedSymbol)) }
                attr_accessor :reasoning_effort

                # Sampling temperature. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :temperature

                # List of tool names. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :tools

                # Nucleus sampling parameter. This is a query parameter used to select responses.
                sig { returns(T.nilable(Float)) }
                attr_accessor :top_p

                # The type of run data source. Always `responses`.
                sig { returns(Symbol) }
                attr_accessor :type

                # List of user identifiers. This is a query parameter used to select responses.
                sig { returns(T.nilable(T::Array[String])) }
                attr_accessor :users

                sig do
                  override
                    .returns({
                      type: Symbol,
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort:
                        T.nilable(OpenAI::ReasoningEffort::TaggedSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String])
                    })
                end
                def to_hash; end

                class << self
                  # A EvalResponsesSource object describing a run data source configuration.
                  sig do
                    params(
                      created_after: T.nilable(Integer),
                      created_before: T.nilable(Integer),
                      instructions_search: T.nilable(String),
                      metadata: T.nilable(T.anything),
                      model: T.nilable(String),
                      reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
                      temperature: T.nilable(Float),
                      tools: T.nilable(T::Array[String]),
                      top_p: T.nilable(Float),
                      users: T.nilable(T::Array[String]),
                      type: Symbol
                    ).returns(T.attached_class)
                  end
                  def new(
                    created_after: nil, # Only include items created after this timestamp (inclusive). This is a query
                                        # parameter used to select responses.
                    created_before: nil, # Only include items created before this timestamp (inclusive). This is a query
                                         # parameter used to select responses.
                    instructions_search: nil, # Optional string to search the 'instructions' field. This is a query parameter
                                              # used to select responses.
                    metadata: nil, # Metadata filter for the responses. This is a query parameter used to select
                                   # responses.
                    model: nil, # The name of the model to find responses for. This is a query parameter used to
                                # select responses.
                    reasoning_effort: nil, # Optional reasoning effort parameter. This is a query parameter used to select
                                           # responses.
                    temperature: nil, # Sampling temperature. This is a query parameter used to select responses.
                    tools: nil, # List of tool names. This is a query parameter used to select responses.
                    top_p: nil, # Nucleus sampling parameter. This is a query parameter used to select responses.
                    users: nil, # List of user identifiers. This is a query parameter used to select responses.
                    type: :responses # The type of run data source. Always `responses`.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::Responses,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              Variants = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileContent,
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::FileID,
                    OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses::Source::Responses
                  )
                end
            end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource,
                OpenAI::Models::Evals::RunRetrieveResponse::DataSource::Responses
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::Evals::RunRetrieveResponse,
              OpenAI::Internal::AnyHash
            )
          end

        class PerModelUsage < OpenAI::Internal::Type::BaseModel
          # The number of tokens retrieved from cache.
          sig { returns(Integer) }
          attr_accessor :cached_tokens

          # The number of completion tokens generated.
          sig { returns(Integer) }
          attr_accessor :completion_tokens

          # The number of invocations.
          sig { returns(Integer) }
          attr_accessor :invocation_count

          # The name of the model.
          sig { returns(String) }
          attr_accessor :model_name

          # The number of prompt tokens used.
          sig { returns(Integer) }
          attr_accessor :prompt_tokens

          # The total number of tokens used.
          sig { returns(Integer) }
          attr_accessor :total_tokens

          sig do
            override
              .returns({
                cached_tokens: Integer,
                completion_tokens: Integer,
                invocation_count: Integer,
                model_name: String,
                prompt_tokens: Integer,
                total_tokens: Integer
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                cached_tokens: Integer,
                completion_tokens: Integer,
                invocation_count: Integer,
                model_name: String,
                prompt_tokens: Integer,
                total_tokens: Integer
              ).returns(T.attached_class)
            end
            def new(
              cached_tokens:, # The number of tokens retrieved from cache.
              completion_tokens:, # The number of completion tokens generated.
              invocation_count:, # The number of invocations.
              model_name:, # The name of the model.
              prompt_tokens:, # The number of prompt tokens used.
              total_tokens: # The total number of tokens used.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunRetrieveResponse::PerModelUsage,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class PerTestingCriteriaResult < OpenAI::Internal::Type::BaseModel
          # Number of tests failed for this criteria.
          sig { returns(Integer) }
          attr_accessor :failed

          # Number of tests passed for this criteria.
          sig { returns(Integer) }
          attr_accessor :passed

          # A description of the testing criteria.
          sig { returns(String) }
          attr_accessor :testing_criteria

          sig { override.returns({ failed: Integer, passed: Integer, testing_criteria: String }) }
          def to_hash; end

          class << self
            sig { params(failed: Integer, passed: Integer, testing_criteria: String).returns(T.attached_class) }
            def new(
              failed:, # Number of tests failed for this criteria.
              passed:, # Number of tests passed for this criteria.
              testing_criteria: # A description of the testing criteria.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunRetrieveResponse::PerTestingCriteriaResult,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ResultCounts < OpenAI::Internal::Type::BaseModel
          # Number of output items that resulted in an error.
          sig { returns(Integer) }
          attr_accessor :errored

          # Number of output items that failed to pass the evaluation.
          sig { returns(Integer) }
          attr_accessor :failed

          # Number of output items that passed the evaluation.
          sig { returns(Integer) }
          attr_accessor :passed

          # Total number of executed output items.
          sig { returns(Integer) }
          attr_accessor :total

          sig do
            override
              .returns({
                errored: Integer,
                failed: Integer,
                passed: Integer,
                total: Integer
              })
          end
          def to_hash; end

          class << self
            # Counters summarizing the outcomes of the evaluation run.
            sig do
              params(
                errored: Integer,
                failed: Integer,
                passed: Integer,
                total: Integer
              ).returns(T.attached_class)
            end
            def new(
              errored:, # Number of output items that resulted in an error.
              failed:, # Number of output items that failed to pass the evaluation.
              passed:, # Number of output items that passed the evaluation.
              total: # Total number of executed output items.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::RunRetrieveResponse::ResultCounts,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      module Runs
        class OutputItemListParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # Identifier for the last output item from the previous pagination request.
          sig { returns(T.nilable(String)) }
          attr_reader :after

          sig { params(after: String).void }
          attr_writer :after

          sig { returns(String) }
          attr_accessor :eval_id

          # Number of output items to retrieve.
          sig { returns(T.nilable(Integer)) }
          attr_reader :limit

          sig { params(limit: Integer).void }
          attr_writer :limit

          # Sort order for output items by timestamp. Use `asc` for ascending order or
          # `desc` for descending order. Defaults to `asc`.
          sig do
            returns(T.nilable(
                OpenAI::Evals::Runs::OutputItemListParams::Order::OrSymbol
              ))
          end
          attr_reader :order

          sig { params(order: OpenAI::Evals::Runs::OutputItemListParams::Order::OrSymbol).void }
          attr_writer :order

          # Filter output items by status. Use `failed` to filter by failed output items or
          # `pass` to filter by passed output items.
          sig do
            returns(T.nilable(
                OpenAI::Evals::Runs::OutputItemListParams::Status::OrSymbol
              ))
          end
          attr_reader :status

          sig { params(status: OpenAI::Evals::Runs::OutputItemListParams::Status::OrSymbol).void }
          attr_writer :status

          sig do
            override
              .returns({
                eval_id: String,
                after: String,
                limit: Integer,
                order:
                  OpenAI::Evals::Runs::OutputItemListParams::Order::OrSymbol,
                status:
                  OpenAI::Evals::Runs::OutputItemListParams::Status::OrSymbol,
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                eval_id: String,
                after: String,
                limit: Integer,
                order: OpenAI::Evals::Runs::OutputItemListParams::Order::OrSymbol,
                status: OpenAI::Evals::Runs::OutputItemListParams::Status::OrSymbol,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              eval_id:,
              after: nil, # Identifier for the last output item from the previous pagination request.
              limit: nil, # Number of output items to retrieve.
              order: nil, # Sort order for output items by timestamp. Use `asc` for ascending order or
                          # `desc` for descending order. Defaults to `asc`.
              status: nil, # Filter output items by status. Use `failed` to filter by failed output items or
                           # `pass` to filter by passed output items.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Evals::Runs::OutputItemListParams,
                OpenAI::Internal::AnyHash
              )
            end

          # Sort order for output items by timestamp. Use `asc` for ascending order or
          # `desc` for descending order. Defaults to `asc`.
          module Order
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Evals::Runs::OutputItemListParams::Order::TaggedSymbol
                ])
              end
              def values; end
            end

            ASC = T.let(
                :asc,
                OpenAI::Evals::Runs::OutputItemListParams::Order::TaggedSymbol
              )

            DESC = T.let(
                :desc,
                OpenAI::Evals::Runs::OutputItemListParams::Order::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Evals::Runs::OutputItemListParams::Order)
              end
          end

          # Filter output items by status. Use `failed` to filter by failed output items or
          # `pass` to filter by passed output items.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Evals::Runs::OutputItemListParams::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            FAIL = T.let(
                :fail,
                OpenAI::Evals::Runs::OutputItemListParams::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            PASS = T.let(
                :pass,
                OpenAI::Evals::Runs::OutputItemListParams::Status::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Evals::Runs::OutputItemListParams::Status)
              end
          end
        end

        class OutputItemListResponse < OpenAI::Internal::Type::BaseModel
          # Unix timestamp (in seconds) when the evaluation run was created.
          sig { returns(Integer) }
          attr_accessor :created_at

          # Details of the input data source item.
          sig { returns(T::Hash[Symbol, T.anything]) }
          attr_accessor :datasource_item

          # The identifier for the data source item.
          sig { returns(Integer) }
          attr_accessor :datasource_item_id

          # The identifier of the evaluation group.
          sig { returns(String) }
          attr_accessor :eval_id

          # Unique identifier for the evaluation run output item.
          sig { returns(String) }
          attr_accessor :id

          # The type of the object. Always "eval.run.output_item".
          sig { returns(Symbol) }
          attr_accessor :object

          # A list of results from the evaluation run.
          sig { returns(T::Array[T::Hash[Symbol, T.anything]]) }
          attr_accessor :results

          # The identifier of the evaluation run associated with this output item.
          sig { returns(String) }
          attr_accessor :run_id

          # A sample containing the input and output of the evaluation run.
          sig { returns(OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample) }
          attr_reader :sample

          sig { params(sample: OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::OrHash).void }
          attr_writer :sample

          # The status of the evaluation run.
          sig { returns(String) }
          attr_accessor :status

          sig do
            override
              .returns({
                id: String,
                created_at: Integer,
                datasource_item: T::Hash[Symbol, T.anything],
                datasource_item_id: Integer,
                eval_id: String,
                object: Symbol,
                results: T::Array[T::Hash[Symbol, T.anything]],
                run_id: String,
                sample:
                  OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample,
                status: String
              })
          end
          def to_hash; end

          class << self
            # A schema representing an evaluation run output item.
            sig do
              params(
                id: String,
                created_at: Integer,
                datasource_item: T::Hash[Symbol, T.anything],
                datasource_item_id: Integer,
                eval_id: String,
                results: T::Array[T::Hash[Symbol, T.anything]],
                run_id: String,
                sample: OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::OrHash,
                status: String,
                object: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # Unique identifier for the evaluation run output item.
              created_at:, # Unix timestamp (in seconds) when the evaluation run was created.
              datasource_item:, # Details of the input data source item.
              datasource_item_id:, # The identifier for the data source item.
              eval_id:, # The identifier of the evaluation group.
              results:, # A list of results from the evaluation run.
              run_id:, # The identifier of the evaluation run associated with this output item.
              sample:, # A sample containing the input and output of the evaluation run.
              status:, # The status of the evaluation run.
              object: :"eval.run.output_item" # The type of the object. Always "eval.run.output_item".
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::Runs::OutputItemListResponse,
                OpenAI::Internal::AnyHash
              )
            end

          class Sample < OpenAI::Internal::Type::BaseModel
            # An object representing an error response from the Eval API.
            sig { returns(OpenAI::Evals::EvalAPIError) }
            attr_reader :error

            sig { params(error: OpenAI::Evals::EvalAPIError::OrHash).void }
            attr_writer :error

            # The reason why the sample generation was finished.
            sig { returns(String) }
            attr_accessor :finish_reason

            # An array of input messages.
            sig do
              returns(T::Array[
                  OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Input
                ])
            end
            attr_accessor :input

            # The maximum number of tokens allowed for completion.
            sig { returns(Integer) }
            attr_accessor :max_completion_tokens

            # The model used for generating the sample.
            sig { returns(String) }
            attr_accessor :model

            # An array of output messages.
            sig do
              returns(T::Array[
                  OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Output
                ])
            end
            attr_accessor :output

            # The seed used for generating the sample.
            sig { returns(Integer) }
            attr_accessor :seed

            # The sampling temperature used.
            sig { returns(Float) }
            attr_accessor :temperature

            # The top_p value used for sampling.
            sig { returns(Float) }
            attr_accessor :top_p

            # Token usage details for the sample.
            sig { returns(OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Usage) }
            attr_reader :usage

            sig { params(usage: OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Usage::OrHash).void }
            attr_writer :usage

            sig do
              override
                .returns({
                  error: OpenAI::Evals::EvalAPIError,
                  finish_reason: String,
                  input:
                    T::Array[
                      OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Input
                    ],
                  max_completion_tokens: Integer,
                  model: String,
                  output:
                    T::Array[
                      OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Output
                    ],
                  seed: Integer,
                  temperature: Float,
                  top_p: Float,
                  usage:
                    OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Usage
                })
            end
            def to_hash; end

            class << self
              # A sample containing the input and output of the evaluation run.
              sig do
                params(
                  error: OpenAI::Evals::EvalAPIError::OrHash,
                  finish_reason: String,
                  input: T::Array[
                    OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Input::OrHash
                  ],
                  max_completion_tokens: Integer,
                  model: String,
                  output: T::Array[
                    OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Output::OrHash
                  ],
                  seed: Integer,
                  temperature: Float,
                  top_p: Float,
                  usage: OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Usage::OrHash
                ).returns(T.attached_class)
              end
              def new(
                error:, # An object representing an error response from the Eval API.
                finish_reason:, # The reason why the sample generation was finished.
                input:, # An array of input messages.
                max_completion_tokens:, # The maximum number of tokens allowed for completion.
                model:, # The model used for generating the sample.
                output:, # An array of output messages.
                seed:, # The seed used for generating the sample.
                temperature:, # The sampling temperature used.
                top_p:, # The top_p value used for sampling.
                usage: # Token usage details for the sample.
); end
            end

            class Input < OpenAI::Internal::Type::BaseModel
              # The content of the message.
              sig { returns(String) }
              attr_accessor :content

              # The role of the message sender (e.g., system, user, developer).
              sig { returns(String) }
              attr_accessor :role

              sig { override.returns({ content: String, role: String }) }
              def to_hash; end

              class << self
                # An input message.
                sig { params(content: String, role: String).returns(T.attached_class) }
                def new(
                  content:, # The content of the message.
                  role: # The role of the message sender (e.g., system, user, developer).
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Input,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample,
                  OpenAI::Internal::AnyHash
                )
              end

            class Output < OpenAI::Internal::Type::BaseModel
              # The content of the message.
              sig { returns(T.nilable(String)) }
              attr_reader :content

              sig { params(content: String).void }
              attr_writer :content

              # The role of the message (e.g. "system", "assistant", "user").
              sig { returns(T.nilable(String)) }
              attr_reader :role

              sig { params(role: String).void }
              attr_writer :role

              sig { override.returns({ content: String, role: String }) }
              def to_hash; end

              class << self
                sig { params(content: String, role: String).returns(T.attached_class) }
                def new(
                  content: nil, # The content of the message.
                  role: nil # The role of the message (e.g. "system", "assistant", "user").
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Output,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            class Usage < OpenAI::Internal::Type::BaseModel
              # The number of tokens retrieved from cache.
              sig { returns(Integer) }
              attr_accessor :cached_tokens

              # The number of completion tokens generated.
              sig { returns(Integer) }
              attr_accessor :completion_tokens

              # The number of prompt tokens used.
              sig { returns(Integer) }
              attr_accessor :prompt_tokens

              # The total number of tokens used.
              sig { returns(Integer) }
              attr_accessor :total_tokens

              sig do
                override
                  .returns({
                    cached_tokens: Integer,
                    completion_tokens: Integer,
                    prompt_tokens: Integer,
                    total_tokens: Integer
                  })
              end
              def to_hash; end

              class << self
                # Token usage details for the sample.
                sig do
                  params(
                    cached_tokens: Integer,
                    completion_tokens: Integer,
                    prompt_tokens: Integer,
                    total_tokens: Integer
                  ).returns(T.attached_class)
                end
                def new(
                  cached_tokens:, # The number of tokens retrieved from cache.
                  completion_tokens:, # The number of completion tokens generated.
                  prompt_tokens:, # The number of prompt tokens used.
                  total_tokens: # The total number of tokens used.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::Runs::OutputItemListResponse::Sample::Usage,
                    OpenAI::Internal::AnyHash
                  )
                end
            end
          end
        end

        class OutputItemRetrieveParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          sig { returns(String) }
          attr_accessor :eval_id

          sig { returns(String) }
          attr_accessor :run_id

          sig do
            override
              .returns({
                eval_id: String,
                run_id: String,
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                eval_id: String,
                run_id: String,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(eval_id:, run_id:, request_options: {}); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Evals::Runs::OutputItemRetrieveParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class OutputItemRetrieveResponse < OpenAI::Internal::Type::BaseModel
          # Unix timestamp (in seconds) when the evaluation run was created.
          sig { returns(Integer) }
          attr_accessor :created_at

          # Details of the input data source item.
          sig { returns(T::Hash[Symbol, T.anything]) }
          attr_accessor :datasource_item

          # The identifier for the data source item.
          sig { returns(Integer) }
          attr_accessor :datasource_item_id

          # The identifier of the evaluation group.
          sig { returns(String) }
          attr_accessor :eval_id

          # Unique identifier for the evaluation run output item.
          sig { returns(String) }
          attr_accessor :id

          # The type of the object. Always "eval.run.output_item".
          sig { returns(Symbol) }
          attr_accessor :object

          # A list of results from the evaluation run.
          sig { returns(T::Array[T::Hash[Symbol, T.anything]]) }
          attr_accessor :results

          # The identifier of the evaluation run associated with this output item.
          sig { returns(String) }
          attr_accessor :run_id

          # A sample containing the input and output of the evaluation run.
          sig { returns(OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample) }
          attr_reader :sample

          sig { params(sample: OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::OrHash).void }
          attr_writer :sample

          # The status of the evaluation run.
          sig { returns(String) }
          attr_accessor :status

          sig do
            override
              .returns({
                id: String,
                created_at: Integer,
                datasource_item: T::Hash[Symbol, T.anything],
                datasource_item_id: Integer,
                eval_id: String,
                object: Symbol,
                results: T::Array[T::Hash[Symbol, T.anything]],
                run_id: String,
                sample:
                  OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample,
                status: String
              })
          end
          def to_hash; end

          class << self
            # A schema representing an evaluation run output item.
            sig do
              params(
                id: String,
                created_at: Integer,
                datasource_item: T::Hash[Symbol, T.anything],
                datasource_item_id: Integer,
                eval_id: String,
                results: T::Array[T::Hash[Symbol, T.anything]],
                run_id: String,
                sample: OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::OrHash,
                status: String,
                object: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # Unique identifier for the evaluation run output item.
              created_at:, # Unix timestamp (in seconds) when the evaluation run was created.
              datasource_item:, # Details of the input data source item.
              datasource_item_id:, # The identifier for the data source item.
              eval_id:, # The identifier of the evaluation group.
              results:, # A list of results from the evaluation run.
              run_id:, # The identifier of the evaluation run associated with this output item.
              sample:, # A sample containing the input and output of the evaluation run.
              status:, # The status of the evaluation run.
              object: :"eval.run.output_item" # The type of the object. Always "eval.run.output_item".
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse,
                OpenAI::Internal::AnyHash
              )
            end

          class Sample < OpenAI::Internal::Type::BaseModel
            # An object representing an error response from the Eval API.
            sig { returns(OpenAI::Evals::EvalAPIError) }
            attr_reader :error

            sig { params(error: OpenAI::Evals::EvalAPIError::OrHash).void }
            attr_writer :error

            # The reason why the sample generation was finished.
            sig { returns(String) }
            attr_accessor :finish_reason

            # An array of input messages.
            sig do
              returns(T::Array[
                  OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Input
                ])
            end
            attr_accessor :input

            # The maximum number of tokens allowed for completion.
            sig { returns(Integer) }
            attr_accessor :max_completion_tokens

            # The model used for generating the sample.
            sig { returns(String) }
            attr_accessor :model

            # An array of output messages.
            sig do
              returns(T::Array[
                  OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Output
                ])
            end
            attr_accessor :output

            # The seed used for generating the sample.
            sig { returns(Integer) }
            attr_accessor :seed

            # The sampling temperature used.
            sig { returns(Float) }
            attr_accessor :temperature

            # The top_p value used for sampling.
            sig { returns(Float) }
            attr_accessor :top_p

            # Token usage details for the sample.
            sig { returns(OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Usage) }
            attr_reader :usage

            sig { params(usage: OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Usage::OrHash).void }
            attr_writer :usage

            sig do
              override
                .returns({
                  error: OpenAI::Evals::EvalAPIError,
                  finish_reason: String,
                  input:
                    T::Array[
                      OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Input
                    ],
                  max_completion_tokens: Integer,
                  model: String,
                  output:
                    T::Array[
                      OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Output
                    ],
                  seed: Integer,
                  temperature: Float,
                  top_p: Float,
                  usage:
                    OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Usage
                })
            end
            def to_hash; end

            class << self
              # A sample containing the input and output of the evaluation run.
              sig do
                params(
                  error: OpenAI::Evals::EvalAPIError::OrHash,
                  finish_reason: String,
                  input: T::Array[
                    OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Input::OrHash
                  ],
                  max_completion_tokens: Integer,
                  model: String,
                  output: T::Array[
                    OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Output::OrHash
                  ],
                  seed: Integer,
                  temperature: Float,
                  top_p: Float,
                  usage: OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Usage::OrHash
                ).returns(T.attached_class)
              end
              def new(
                error:, # An object representing an error response from the Eval API.
                finish_reason:, # The reason why the sample generation was finished.
                input:, # An array of input messages.
                max_completion_tokens:, # The maximum number of tokens allowed for completion.
                model:, # The model used for generating the sample.
                output:, # An array of output messages.
                seed:, # The seed used for generating the sample.
                temperature:, # The sampling temperature used.
                top_p:, # The top_p value used for sampling.
                usage: # Token usage details for the sample.
); end
            end

            class Input < OpenAI::Internal::Type::BaseModel
              # The content of the message.
              sig { returns(String) }
              attr_accessor :content

              # The role of the message sender (e.g., system, user, developer).
              sig { returns(String) }
              attr_accessor :role

              sig { override.returns({ content: String, role: String }) }
              def to_hash; end

              class << self
                # An input message.
                sig { params(content: String, role: String).returns(T.attached_class) }
                def new(
                  content:, # The content of the message.
                  role: # The role of the message sender (e.g., system, user, developer).
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Input,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample,
                  OpenAI::Internal::AnyHash
                )
              end

            class Output < OpenAI::Internal::Type::BaseModel
              # The content of the message.
              sig { returns(T.nilable(String)) }
              attr_reader :content

              sig { params(content: String).void }
              attr_writer :content

              # The role of the message (e.g. "system", "assistant", "user").
              sig { returns(T.nilable(String)) }
              attr_reader :role

              sig { params(role: String).void }
              attr_writer :role

              sig { override.returns({ content: String, role: String }) }
              def to_hash; end

              class << self
                sig { params(content: String, role: String).returns(T.attached_class) }
                def new(
                  content: nil, # The content of the message.
                  role: nil # The role of the message (e.g. "system", "assistant", "user").
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Output,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            class Usage < OpenAI::Internal::Type::BaseModel
              # The number of tokens retrieved from cache.
              sig { returns(Integer) }
              attr_accessor :cached_tokens

              # The number of completion tokens generated.
              sig { returns(Integer) }
              attr_accessor :completion_tokens

              # The number of prompt tokens used.
              sig { returns(Integer) }
              attr_accessor :prompt_tokens

              # The total number of tokens used.
              sig { returns(Integer) }
              attr_accessor :total_tokens

              sig do
                override
                  .returns({
                    cached_tokens: Integer,
                    completion_tokens: Integer,
                    prompt_tokens: Integer,
                    total_tokens: Integer
                  })
              end
              def to_hash; end

              class << self
                # Token usage details for the sample.
                sig do
                  params(
                    cached_tokens: Integer,
                    completion_tokens: Integer,
                    prompt_tokens: Integer,
                    total_tokens: Integer
                  ).returns(T.attached_class)
                end
                def new(
                  cached_tokens:, # The number of tokens retrieved from cache.
                  completion_tokens:, # The number of completion tokens generated.
                  prompt_tokens:, # The number of prompt tokens used.
                  total_tokens: # The total number of tokens used.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse::Sample::Usage,
                    OpenAI::Internal::AnyHash
                  )
                end
            end
          end
        end
      end
    end

    # The strategy used to chunk the file.
    module FileChunkingStrategy
      extend OpenAI::Internal::Type::Union

      class << self
        sig { override.returns(T::Array[OpenAI::FileChunkingStrategy::Variants]) }
        def variants; end
      end

      Variants = T.type_alias do
          T.any(
            OpenAI::StaticFileChunkingStrategyObject,
            OpenAI::OtherFileChunkingStrategyObject
          )
        end
    end

    # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
    # strategy. Only applicable if `file_ids` is non-empty.
    module FileChunkingStrategyParam
      extend OpenAI::Internal::Type::Union

      class << self
        sig { override.returns(T::Array[OpenAI::FileChunkingStrategyParam::Variants]) }
        def variants; end
      end

      Variants = T.type_alias do
          T.any(
            OpenAI::AutoFileChunkingStrategyParam,
            OpenAI::StaticFileChunkingStrategyObjectParam
          )
        end
    end

    FileContent = String

    class FileContentParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::FileContentParams, OpenAI::Internal::AnyHash)
        end
    end

    class FileCreateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # The File object (not file name) to be uploaded.
      sig { returns(OpenAI::Internal::FileInput) }
      attr_accessor :file

      # The intended purpose of the uploaded file. One of: - `assistants`: Used in the
      # Assistants API - `batch`: Used in the Batch API - `fine-tune`: Used for
      # fine-tuning - `vision`: Images used for vision fine-tuning - `user_data`:
      # Flexible file type for any purpose - `evals`: Used for eval data sets
      sig { returns(OpenAI::FilePurpose::OrSymbol) }
      attr_accessor :purpose

      sig do
        override
          .returns({
            file: OpenAI::Internal::FileInput,
            purpose: OpenAI::FilePurpose::OrSymbol,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            file: OpenAI::Internal::FileInput,
            purpose: OpenAI::FilePurpose::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          file:, # The File object (not file name) to be uploaded.
          purpose:, # The intended purpose of the uploaded file. One of: - `assistants`: Used in the
                    # Assistants API - `batch`: Used in the Batch API - `fine-tune`: Used for
                    # fine-tuning - `vision`: Images used for vision fine-tuning - `user_data`:
                    # Flexible file type for any purpose - `evals`: Used for eval data sets
          request_options: {}
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::FileCreateParams, OpenAI::Internal::AnyHash)
        end
    end

    class FileDeleteParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::FileDeleteParams, OpenAI::Internal::AnyHash)
        end
    end

    class FileDeleted < OpenAI::Internal::Type::BaseModel
      sig { returns(T::Boolean) }
      attr_accessor :deleted

      sig { returns(String) }
      attr_accessor :id

      sig { returns(Symbol) }
      attr_accessor :object

      sig { override.returns({ id: String, deleted: T::Boolean, object: Symbol }) }
      def to_hash; end

      class << self
        sig { params(id: String, deleted: T::Boolean, object: Symbol).returns(T.attached_class) }
        def new(id:, deleted:, object: :file); end
      end

      OrHash = T.type_alias { T.any(OpenAI::FileDeleted, OpenAI::Internal::AnyHash) }
    end

    class FileListParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # A cursor for use in pagination. `after` is an object ID that defines your place
      # in the list. For instance, if you make a list request and receive 100 objects,
      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
      # fetch the next page of the list.
      sig { returns(T.nilable(String)) }
      attr_reader :after

      sig { params(after: String).void }
      attr_writer :after

      # A limit on the number of objects to be returned. Limit can range between 1 and
      # 10,000, and the default is 10,000.
      sig { returns(T.nilable(Integer)) }
      attr_reader :limit

      sig { params(limit: Integer).void }
      attr_writer :limit

      # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
      # order and `desc` for descending order.
      sig { returns(T.nilable(OpenAI::FileListParams::Order::OrSymbol)) }
      attr_reader :order

      sig { params(order: OpenAI::FileListParams::Order::OrSymbol).void }
      attr_writer :order

      # Only return files with the given purpose.
      sig { returns(T.nilable(String)) }
      attr_reader :purpose

      sig { params(purpose: String).void }
      attr_writer :purpose

      sig do
        override
          .returns({
            after: String,
            limit: Integer,
            order: OpenAI::FileListParams::Order::OrSymbol,
            purpose: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            after: String,
            limit: Integer,
            order: OpenAI::FileListParams::Order::OrSymbol,
            purpose: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                      # in the list. For instance, if you make a list request and receive 100 objects,
                      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                      # fetch the next page of the list.
          limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                      # 10,000, and the default is 10,000.
          order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                      # order and `desc` for descending order.
          purpose: nil, # Only return files with the given purpose.
          request_options: {}
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::FileListParams, OpenAI::Internal::AnyHash)
        end

      # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
      # order and `desc` for descending order.
      module Order
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::FileListParams::Order::TaggedSymbol]) }
          def values; end
        end

        ASC = T.let(:asc, OpenAI::FileListParams::Order::TaggedSymbol)
        DESC = T.let(:desc, OpenAI::FileListParams::Order::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::FileListParams::Order) }
      end
    end

    class FileObject < OpenAI::Internal::Type::BaseModel
      # The size of the file, in bytes.
      sig { returns(Integer) }
      attr_accessor :bytes

      # The Unix timestamp (in seconds) for when the file was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The Unix timestamp (in seconds) for when the file will expire.
      sig { returns(T.nilable(Integer)) }
      attr_reader :expires_at

      sig { params(expires_at: Integer).void }
      attr_writer :expires_at

      # The name of the file.
      sig { returns(String) }
      attr_accessor :filename

      # The file identifier, which can be referenced in the API endpoints.
      sig { returns(String) }
      attr_accessor :id

      # The object type, which is always `file`.
      sig { returns(Symbol) }
      attr_accessor :object

      # The intended purpose of the file. Supported values are `assistants`,
      # `assistants_output`, `batch`, `batch_output`, `fine-tune`, `fine-tune-results`,
      # `vision`, and `user_data`.
      sig { returns(OpenAI::FileObject::Purpose::TaggedSymbol) }
      attr_accessor :purpose

      # Deprecated. The current status of the file, which can be either `uploaded`,
      # `processed`, or `error`.
      sig { returns(OpenAI::FileObject::Status::TaggedSymbol) }
      attr_accessor :status

      # Deprecated. For details on why a fine-tuning training file failed validation,
      # see the `error` field on `fine_tuning.job`.
      sig { returns(T.nilable(String)) }
      attr_reader :status_details

      sig { params(status_details: String).void }
      attr_writer :status_details

      sig do
        override
          .returns({
            id: String,
            bytes: Integer,
            created_at: Integer,
            filename: String,
            object: Symbol,
            purpose: OpenAI::FileObject::Purpose::TaggedSymbol,
            status: OpenAI::FileObject::Status::TaggedSymbol,
            expires_at: Integer,
            status_details: String
          })
      end
      def to_hash; end

      class << self
        # The `File` object represents a document that has been uploaded to OpenAI.
        sig do
          params(
            id: String,
            bytes: Integer,
            created_at: Integer,
            filename: String,
            purpose: OpenAI::FileObject::Purpose::OrSymbol,
            status: OpenAI::FileObject::Status::OrSymbol,
            expires_at: Integer,
            status_details: String,
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          id:, # The file identifier, which can be referenced in the API endpoints.
          bytes:, # The size of the file, in bytes.
          created_at:, # The Unix timestamp (in seconds) for when the file was created.
          filename:, # The name of the file.
          purpose:, # The intended purpose of the file. Supported values are `assistants`,
                    # `assistants_output`, `batch`, `batch_output`, `fine-tune`, `fine-tune-results`,
                    # `vision`, and `user_data`.
          status:, # Deprecated. The current status of the file, which can be either `uploaded`,
                   # `processed`, or `error`.
          expires_at: nil, # The Unix timestamp (in seconds) for when the file will expire.
          status_details: nil, # Deprecated. For details on why a fine-tuning training file failed validation,
                               # see the `error` field on `fine_tuning.job`.
          object: :file # The object type, which is always `file`.
); end
      end

      OrHash = T.type_alias { T.any(OpenAI::FileObject, OpenAI::Internal::AnyHash) }

      # The intended purpose of the file. Supported values are `assistants`,
      # `assistants_output`, `batch`, `batch_output`, `fine-tune`, `fine-tune-results`,
      # `vision`, and `user_data`.
      module Purpose
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::FileObject::Purpose::TaggedSymbol]) }
          def values; end
        end

        ASSISTANTS = T.let(:assistants, OpenAI::FileObject::Purpose::TaggedSymbol)

        ASSISTANTS_OUTPUT = T.let(:assistants_output, OpenAI::FileObject::Purpose::TaggedSymbol)

        BATCH = T.let(:batch, OpenAI::FileObject::Purpose::TaggedSymbol)

        BATCH_OUTPUT = T.let(:batch_output, OpenAI::FileObject::Purpose::TaggedSymbol)

        FINE_TUNE = T.let(:"fine-tune", OpenAI::FileObject::Purpose::TaggedSymbol)

        FINE_TUNE_RESULTS = T.let(:"fine-tune-results", OpenAI::FileObject::Purpose::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::FileObject::Purpose) }

        USER_DATA = T.let(:user_data, OpenAI::FileObject::Purpose::TaggedSymbol)
        VISION = T.let(:vision, OpenAI::FileObject::Purpose::TaggedSymbol)
      end

      # Deprecated. The current status of the file, which can be either `uploaded`,
      # `processed`, or `error`.
      module Status
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::FileObject::Status::TaggedSymbol]) }
          def values; end
        end

        ERROR = T.let(:error, OpenAI::FileObject::Status::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }
        PROCESSED = T.let(:processed, OpenAI::FileObject::Status::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::FileObject::Status) }

        UPLOADED = T.let(:uploaded, OpenAI::FileObject::Status::TaggedSymbol)
      end
    end

    # The intended purpose of the uploaded file. One of: - `assistants`: Used in the
    # Assistants API - `batch`: Used in the Batch API - `fine-tune`: Used for
    # fine-tuning - `vision`: Images used for vision fine-tuning - `user_data`:
    # Flexible file type for any purpose - `evals`: Used for eval data sets
    module FilePurpose
      extend OpenAI::Internal::Type::Enum

      class << self
        sig { override.returns(T::Array[OpenAI::FilePurpose::TaggedSymbol]) }
        def values; end
      end

      ASSISTANTS = T.let(:assistants, OpenAI::FilePurpose::TaggedSymbol)
      BATCH = T.let(:batch, OpenAI::FilePurpose::TaggedSymbol)
      EVALS = T.let(:evals, OpenAI::FilePurpose::TaggedSymbol)
      FINE_TUNE = T.let(:"fine-tune", OpenAI::FilePurpose::TaggedSymbol)
      OrSymbol = T.type_alias { T.any(Symbol, String) }
      TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::FilePurpose) }
      USER_DATA = T.let(:user_data, OpenAI::FilePurpose::TaggedSymbol)
      VISION = T.let(:vision, OpenAI::FilePurpose::TaggedSymbol)
    end

    class FileRetrieveParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::FileRetrieveParams, OpenAI::Internal::AnyHash)
        end
    end

    module FineTuning
      module Alpha
        class GraderRunParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # The grader used for the fine-tuning job.
          sig do
            returns(T.any(
                OpenAI::Graders::StringCheckGrader,
                OpenAI::Graders::TextSimilarityGrader,
                OpenAI::Graders::PythonGrader,
                OpenAI::Graders::ScoreModelGrader,
                OpenAI::Graders::MultiGrader
              ))
          end
          attr_accessor :grader

          # The dataset item provided to the grader. This will be used to populate the
          # `item` namespace. See
          # [the guide](https://platform.openai.com/docs/guides/graders) for more details.
          sig { returns(T.nilable(T.anything)) }
          attr_reader :item

          sig { params(item: T.anything).void }
          attr_writer :item

          # The model sample to be evaluated. This value will be used to populate the
          # `sample` namespace. See
          # [the guide](https://platform.openai.com/docs/guides/graders) for more details.
          # The `output_json` variable will be populated if the model sample is a valid JSON
          # string.
          sig { returns(String) }
          attr_accessor :model_sample

          sig do
            override
              .returns({
                grader:
                  T.any(
                    OpenAI::Graders::StringCheckGrader,
                    OpenAI::Graders::TextSimilarityGrader,
                    OpenAI::Graders::PythonGrader,
                    OpenAI::Graders::ScoreModelGrader,
                    OpenAI::Graders::MultiGrader
                  ),
                model_sample: String,
                item: T.anything,
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                grader: T.any(
                  OpenAI::Graders::StringCheckGrader::OrHash,
                  OpenAI::Graders::TextSimilarityGrader::OrHash,
                  OpenAI::Graders::PythonGrader::OrHash,
                  OpenAI::Graders::ScoreModelGrader::OrHash,
                  OpenAI::Graders::MultiGrader::OrHash
                ),
                model_sample: String,
                item: T.anything,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              grader:, # The grader used for the fine-tuning job.
              model_sample:, # The model sample to be evaluated. This value will be used to populate the
                             # `sample` namespace. See
                             # [the guide](https://platform.openai.com/docs/guides/graders) for more details.
                             # The `output_json` variable will be populated if the model sample is a valid JSON
                             # string.
              item: nil, # The dataset item provided to the grader. This will be used to populate the
                         # `item` namespace. See
                         # [the guide](https://platform.openai.com/docs/guides/graders) for more details.
              request_options: {}
); end
          end

          # The grader used for the fine-tuning job.
          module Grader
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::Alpha::GraderRunParams::Grader::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias do
                T.any(
                  OpenAI::Graders::StringCheckGrader,
                  OpenAI::Graders::TextSimilarityGrader,
                  OpenAI::Graders::PythonGrader,
                  OpenAI::Graders::ScoreModelGrader,
                  OpenAI::Graders::MultiGrader
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::Alpha::GraderRunParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class GraderRunResponse < OpenAI::Internal::Type::BaseModel
          sig { returns(OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata) }
          attr_reader :metadata

          sig { params(metadata: OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata::OrHash).void }
          attr_writer :metadata

          sig { returns(T::Hash[Symbol, T.anything]) }
          attr_accessor :model_grader_token_usage_per_model

          sig { returns(Float) }
          attr_accessor :reward

          sig { returns(T::Hash[Symbol, T.anything]) }
          attr_accessor :sub_rewards

          sig do
            override
              .returns({
                metadata:
                  OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata,
                model_grader_token_usage_per_model: T::Hash[Symbol, T.anything],
                reward: Float,
                sub_rewards: T::Hash[Symbol, T.anything]
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                metadata: OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata::OrHash,
                model_grader_token_usage_per_model: T::Hash[Symbol, T.anything],
                reward: Float,
                sub_rewards: T::Hash[Symbol, T.anything]
              ).returns(T.attached_class)
            end
            def new(metadata:, model_grader_token_usage_per_model:, reward:, sub_rewards:); end
          end

          class Metadata < OpenAI::Internal::Type::BaseModel
            sig { returns(OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata::Errors) }
            attr_reader :errors

            sig { params(errors: OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata::Errors::OrHash).void }
            attr_writer :errors

            sig { returns(Float) }
            attr_accessor :execution_time

            sig { returns(String) }
            attr_accessor :name

            sig { returns(T.nilable(String)) }
            attr_accessor :sampled_model_name

            sig { returns(T::Hash[Symbol, T.anything]) }
            attr_accessor :scores

            sig { returns(T.nilable(Integer)) }
            attr_accessor :token_usage

            sig { returns(String) }
            attr_accessor :type

            sig do
              override
                .returns({
                  errors:
                    OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata::Errors,
                  execution_time: Float,
                  name: String,
                  sampled_model_name: T.nilable(String),
                  scores: T::Hash[Symbol, T.anything],
                  token_usage: T.nilable(Integer),
                  type: String
                })
            end
            def to_hash; end

            class << self
              sig do
                params(
                  errors: OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata::Errors::OrHash,
                  execution_time: Float,
                  name: String,
                  sampled_model_name: T.nilable(String),
                  scores: T::Hash[Symbol, T.anything],
                  token_usage: T.nilable(Integer),
                  type: String
                ).returns(T.attached_class)
              end
              def new(errors:, execution_time:, name:, sampled_model_name:, scores:, token_usage:, type:); end
            end

            class Errors < OpenAI::Internal::Type::BaseModel
              sig { returns(T::Boolean) }
              attr_accessor :formula_parse_error

              sig { returns(T::Boolean) }
              attr_accessor :invalid_variable_error

              sig { returns(T::Boolean) }
              attr_accessor :model_grader_parse_error

              sig { returns(T::Boolean) }
              attr_accessor :model_grader_refusal_error

              sig { returns(T::Boolean) }
              attr_accessor :model_grader_server_error

              sig { returns(T.nilable(String)) }
              attr_accessor :model_grader_server_error_details

              sig { returns(T::Boolean) }
              attr_accessor :other_error

              sig { returns(T::Boolean) }
              attr_accessor :python_grader_runtime_error

              sig { returns(T.nilable(String)) }
              attr_accessor :python_grader_runtime_error_details

              sig { returns(T::Boolean) }
              attr_accessor :python_grader_server_error

              sig { returns(T.nilable(String)) }
              attr_accessor :python_grader_server_error_type

              sig { returns(T::Boolean) }
              attr_accessor :sample_parse_error

              sig { returns(T::Boolean) }
              attr_accessor :truncated_observation_error

              sig { returns(T::Boolean) }
              attr_accessor :unresponsive_reward_error

              sig do
                override
                  .returns({
                    formula_parse_error: T::Boolean,
                    invalid_variable_error: T::Boolean,
                    model_grader_parse_error: T::Boolean,
                    model_grader_refusal_error: T::Boolean,
                    model_grader_server_error: T::Boolean,
                    model_grader_server_error_details: T.nilable(String),
                    other_error: T::Boolean,
                    python_grader_runtime_error: T::Boolean,
                    python_grader_runtime_error_details: T.nilable(String),
                    python_grader_server_error: T::Boolean,
                    python_grader_server_error_type: T.nilable(String),
                    sample_parse_error: T::Boolean,
                    truncated_observation_error: T::Boolean,
                    unresponsive_reward_error: T::Boolean
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    formula_parse_error: T::Boolean,
                    invalid_variable_error: T::Boolean,
                    model_grader_parse_error: T::Boolean,
                    model_grader_refusal_error: T::Boolean,
                    model_grader_server_error: T::Boolean,
                    model_grader_server_error_details: T.nilable(String),
                    other_error: T::Boolean,
                    python_grader_runtime_error: T::Boolean,
                    python_grader_runtime_error_details: T.nilable(String),
                    python_grader_server_error: T::Boolean,
                    python_grader_server_error_type: T.nilable(String),
                    sample_parse_error: T::Boolean,
                    truncated_observation_error: T::Boolean,
                    unresponsive_reward_error: T::Boolean
                  ).returns(T.attached_class)
                end
                def new(formula_parse_error:, invalid_variable_error:, model_grader_parse_error:, model_grader_refusal_error:, model_grader_server_error:, model_grader_server_error_details:, other_error:, python_grader_runtime_error:, python_grader_runtime_error_details:, python_grader_server_error:, python_grader_server_error_type:, sample_parse_error:, truncated_observation_error:, unresponsive_reward_error:); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata::Errors,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Models::FineTuning::Alpha::GraderRunResponse::Metadata,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::FineTuning::Alpha::GraderRunResponse,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class GraderValidateParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # The grader used for the fine-tuning job.
          sig do
            returns(T.any(
                OpenAI::Graders::StringCheckGrader,
                OpenAI::Graders::TextSimilarityGrader,
                OpenAI::Graders::PythonGrader,
                OpenAI::Graders::ScoreModelGrader,
                OpenAI::Graders::MultiGrader
              ))
          end
          attr_accessor :grader

          sig do
            override
              .returns({
                grader:
                  T.any(
                    OpenAI::Graders::StringCheckGrader,
                    OpenAI::Graders::TextSimilarityGrader,
                    OpenAI::Graders::PythonGrader,
                    OpenAI::Graders::ScoreModelGrader,
                    OpenAI::Graders::MultiGrader
                  ),
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                grader: T.any(
                  OpenAI::Graders::StringCheckGrader::OrHash,
                  OpenAI::Graders::TextSimilarityGrader::OrHash,
                  OpenAI::Graders::PythonGrader::OrHash,
                  OpenAI::Graders::ScoreModelGrader::OrHash,
                  OpenAI::Graders::MultiGrader::OrHash
                ),
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              grader:, # The grader used for the fine-tuning job.
              request_options: {}
); end
          end

          # The grader used for the fine-tuning job.
          module Grader
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::Alpha::GraderValidateParams::Grader::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias do
                T.any(
                  OpenAI::Graders::StringCheckGrader,
                  OpenAI::Graders::TextSimilarityGrader,
                  OpenAI::Graders::PythonGrader,
                  OpenAI::Graders::ScoreModelGrader,
                  OpenAI::Graders::MultiGrader
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::Alpha::GraderValidateParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class GraderValidateResponse < OpenAI::Internal::Type::BaseModel
          # The grader used for the fine-tuning job.
          sig do
            returns(T.nilable(
                OpenAI::Models::FineTuning::Alpha::GraderValidateResponse::Grader::Variants
              ))
          end
          attr_reader :grader

          sig do
            params(
              grader: T.any(
                  OpenAI::Graders::StringCheckGrader::OrHash,
                  OpenAI::Graders::TextSimilarityGrader::OrHash,
                  OpenAI::Graders::PythonGrader::OrHash,
                  OpenAI::Graders::ScoreModelGrader::OrHash,
                  OpenAI::Graders::MultiGrader::OrHash
                )
            ).void
          end
          attr_writer :grader

          sig do
            override
              .returns({
                grader:
                  OpenAI::Models::FineTuning::Alpha::GraderValidateResponse::Grader::Variants
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                grader: T.any(
                  OpenAI::Graders::StringCheckGrader::OrHash,
                  OpenAI::Graders::TextSimilarityGrader::OrHash,
                  OpenAI::Graders::PythonGrader::OrHash,
                  OpenAI::Graders::ScoreModelGrader::OrHash,
                  OpenAI::Graders::MultiGrader::OrHash
                )
              ).returns(T.attached_class)
            end
            def new(
              grader: nil # The grader used for the fine-tuning job.
); end
          end

          # The grader used for the fine-tuning job.
          module Grader
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Models::FineTuning::Alpha::GraderValidateResponse::Grader::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias do
                T.any(
                  OpenAI::Graders::StringCheckGrader,
                  OpenAI::Graders::TextSimilarityGrader,
                  OpenAI::Graders::PythonGrader,
                  OpenAI::Graders::ScoreModelGrader,
                  OpenAI::Graders::MultiGrader
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::FineTuning::Alpha::GraderValidateResponse,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      module Checkpoints
        class PermissionCreateParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # The project identifiers to grant access to.
          sig { returns(T::Array[String]) }
          attr_accessor :project_ids

          sig do
            override
              .returns({
                project_ids: T::Array[String],
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                project_ids: T::Array[String],
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              project_ids:, # The project identifiers to grant access to.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::Checkpoints::PermissionCreateParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class PermissionCreateResponse < OpenAI::Internal::Type::BaseModel
          # The Unix timestamp (in seconds) for when the permission was created.
          sig { returns(Integer) }
          attr_accessor :created_at

          # The permission identifier, which can be referenced in the API endpoints.
          sig { returns(String) }
          attr_accessor :id

          # The object type, which is always "checkpoint.permission".
          sig { returns(Symbol) }
          attr_accessor :object

          # The project identifier that the permission is for.
          sig { returns(String) }
          attr_accessor :project_id

          sig do
            override
              .returns({
                id: String,
                created_at: Integer,
                object: Symbol,
                project_id: String
              })
          end
          def to_hash; end

          class << self
            # The `checkpoint.permission` object represents a permission for a fine-tuned
            # model checkpoint.
            sig do
              params(
                id: String,
                created_at: Integer,
                project_id: String,
                object: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The permission identifier, which can be referenced in the API endpoints.
              created_at:, # The Unix timestamp (in seconds) for when the permission was created.
              project_id:, # The project identifier that the permission is for.
              object: :"checkpoint.permission" # The object type, which is always "checkpoint.permission".
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::FineTuning::Checkpoints::PermissionCreateResponse,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class PermissionDeleteParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          sig { returns(String) }
          attr_accessor :fine_tuned_model_checkpoint

          sig do
            override
              .returns({
                fine_tuned_model_checkpoint: String,
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                fine_tuned_model_checkpoint: String,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(fine_tuned_model_checkpoint:, request_options: {}); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::Checkpoints::PermissionDeleteParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class PermissionDeleteResponse < OpenAI::Internal::Type::BaseModel
          # Whether the fine-tuned model checkpoint permission was successfully deleted.
          sig { returns(T::Boolean) }
          attr_accessor :deleted

          # The ID of the fine-tuned model checkpoint permission that was deleted.
          sig { returns(String) }
          attr_accessor :id

          # The object type, which is always "checkpoint.permission".
          sig { returns(Symbol) }
          attr_accessor :object

          sig { override.returns({ id: String, deleted: T::Boolean, object: Symbol }) }
          def to_hash; end

          class << self
            sig { params(id: String, deleted: T::Boolean, object: Symbol).returns(T.attached_class) }
            def new(
              id:, # The ID of the fine-tuned model checkpoint permission that was deleted.
              deleted:, # Whether the fine-tuned model checkpoint permission was successfully deleted.
              object: :"checkpoint.permission" # The object type, which is always "checkpoint.permission".
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::FineTuning::Checkpoints::PermissionDeleteResponse,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class PermissionRetrieveParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # Identifier for the last permission ID from the previous pagination request.
          sig { returns(T.nilable(String)) }
          attr_reader :after

          sig { params(after: String).void }
          attr_writer :after

          # Number of permissions to retrieve.
          sig { returns(T.nilable(Integer)) }
          attr_reader :limit

          sig { params(limit: Integer).void }
          attr_writer :limit

          # The order in which to retrieve permissions.
          sig do
            returns(T.nilable(
                OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams::Order::OrSymbol
              ))
          end
          attr_reader :order

          sig { params(order: OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams::Order::OrSymbol).void }
          attr_writer :order

          # The ID of the project to get permissions for.
          sig { returns(T.nilable(String)) }
          attr_reader :project_id

          sig { params(project_id: String).void }
          attr_writer :project_id

          sig do
            override
              .returns({
                after: String,
                limit: Integer,
                order:
                  OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams::Order::OrSymbol,
                project_id: String,
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                after: String,
                limit: Integer,
                order: OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams::Order::OrSymbol,
                project_id: String,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              after: nil, # Identifier for the last permission ID from the previous pagination request.
              limit: nil, # Number of permissions to retrieve.
              order: nil, # The order in which to retrieve permissions.
              project_id: nil, # The ID of the project to get permissions for.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams,
                OpenAI::Internal::AnyHash
              )
            end

          # The order in which to retrieve permissions.
          module Order
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams::Order::TaggedSymbol
                ])
              end
              def values; end
            end

            ASCENDING = T.let(
                :ascending,
                OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams::Order::TaggedSymbol
              )

            DESCENDING = T.let(
                :descending,
                OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams::Order::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams::Order
                )
              end
          end
        end

        class PermissionRetrieveResponse < OpenAI::Internal::Type::BaseModel
          sig do
            returns(T::Array[
                OpenAI::Models::FineTuning::Checkpoints::PermissionRetrieveResponse::Data
              ])
          end
          attr_accessor :data

          sig { returns(T.nilable(String)) }
          attr_accessor :first_id

          sig { returns(T::Boolean) }
          attr_accessor :has_more

          sig { returns(T.nilable(String)) }
          attr_accessor :last_id

          sig { returns(Symbol) }
          attr_accessor :object

          sig do
            override
              .returns({
                data:
                  T::Array[
                    OpenAI::Models::FineTuning::Checkpoints::PermissionRetrieveResponse::Data
                  ],
                has_more: T::Boolean,
                object: Symbol,
                first_id: T.nilable(String),
                last_id: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                data: T::Array[
                  OpenAI::Models::FineTuning::Checkpoints::PermissionRetrieveResponse::Data::OrHash
                ],
                has_more: T::Boolean,
                first_id: T.nilable(String),
                last_id: T.nilable(String),
                object: Symbol
              ).returns(T.attached_class)
            end
            def new(data:, has_more:, first_id: nil, last_id: nil, object: :list); end
          end

          class Data < OpenAI::Internal::Type::BaseModel
            # The Unix timestamp (in seconds) for when the permission was created.
            sig { returns(Integer) }
            attr_accessor :created_at

            # The permission identifier, which can be referenced in the API endpoints.
            sig { returns(String) }
            attr_accessor :id

            # The object type, which is always "checkpoint.permission".
            sig { returns(Symbol) }
            attr_accessor :object

            # The project identifier that the permission is for.
            sig { returns(String) }
            attr_accessor :project_id

            sig do
              override
                .returns({
                  id: String,
                  created_at: Integer,
                  object: Symbol,
                  project_id: String
                })
            end
            def to_hash; end

            class << self
              # The `checkpoint.permission` object represents a permission for a fine-tuned
              # model checkpoint.
              sig do
                params(
                  id: String,
                  created_at: Integer,
                  project_id: String,
                  object: Symbol
                ).returns(T.attached_class)
              end
              def new(
                id:, # The permission identifier, which can be referenced in the API endpoints.
                created_at:, # The Unix timestamp (in seconds) for when the permission was created.
                project_id:, # The project identifier that the permission is for.
                object: :"checkpoint.permission" # The object type, which is always "checkpoint.permission".
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Models::FineTuning::Checkpoints::PermissionRetrieveResponse::Data,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Models::FineTuning::Checkpoints::PermissionRetrieveResponse,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class DpoHyperparameters < OpenAI::Internal::Type::BaseModel
        # Number of examples in each batch. A larger batch size means that model
        # parameters are updated less frequently, but with lower variance.
        sig { returns(T.nilable(T.any(Symbol, Integer))) }
        attr_reader :batch_size

        sig { params(batch_size: T.any(Symbol, Integer)).void }
        attr_writer :batch_size

        # The beta value for the DPO method. A higher beta value will increase the weight
        # of the penalty between the policy and reference model.
        sig { returns(T.nilable(T.any(Symbol, Float))) }
        attr_reader :beta

        sig { params(beta: T.any(Symbol, Float)).void }
        attr_writer :beta

        # Scaling factor for the learning rate. A smaller learning rate may be useful to
        # avoid overfitting.
        sig { returns(T.nilable(T.any(Symbol, Float))) }
        attr_reader :learning_rate_multiplier

        sig { params(learning_rate_multiplier: T.any(Symbol, Float)).void }
        attr_writer :learning_rate_multiplier

        # The number of epochs to train the model for. An epoch refers to one full cycle
        # through the training dataset.
        sig { returns(T.nilable(T.any(Symbol, Integer))) }
        attr_reader :n_epochs

        sig { params(n_epochs: T.any(Symbol, Integer)).void }
        attr_writer :n_epochs

        sig do
          override
            .returns({
              batch_size: T.any(Symbol, Integer),
              beta: T.any(Symbol, Float),
              learning_rate_multiplier: T.any(Symbol, Float),
              n_epochs: T.any(Symbol, Integer)
            })
        end
        def to_hash; end

        class << self
          # The hyperparameters used for the DPO fine-tuning job.
          sig do
            params(
              batch_size: T.any(Symbol, Integer),
              beta: T.any(Symbol, Float),
              learning_rate_multiplier: T.any(Symbol, Float),
              n_epochs: T.any(Symbol, Integer)
            ).returns(T.attached_class)
          end
          def new(
            batch_size: nil, # Number of examples in each batch. A larger batch size means that model
                             # parameters are updated less frequently, but with lower variance.
            beta: nil, # The beta value for the DPO method. A higher beta value will increase the weight
                       # of the penalty between the policy and reference model.
            learning_rate_multiplier: nil, # Scaling factor for the learning rate. A smaller learning rate may be useful to
                                           # avoid overfitting.
            n_epochs: nil # The number of epochs to train the model for. An epoch refers to one full cycle
                          # through the training dataset.
); end
        end

        # Number of examples in each batch. A larger batch size means that model
        # parameters are updated less frequently, but with lower variance.
        module BatchSize
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::DpoHyperparameters::BatchSize::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Integer) }
        end

        # The beta value for the DPO method. A higher beta value will increase the weight
        # of the penalty between the policy and reference model.
        module Beta
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::FineTuning::DpoHyperparameters::Beta::Variants]) }
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Float) }
        end

        # Scaling factor for the learning rate. A smaller learning rate may be useful to
        # avoid overfitting.
        module LearningRateMultiplier
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::DpoHyperparameters::LearningRateMultiplier::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Float) }
        end

        # The number of epochs to train the model for. An epoch refers to one full cycle
        # through the training dataset.
        module NEpochs
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::DpoHyperparameters::NEpochs::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Integer) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::DpoHyperparameters,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class DpoMethod < OpenAI::Internal::Type::BaseModel
        # The hyperparameters used for the DPO fine-tuning job.
        sig { returns(T.nilable(OpenAI::FineTuning::DpoHyperparameters)) }
        attr_reader :hyperparameters

        sig { params(hyperparameters: OpenAI::FineTuning::DpoHyperparameters::OrHash).void }
        attr_writer :hyperparameters

        sig { override.returns({ hyperparameters: OpenAI::FineTuning::DpoHyperparameters }) }
        def to_hash; end

        class << self
          # Configuration for the DPO fine-tuning method.
          sig { params(hyperparameters: OpenAI::FineTuning::DpoHyperparameters::OrHash).returns(T.attached_class) }
          def new(
            hyperparameters: nil # The hyperparameters used for the DPO fine-tuning job.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::FineTuning::DpoMethod, OpenAI::Internal::AnyHash)
          end
      end

      class FineTuningJob < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) for when the fine-tuning job was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # For fine-tuning jobs that have `failed`, this will contain more information on
        # the cause of the failure.
        sig { returns(T.nilable(OpenAI::FineTuning::FineTuningJob::Error)) }
        attr_reader :error

        sig { params(error: T.nilable(OpenAI::FineTuning::FineTuningJob::Error::OrHash)).void }
        attr_writer :error

        # The Unix timestamp (in seconds) for when the fine-tuning job is estimated to
        # finish. The value will be null if the fine-tuning job is not running.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :estimated_finish

        # The name of the fine-tuned model that is being created. The value will be null
        # if the fine-tuning job is still running.
        sig { returns(T.nilable(String)) }
        attr_accessor :fine_tuned_model

        # The Unix timestamp (in seconds) for when the fine-tuning job was finished. The
        # value will be null if the fine-tuning job is still running.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :finished_at

        # The hyperparameters used for the fine-tuning job. This value will only be
        # returned when running `supervised` jobs.
        sig { returns(OpenAI::FineTuning::FineTuningJob::Hyperparameters) }
        attr_reader :hyperparameters

        sig { params(hyperparameters: OpenAI::FineTuning::FineTuningJob::Hyperparameters::OrHash).void }
        attr_writer :hyperparameters

        # The object identifier, which can be referenced in the API endpoints.
        sig { returns(String) }
        attr_accessor :id

        # A list of integrations to enable for this fine-tuning job.
        sig do
          returns(T.nilable(
              T::Array[OpenAI::FineTuning::FineTuningJobWandbIntegrationObject]
            ))
        end
        attr_accessor :integrations

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The method used for fine-tuning.
        sig { returns(T.nilable(OpenAI::FineTuning::FineTuningJob::Method)) }
        attr_reader :method_

        sig { params(method_: OpenAI::FineTuning::FineTuningJob::Method::OrHash).void }
        attr_writer :method_

        # The base model that is being fine-tuned.
        sig { returns(String) }
        attr_accessor :model

        # The object type, which is always "fine_tuning.job".
        sig { returns(Symbol) }
        attr_accessor :object

        # The organization that owns the fine-tuning job.
        sig { returns(String) }
        attr_accessor :organization_id

        # The compiled results file ID(s) for the fine-tuning job. You can retrieve the
        # results with the
        # [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).
        sig { returns(T::Array[String]) }
        attr_accessor :result_files

        # The seed used for the fine-tuning job.
        sig { returns(Integer) }
        attr_accessor :seed

        # The current status of the fine-tuning job, which can be either
        # `validating_files`, `queued`, `running`, `succeeded`, `failed`, or `cancelled`.
        sig { returns(OpenAI::FineTuning::FineTuningJob::Status::TaggedSymbol) }
        attr_accessor :status

        # The total number of billable tokens processed by this fine-tuning job. The value
        # will be null if the fine-tuning job is still running.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :trained_tokens

        # The file ID used for training. You can retrieve the training data with the
        # [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).
        sig { returns(String) }
        attr_accessor :training_file

        # The file ID used for validation. You can retrieve the validation results with
        # the
        # [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).
        sig { returns(T.nilable(String)) }
        attr_accessor :validation_file

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              error: T.nilable(OpenAI::FineTuning::FineTuningJob::Error),
              fine_tuned_model: T.nilable(String),
              finished_at: T.nilable(Integer),
              hyperparameters:
                OpenAI::FineTuning::FineTuningJob::Hyperparameters,
              model: String,
              object: Symbol,
              organization_id: String,
              result_files: T::Array[String],
              seed: Integer,
              status: OpenAI::FineTuning::FineTuningJob::Status::TaggedSymbol,
              trained_tokens: T.nilable(Integer),
              training_file: String,
              validation_file: T.nilable(String),
              estimated_finish: T.nilable(Integer),
              integrations:
                T.nilable(
                  T::Array[
                    OpenAI::FineTuning::FineTuningJobWandbIntegrationObject
                  ]
                ),
              metadata: T.nilable(T::Hash[Symbol, String]),
              method_: OpenAI::FineTuning::FineTuningJob::Method
            })
        end
        def to_hash; end

        class << self
          # The `fine_tuning.job` object represents a fine-tuning job that has been created
          # through the API.
          sig do
            params(
              id: String,
              created_at: Integer,
              error: T.nilable(OpenAI::FineTuning::FineTuningJob::Error::OrHash),
              fine_tuned_model: T.nilable(String),
              finished_at: T.nilable(Integer),
              hyperparameters: OpenAI::FineTuning::FineTuningJob::Hyperparameters::OrHash,
              model: String,
              organization_id: String,
              result_files: T::Array[String],
              seed: Integer,
              status: OpenAI::FineTuning::FineTuningJob::Status::OrSymbol,
              trained_tokens: T.nilable(Integer),
              training_file: String,
              validation_file: T.nilable(String),
              estimated_finish: T.nilable(Integer),
              integrations: T.nilable(
                T::Array[
                  OpenAI::FineTuning::FineTuningJobWandbIntegrationObject::OrHash
                ]
              ),
              metadata: T.nilable(T::Hash[Symbol, String]),
              method_: OpenAI::FineTuning::FineTuningJob::Method::OrHash,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The object identifier, which can be referenced in the API endpoints.
            created_at:, # The Unix timestamp (in seconds) for when the fine-tuning job was created.
            error:, # For fine-tuning jobs that have `failed`, this will contain more information on
                    # the cause of the failure.
            fine_tuned_model:, # The name of the fine-tuned model that is being created. The value will be null
                               # if the fine-tuning job is still running.
            finished_at:, # The Unix timestamp (in seconds) for when the fine-tuning job was finished. The
                          # value will be null if the fine-tuning job is still running.
            hyperparameters:, # The hyperparameters used for the fine-tuning job. This value will only be
                              # returned when running `supervised` jobs.
            model:, # The base model that is being fine-tuned.
            organization_id:, # The organization that owns the fine-tuning job.
            result_files:, # The compiled results file ID(s) for the fine-tuning job. You can retrieve the
                           # results with the
                           # [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).
            seed:, # The seed used for the fine-tuning job.
            status:, # The current status of the fine-tuning job, which can be either
                     # `validating_files`, `queued`, `running`, `succeeded`, `failed`, or `cancelled`.
            trained_tokens:, # The total number of billable tokens processed by this fine-tuning job. The value
                             # will be null if the fine-tuning job is still running.
            training_file:, # The file ID used for training. You can retrieve the training data with the
                            # [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).
            validation_file:, # The file ID used for validation. You can retrieve the validation results with
                              # the
                              # [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).
            estimated_finish: nil, # The Unix timestamp (in seconds) for when the fine-tuning job is estimated to
                                   # finish. The value will be null if the fine-tuning job is not running.
            integrations: nil, # A list of integrations to enable for this fine-tuning job.
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            method_: nil, # The method used for fine-tuning.
            object: :"fine_tuning.job" # The object type, which is always "fine_tuning.job".
); end
        end

        class Error < OpenAI::Internal::Type::BaseModel
          # A machine-readable error code.
          sig { returns(String) }
          attr_accessor :code

          # A human-readable error message.
          sig { returns(String) }
          attr_accessor :message

          # The parameter that was invalid, usually `training_file` or `validation_file`.
          # This field will be null if the failure was not parameter-specific.
          sig { returns(T.nilable(String)) }
          attr_accessor :param

          sig { override.returns({ code: String, message: String, param: T.nilable(String) }) }
          def to_hash; end

          class << self
            # For fine-tuning jobs that have `failed`, this will contain more information on
            # the cause of the failure.
            sig { params(code: String, message: String, param: T.nilable(String)).returns(T.attached_class) }
            def new(
              code:, # A machine-readable error code.
              message:, # A human-readable error message.
              param: # The parameter that was invalid, usually `training_file` or `validation_file`.
                     # This field will be null if the failure was not parameter-specific.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::FineTuningJob::Error,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class Hyperparameters < OpenAI::Internal::Type::BaseModel
          # Number of examples in each batch. A larger batch size means that model
          # parameters are updated less frequently, but with lower variance.
          sig do
            returns(T.nilable(
                OpenAI::FineTuning::FineTuningJob::Hyperparameters::BatchSize::Variants
              ))
          end
          attr_accessor :batch_size

          # Scaling factor for the learning rate. A smaller learning rate may be useful to
          # avoid overfitting.
          sig do
            returns(T.nilable(
                OpenAI::FineTuning::FineTuningJob::Hyperparameters::LearningRateMultiplier::Variants
              ))
          end
          attr_reader :learning_rate_multiplier

          sig { params(learning_rate_multiplier: T.any(Symbol, Float)).void }
          attr_writer :learning_rate_multiplier

          # The number of epochs to train the model for. An epoch refers to one full cycle
          # through the training dataset.
          sig do
            returns(T.nilable(
                OpenAI::FineTuning::FineTuningJob::Hyperparameters::NEpochs::Variants
              ))
          end
          attr_reader :n_epochs

          sig { params(n_epochs: T.any(Symbol, Integer)).void }
          attr_writer :n_epochs

          sig do
            override
              .returns({
                batch_size:
                  T.nilable(
                    OpenAI::FineTuning::FineTuningJob::Hyperparameters::BatchSize::Variants
                  ),
                learning_rate_multiplier:
                  OpenAI::FineTuning::FineTuningJob::Hyperparameters::LearningRateMultiplier::Variants,
                n_epochs:
                  OpenAI::FineTuning::FineTuningJob::Hyperparameters::NEpochs::Variants
              })
          end
          def to_hash; end

          class << self
            # The hyperparameters used for the fine-tuning job. This value will only be
            # returned when running `supervised` jobs.
            sig do
              params(
                batch_size: T.nilable(T.any(Symbol, Integer)),
                learning_rate_multiplier: T.any(Symbol, Float),
                n_epochs: T.any(Symbol, Integer)
              ).returns(T.attached_class)
            end
            def new(
              batch_size: nil, # Number of examples in each batch. A larger batch size means that model
                               # parameters are updated less frequently, but with lower variance.
              learning_rate_multiplier: nil, # Scaling factor for the learning rate. A smaller learning rate may be useful to
                                             # avoid overfitting.
              n_epochs: nil # The number of epochs to train the model for. An epoch refers to one full cycle
                            # through the training dataset.
); end
          end

          # Number of examples in each batch. A larger batch size means that model
          # parameters are updated less frequently, but with lower variance.
          module BatchSize
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::FineTuningJob::Hyperparameters::BatchSize::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias { T.any(Symbol, Integer) }
          end

          # Scaling factor for the learning rate. A smaller learning rate may be useful to
          # avoid overfitting.
          module LearningRateMultiplier
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::FineTuningJob::Hyperparameters::LearningRateMultiplier::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias { T.any(Symbol, Float) }
          end

          # The number of epochs to train the model for. An epoch refers to one full cycle
          # through the training dataset.
          module NEpochs
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::FineTuningJob::Hyperparameters::NEpochs::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias { T.any(Symbol, Integer) }
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::FineTuningJob::Hyperparameters,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class Method < OpenAI::Internal::Type::BaseModel
          # Configuration for the DPO fine-tuning method.
          sig { returns(T.nilable(OpenAI::FineTuning::DpoMethod)) }
          attr_reader :dpo

          sig { params(dpo: OpenAI::FineTuning::DpoMethod::OrHash).void }
          attr_writer :dpo

          # Configuration for the reinforcement fine-tuning method.
          sig { returns(T.nilable(OpenAI::FineTuning::ReinforcementMethod)) }
          attr_reader :reinforcement

          sig { params(reinforcement: OpenAI::FineTuning::ReinforcementMethod::OrHash).void }
          attr_writer :reinforcement

          # Configuration for the supervised fine-tuning method.
          sig { returns(T.nilable(OpenAI::FineTuning::SupervisedMethod)) }
          attr_reader :supervised

          sig { params(supervised: OpenAI::FineTuning::SupervisedMethod::OrHash).void }
          attr_writer :supervised

          # The type of method. Is either `supervised`, `dpo`, or `reinforcement`.
          sig { returns(OpenAI::FineTuning::FineTuningJob::Method::Type::TaggedSymbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                type:
                  OpenAI::FineTuning::FineTuningJob::Method::Type::TaggedSymbol,
                dpo: OpenAI::FineTuning::DpoMethod,
                reinforcement: OpenAI::FineTuning::ReinforcementMethod,
                supervised: OpenAI::FineTuning::SupervisedMethod
              })
          end
          def to_hash; end

          class << self
            # The method used for fine-tuning.
            sig do
              params(
                type: OpenAI::FineTuning::FineTuningJob::Method::Type::OrSymbol,
                dpo: OpenAI::FineTuning::DpoMethod::OrHash,
                reinforcement: OpenAI::FineTuning::ReinforcementMethod::OrHash,
                supervised: OpenAI::FineTuning::SupervisedMethod::OrHash
              ).returns(T.attached_class)
            end
            def new(
              type:, # The type of method. Is either `supervised`, `dpo`, or `reinforcement`.
              dpo: nil, # Configuration for the DPO fine-tuning method.
              reinforcement: nil, # Configuration for the reinforcement fine-tuning method.
              supervised: nil # Configuration for the supervised fine-tuning method.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::FineTuningJob::Method,
                OpenAI::Internal::AnyHash
              )
            end

          # The type of method. Is either `supervised`, `dpo`, or `reinforcement`.
          module Type
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::FineTuningJob::Method::Type::TaggedSymbol
                ])
              end
              def values; end
            end

            DPO = T.let(
                :dpo,
                OpenAI::FineTuning::FineTuningJob::Method::Type::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            REINFORCEMENT = T.let(
                :reinforcement,
                OpenAI::FineTuning::FineTuningJob::Method::Type::TaggedSymbol
              )

            SUPERVISED = T.let(
                :supervised,
                OpenAI::FineTuning::FineTuningJob::Method::Type::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::FineTuning::FineTuningJob::Method::Type)
              end
          end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::FineTuning::FineTuningJob, OpenAI::Internal::AnyHash)
          end

        # The current status of the fine-tuning job, which can be either
        # `validating_files`, `queued`, `running`, `succeeded`, `failed`, or `cancelled`.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::FineTuning::FineTuningJob::Status::TaggedSymbol]) }
            def values; end
          end

          CANCELLED = T.let(
              :cancelled,
              OpenAI::FineTuning::FineTuningJob::Status::TaggedSymbol
            )

          FAILED = T.let(
              :failed,
              OpenAI::FineTuning::FineTuningJob::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          QUEUED = T.let(
              :queued,
              OpenAI::FineTuning::FineTuningJob::Status::TaggedSymbol
            )

          RUNNING = T.let(
              :running,
              OpenAI::FineTuning::FineTuningJob::Status::TaggedSymbol
            )

          SUCCEEDED = T.let(
              :succeeded,
              OpenAI::FineTuning::FineTuningJob::Status::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::FineTuning::FineTuningJob::Status)
            end

          VALIDATING_FILES = T.let(
              :validating_files,
              OpenAI::FineTuning::FineTuningJob::Status::TaggedSymbol
            )
        end
      end

      class FineTuningJobEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) for when the fine-tuning job was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # The data associated with the event.
        sig { returns(T.nilable(T.anything)) }
        attr_reader :data

        sig { params(data: T.anything).void }
        attr_writer :data

        # The object identifier.
        sig { returns(String) }
        attr_accessor :id

        # The log level of the event.
        sig { returns(OpenAI::FineTuning::FineTuningJobEvent::Level::TaggedSymbol) }
        attr_accessor :level

        # The message of the event.
        sig { returns(String) }
        attr_accessor :message

        # The object type, which is always "fine_tuning.job.event".
        sig { returns(Symbol) }
        attr_accessor :object

        # The type of event.
        sig do
          returns(T.nilable(
              OpenAI::FineTuning::FineTuningJobEvent::Type::TaggedSymbol
            ))
        end
        attr_reader :type

        sig { params(type: OpenAI::FineTuning::FineTuningJobEvent::Type::OrSymbol).void }
        attr_writer :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              level:
                OpenAI::FineTuning::FineTuningJobEvent::Level::TaggedSymbol,
              message: String,
              object: Symbol,
              data: T.anything,
              type: OpenAI::FineTuning::FineTuningJobEvent::Type::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Fine-tuning job event object
          sig do
            params(
              id: String,
              created_at: Integer,
              level: OpenAI::FineTuning::FineTuningJobEvent::Level::OrSymbol,
              message: String,
              data: T.anything,
              type: OpenAI::FineTuning::FineTuningJobEvent::Type::OrSymbol,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The object identifier.
            created_at:, # The Unix timestamp (in seconds) for when the fine-tuning job was created.
            level:, # The log level of the event.
            message:, # The message of the event.
            data: nil, # The data associated with the event.
            type: nil, # The type of event.
            object: :"fine_tuning.job.event" # The object type, which is always "fine_tuning.job.event".
); end
        end

        # The log level of the event.
        module Level
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::FineTuningJobEvent::Level::TaggedSymbol
              ])
            end
            def values; end
          end

          ERROR = T.let(
              :error,
              OpenAI::FineTuning::FineTuningJobEvent::Level::TaggedSymbol
            )

          INFO = T.let(
              :info,
              OpenAI::FineTuning::FineTuningJobEvent::Level::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::FineTuning::FineTuningJobEvent::Level)
            end

          WARN = T.let(
              :warn,
              OpenAI::FineTuning::FineTuningJobEvent::Level::TaggedSymbol
            )
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::FineTuningJobEvent,
              OpenAI::Internal::AnyHash
            )
          end

        # The type of event.
        module Type
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::FineTuningJobEvent::Type::TaggedSymbol
              ])
            end
            def values; end
          end

          MESSAGE = T.let(
              :message,
              OpenAI::FineTuning::FineTuningJobEvent::Type::TaggedSymbol
            )

          METRICS = T.let(
              :metrics,
              OpenAI::FineTuning::FineTuningJobEvent::Type::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::FineTuning::FineTuningJobEvent::Type)
            end
        end
      end

      FineTuningJobIntegration = OpenAI::Models::FineTuning::FineTuningJobWandbIntegrationObject

      class FineTuningJobWandbIntegration < OpenAI::Internal::Type::BaseModel
        # The entity to use for the run. This allows you to set the team or username of
        # the WandB user that you would like associated with the run. If not set, the
        # default entity for the registered WandB API key is used.
        sig { returns(T.nilable(String)) }
        attr_accessor :entity

        # A display name to set for the run. If not set, we will use the Job ID as the
        # name.
        sig { returns(T.nilable(String)) }
        attr_accessor :name

        # The name of the project that the new run will be created under.
        sig { returns(String) }
        attr_accessor :project

        # A list of tags to be attached to the newly created run. These tags are passed
        # through directly to WandB. Some default tags are generated by OpenAI:
        # "openai/finetune", "openai/{base-model}", "openai/{ftjob-abcdef}".
        sig { returns(T.nilable(T::Array[String])) }
        attr_reader :tags

        sig { params(tags: T::Array[String]).void }
        attr_writer :tags

        sig do
          override
            .returns({
              project: String,
              entity: T.nilable(String),
              name: T.nilable(String),
              tags: T::Array[String]
            })
        end
        def to_hash; end

        class << self
          # The settings for your integration with Weights and Biases. This payload
          # specifies the project that metrics will be sent to. Optionally, you can set an
          # explicit display name for your run, add tags to your run, and set a default
          # entity (team, username, etc) to be associated with your run.
          sig do
            params(
              project: String,
              entity: T.nilable(String),
              name: T.nilable(String),
              tags: T::Array[String]
            ).returns(T.attached_class)
          end
          def new(
            project:, # The name of the project that the new run will be created under.
            entity: nil, # The entity to use for the run. This allows you to set the team or username of
                         # the WandB user that you would like associated with the run. If not set, the
                         # default entity for the registered WandB API key is used.
            name: nil, # A display name to set for the run. If not set, we will use the Job ID as the
                       # name.
            tags: nil # A list of tags to be attached to the newly created run. These tags are passed
                      # through directly to WandB. Some default tags are generated by OpenAI:
                      # "openai/finetune", "openai/{base-model}", "openai/{ftjob-abcdef}".
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::FineTuningJobWandbIntegration,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FineTuningJobWandbIntegrationObject < OpenAI::Internal::Type::BaseModel
        # The type of the integration being enabled for the fine-tuning job
        sig { returns(Symbol) }
        attr_accessor :type

        # The settings for your integration with Weights and Biases. This payload
        # specifies the project that metrics will be sent to. Optionally, you can set an
        # explicit display name for your run, add tags to your run, and set a default
        # entity (team, username, etc) to be associated with your run.
        sig { returns(OpenAI::FineTuning::FineTuningJobWandbIntegration) }
        attr_reader :wandb

        sig { params(wandb: OpenAI::FineTuning::FineTuningJobWandbIntegration::OrHash).void }
        attr_writer :wandb

        sig do
          override
            .returns({
              type: Symbol,
              wandb: OpenAI::FineTuning::FineTuningJobWandbIntegration
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              wandb: OpenAI::FineTuning::FineTuningJobWandbIntegration::OrHash,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            wandb:, # The settings for your integration with Weights and Biases. This payload
                    # specifies the project that metrics will be sent to. Optionally, you can set an
                    # explicit display name for your run, add tags to your run, and set a default
                    # entity (team, username, etc) to be associated with your run.
            type: :wandb # The type of the integration being enabled for the fine-tuning job
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::FineTuningJobWandbIntegrationObject,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class JobCancelParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::JobCancelParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class JobCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # The hyperparameters used for the fine-tuning job. This value is now deprecated
        # in favor of `method`, and should be passed in under the `method` parameter.
        sig { returns(T.nilable(OpenAI::FineTuning::JobCreateParams::Hyperparameters)) }
        attr_reader :hyperparameters

        sig { params(hyperparameters: OpenAI::FineTuning::JobCreateParams::Hyperparameters::OrHash).void }
        attr_writer :hyperparameters

        # A list of integrations to enable for your fine-tuning job.
        sig do
          returns(T.nilable(
              T::Array[OpenAI::FineTuning::JobCreateParams::Integration]
            ))
        end
        attr_accessor :integrations

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # The method used for fine-tuning.
        sig { returns(T.nilable(OpenAI::FineTuning::JobCreateParams::Method)) }
        attr_reader :method_

        sig { params(method_: OpenAI::FineTuning::JobCreateParams::Method::OrHash).void }
        attr_writer :method_

        # The name of the model to fine-tune. You can select one of the
        # [supported models](https://platform.openai.com/docs/guides/fine-tuning#which-models-can-be-fine-tuned).
        sig { returns(T.any(String, OpenAI::FineTuning::JobCreateParams::Model::OrSymbol)) }
        attr_accessor :model

        # The seed controls the reproducibility of the job. Passing in the same seed and
        # job parameters should produce the same results, but may differ in rare cases. If
        # a seed is not specified, one will be generated for you.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :seed

        # A string of up to 64 characters that will be added to your fine-tuned model
        # name.
        #
        # For example, a `suffix` of "custom-model-name" would produce a model name like
        # `ft:gpt-4o-mini:openai:custom-model-name:7p4lURel`.
        sig { returns(T.nilable(String)) }
        attr_accessor :suffix

        # The ID of an uploaded file that contains training data.
        #
        # See [upload file](https://platform.openai.com/docs/api-reference/files/create)
        # for how to upload a file.
        #
        # Your dataset must be formatted as a JSONL file. Additionally, you must upload
        # your file with the purpose `fine-tune`.
        #
        # The contents of the file should differ depending on if the model uses the
        # [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input),
        # [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)
        # format, or if the fine-tuning method uses the
        # [preference](https://platform.openai.com/docs/api-reference/fine-tuning/preference-input)
        # format.
        #
        # See the
        # [fine-tuning guide](https://platform.openai.com/docs/guides/model-optimization)
        # for more details.
        sig { returns(String) }
        attr_accessor :training_file

        # The ID of an uploaded file that contains validation data.
        #
        # If you provide this file, the data is used to generate validation metrics
        # periodically during fine-tuning. These metrics can be viewed in the fine-tuning
        # results file. The same data should not be present in both train and validation
        # files.
        #
        # Your dataset must be formatted as a JSONL file. You must upload your file with
        # the purpose `fine-tune`.
        #
        # See the
        # [fine-tuning guide](https://platform.openai.com/docs/guides/model-optimization)
        # for more details.
        sig { returns(T.nilable(String)) }
        attr_accessor :validation_file

        sig do
          override
            .returns({
              model:
                T.any(
                  String,
                  OpenAI::FineTuning::JobCreateParams::Model::OrSymbol
                ),
              training_file: String,
              hyperparameters:
                OpenAI::FineTuning::JobCreateParams::Hyperparameters,
              integrations:
                T.nilable(
                  T::Array[OpenAI::FineTuning::JobCreateParams::Integration]
                ),
              metadata: T.nilable(T::Hash[Symbol, String]),
              method_: OpenAI::FineTuning::JobCreateParams::Method,
              seed: T.nilable(Integer),
              suffix: T.nilable(String),
              validation_file: T.nilable(String),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              model: T.any(
                String,
                OpenAI::FineTuning::JobCreateParams::Model::OrSymbol
              ),
              training_file: String,
              hyperparameters: OpenAI::FineTuning::JobCreateParams::Hyperparameters::OrHash,
              integrations: T.nilable(
                T::Array[
                  OpenAI::FineTuning::JobCreateParams::Integration::OrHash
                ]
              ),
              metadata: T.nilable(T::Hash[Symbol, String]),
              method_: OpenAI::FineTuning::JobCreateParams::Method::OrHash,
              seed: T.nilable(Integer),
              suffix: T.nilable(String),
              validation_file: T.nilable(String),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            model:, # The name of the model to fine-tune. You can select one of the
                    # [supported models](https://platform.openai.com/docs/guides/fine-tuning#which-models-can-be-fine-tuned).
            training_file:, # The ID of an uploaded file that contains training data.
                            # See [upload file](https://platform.openai.com/docs/api-reference/files/create)
                            # for how to upload a file.
                            # Your dataset must be formatted as a JSONL file. Additionally, you must upload
                            # your file with the purpose `fine-tune`.
                            # The contents of the file should differ depending on if the model uses the
                            # [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input),
                            # [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)
                            # format, or if the fine-tuning method uses the
                            # [preference](https://platform.openai.com/docs/api-reference/fine-tuning/preference-input)
                            # format.
                            # See the
                            # [fine-tuning guide](https://platform.openai.com/docs/guides/model-optimization)
                            # for more details.
            hyperparameters: nil, # The hyperparameters used for the fine-tuning job. This value is now deprecated
                                  # in favor of `method`, and should be passed in under the `method` parameter.
            integrations: nil, # A list of integrations to enable for your fine-tuning job.
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            method_: nil, # The method used for fine-tuning.
            seed: nil, # The seed controls the reproducibility of the job. Passing in the same seed and
                       # job parameters should produce the same results, but may differ in rare cases. If
                       # a seed is not specified, one will be generated for you.
            suffix: nil, # A string of up to 64 characters that will be added to your fine-tuned model
                         # name.
                         # For example, a `suffix` of "custom-model-name" would produce a model name like
                         # `ft:gpt-4o-mini:openai:custom-model-name:7p4lURel`.
            validation_file: nil, # The ID of an uploaded file that contains validation data.
                                  # If you provide this file, the data is used to generate validation metrics
                                  # periodically during fine-tuning. These metrics can be viewed in the fine-tuning
                                  # results file. The same data should not be present in both train and validation
                                  # files.
                                  # Your dataset must be formatted as a JSONL file. You must upload your file with
                                  # the purpose `fine-tune`.
                                  # See the
                                  # [fine-tuning guide](https://platform.openai.com/docs/guides/model-optimization)
                                  # for more details.
            request_options: {}
); end
        end

        class Hyperparameters < OpenAI::Internal::Type::BaseModel
          # Number of examples in each batch. A larger batch size means that model
          # parameters are updated less frequently, but with lower variance.
          sig { returns(T.nilable(T.any(Symbol, Integer))) }
          attr_reader :batch_size

          sig { params(batch_size: T.any(Symbol, Integer)).void }
          attr_writer :batch_size

          # Scaling factor for the learning rate. A smaller learning rate may be useful to
          # avoid overfitting.
          sig { returns(T.nilable(T.any(Symbol, Float))) }
          attr_reader :learning_rate_multiplier

          sig { params(learning_rate_multiplier: T.any(Symbol, Float)).void }
          attr_writer :learning_rate_multiplier

          # The number of epochs to train the model for. An epoch refers to one full cycle
          # through the training dataset.
          sig { returns(T.nilable(T.any(Symbol, Integer))) }
          attr_reader :n_epochs

          sig { params(n_epochs: T.any(Symbol, Integer)).void }
          attr_writer :n_epochs

          sig do
            override
              .returns({
                batch_size: T.any(Symbol, Integer),
                learning_rate_multiplier: T.any(Symbol, Float),
                n_epochs: T.any(Symbol, Integer)
              })
          end
          def to_hash; end

          class << self
            # The hyperparameters used for the fine-tuning job. This value is now deprecated
            # in favor of `method`, and should be passed in under the `method` parameter.
            sig do
              params(
                batch_size: T.any(Symbol, Integer),
                learning_rate_multiplier: T.any(Symbol, Float),
                n_epochs: T.any(Symbol, Integer)
              ).returns(T.attached_class)
            end
            def new(
              batch_size: nil, # Number of examples in each batch. A larger batch size means that model
                               # parameters are updated less frequently, but with lower variance.
              learning_rate_multiplier: nil, # Scaling factor for the learning rate. A smaller learning rate may be useful to
                                             # avoid overfitting.
              n_epochs: nil # The number of epochs to train the model for. An epoch refers to one full cycle
                            # through the training dataset.
); end
          end

          # Number of examples in each batch. A larger batch size means that model
          # parameters are updated less frequently, but with lower variance.
          module BatchSize
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::JobCreateParams::Hyperparameters::BatchSize::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias { T.any(Symbol, Integer) }
          end

          # Scaling factor for the learning rate. A smaller learning rate may be useful to
          # avoid overfitting.
          module LearningRateMultiplier
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::JobCreateParams::Hyperparameters::LearningRateMultiplier::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias { T.any(Symbol, Float) }
          end

          # The number of epochs to train the model for. An epoch refers to one full cycle
          # through the training dataset.
          module NEpochs
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::JobCreateParams::Hyperparameters::NEpochs::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias { T.any(Symbol, Integer) }
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::JobCreateParams::Hyperparameters,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class Integration < OpenAI::Internal::Type::BaseModel
          # The type of integration to enable. Currently, only "wandb" (Weights and Biases)
          # is supported.
          sig { returns(Symbol) }
          attr_accessor :type

          # The settings for your integration with Weights and Biases. This payload
          # specifies the project that metrics will be sent to. Optionally, you can set an
          # explicit display name for your run, add tags to your run, and set a default
          # entity (team, username, etc) to be associated with your run.
          sig { returns(OpenAI::FineTuning::JobCreateParams::Integration::Wandb) }
          attr_reader :wandb

          sig { params(wandb: OpenAI::FineTuning::JobCreateParams::Integration::Wandb::OrHash).void }
          attr_writer :wandb

          sig do
            override
              .returns({
                type: Symbol,
                wandb: OpenAI::FineTuning::JobCreateParams::Integration::Wandb
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                wandb: OpenAI::FineTuning::JobCreateParams::Integration::Wandb::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              wandb:, # The settings for your integration with Weights and Biases. This payload
                      # specifies the project that metrics will be sent to. Optionally, you can set an
                      # explicit display name for your run, add tags to your run, and set a default
                      # entity (team, username, etc) to be associated with your run.
              type: :wandb # The type of integration to enable. Currently, only "wandb" (Weights and Biases)
                           # is supported.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::JobCreateParams::Integration,
                OpenAI::Internal::AnyHash
              )
            end

          class Wandb < OpenAI::Internal::Type::BaseModel
            # The entity to use for the run. This allows you to set the team or username of
            # the WandB user that you would like associated with the run. If not set, the
            # default entity for the registered WandB API key is used.
            sig { returns(T.nilable(String)) }
            attr_accessor :entity

            # A display name to set for the run. If not set, we will use the Job ID as the
            # name.
            sig { returns(T.nilable(String)) }
            attr_accessor :name

            # The name of the project that the new run will be created under.
            sig { returns(String) }
            attr_accessor :project

            # A list of tags to be attached to the newly created run. These tags are passed
            # through directly to WandB. Some default tags are generated by OpenAI:
            # "openai/finetune", "openai/{base-model}", "openai/{ftjob-abcdef}".
            sig { returns(T.nilable(T::Array[String])) }
            attr_reader :tags

            sig { params(tags: T::Array[String]).void }
            attr_writer :tags

            sig do
              override
                .returns({
                  project: String,
                  entity: T.nilable(String),
                  name: T.nilable(String),
                  tags: T::Array[String]
                })
            end
            def to_hash; end

            class << self
              # The settings for your integration with Weights and Biases. This payload
              # specifies the project that metrics will be sent to. Optionally, you can set an
              # explicit display name for your run, add tags to your run, and set a default
              # entity (team, username, etc) to be associated with your run.
              sig do
                params(
                  project: String,
                  entity: T.nilable(String),
                  name: T.nilable(String),
                  tags: T::Array[String]
                ).returns(T.attached_class)
              end
              def new(
                project:, # The name of the project that the new run will be created under.
                entity: nil, # The entity to use for the run. This allows you to set the team or username of
                             # the WandB user that you would like associated with the run. If not set, the
                             # default entity for the registered WandB API key is used.
                name: nil, # A display name to set for the run. If not set, we will use the Job ID as the
                           # name.
                tags: nil # A list of tags to be attached to the newly created run. These tags are passed
                          # through directly to WandB. Some default tags are generated by OpenAI:
                          # "openai/finetune", "openai/{base-model}", "openai/{ftjob-abcdef}".
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::FineTuning::JobCreateParams::Integration::Wandb,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        class Method < OpenAI::Internal::Type::BaseModel
          # Configuration for the DPO fine-tuning method.
          sig { returns(T.nilable(OpenAI::FineTuning::DpoMethod)) }
          attr_reader :dpo

          sig { params(dpo: OpenAI::FineTuning::DpoMethod::OrHash).void }
          attr_writer :dpo

          # Configuration for the reinforcement fine-tuning method.
          sig { returns(T.nilable(OpenAI::FineTuning::ReinforcementMethod)) }
          attr_reader :reinforcement

          sig { params(reinforcement: OpenAI::FineTuning::ReinforcementMethod::OrHash).void }
          attr_writer :reinforcement

          # Configuration for the supervised fine-tuning method.
          sig { returns(T.nilable(OpenAI::FineTuning::SupervisedMethod)) }
          attr_reader :supervised

          sig { params(supervised: OpenAI::FineTuning::SupervisedMethod::OrHash).void }
          attr_writer :supervised

          # The type of method. Is either `supervised`, `dpo`, or `reinforcement`.
          sig { returns(OpenAI::FineTuning::JobCreateParams::Method::Type::OrSymbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                type:
                  OpenAI::FineTuning::JobCreateParams::Method::Type::OrSymbol,
                dpo: OpenAI::FineTuning::DpoMethod,
                reinforcement: OpenAI::FineTuning::ReinforcementMethod,
                supervised: OpenAI::FineTuning::SupervisedMethod
              })
          end
          def to_hash; end

          class << self
            # The method used for fine-tuning.
            sig do
              params(
                type: OpenAI::FineTuning::JobCreateParams::Method::Type::OrSymbol,
                dpo: OpenAI::FineTuning::DpoMethod::OrHash,
                reinforcement: OpenAI::FineTuning::ReinforcementMethod::OrHash,
                supervised: OpenAI::FineTuning::SupervisedMethod::OrHash
              ).returns(T.attached_class)
            end
            def new(
              type:, # The type of method. Is either `supervised`, `dpo`, or `reinforcement`.
              dpo: nil, # Configuration for the DPO fine-tuning method.
              reinforcement: nil, # Configuration for the reinforcement fine-tuning method.
              supervised: nil # Configuration for the supervised fine-tuning method.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::JobCreateParams::Method,
                OpenAI::Internal::AnyHash
              )
            end

          # The type of method. Is either `supervised`, `dpo`, or `reinforcement`.
          module Type
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::FineTuning::JobCreateParams::Method::Type::TaggedSymbol
                ])
              end
              def values; end
            end

            DPO = T.let(
                :dpo,
                OpenAI::FineTuning::JobCreateParams::Method::Type::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            REINFORCEMENT = T.let(
                :reinforcement,
                OpenAI::FineTuning::JobCreateParams::Method::Type::TaggedSymbol
              )

            SUPERVISED = T.let(
                :supervised,
                OpenAI::FineTuning::JobCreateParams::Method::Type::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::FineTuning::JobCreateParams::Method::Type)
              end
          end
        end

        # The name of the model to fine-tune. You can select one of the
        # [supported models](https://platform.openai.com/docs/guides/fine-tuning#which-models-can-be-fine-tuned).
        module Model
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::FineTuning::JobCreateParams::Model::Variants]) }
            def variants; end
          end

          BABBAGE_002 = T.let(
              :"babbage-002",
              OpenAI::FineTuning::JobCreateParams::Model::TaggedSymbol
            )

          DAVINCI_002 = T.let(
              :"davinci-002",
              OpenAI::FineTuning::JobCreateParams::Model::TaggedSymbol
            )

          GPT_3_5_TURBO = T.let(
              :"gpt-3.5-turbo",
              OpenAI::FineTuning::JobCreateParams::Model::TaggedSymbol
            )

          GPT_4O_MINI = T.let(
              :"gpt-4o-mini",
              OpenAI::FineTuning::JobCreateParams::Model::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::FineTuning::JobCreateParams::Model)
            end

          Variants = T.type_alias do
              T.any(
                String,
                OpenAI::FineTuning::JobCreateParams::Model::TaggedSymbol
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::JobCreateParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class JobListEventsParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Identifier for the last event from the previous pagination request.
        sig { returns(T.nilable(String)) }
        attr_reader :after

        sig { params(after: String).void }
        attr_writer :after

        # Number of events to retrieve.
        sig { returns(T.nilable(Integer)) }
        attr_reader :limit

        sig { params(limit: Integer).void }
        attr_writer :limit

        sig do
          override
            .returns({
              after: String,
              limit: Integer,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              after: String,
              limit: Integer,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            after: nil, # Identifier for the last event from the previous pagination request.
            limit: nil, # Number of events to retrieve.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::JobListEventsParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class JobListParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Identifier for the last job from the previous pagination request.
        sig { returns(T.nilable(String)) }
        attr_reader :after

        sig { params(after: String).void }
        attr_writer :after

        # Number of fine-tuning jobs to retrieve.
        sig { returns(T.nilable(Integer)) }
        attr_reader :limit

        sig { params(limit: Integer).void }
        attr_writer :limit

        # Optional metadata filter. To filter, use the syntax `metadata[k]=v`.
        # Alternatively, set `metadata=null` to indicate no metadata.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        sig do
          override
            .returns({
              after: String,
              limit: Integer,
              metadata: T.nilable(T::Hash[Symbol, String]),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              after: String,
              limit: Integer,
              metadata: T.nilable(T::Hash[Symbol, String]),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            after: nil, # Identifier for the last job from the previous pagination request.
            limit: nil, # Number of fine-tuning jobs to retrieve.
            metadata: nil, # Optional metadata filter. To filter, use the syntax `metadata[k]=v`.
                           # Alternatively, set `metadata=null` to indicate no metadata.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::FineTuning::JobListParams, OpenAI::Internal::AnyHash)
          end
      end

      class JobPauseParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::FineTuning::JobPauseParams, OpenAI::Internal::AnyHash)
          end
      end

      class JobResumeParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::JobResumeParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class JobRetrieveParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::JobRetrieveParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      module Jobs
        class CheckpointListParams < OpenAI::Internal::Type::BaseModel
          extend OpenAI::Internal::Type::RequestParameters::Converter
          include OpenAI::Internal::Type::RequestParameters

          # Identifier for the last checkpoint ID from the previous pagination request.
          sig { returns(T.nilable(String)) }
          attr_reader :after

          sig { params(after: String).void }
          attr_writer :after

          # Number of checkpoints to retrieve.
          sig { returns(T.nilable(Integer)) }
          attr_reader :limit

          sig { params(limit: Integer).void }
          attr_writer :limit

          sig do
            override
              .returns({
                after: String,
                limit: Integer,
                request_options: OpenAI::RequestOptions
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                after: String,
                limit: Integer,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(T.attached_class)
            end
            def new(
              after: nil, # Identifier for the last checkpoint ID from the previous pagination request.
              limit: nil, # Number of checkpoints to retrieve.
              request_options: {}
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::Jobs::CheckpointListParams,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class FineTuningJobCheckpoint < OpenAI::Internal::Type::BaseModel
          # The Unix timestamp (in seconds) for when the checkpoint was created.
          sig { returns(Integer) }
          attr_accessor :created_at

          # The name of the fine-tuned checkpoint model that is created.
          sig { returns(String) }
          attr_accessor :fine_tuned_model_checkpoint

          # The name of the fine-tuning job that this checkpoint was created from.
          sig { returns(String) }
          attr_accessor :fine_tuning_job_id

          # The checkpoint identifier, which can be referenced in the API endpoints.
          sig { returns(String) }
          attr_accessor :id

          # Metrics at the step number during the fine-tuning job.
          sig { returns(OpenAI::FineTuning::Jobs::FineTuningJobCheckpoint::Metrics) }
          attr_reader :metrics

          sig { params(metrics: OpenAI::FineTuning::Jobs::FineTuningJobCheckpoint::Metrics::OrHash).void }
          attr_writer :metrics

          # The object type, which is always "fine_tuning.job.checkpoint".
          sig { returns(Symbol) }
          attr_accessor :object

          # The step number that the checkpoint was created at.
          sig { returns(Integer) }
          attr_accessor :step_number

          sig do
            override
              .returns({
                id: String,
                created_at: Integer,
                fine_tuned_model_checkpoint: String,
                fine_tuning_job_id: String,
                metrics:
                  OpenAI::FineTuning::Jobs::FineTuningJobCheckpoint::Metrics,
                object: Symbol,
                step_number: Integer
              })
          end
          def to_hash; end

          class << self
            # The `fine_tuning.job.checkpoint` object represents a model checkpoint for a
            # fine-tuning job that is ready to use.
            sig do
              params(
                id: String,
                created_at: Integer,
                fine_tuned_model_checkpoint: String,
                fine_tuning_job_id: String,
                metrics: OpenAI::FineTuning::Jobs::FineTuningJobCheckpoint::Metrics::OrHash,
                step_number: Integer,
                object: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The checkpoint identifier, which can be referenced in the API endpoints.
              created_at:, # The Unix timestamp (in seconds) for when the checkpoint was created.
              fine_tuned_model_checkpoint:, # The name of the fine-tuned checkpoint model that is created.
              fine_tuning_job_id:, # The name of the fine-tuning job that this checkpoint was created from.
              metrics:, # Metrics at the step number during the fine-tuning job.
              step_number:, # The step number that the checkpoint was created at.
              object: :"fine_tuning.job.checkpoint" # The object type, which is always "fine_tuning.job.checkpoint".
); end
          end

          class Metrics < OpenAI::Internal::Type::BaseModel
            sig { returns(T.nilable(Float)) }
            attr_reader :full_valid_loss

            sig { params(full_valid_loss: Float).void }
            attr_writer :full_valid_loss

            sig { returns(T.nilable(Float)) }
            attr_reader :full_valid_mean_token_accuracy

            sig { params(full_valid_mean_token_accuracy: Float).void }
            attr_writer :full_valid_mean_token_accuracy

            sig { returns(T.nilable(Float)) }
            attr_reader :step

            sig { params(step: Float).void }
            attr_writer :step

            sig { returns(T.nilable(Float)) }
            attr_reader :train_loss

            sig { params(train_loss: Float).void }
            attr_writer :train_loss

            sig { returns(T.nilable(Float)) }
            attr_reader :train_mean_token_accuracy

            sig { params(train_mean_token_accuracy: Float).void }
            attr_writer :train_mean_token_accuracy

            sig { returns(T.nilable(Float)) }
            attr_reader :valid_loss

            sig { params(valid_loss: Float).void }
            attr_writer :valid_loss

            sig { returns(T.nilable(Float)) }
            attr_reader :valid_mean_token_accuracy

            sig { params(valid_mean_token_accuracy: Float).void }
            attr_writer :valid_mean_token_accuracy

            sig do
              override
                .returns({
                  full_valid_loss: Float,
                  full_valid_mean_token_accuracy: Float,
                  step: Float,
                  train_loss: Float,
                  train_mean_token_accuracy: Float,
                  valid_loss: Float,
                  valid_mean_token_accuracy: Float
                })
            end
            def to_hash; end

            class << self
              # Metrics at the step number during the fine-tuning job.
              sig do
                params(
                  full_valid_loss: Float,
                  full_valid_mean_token_accuracy: Float,
                  step: Float,
                  train_loss: Float,
                  train_mean_token_accuracy: Float,
                  valid_loss: Float,
                  valid_mean_token_accuracy: Float
                ).returns(T.attached_class)
              end
              def new(full_valid_loss: nil, full_valid_mean_token_accuracy: nil, step: nil, train_loss: nil, train_mean_token_accuracy: nil, valid_loss: nil, valid_mean_token_accuracy: nil); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::FineTuning::Jobs::FineTuningJobCheckpoint::Metrics,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::FineTuning::Jobs::FineTuningJobCheckpoint,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class ReinforcementHyperparameters < OpenAI::Internal::Type::BaseModel
        # Number of examples in each batch. A larger batch size means that model
        # parameters are updated less frequently, but with lower variance.
        sig { returns(T.nilable(T.any(Symbol, Integer))) }
        attr_reader :batch_size

        sig { params(batch_size: T.any(Symbol, Integer)).void }
        attr_writer :batch_size

        # Multiplier on amount of compute used for exploring search space during training.
        sig { returns(T.nilable(T.any(Symbol, Float))) }
        attr_reader :compute_multiplier

        sig { params(compute_multiplier: T.any(Symbol, Float)).void }
        attr_writer :compute_multiplier

        # The number of training steps between evaluation runs.
        sig { returns(T.nilable(T.any(Symbol, Integer))) }
        attr_reader :eval_interval

        sig { params(eval_interval: T.any(Symbol, Integer)).void }
        attr_writer :eval_interval

        # Number of evaluation samples to generate per training step.
        sig { returns(T.nilable(T.any(Symbol, Integer))) }
        attr_reader :eval_samples

        sig { params(eval_samples: T.any(Symbol, Integer)).void }
        attr_writer :eval_samples

        # Scaling factor for the learning rate. A smaller learning rate may be useful to
        # avoid overfitting.
        sig { returns(T.nilable(T.any(Symbol, Float))) }
        attr_reader :learning_rate_multiplier

        sig { params(learning_rate_multiplier: T.any(Symbol, Float)).void }
        attr_writer :learning_rate_multiplier

        # The number of epochs to train the model for. An epoch refers to one full cycle
        # through the training dataset.
        sig { returns(T.nilable(T.any(Symbol, Integer))) }
        attr_reader :n_epochs

        sig { params(n_epochs: T.any(Symbol, Integer)).void }
        attr_writer :n_epochs

        # Level of reasoning effort.
        sig do
          returns(T.nilable(
              OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort::OrSymbol
            ))
        end
        attr_reader :reasoning_effort

        sig do
          params(
            reasoning_effort: OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort::OrSymbol
          ).void
        end
        attr_writer :reasoning_effort

        sig do
          override
            .returns({
              batch_size: T.any(Symbol, Integer),
              compute_multiplier: T.any(Symbol, Float),
              eval_interval: T.any(Symbol, Integer),
              eval_samples: T.any(Symbol, Integer),
              learning_rate_multiplier: T.any(Symbol, Float),
              n_epochs: T.any(Symbol, Integer),
              reasoning_effort:
                OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort::OrSymbol
            })
        end
        def to_hash; end

        class << self
          # The hyperparameters used for the reinforcement fine-tuning job.
          sig do
            params(
              batch_size: T.any(Symbol, Integer),
              compute_multiplier: T.any(Symbol, Float),
              eval_interval: T.any(Symbol, Integer),
              eval_samples: T.any(Symbol, Integer),
              learning_rate_multiplier: T.any(Symbol, Float),
              n_epochs: T.any(Symbol, Integer),
              reasoning_effort: OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort::OrSymbol
            ).returns(T.attached_class)
          end
          def new(
            batch_size: nil, # Number of examples in each batch. A larger batch size means that model
                             # parameters are updated less frequently, but with lower variance.
            compute_multiplier: nil, # Multiplier on amount of compute used for exploring search space during training.
            eval_interval: nil, # The number of training steps between evaluation runs.
            eval_samples: nil, # Number of evaluation samples to generate per training step.
            learning_rate_multiplier: nil, # Scaling factor for the learning rate. A smaller learning rate may be useful to
                                           # avoid overfitting.
            n_epochs: nil, # The number of epochs to train the model for. An epoch refers to one full cycle
                           # through the training dataset.
            reasoning_effort: nil # Level of reasoning effort.
); end
        end

        # Number of examples in each batch. A larger batch size means that model
        # parameters are updated less frequently, but with lower variance.
        module BatchSize
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::ReinforcementHyperparameters::BatchSize::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Integer) }
        end

        # Multiplier on amount of compute used for exploring search space during training.
        module ComputeMultiplier
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::ReinforcementHyperparameters::ComputeMultiplier::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Float) }
        end

        # The number of training steps between evaluation runs.
        module EvalInterval
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::ReinforcementHyperparameters::EvalInterval::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Integer) }
        end

        # Number of evaluation samples to generate per training step.
        module EvalSamples
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::ReinforcementHyperparameters::EvalSamples::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Integer) }
        end

        # Scaling factor for the learning rate. A smaller learning rate may be useful to
        # avoid overfitting.
        module LearningRateMultiplier
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::ReinforcementHyperparameters::LearningRateMultiplier::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Float) }
        end

        # The number of epochs to train the model for. An epoch refers to one full cycle
        # through the training dataset.
        module NEpochs
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::ReinforcementHyperparameters::NEpochs::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Integer) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::ReinforcementHyperparameters,
              OpenAI::Internal::AnyHash
            )
          end

        # Level of reasoning effort.
        module ReasoningEffort
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort::TaggedSymbol
              ])
            end
            def values; end
          end

          DEFAULT = T.let(
              :default,
              OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort::TaggedSymbol
            )

          HIGH = T.let(
              :high,
              OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort::TaggedSymbol
            )

          LOW = T.let(
              :low,
              OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort::TaggedSymbol
            )

          MEDIUM = T.let(
              :medium,
              OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::FineTuning::ReinforcementHyperparameters::ReasoningEffort
              )
            end
        end
      end

      class ReinforcementMethod < OpenAI::Internal::Type::BaseModel
        # The grader used for the fine-tuning job.
        sig do
          returns(T.any(
              OpenAI::Graders::StringCheckGrader,
              OpenAI::Graders::TextSimilarityGrader,
              OpenAI::Graders::PythonGrader,
              OpenAI::Graders::ScoreModelGrader,
              OpenAI::Graders::MultiGrader
            ))
        end
        attr_accessor :grader

        # The hyperparameters used for the reinforcement fine-tuning job.
        sig { returns(T.nilable(OpenAI::FineTuning::ReinforcementHyperparameters)) }
        attr_reader :hyperparameters

        sig { params(hyperparameters: OpenAI::FineTuning::ReinforcementHyperparameters::OrHash).void }
        attr_writer :hyperparameters

        sig do
          override
            .returns({
              grader:
                T.any(
                  OpenAI::Graders::StringCheckGrader,
                  OpenAI::Graders::TextSimilarityGrader,
                  OpenAI::Graders::PythonGrader,
                  OpenAI::Graders::ScoreModelGrader,
                  OpenAI::Graders::MultiGrader
                ),
              hyperparameters: OpenAI::FineTuning::ReinforcementHyperparameters
            })
        end
        def to_hash; end

        class << self
          # Configuration for the reinforcement fine-tuning method.
          sig do
            params(
              grader: T.any(
                OpenAI::Graders::StringCheckGrader::OrHash,
                OpenAI::Graders::TextSimilarityGrader::OrHash,
                OpenAI::Graders::PythonGrader::OrHash,
                OpenAI::Graders::ScoreModelGrader::OrHash,
                OpenAI::Graders::MultiGrader::OrHash
              ),
              hyperparameters: OpenAI::FineTuning::ReinforcementHyperparameters::OrHash
            ).returns(T.attached_class)
          end
          def new(
            grader:, # The grader used for the fine-tuning job.
            hyperparameters: nil # The hyperparameters used for the reinforcement fine-tuning job.
); end
        end

        # The grader used for the fine-tuning job.
        module Grader
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::ReinforcementMethod::Grader::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Graders::StringCheckGrader,
                OpenAI::Graders::TextSimilarityGrader,
                OpenAI::Graders::PythonGrader,
                OpenAI::Graders::ScoreModelGrader,
                OpenAI::Graders::MultiGrader
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::ReinforcementMethod,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class SupervisedHyperparameters < OpenAI::Internal::Type::BaseModel
        # Number of examples in each batch. A larger batch size means that model
        # parameters are updated less frequently, but with lower variance.
        sig { returns(T.nilable(T.any(Symbol, Integer))) }
        attr_reader :batch_size

        sig { params(batch_size: T.any(Symbol, Integer)).void }
        attr_writer :batch_size

        # Scaling factor for the learning rate. A smaller learning rate may be useful to
        # avoid overfitting.
        sig { returns(T.nilable(T.any(Symbol, Float))) }
        attr_reader :learning_rate_multiplier

        sig { params(learning_rate_multiplier: T.any(Symbol, Float)).void }
        attr_writer :learning_rate_multiplier

        # The number of epochs to train the model for. An epoch refers to one full cycle
        # through the training dataset.
        sig { returns(T.nilable(T.any(Symbol, Integer))) }
        attr_reader :n_epochs

        sig { params(n_epochs: T.any(Symbol, Integer)).void }
        attr_writer :n_epochs

        sig do
          override
            .returns({
              batch_size: T.any(Symbol, Integer),
              learning_rate_multiplier: T.any(Symbol, Float),
              n_epochs: T.any(Symbol, Integer)
            })
        end
        def to_hash; end

        class << self
          # The hyperparameters used for the fine-tuning job.
          sig do
            params(
              batch_size: T.any(Symbol, Integer),
              learning_rate_multiplier: T.any(Symbol, Float),
              n_epochs: T.any(Symbol, Integer)
            ).returns(T.attached_class)
          end
          def new(
            batch_size: nil, # Number of examples in each batch. A larger batch size means that model
                             # parameters are updated less frequently, but with lower variance.
            learning_rate_multiplier: nil, # Scaling factor for the learning rate. A smaller learning rate may be useful to
                                           # avoid overfitting.
            n_epochs: nil # The number of epochs to train the model for. An epoch refers to one full cycle
                          # through the training dataset.
); end
        end

        # Number of examples in each batch. A larger batch size means that model
        # parameters are updated less frequently, but with lower variance.
        module BatchSize
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::SupervisedHyperparameters::BatchSize::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Integer) }
        end

        # Scaling factor for the learning rate. A smaller learning rate may be useful to
        # avoid overfitting.
        module LearningRateMultiplier
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::SupervisedHyperparameters::LearningRateMultiplier::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Float) }
        end

        # The number of epochs to train the model for. An epoch refers to one full cycle
        # through the training dataset.
        module NEpochs
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::FineTuning::SupervisedHyperparameters::NEpochs::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(Symbol, Integer) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::SupervisedHyperparameters,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class SupervisedMethod < OpenAI::Internal::Type::BaseModel
        # The hyperparameters used for the fine-tuning job.
        sig { returns(T.nilable(OpenAI::FineTuning::SupervisedHyperparameters)) }
        attr_reader :hyperparameters

        sig { params(hyperparameters: OpenAI::FineTuning::SupervisedHyperparameters::OrHash).void }
        attr_writer :hyperparameters

        sig { override.returns({ hyperparameters: OpenAI::FineTuning::SupervisedHyperparameters }) }
        def to_hash; end

        class << self
          # Configuration for the supervised fine-tuning method.
          sig do
            params(
              hyperparameters: OpenAI::FineTuning::SupervisedHyperparameters::OrHash
            ).returns(T.attached_class)
          end
          def new(
            hyperparameters: nil # The hyperparameters used for the fine-tuning job.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::FineTuning::SupervisedMethod,
              OpenAI::Internal::AnyHash
            )
          end
      end
    end

    FineTuningJob = FineTuning::FineTuningJob
    FineTuningJobEvent = FineTuning::FineTuningJobEvent
    FineTuningJobIntegration = FineTuning::FineTuningJobIntegration
    FineTuningJobWandbIntegration = FineTuning::FineTuningJobWandbIntegration

    FineTuningJobWandbIntegrationObject = FineTuning::FineTuningJobWandbIntegrationObject

    class FunctionDefinition < OpenAI::Internal::Type::BaseModel
      # A description of what the function does, used by the model to choose when and
      # how to call the function.
      sig { returns(T.nilable(String)) }
      attr_reader :description

      sig { params(description: String).void }
      attr_writer :description

      # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
      # underscores and dashes, with a maximum length of 64.
      sig { returns(String) }
      attr_accessor :name

      # The parameters the functions accepts, described as a JSON Schema object. See the
      # [guide](https://platform.openai.com/docs/guides/function-calling) for examples,
      # and the
      # [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
      # documentation about the format.
      #
      # Omitting `parameters` defines a function with an empty parameter list.
      sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
      attr_reader :parameters

      sig { params(parameters: T::Hash[Symbol, T.anything]).void }
      attr_writer :parameters

      # Whether to enable strict schema adherence when generating the function call. If
      # set to true, the model will follow the exact schema defined in the `parameters`
      # field. Only a subset of JSON Schema is supported when `strict` is `true`. Learn
      # more about Structured Outputs in the
      # [function calling guide](https://platform.openai.com/docs/guides/function-calling).
      sig { returns(T.nilable(T::Boolean)) }
      attr_accessor :strict

      sig do
        override
          .returns({
            name: String,
            description: String,
            parameters: T::Hash[Symbol, T.anything],
            strict: T.nilable(T::Boolean)
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            name: String,
            description: String,
            parameters: T::Hash[Symbol, T.anything],
            strict: T.nilable(T::Boolean)
          ).returns(T.attached_class)
        end
        def new(
          name:, # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
                 # underscores and dashes, with a maximum length of 64.
          description: nil, # A description of what the function does, used by the model to choose when and
                            # how to call the function.
          parameters: nil, # The parameters the functions accepts, described as a JSON Schema object. See the
                           # [guide](https://platform.openai.com/docs/guides/function-calling) for examples,
                           # and the
                           # [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
                           # documentation about the format.
                           # Omitting `parameters` defines a function with an empty parameter list.
          strict: nil # Whether to enable strict schema adherence when generating the function call. If
                      # set to true, the model will follow the exact schema defined in the `parameters`
                      # field. Only a subset of JSON Schema is supported when `strict` is `true`. Learn
                      # more about Structured Outputs in the
                      # [function calling guide](https://platform.openai.com/docs/guides/function-calling).
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::FunctionDefinition, OpenAI::Internal::AnyHash)
        end
    end

    FunctionParameters = T.let(
        OpenAI::Internal::Type::HashOf[OpenAI::Internal::Type::Unknown],
        OpenAI::Internal::Type::Converter
      )

    module Graders
      class LabelModelGrader < OpenAI::Internal::Type::BaseModel
        sig { returns(T::Array[OpenAI::Graders::LabelModelGrader::Input]) }
        attr_accessor :input

        # The labels to assign to each item in the evaluation.
        sig { returns(T::Array[String]) }
        attr_accessor :labels

        # The model to use for the evaluation. Must support structured outputs.
        sig { returns(String) }
        attr_accessor :model

        # The name of the grader.
        sig { returns(String) }
        attr_accessor :name

        # The labels that indicate a passing result. Must be a subset of labels.
        sig { returns(T::Array[String]) }
        attr_accessor :passing_labels

        # The object type, which is always `label_model`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              input: T::Array[OpenAI::Graders::LabelModelGrader::Input],
              labels: T::Array[String],
              model: String,
              name: String,
              passing_labels: T::Array[String],
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # A LabelModelGrader object which uses a model to assign labels to each item in
          # the evaluation.
          sig do
            params(
              input: T::Array[OpenAI::Graders::LabelModelGrader::Input::OrHash],
              labels: T::Array[String],
              model: String,
              name: String,
              passing_labels: T::Array[String],
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            input:,
            labels:, # The labels to assign to each item in the evaluation.
            model:, # The model to use for the evaluation. Must support structured outputs.
            name:, # The name of the grader.
            passing_labels:, # The labels that indicate a passing result. Must be a subset of labels.
            type: :label_model # The object type, which is always `label_model`.
); end
        end

        class Input < OpenAI::Internal::Type::BaseModel
          # Inputs to the model - can contain template strings.
          sig do
            returns(T.any(
                String,
                OpenAI::Responses::ResponseInputText,
                OpenAI::Graders::LabelModelGrader::Input::Content::OutputText,
                OpenAI::Graders::LabelModelGrader::Input::Content::InputImage,
                T::Array[T.anything]
              ))
          end
          attr_accessor :content

          # The role of the message input. One of `user`, `assistant`, `system`, or
          # `developer`.
          sig { returns(OpenAI::Graders::LabelModelGrader::Input::Role::OrSymbol) }
          attr_accessor :role

          # The type of the message input. Always `message`.
          sig do
            returns(T.nilable(
                OpenAI::Graders::LabelModelGrader::Input::Type::OrSymbol
              ))
          end
          attr_reader :type

          sig { params(type: OpenAI::Graders::LabelModelGrader::Input::Type::OrSymbol).void }
          attr_writer :type

          sig do
            override
              .returns({
                content:
                  T.any(
                    String,
                    OpenAI::Responses::ResponseInputText,
                    OpenAI::Graders::LabelModelGrader::Input::Content::OutputText,
                    OpenAI::Graders::LabelModelGrader::Input::Content::InputImage,
                    T::Array[T.anything]
                  ),
                role: OpenAI::Graders::LabelModelGrader::Input::Role::OrSymbol,
                type: OpenAI::Graders::LabelModelGrader::Input::Type::OrSymbol
              })
          end
          def to_hash; end

          class << self
            # A message input to the model with a role indicating instruction following
            # hierarchy. Instructions given with the `developer` or `system` role take
            # precedence over instructions given with the `user` role. Messages with the
            # `assistant` role are presumed to have been generated by the model in previous
            # interactions.
            sig do
              params(
                content: T.any(
                  String,
                  OpenAI::Responses::ResponseInputText::OrHash,
                  OpenAI::Graders::LabelModelGrader::Input::Content::OutputText::OrHash,
                  OpenAI::Graders::LabelModelGrader::Input::Content::InputImage::OrHash,
                  T::Array[T.anything]
                ),
                role: OpenAI::Graders::LabelModelGrader::Input::Role::OrSymbol,
                type: OpenAI::Graders::LabelModelGrader::Input::Type::OrSymbol
              ).returns(T.attached_class)
            end
            def new(
              content:, # Inputs to the model - can contain template strings.
              role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                     # `developer`.
              type: nil # The type of the message input. Always `message`.
); end
          end

          # Inputs to the model - can contain template strings.
          module Content
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Graders::LabelModelGrader::Input::Content::Variants
                ])
              end
              def variants; end
            end

            AnArrayOfInputTextAndInputImageArray = T.let(
                OpenAI::Internal::Type::ArrayOf[
                  OpenAI::Internal::Type::Unknown
                ],
                OpenAI::Internal::Type::Converter
              )

            class InputImage < OpenAI::Internal::Type::BaseModel
              # The detail level of the image to be sent to the model. One of `high`, `low`, or
              # `auto`. Defaults to `auto`.
              sig { returns(T.nilable(String)) }
              attr_reader :detail

              sig { params(detail: String).void }
              attr_writer :detail

              # The URL of the image input.
              sig { returns(String) }
              attr_accessor :image_url

              # The type of the image input. Always `input_image`.
              sig { returns(Symbol) }
              attr_accessor :type

              sig { override.returns({ image_url: String, type: Symbol, detail: String }) }
              def to_hash; end

              class << self
                # An image input to the model.
                sig { params(image_url: String, detail: String, type: Symbol).returns(T.attached_class) }
                def new(
                  image_url:, # The URL of the image input.
                  detail: nil, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                               # `auto`. Defaults to `auto`.
                  type: :input_image # The type of the image input. Always `input_image`.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Graders::LabelModelGrader::Input::Content::InputImage,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            class OutputText < OpenAI::Internal::Type::BaseModel
              # The text output from the model.
              sig { returns(String) }
              attr_accessor :text

              # The type of the output text. Always `output_text`.
              sig { returns(Symbol) }
              attr_accessor :type

              sig { override.returns({ text: String, type: Symbol }) }
              def to_hash; end

              class << self
                # A text output from the model.
                sig { params(text: String, type: Symbol).returns(T.attached_class) }
                def new(
                  text:, # The text output from the model.
                  type: :output_text # The type of the output text. Always `output_text`.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Graders::LabelModelGrader::Input::Content::OutputText,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            Variants = T.type_alias do
                T.any(
                  String,
                  OpenAI::Responses::ResponseInputText,
                  OpenAI::Graders::LabelModelGrader::Input::Content::OutputText,
                  OpenAI::Graders::LabelModelGrader::Input::Content::InputImage,
                  T::Array[T.anything]
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Graders::LabelModelGrader::Input,
                OpenAI::Internal::AnyHash
              )
            end

          # The role of the message input. One of `user`, `assistant`, `system`, or
          # `developer`.
          module Role
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Graders::LabelModelGrader::Input::Role::TaggedSymbol
                ])
              end
              def values; end
            end

            ASSISTANT = T.let(
                :assistant,
                OpenAI::Graders::LabelModelGrader::Input::Role::TaggedSymbol
              )

            DEVELOPER = T.let(
                :developer,
                OpenAI::Graders::LabelModelGrader::Input::Role::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            SYSTEM = T.let(
                :system,
                OpenAI::Graders::LabelModelGrader::Input::Role::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Graders::LabelModelGrader::Input::Role)
              end

            USER = T.let(
                :user,
                OpenAI::Graders::LabelModelGrader::Input::Role::TaggedSymbol
              )
          end

          # The type of the message input. Always `message`.
          module Type
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Graders::LabelModelGrader::Input::Type::TaggedSymbol
                ])
              end
              def values; end
            end

            MESSAGE = T.let(
                :message,
                OpenAI::Graders::LabelModelGrader::Input::Type::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Graders::LabelModelGrader::Input::Type)
              end
          end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Graders::LabelModelGrader, OpenAI::Internal::AnyHash)
          end
      end

      class MultiGrader < OpenAI::Internal::Type::BaseModel
        # A formula to calculate the output based on grader results.
        sig { returns(String) }
        attr_accessor :calculate_output

        # A StringCheckGrader object that performs a string comparison between input and
        # reference using a specified operation.
        sig do
          returns(T.any(
              OpenAI::Graders::StringCheckGrader,
              OpenAI::Graders::TextSimilarityGrader,
              OpenAI::Graders::PythonGrader,
              OpenAI::Graders::ScoreModelGrader,
              OpenAI::Graders::LabelModelGrader
            ))
        end
        attr_accessor :graders

        # The name of the grader.
        sig { returns(String) }
        attr_accessor :name

        # The object type, which is always `multi`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              calculate_output: String,
              graders:
                T.any(
                  OpenAI::Graders::StringCheckGrader,
                  OpenAI::Graders::TextSimilarityGrader,
                  OpenAI::Graders::PythonGrader,
                  OpenAI::Graders::ScoreModelGrader,
                  OpenAI::Graders::LabelModelGrader
                ),
              name: String,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # A MultiGrader object combines the output of multiple graders to produce a single
          # score.
          sig do
            params(
              calculate_output: String,
              graders: T.any(
                OpenAI::Graders::StringCheckGrader::OrHash,
                OpenAI::Graders::TextSimilarityGrader::OrHash,
                OpenAI::Graders::PythonGrader::OrHash,
                OpenAI::Graders::ScoreModelGrader::OrHash,
                OpenAI::Graders::LabelModelGrader::OrHash
              ),
              name: String,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            calculate_output:, # A formula to calculate the output based on grader results.
            graders:, # A StringCheckGrader object that performs a string comparison between input and
                      # reference using a specified operation.
            name:, # The name of the grader.
            type: :multi # The object type, which is always `multi`.
); end
        end

        # A StringCheckGrader object that performs a string comparison between input and
        # reference using a specified operation.
        module Graders
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Graders::MultiGrader::Graders::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Graders::StringCheckGrader,
                OpenAI::Graders::TextSimilarityGrader,
                OpenAI::Graders::PythonGrader,
                OpenAI::Graders::ScoreModelGrader,
                OpenAI::Graders::LabelModelGrader
              )
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Graders::MultiGrader, OpenAI::Internal::AnyHash)
          end
      end

      class PythonGrader < OpenAI::Internal::Type::BaseModel
        # The image tag to use for the python script.
        sig { returns(T.nilable(String)) }
        attr_reader :image_tag

        sig { params(image_tag: String).void }
        attr_writer :image_tag

        # The name of the grader.
        sig { returns(String) }
        attr_accessor :name

        # The source code of the python script.
        sig { returns(String) }
        attr_accessor :source

        # The object type, which is always `python`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ name: String, source: String, type: Symbol, image_tag: String }) }
        def to_hash; end

        class << self
          # A PythonGrader object that runs a python script on the input.
          sig { params(name: String, source: String, image_tag: String, type: Symbol).returns(T.attached_class) }
          def new(
            name:, # The name of the grader.
            source:, # The source code of the python script.
            image_tag: nil, # The image tag to use for the python script.
            type: :python # The object type, which is always `python`.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Graders::PythonGrader, OpenAI::Internal::AnyHash)
          end
      end

      class ScoreModelGrader < OpenAI::Internal::Type::BaseModel
        # The input text. This may include template strings.
        sig { returns(T::Array[OpenAI::Graders::ScoreModelGrader::Input]) }
        attr_accessor :input

        # The model to use for the evaluation.
        sig { returns(String) }
        attr_accessor :model

        # The name of the grader.
        sig { returns(String) }
        attr_accessor :name

        # The range of the score. Defaults to `[0, 1]`.
        sig { returns(T.nilable(T::Array[Float])) }
        attr_reader :range

        sig { params(range: T::Array[Float]).void }
        attr_writer :range

        # The sampling parameters for the model.
        sig { returns(T.nilable(T.anything)) }
        attr_reader :sampling_params

        sig { params(sampling_params: T.anything).void }
        attr_writer :sampling_params

        # The object type, which is always `score_model`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              input: T::Array[OpenAI::Graders::ScoreModelGrader::Input],
              model: String,
              name: String,
              type: Symbol,
              range: T::Array[Float],
              sampling_params: T.anything
            })
        end
        def to_hash; end

        class << self
          # A ScoreModelGrader object that uses a model to assign a score to the input.
          sig do
            params(
              input: T::Array[OpenAI::Graders::ScoreModelGrader::Input::OrHash],
              model: String,
              name: String,
              range: T::Array[Float],
              sampling_params: T.anything,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            input:, # The input text. This may include template strings.
            model:, # The model to use for the evaluation.
            name:, # The name of the grader.
            range: nil, # The range of the score. Defaults to `[0, 1]`.
            sampling_params: nil, # The sampling parameters for the model.
            type: :score_model # The object type, which is always `score_model`.
); end
        end

        class Input < OpenAI::Internal::Type::BaseModel
          # Inputs to the model - can contain template strings.
          sig do
            returns(T.any(
                String,
                OpenAI::Responses::ResponseInputText,
                OpenAI::Graders::ScoreModelGrader::Input::Content::OutputText,
                OpenAI::Graders::ScoreModelGrader::Input::Content::InputImage,
                T::Array[T.anything]
              ))
          end
          attr_accessor :content

          # The role of the message input. One of `user`, `assistant`, `system`, or
          # `developer`.
          sig { returns(OpenAI::Graders::ScoreModelGrader::Input::Role::OrSymbol) }
          attr_accessor :role

          # The type of the message input. Always `message`.
          sig do
            returns(T.nilable(
                OpenAI::Graders::ScoreModelGrader::Input::Type::OrSymbol
              ))
          end
          attr_reader :type

          sig { params(type: OpenAI::Graders::ScoreModelGrader::Input::Type::OrSymbol).void }
          attr_writer :type

          sig do
            override
              .returns({
                content:
                  T.any(
                    String,
                    OpenAI::Responses::ResponseInputText,
                    OpenAI::Graders::ScoreModelGrader::Input::Content::OutputText,
                    OpenAI::Graders::ScoreModelGrader::Input::Content::InputImage,
                    T::Array[T.anything]
                  ),
                role: OpenAI::Graders::ScoreModelGrader::Input::Role::OrSymbol,
                type: OpenAI::Graders::ScoreModelGrader::Input::Type::OrSymbol
              })
          end
          def to_hash; end

          class << self
            # A message input to the model with a role indicating instruction following
            # hierarchy. Instructions given with the `developer` or `system` role take
            # precedence over instructions given with the `user` role. Messages with the
            # `assistant` role are presumed to have been generated by the model in previous
            # interactions.
            sig do
              params(
                content: T.any(
                  String,
                  OpenAI::Responses::ResponseInputText::OrHash,
                  OpenAI::Graders::ScoreModelGrader::Input::Content::OutputText::OrHash,
                  OpenAI::Graders::ScoreModelGrader::Input::Content::InputImage::OrHash,
                  T::Array[T.anything]
                ),
                role: OpenAI::Graders::ScoreModelGrader::Input::Role::OrSymbol,
                type: OpenAI::Graders::ScoreModelGrader::Input::Type::OrSymbol
              ).returns(T.attached_class)
            end
            def new(
              content:, # Inputs to the model - can contain template strings.
              role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                     # `developer`.
              type: nil # The type of the message input. Always `message`.
); end
          end

          # Inputs to the model - can contain template strings.
          module Content
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Graders::ScoreModelGrader::Input::Content::Variants
                ])
              end
              def variants; end
            end

            AnArrayOfInputTextAndInputImageArray = T.let(
                OpenAI::Internal::Type::ArrayOf[
                  OpenAI::Internal::Type::Unknown
                ],
                OpenAI::Internal::Type::Converter
              )

            class InputImage < OpenAI::Internal::Type::BaseModel
              # The detail level of the image to be sent to the model. One of `high`, `low`, or
              # `auto`. Defaults to `auto`.
              sig { returns(T.nilable(String)) }
              attr_reader :detail

              sig { params(detail: String).void }
              attr_writer :detail

              # The URL of the image input.
              sig { returns(String) }
              attr_accessor :image_url

              # The type of the image input. Always `input_image`.
              sig { returns(Symbol) }
              attr_accessor :type

              sig { override.returns({ image_url: String, type: Symbol, detail: String }) }
              def to_hash; end

              class << self
                # An image input to the model.
                sig { params(image_url: String, detail: String, type: Symbol).returns(T.attached_class) }
                def new(
                  image_url:, # The URL of the image input.
                  detail: nil, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                               # `auto`. Defaults to `auto`.
                  type: :input_image # The type of the image input. Always `input_image`.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Graders::ScoreModelGrader::Input::Content::InputImage,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            class OutputText < OpenAI::Internal::Type::BaseModel
              # The text output from the model.
              sig { returns(String) }
              attr_accessor :text

              # The type of the output text. Always `output_text`.
              sig { returns(Symbol) }
              attr_accessor :type

              sig { override.returns({ text: String, type: Symbol }) }
              def to_hash; end

              class << self
                # A text output from the model.
                sig { params(text: String, type: Symbol).returns(T.attached_class) }
                def new(
                  text:, # The text output from the model.
                  type: :output_text # The type of the output text. Always `output_text`.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Graders::ScoreModelGrader::Input::Content::OutputText,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            Variants = T.type_alias do
                T.any(
                  String,
                  OpenAI::Responses::ResponseInputText,
                  OpenAI::Graders::ScoreModelGrader::Input::Content::OutputText,
                  OpenAI::Graders::ScoreModelGrader::Input::Content::InputImage,
                  T::Array[T.anything]
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Graders::ScoreModelGrader::Input,
                OpenAI::Internal::AnyHash
              )
            end

          # The role of the message input. One of `user`, `assistant`, `system`, or
          # `developer`.
          module Role
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Graders::ScoreModelGrader::Input::Role::TaggedSymbol
                ])
              end
              def values; end
            end

            ASSISTANT = T.let(
                :assistant,
                OpenAI::Graders::ScoreModelGrader::Input::Role::TaggedSymbol
              )

            DEVELOPER = T.let(
                :developer,
                OpenAI::Graders::ScoreModelGrader::Input::Role::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            SYSTEM = T.let(
                :system,
                OpenAI::Graders::ScoreModelGrader::Input::Role::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Graders::ScoreModelGrader::Input::Role)
              end

            USER = T.let(
                :user,
                OpenAI::Graders::ScoreModelGrader::Input::Role::TaggedSymbol
              )
          end

          # The type of the message input. Always `message`.
          module Type
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Graders::ScoreModelGrader::Input::Type::TaggedSymbol
                ])
              end
              def values; end
            end

            MESSAGE = T.let(
                :message,
                OpenAI::Graders::ScoreModelGrader::Input::Type::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Graders::ScoreModelGrader::Input::Type)
              end
          end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Graders::ScoreModelGrader, OpenAI::Internal::AnyHash)
          end
      end

      class StringCheckGrader < OpenAI::Internal::Type::BaseModel
        # The input text. This may include template strings.
        sig { returns(String) }
        attr_accessor :input

        # The name of the grader.
        sig { returns(String) }
        attr_accessor :name

        # The string check operation to perform. One of `eq`, `ne`, `like`, or `ilike`.
        sig { returns(OpenAI::Graders::StringCheckGrader::Operation::OrSymbol) }
        attr_accessor :operation

        # The reference text. This may include template strings.
        sig { returns(String) }
        attr_accessor :reference

        # The object type, which is always `string_check`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              input: String,
              name: String,
              operation:
                OpenAI::Graders::StringCheckGrader::Operation::OrSymbol,
              reference: String,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # A StringCheckGrader object that performs a string comparison between input and
          # reference using a specified operation.
          sig do
            params(
              input: String,
              name: String,
              operation: OpenAI::Graders::StringCheckGrader::Operation::OrSymbol,
              reference: String,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            input:, # The input text. This may include template strings.
            name:, # The name of the grader.
            operation:, # The string check operation to perform. One of `eq`, `ne`, `like`, or `ilike`.
            reference:, # The reference text. This may include template strings.
            type: :string_check # The object type, which is always `string_check`.
); end
        end

        # The string check operation to perform. One of `eq`, `ne`, `like`, or `ilike`.
        module Operation
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Graders::StringCheckGrader::Operation::TaggedSymbol
              ])
            end
            def values; end
          end

          EQ = T.let(
              :eq,
              OpenAI::Graders::StringCheckGrader::Operation::TaggedSymbol
            )

          ILIKE = T.let(
              :ilike,
              OpenAI::Graders::StringCheckGrader::Operation::TaggedSymbol
            )

          LIKE = T.let(
              :like,
              OpenAI::Graders::StringCheckGrader::Operation::TaggedSymbol
            )

          NE = T.let(
              :ne,
              OpenAI::Graders::StringCheckGrader::Operation::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Graders::StringCheckGrader::Operation)
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Graders::StringCheckGrader, OpenAI::Internal::AnyHash)
          end
      end

      class TextSimilarityGrader < OpenAI::Internal::Type::BaseModel
        # The evaluation metric to use. One of `fuzzy_match`, `bleu`, `gleu`, `meteor`,
        # `rouge_1`, `rouge_2`, `rouge_3`, `rouge_4`, `rouge_5`, or `rouge_l`.
        sig { returns(OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::OrSymbol) }
        attr_accessor :evaluation_metric

        # The text being graded.
        sig { returns(String) }
        attr_accessor :input

        # The name of the grader.
        sig { returns(String) }
        attr_accessor :name

        # The text being graded against.
        sig { returns(String) }
        attr_accessor :reference

        # The type of grader.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              evaluation_metric:
                OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::OrSymbol,
              input: String,
              name: String,
              reference: String,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # A TextSimilarityGrader object which grades text based on similarity metrics.
          sig do
            params(
              evaluation_metric: OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::OrSymbol,
              input: String,
              name: String,
              reference: String,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            evaluation_metric:, # The evaluation metric to use. One of `fuzzy_match`, `bleu`, `gleu`, `meteor`,
                                # `rouge_1`, `rouge_2`, `rouge_3`, `rouge_4`, `rouge_5`, or `rouge_l`.
            input:, # The text being graded.
            name:, # The name of the grader.
            reference:, # The text being graded against.
            type: :text_similarity # The type of grader.
); end
        end

        # The evaluation metric to use. One of `fuzzy_match`, `bleu`, `gleu`, `meteor`,
        # `rouge_1`, `rouge_2`, `rouge_3`, `rouge_4`, `rouge_5`, or `rouge_l`.
        module EvaluationMetric
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
              ])
            end
            def values; end
          end

          BLEU = T.let(
              :bleu,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          FUZZY_MATCH = T.let(
              :fuzzy_match,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          GLEU = T.let(
              :gleu,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          METEOR = T.let(
              :meteor,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          ROUGE_1 = T.let(
              :rouge_1,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          ROUGE_2 = T.let(
              :rouge_2,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          ROUGE_3 = T.let(
              :rouge_3,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          ROUGE_4 = T.let(
              :rouge_4,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          ROUGE_5 = T.let(
              :rouge_5,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          ROUGE_L = T.let(
              :rouge_l,
              OpenAI::Graders::TextSimilarityGrader::EvaluationMetric::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Graders::TextSimilarityGrader::EvaluationMetric
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Graders::TextSimilarityGrader,
              OpenAI::Internal::AnyHash
            )
          end
      end
    end

    class Image < OpenAI::Internal::Type::BaseModel
      # The base64-encoded JSON of the generated image. Default value for `gpt-image-1`,
      # and only present if `response_format` is set to `b64_json` for `dall-e-2` and
      # `dall-e-3`.
      sig { returns(T.nilable(String)) }
      attr_reader :b64_json

      sig { params(b64_json: String).void }
      attr_writer :b64_json

      # For `dall-e-3` only, the revised prompt that was used to generate the image.
      sig { returns(T.nilable(String)) }
      attr_reader :revised_prompt

      sig { params(revised_prompt: String).void }
      attr_writer :revised_prompt

      # When using `dall-e-2` or `dall-e-3`, the URL of the generated image if
      # `response_format` is set to `url` (default value). Unsupported for
      # `gpt-image-1`.
      sig { returns(T.nilable(String)) }
      attr_reader :url

      sig { params(url: String).void }
      attr_writer :url

      sig { override.returns({ b64_json: String, revised_prompt: String, url: String }) }
      def to_hash; end

      class << self
        # Represents the content or the URL of an image generated by the OpenAI API.
        sig { params(b64_json: String, revised_prompt: String, url: String).returns(T.attached_class) }
        def new(
          b64_json: nil, # The base64-encoded JSON of the generated image. Default value for `gpt-image-1`,
                         # and only present if `response_format` is set to `b64_json` for `dall-e-2` and
                         # `dall-e-3`.
          revised_prompt: nil, # For `dall-e-3` only, the revised prompt that was used to generate the image.
          url: nil # When using `dall-e-2` or `dall-e-3`, the URL of the generated image if
                   # `response_format` is set to `url` (default value). Unsupported for
                   # `gpt-image-1`.
); end
      end

      OrHash = T.type_alias { T.any(OpenAI::Image, OpenAI::Internal::AnyHash) }
    end

    class ImageCreateVariationParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # The image to use as the basis for the variation(s). Must be a valid PNG file,
      # less than 4MB, and square.
      sig { returns(OpenAI::Internal::FileInput) }
      attr_accessor :image

      # The model to use for image generation. Only `dall-e-2` is supported at this
      # time.
      sig { returns(T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol))) }
      attr_accessor :model

      # The number of images to generate. Must be between 1 and 10.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :n

      # The format in which the generated images are returned. Must be one of `url` or
      # `b64_json`. URLs are only valid for 60 minutes after the image has been
      # generated.
      sig { returns(T.nilable(
            OpenAI::ImageCreateVariationParams::ResponseFormat::OrSymbol
          )) }
      attr_accessor :response_format

      # The size of the generated images. Must be one of `256x256`, `512x512`, or
      # `1024x1024`.
      sig { returns(T.nilable(OpenAI::ImageCreateVariationParams::Size::OrSymbol)) }
      attr_accessor :size

      # A unique identifier representing your end-user, which can help OpenAI to monitor
      # and detect abuse.
      # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
      sig { returns(T.nilable(String)) }
      attr_reader :user

      sig { params(user: String).void }
      attr_writer :user

      sig do
        override
          .returns({
            image: OpenAI::Internal::FileInput,
            model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
            n: T.nilable(Integer),
            response_format:
              T.nilable(
                OpenAI::ImageCreateVariationParams::ResponseFormat::OrSymbol
              ),
            size: T.nilable(OpenAI::ImageCreateVariationParams::Size::OrSymbol),
            user: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            image: OpenAI::Internal::FileInput,
            model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
            n: T.nilable(Integer),
            response_format: T.nilable(
              OpenAI::ImageCreateVariationParams::ResponseFormat::OrSymbol
            ),
            size: T.nilable(OpenAI::ImageCreateVariationParams::Size::OrSymbol),
            user: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          image:, # The image to use as the basis for the variation(s). Must be a valid PNG file,
                  # less than 4MB, and square.
          model: nil, # The model to use for image generation. Only `dall-e-2` is supported at this
                      # time.
          n: nil, # The number of images to generate. Must be between 1 and 10.
          response_format: nil, # The format in which the generated images are returned. Must be one of `url` or
                                # `b64_json`. URLs are only valid for 60 minutes after the image has been
                                # generated.
          size: nil, # The size of the generated images. Must be one of `256x256`, `512x512`, or
                     # `1024x1024`.
          user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                     # and detect abuse.
                     # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
          request_options: {}
); end
      end

      # The model to use for image generation. Only `dall-e-2` is supported at this
      # time.
      module Model
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::ImageCreateVariationParams::Model::Variants]) }
          def variants; end
        end

        Variants = T.type_alias { T.any(String, OpenAI::ImageModel::TaggedSymbol) }
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ImageCreateVariationParams, OpenAI::Internal::AnyHash)
        end

      # The format in which the generated images are returned. Must be one of `url` or
      # `b64_json`. URLs are only valid for 60 minutes after the image has been
      # generated.
      module ResponseFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::ImageCreateVariationParams::ResponseFormat::TaggedSymbol
            ])
          end
          def values; end
        end

        B64_JSON = T.let(
            :b64_json,
            OpenAI::ImageCreateVariationParams::ResponseFormat::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageCreateVariationParams::ResponseFormat)
          end

        URL = T.let(
            :url,
            OpenAI::ImageCreateVariationParams::ResponseFormat::TaggedSymbol
          )
      end

      # The size of the generated images. Must be one of `256x256`, `512x512`, or
      # `1024x1024`.
      module Size
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageCreateVariationParams::Size::TaggedSymbol]) }
          def values; end
        end

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        SIZE_1024X1024 = T.let(
            :"1024x1024",
            OpenAI::ImageCreateVariationParams::Size::TaggedSymbol
          )

        SIZE_256X256 = T.let(
            :"256x256",
            OpenAI::ImageCreateVariationParams::Size::TaggedSymbol
          )

        SIZE_512X512 = T.let(
            :"512x512",
            OpenAI::ImageCreateVariationParams::Size::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageCreateVariationParams::Size)
          end
      end
    end

    class ImageEditCompletedEvent < OpenAI::Internal::Type::BaseModel
      # Base64-encoded final edited image data, suitable for rendering as an image.
      sig { returns(String) }
      attr_accessor :b64_json

      # The background setting for the edited image.
      sig { returns(OpenAI::ImageEditCompletedEvent::Background::TaggedSymbol) }
      attr_accessor :background

      # The Unix timestamp when the event was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The output format for the edited image.
      sig { returns(OpenAI::ImageEditCompletedEvent::OutputFormat::TaggedSymbol) }
      attr_accessor :output_format

      # The quality setting for the edited image.
      sig { returns(OpenAI::ImageEditCompletedEvent::Quality::TaggedSymbol) }
      attr_accessor :quality

      # The size of the edited image.
      sig { returns(OpenAI::ImageEditCompletedEvent::Size::TaggedSymbol) }
      attr_accessor :size

      # The type of the event. Always `image_edit.completed`.
      sig { returns(Symbol) }
      attr_accessor :type

      # For `gpt-image-1` only, the token usage information for the image generation.
      sig { returns(OpenAI::ImageEditCompletedEvent::Usage) }
      attr_reader :usage

      sig { params(usage: OpenAI::ImageEditCompletedEvent::Usage::OrHash).void }
      attr_writer :usage

      sig do
        override
          .returns({
            b64_json: String,
            background:
              OpenAI::ImageEditCompletedEvent::Background::TaggedSymbol,
            created_at: Integer,
            output_format:
              OpenAI::ImageEditCompletedEvent::OutputFormat::TaggedSymbol,
            quality: OpenAI::ImageEditCompletedEvent::Quality::TaggedSymbol,
            size: OpenAI::ImageEditCompletedEvent::Size::TaggedSymbol,
            type: Symbol,
            usage: OpenAI::ImageEditCompletedEvent::Usage
          })
      end
      def to_hash; end

      class << self
        # Emitted when image editing has completed and the final image is available.
        sig do
          params(
            b64_json: String,
            background: OpenAI::ImageEditCompletedEvent::Background::OrSymbol,
            created_at: Integer,
            output_format: OpenAI::ImageEditCompletedEvent::OutputFormat::OrSymbol,
            quality: OpenAI::ImageEditCompletedEvent::Quality::OrSymbol,
            size: OpenAI::ImageEditCompletedEvent::Size::OrSymbol,
            usage: OpenAI::ImageEditCompletedEvent::Usage::OrHash,
            type: Symbol
          ).returns(T.attached_class)
        end
        def new(
          b64_json:, # Base64-encoded final edited image data, suitable for rendering as an image.
          background:, # The background setting for the edited image.
          created_at:, # The Unix timestamp when the event was created.
          output_format:, # The output format for the edited image.
          quality:, # The quality setting for the edited image.
          size:, # The size of the edited image.
          usage:, # For `gpt-image-1` only, the token usage information for the image generation.
          type: :"image_edit.completed" # The type of the event. Always `image_edit.completed`.
); end
      end

      # The background setting for the edited image.
      module Background
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditCompletedEvent::Background::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(
            :auto,
            OpenAI::ImageEditCompletedEvent::Background::TaggedSymbol
          )

        OPAQUE = T.let(
            :opaque,
            OpenAI::ImageEditCompletedEvent::Background::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TRANSPARENT = T.let(
            :transparent,
            OpenAI::ImageEditCompletedEvent::Background::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageEditCompletedEvent::Background)
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ImageEditCompletedEvent, OpenAI::Internal::AnyHash)
        end

      # The output format for the edited image.
      module OutputFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::ImageEditCompletedEvent::OutputFormat::TaggedSymbol
            ])
          end
          def values; end
        end

        JPEG = T.let(
            :jpeg,
            OpenAI::ImageEditCompletedEvent::OutputFormat::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        PNG = T.let(
            :png,
            OpenAI::ImageEditCompletedEvent::OutputFormat::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageEditCompletedEvent::OutputFormat)
          end

        WEBP = T.let(
            :webp,
            OpenAI::ImageEditCompletedEvent::OutputFormat::TaggedSymbol
          )
      end

      # The quality setting for the edited image.
      module Quality
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditCompletedEvent::Quality::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageEditCompletedEvent::Quality::TaggedSymbol)

        HIGH = T.let(:high, OpenAI::ImageEditCompletedEvent::Quality::TaggedSymbol)

        LOW = T.let(:low, OpenAI::ImageEditCompletedEvent::Quality::TaggedSymbol)

        MEDIUM = T.let(:medium, OpenAI::ImageEditCompletedEvent::Quality::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageEditCompletedEvent::Quality)
          end
      end

      # The size of the edited image.
      module Size
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditCompletedEvent::Size::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageEditCompletedEvent::Size::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        SIZE_1024X1024 = T.let(
            :"1024x1024",
            OpenAI::ImageEditCompletedEvent::Size::TaggedSymbol
          )

        SIZE_1024X1536 = T.let(
            :"1024x1536",
            OpenAI::ImageEditCompletedEvent::Size::TaggedSymbol
          )

        SIZE_1536X1024 = T.let(
            :"1536x1024",
            OpenAI::ImageEditCompletedEvent::Size::TaggedSymbol
          )

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageEditCompletedEvent::Size) }
      end

      class Usage < OpenAI::Internal::Type::BaseModel
        # The number of tokens (images and text) in the input prompt.
        sig { returns(Integer) }
        attr_accessor :input_tokens

        # The input tokens detailed information for the image generation.
        sig { returns(OpenAI::ImageEditCompletedEvent::Usage::InputTokensDetails) }
        attr_reader :input_tokens_details

        sig { params(input_tokens_details: OpenAI::ImageEditCompletedEvent::Usage::InputTokensDetails::OrHash).void }
        attr_writer :input_tokens_details

        # The number of image tokens in the output image.
        sig { returns(Integer) }
        attr_accessor :output_tokens

        # The total number of tokens (images and text) used for the image generation.
        sig { returns(Integer) }
        attr_accessor :total_tokens

        sig do
          override
            .returns({
              input_tokens: Integer,
              input_tokens_details:
                OpenAI::ImageEditCompletedEvent::Usage::InputTokensDetails,
              output_tokens: Integer,
              total_tokens: Integer
            })
        end
        def to_hash; end

        class << self
          # For `gpt-image-1` only, the token usage information for the image generation.
          sig do
            params(
              input_tokens: Integer,
              input_tokens_details: OpenAI::ImageEditCompletedEvent::Usage::InputTokensDetails::OrHash,
              output_tokens: Integer,
              total_tokens: Integer
            ).returns(T.attached_class)
          end
          def new(
            input_tokens:, # The number of tokens (images and text) in the input prompt.
            input_tokens_details:, # The input tokens detailed information for the image generation.
            output_tokens:, # The number of image tokens in the output image.
            total_tokens: # The total number of tokens (images and text) used for the image generation.
); end
        end

        class InputTokensDetails < OpenAI::Internal::Type::BaseModel
          # The number of image tokens in the input prompt.
          sig { returns(Integer) }
          attr_accessor :image_tokens

          # The number of text tokens in the input prompt.
          sig { returns(Integer) }
          attr_accessor :text_tokens

          sig { override.returns({ image_tokens: Integer, text_tokens: Integer }) }
          def to_hash; end

          class << self
            # The input tokens detailed information for the image generation.
            sig { params(image_tokens: Integer, text_tokens: Integer).returns(T.attached_class) }
            def new(
              image_tokens:, # The number of image tokens in the input prompt.
              text_tokens: # The number of text tokens in the input prompt.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::ImageEditCompletedEvent::Usage::InputTokensDetails,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::ImageEditCompletedEvent::Usage,
              OpenAI::Internal::AnyHash
            )
          end
      end
    end

    class ImageEditParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # Allows to set transparency for the background of the generated image(s). This
      # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
      # `opaque` or `auto` (default value). When `auto` is used, the model will
      # automatically determine the best background for the image.
      #
      # If `transparent`, the output format needs to support transparency, so it should
      # be set to either `png` (default value) or `webp`.
      sig { returns(T.nilable(OpenAI::ImageEditParams::Background::OrSymbol)) }
      attr_accessor :background

      # The image(s) to edit. Must be a supported image file or an array of images.
      #
      # For `gpt-image-1`, each image should be a `png`, `webp`, or `jpg` file less than
      # 50MB. You can provide up to 16 images.
      #
      # For `dall-e-2`, you can only provide one image, and it should be a square `png`
      # file less than 4MB.
      sig { returns(OpenAI::ImageEditParams::Image::Variants) }
      attr_accessor :image

      # Control how much effort the model will exert to match the style and features,
      # especially facial features, of input images. This parameter is only supported
      # for `gpt-image-1`. Supports `high` and `low`. Defaults to `low`.
      sig { returns(T.nilable(OpenAI::ImageEditParams::InputFidelity::OrSymbol)) }
      attr_accessor :input_fidelity

      # An additional image whose fully transparent areas (e.g. where alpha is zero)
      # indicate where `image` should be edited. If there are multiple images provided,
      # the mask will be applied on the first image. Must be a valid PNG file, less than
      # 4MB, and have the same dimensions as `image`.
      sig { returns(T.nilable(OpenAI::Internal::FileInput)) }
      attr_reader :mask

      sig { params(mask: OpenAI::Internal::FileInput).void }
      attr_writer :mask

      # The model to use for image generation. Only `dall-e-2` and `gpt-image-1` are
      # supported. Defaults to `dall-e-2` unless a parameter specific to `gpt-image-1`
      # is used.
      sig { returns(T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol))) }
      attr_accessor :model

      # The number of images to generate. Must be between 1 and 10.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :n

      # The compression level (0-100%) for the generated images. This parameter is only
      # supported for `gpt-image-1` with the `webp` or `jpeg` output formats, and
      # defaults to 100.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :output_compression

      # The format in which the generated images are returned. This parameter is only
      # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`. The
      # default value is `png`.
      sig { returns(T.nilable(OpenAI::ImageEditParams::OutputFormat::OrSymbol)) }
      attr_accessor :output_format

      # The number of partial images to generate. This parameter is used for streaming
      # responses that return partial images. Value must be between 0 and 3. When set to
      # 0, the response will be a single image sent in one streaming event.
      #
      # Note that the final image may be sent before the full number of partial images
      # are generated if the full image is generated more quickly.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :partial_images

      # A text description of the desired image(s). The maximum length is 1000
      # characters for `dall-e-2`, and 32000 characters for `gpt-image-1`.
      sig { returns(String) }
      attr_accessor :prompt

      # The quality of the image that will be generated. `high`, `medium` and `low` are
      # only supported for `gpt-image-1`. `dall-e-2` only supports `standard` quality.
      # Defaults to `auto`.
      sig { returns(T.nilable(OpenAI::ImageEditParams::Quality::OrSymbol)) }
      attr_accessor :quality

      # The format in which the generated images are returned. Must be one of `url` or
      # `b64_json`. URLs are only valid for 60 minutes after the image has been
      # generated. This parameter is only supported for `dall-e-2`, as `gpt-image-1`
      # will always return base64-encoded images.
      sig { returns(T.nilable(OpenAI::ImageEditParams::ResponseFormat::OrSymbol)) }
      attr_accessor :response_format

      # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
      # (landscape), `1024x1536` (portrait), or `auto` (default value) for
      # `gpt-image-1`, and one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`.
      sig { returns(T.nilable(OpenAI::ImageEditParams::Size::OrSymbol)) }
      attr_accessor :size

      # A unique identifier representing your end-user, which can help OpenAI to monitor
      # and detect abuse.
      # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
      sig { returns(T.nilable(String)) }
      attr_reader :user

      sig { params(user: String).void }
      attr_writer :user

      sig do
        override
          .returns({
            image: OpenAI::ImageEditParams::Image::Variants,
            prompt: String,
            background:
              T.nilable(OpenAI::ImageEditParams::Background::OrSymbol),
            input_fidelity:
              T.nilable(OpenAI::ImageEditParams::InputFidelity::OrSymbol),
            mask: OpenAI::Internal::FileInput,
            model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
            n: T.nilable(Integer),
            output_compression: T.nilable(Integer),
            output_format:
              T.nilable(OpenAI::ImageEditParams::OutputFormat::OrSymbol),
            partial_images: T.nilable(Integer),
            quality: T.nilable(OpenAI::ImageEditParams::Quality::OrSymbol),
            response_format:
              T.nilable(OpenAI::ImageEditParams::ResponseFormat::OrSymbol),
            size: T.nilable(OpenAI::ImageEditParams::Size::OrSymbol),
            user: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            image: OpenAI::ImageEditParams::Image::Variants,
            prompt: String,
            background: T.nilable(OpenAI::ImageEditParams::Background::OrSymbol),
            input_fidelity: T.nilable(OpenAI::ImageEditParams::InputFidelity::OrSymbol),
            mask: OpenAI::Internal::FileInput,
            model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
            n: T.nilable(Integer),
            output_compression: T.nilable(Integer),
            output_format: T.nilable(OpenAI::ImageEditParams::OutputFormat::OrSymbol),
            partial_images: T.nilable(Integer),
            quality: T.nilable(OpenAI::ImageEditParams::Quality::OrSymbol),
            response_format: T.nilable(OpenAI::ImageEditParams::ResponseFormat::OrSymbol),
            size: T.nilable(OpenAI::ImageEditParams::Size::OrSymbol),
            user: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          image:, # The image(s) to edit. Must be a supported image file or an array of images.
                  # For `gpt-image-1`, each image should be a `png`, `webp`, or `jpg` file less than
                  # 50MB. You can provide up to 16 images.
                  # For `dall-e-2`, you can only provide one image, and it should be a square `png`
                  # file less than 4MB.
          prompt:, # A text description of the desired image(s). The maximum length is 1000
                   # characters for `dall-e-2`, and 32000 characters for `gpt-image-1`.
          background: nil, # Allows to set transparency for the background of the generated image(s). This
                           # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
                           # `opaque` or `auto` (default value). When `auto` is used, the model will
                           # automatically determine the best background for the image.
                           # If `transparent`, the output format needs to support transparency, so it should
                           # be set to either `png` (default value) or `webp`.
          input_fidelity: nil, # Control how much effort the model will exert to match the style and features,
                               # especially facial features, of input images. This parameter is only supported
                               # for `gpt-image-1`. Supports `high` and `low`. Defaults to `low`.
          mask: nil, # An additional image whose fully transparent areas (e.g. where alpha is zero)
                     # indicate where `image` should be edited. If there are multiple images provided,
                     # the mask will be applied on the first image. Must be a valid PNG file, less than
                     # 4MB, and have the same dimensions as `image`.
          model: nil, # The model to use for image generation. Only `dall-e-2` and `gpt-image-1` are
                      # supported. Defaults to `dall-e-2` unless a parameter specific to `gpt-image-1`
                      # is used.
          n: nil, # The number of images to generate. Must be between 1 and 10.
          output_compression: nil, # The compression level (0-100%) for the generated images. This parameter is only
                                   # supported for `gpt-image-1` with the `webp` or `jpeg` output formats, and
                                   # defaults to 100.
          output_format: nil, # The format in which the generated images are returned. This parameter is only
                              # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`. The
                              # default value is `png`.
          partial_images: nil, # The number of partial images to generate. This parameter is used for streaming
                               # responses that return partial images. Value must be between 0 and 3. When set to
                               # 0, the response will be a single image sent in one streaming event.
                               # Note that the final image may be sent before the full number of partial images
                               # are generated if the full image is generated more quickly.
          quality: nil, # The quality of the image that will be generated. `high`, `medium` and `low` are
                        # only supported for `gpt-image-1`. `dall-e-2` only supports `standard` quality.
                        # Defaults to `auto`.
          response_format: nil, # The format in which the generated images are returned. Must be one of `url` or
                                # `b64_json`. URLs are only valid for 60 minutes after the image has been
                                # generated. This parameter is only supported for `dall-e-2`, as `gpt-image-1`
                                # will always return base64-encoded images.
          size: nil, # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
                     # (landscape), `1024x1536` (portrait), or `auto` (default value) for
                     # `gpt-image-1`, and one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`.
          user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                     # and detect abuse.
                     # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
          request_options: {}
); end
      end

      # Allows to set transparency for the background of the generated image(s). This
      # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
      # `opaque` or `auto` (default value). When `auto` is used, the model will
      # automatically determine the best background for the image.
      #
      # If `transparent`, the output format needs to support transparency, so it should
      # be set to either `png` (default value) or `webp`.
      module Background
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditParams::Background::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageEditParams::Background::TaggedSymbol)

        OPAQUE = T.let(:opaque, OpenAI::ImageEditParams::Background::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TRANSPARENT = T.let(:transparent, OpenAI::ImageEditParams::Background::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageEditParams::Background) }
      end

      # The image(s) to edit. Must be a supported image file or an array of images.
      #
      # For `gpt-image-1`, each image should be a `png`, `webp`, or `jpg` file less than
      # 50MB. You can provide up to 16 images.
      #
      # For `dall-e-2`, you can only provide one image, and it should be a square `png`
      # file less than 4MB.
      module Image
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditParams::Image::Variants]) }
          def variants; end
        end

        StringArray = T.let(
            OpenAI::Internal::Type::ArrayOf[OpenAI::Internal::Type::FileInput],
            OpenAI::Internal::Type::Converter
          )

        Variants = T.type_alias { T.any(StringIO, T::Array[StringIO]) }
      end

      # Control how much effort the model will exert to match the style and features,
      # especially facial features, of input images. This parameter is only supported
      # for `gpt-image-1`. Supports `high` and `low`. Defaults to `low`.
      module InputFidelity
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditParams::InputFidelity::TaggedSymbol]) }
          def values; end
        end

        HIGH = T.let(:high, OpenAI::ImageEditParams::InputFidelity::TaggedSymbol)

        LOW = T.let(:low, OpenAI::ImageEditParams::InputFidelity::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageEditParams::InputFidelity) }
      end

      # The model to use for image generation. Only `dall-e-2` and `gpt-image-1` are
      # supported. Defaults to `dall-e-2` unless a parameter specific to `gpt-image-1`
      # is used.
      module Model
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditParams::Model::Variants]) }
          def variants; end
        end

        Variants = T.type_alias { T.any(String, OpenAI::ImageModel::TaggedSymbol) }
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ImageEditParams, OpenAI::Internal::AnyHash)
        end

      # The format in which the generated images are returned. This parameter is only
      # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`. The
      # default value is `png`.
      module OutputFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditParams::OutputFormat::TaggedSymbol]) }
          def values; end
        end

        JPEG = T.let(:jpeg, OpenAI::ImageEditParams::OutputFormat::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }
        PNG = T.let(:png, OpenAI::ImageEditParams::OutputFormat::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageEditParams::OutputFormat) }

        WEBP = T.let(:webp, OpenAI::ImageEditParams::OutputFormat::TaggedSymbol)
      end

      # The quality of the image that will be generated. `high`, `medium` and `low` are
      # only supported for `gpt-image-1`. `dall-e-2` only supports `standard` quality.
      # Defaults to `auto`.
      module Quality
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditParams::Quality::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageEditParams::Quality::TaggedSymbol)
        HIGH = T.let(:high, OpenAI::ImageEditParams::Quality::TaggedSymbol)
        LOW = T.let(:low, OpenAI::ImageEditParams::Quality::TaggedSymbol)
        MEDIUM = T.let(:medium, OpenAI::ImageEditParams::Quality::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        STANDARD = T.let(:standard, OpenAI::ImageEditParams::Quality::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageEditParams::Quality) }
      end

      # The format in which the generated images are returned. Must be one of `url` or
      # `b64_json`. URLs are only valid for 60 minutes after the image has been
      # generated. This parameter is only supported for `dall-e-2`, as `gpt-image-1`
      # will always return base64-encoded images.
      module ResponseFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditParams::ResponseFormat::TaggedSymbol]) }
          def values; end
        end

        B64_JSON = T.let(
            :b64_json,
            OpenAI::ImageEditParams::ResponseFormat::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageEditParams::ResponseFormat)
          end

        URL = T.let(:url, OpenAI::ImageEditParams::ResponseFormat::TaggedSymbol)
      end

      # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
      # (landscape), `1024x1536` (portrait), or `auto` (default value) for
      # `gpt-image-1`, and one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`.
      module Size
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditParams::Size::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageEditParams::Size::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        SIZE_1024X1024 = T.let(:"1024x1024", OpenAI::ImageEditParams::Size::TaggedSymbol)

        SIZE_1024X1536 = T.let(:"1024x1536", OpenAI::ImageEditParams::Size::TaggedSymbol)

        SIZE_1536X1024 = T.let(:"1536x1024", OpenAI::ImageEditParams::Size::TaggedSymbol)

        SIZE_256X256 = T.let(:"256x256", OpenAI::ImageEditParams::Size::TaggedSymbol)

        SIZE_512X512 = T.let(:"512x512", OpenAI::ImageEditParams::Size::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageEditParams::Size) }
      end
    end

    class ImageEditPartialImageEvent < OpenAI::Internal::Type::BaseModel
      # Base64-encoded partial image data, suitable for rendering as an image.
      sig { returns(String) }
      attr_accessor :b64_json

      # The background setting for the requested edited image.
      sig { returns(OpenAI::ImageEditPartialImageEvent::Background::TaggedSymbol) }
      attr_accessor :background

      # The Unix timestamp when the event was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The output format for the requested edited image.
      sig { returns(OpenAI::ImageEditPartialImageEvent::OutputFormat::TaggedSymbol) }
      attr_accessor :output_format

      # 0-based index for the partial image (streaming).
      sig { returns(Integer) }
      attr_accessor :partial_image_index

      # The quality setting for the requested edited image.
      sig { returns(OpenAI::ImageEditPartialImageEvent::Quality::TaggedSymbol) }
      attr_accessor :quality

      # The size of the requested edited image.
      sig { returns(OpenAI::ImageEditPartialImageEvent::Size::TaggedSymbol) }
      attr_accessor :size

      # The type of the event. Always `image_edit.partial_image`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig do
        override
          .returns({
            b64_json: String,
            background:
              OpenAI::ImageEditPartialImageEvent::Background::TaggedSymbol,
            created_at: Integer,
            output_format:
              OpenAI::ImageEditPartialImageEvent::OutputFormat::TaggedSymbol,
            partial_image_index: Integer,
            quality: OpenAI::ImageEditPartialImageEvent::Quality::TaggedSymbol,
            size: OpenAI::ImageEditPartialImageEvent::Size::TaggedSymbol,
            type: Symbol
          })
      end
      def to_hash; end

      class << self
        # Emitted when a partial image is available during image editing streaming.
        sig do
          params(
            b64_json: String,
            background: OpenAI::ImageEditPartialImageEvent::Background::OrSymbol,
            created_at: Integer,
            output_format: OpenAI::ImageEditPartialImageEvent::OutputFormat::OrSymbol,
            partial_image_index: Integer,
            quality: OpenAI::ImageEditPartialImageEvent::Quality::OrSymbol,
            size: OpenAI::ImageEditPartialImageEvent::Size::OrSymbol,
            type: Symbol
          ).returns(T.attached_class)
        end
        def new(
          b64_json:, # Base64-encoded partial image data, suitable for rendering as an image.
          background:, # The background setting for the requested edited image.
          created_at:, # The Unix timestamp when the event was created.
          output_format:, # The output format for the requested edited image.
          partial_image_index:, # 0-based index for the partial image (streaming).
          quality:, # The quality setting for the requested edited image.
          size:, # The size of the requested edited image.
          type: :"image_edit.partial_image" # The type of the event. Always `image_edit.partial_image`.
); end
      end

      # The background setting for the requested edited image.
      module Background
        extend OpenAI::Internal::Type::Enum

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::ImageEditPartialImageEvent::Background::TaggedSymbol
            ])
          end
          def values; end
        end

        AUTO = T.let(
            :auto,
            OpenAI::ImageEditPartialImageEvent::Background::TaggedSymbol
          )

        OPAQUE = T.let(
            :opaque,
            OpenAI::ImageEditPartialImageEvent::Background::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TRANSPARENT = T.let(
            :transparent,
            OpenAI::ImageEditPartialImageEvent::Background::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageEditPartialImageEvent::Background)
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ImageEditPartialImageEvent, OpenAI::Internal::AnyHash)
        end

      # The output format for the requested edited image.
      module OutputFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::ImageEditPartialImageEvent::OutputFormat::TaggedSymbol
            ])
          end
          def values; end
        end

        JPEG = T.let(
            :jpeg,
            OpenAI::ImageEditPartialImageEvent::OutputFormat::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        PNG = T.let(
            :png,
            OpenAI::ImageEditPartialImageEvent::OutputFormat::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageEditPartialImageEvent::OutputFormat)
          end

        WEBP = T.let(
            :webp,
            OpenAI::ImageEditPartialImageEvent::OutputFormat::TaggedSymbol
          )
      end

      # The quality setting for the requested edited image.
      module Quality
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditPartialImageEvent::Quality::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(
            :auto,
            OpenAI::ImageEditPartialImageEvent::Quality::TaggedSymbol
          )

        HIGH = T.let(
            :high,
            OpenAI::ImageEditPartialImageEvent::Quality::TaggedSymbol
          )

        LOW = T.let(:low, OpenAI::ImageEditPartialImageEvent::Quality::TaggedSymbol)

        MEDIUM = T.let(
            :medium,
            OpenAI::ImageEditPartialImageEvent::Quality::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageEditPartialImageEvent::Quality)
          end
      end

      # The size of the requested edited image.
      module Size
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageEditPartialImageEvent::Size::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageEditPartialImageEvent::Size::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        SIZE_1024X1024 = T.let(
            :"1024x1024",
            OpenAI::ImageEditPartialImageEvent::Size::TaggedSymbol
          )

        SIZE_1024X1536 = T.let(
            :"1024x1536",
            OpenAI::ImageEditPartialImageEvent::Size::TaggedSymbol
          )

        SIZE_1536X1024 = T.let(
            :"1536x1024",
            OpenAI::ImageEditPartialImageEvent::Size::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageEditPartialImageEvent::Size)
          end
      end
    end

    # Emitted when a partial image is available during image editing streaming.
    module ImageEditStreamEvent
      extend OpenAI::Internal::Type::Union

      class << self
        sig { override.returns(T::Array[OpenAI::ImageEditStreamEvent::Variants]) }
        def variants; end
      end

      Variants = T.type_alias do
          T.any(
            OpenAI::ImageEditPartialImageEvent,
            OpenAI::ImageEditCompletedEvent
          )
        end
    end

    class ImageGenCompletedEvent < OpenAI::Internal::Type::BaseModel
      # Base64-encoded image data, suitable for rendering as an image.
      sig { returns(String) }
      attr_accessor :b64_json

      # The background setting for the generated image.
      sig { returns(OpenAI::ImageGenCompletedEvent::Background::TaggedSymbol) }
      attr_accessor :background

      # The Unix timestamp when the event was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The output format for the generated image.
      sig { returns(OpenAI::ImageGenCompletedEvent::OutputFormat::TaggedSymbol) }
      attr_accessor :output_format

      # The quality setting for the generated image.
      sig { returns(OpenAI::ImageGenCompletedEvent::Quality::TaggedSymbol) }
      attr_accessor :quality

      # The size of the generated image.
      sig { returns(OpenAI::ImageGenCompletedEvent::Size::TaggedSymbol) }
      attr_accessor :size

      # The type of the event. Always `image_generation.completed`.
      sig { returns(Symbol) }
      attr_accessor :type

      # For `gpt-image-1` only, the token usage information for the image generation.
      sig { returns(OpenAI::ImageGenCompletedEvent::Usage) }
      attr_reader :usage

      sig { params(usage: OpenAI::ImageGenCompletedEvent::Usage::OrHash).void }
      attr_writer :usage

      sig do
        override
          .returns({
            b64_json: String,
            background:
              OpenAI::ImageGenCompletedEvent::Background::TaggedSymbol,
            created_at: Integer,
            output_format:
              OpenAI::ImageGenCompletedEvent::OutputFormat::TaggedSymbol,
            quality: OpenAI::ImageGenCompletedEvent::Quality::TaggedSymbol,
            size: OpenAI::ImageGenCompletedEvent::Size::TaggedSymbol,
            type: Symbol,
            usage: OpenAI::ImageGenCompletedEvent::Usage
          })
      end
      def to_hash; end

      class << self
        # Emitted when image generation has completed and the final image is available.
        sig do
          params(
            b64_json: String,
            background: OpenAI::ImageGenCompletedEvent::Background::OrSymbol,
            created_at: Integer,
            output_format: OpenAI::ImageGenCompletedEvent::OutputFormat::OrSymbol,
            quality: OpenAI::ImageGenCompletedEvent::Quality::OrSymbol,
            size: OpenAI::ImageGenCompletedEvent::Size::OrSymbol,
            usage: OpenAI::ImageGenCompletedEvent::Usage::OrHash,
            type: Symbol
          ).returns(T.attached_class)
        end
        def new(
          b64_json:, # Base64-encoded image data, suitable for rendering as an image.
          background:, # The background setting for the generated image.
          created_at:, # The Unix timestamp when the event was created.
          output_format:, # The output format for the generated image.
          quality:, # The quality setting for the generated image.
          size:, # The size of the generated image.
          usage:, # For `gpt-image-1` only, the token usage information for the image generation.
          type: :"image_generation.completed" # The type of the event. Always `image_generation.completed`.
); end
      end

      # The background setting for the generated image.
      module Background
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenCompletedEvent::Background::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageGenCompletedEvent::Background::TaggedSymbol)

        OPAQUE = T.let(
            :opaque,
            OpenAI::ImageGenCompletedEvent::Background::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TRANSPARENT = T.let(
            :transparent,
            OpenAI::ImageGenCompletedEvent::Background::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenCompletedEvent::Background)
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ImageGenCompletedEvent, OpenAI::Internal::AnyHash)
        end

      # The output format for the generated image.
      module OutputFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenCompletedEvent::OutputFormat::TaggedSymbol]) }
          def values; end
        end

        JPEG = T.let(
            :jpeg,
            OpenAI::ImageGenCompletedEvent::OutputFormat::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        PNG = T.let(
            :png,
            OpenAI::ImageGenCompletedEvent::OutputFormat::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenCompletedEvent::OutputFormat)
          end

        WEBP = T.let(
            :webp,
            OpenAI::ImageGenCompletedEvent::OutputFormat::TaggedSymbol
          )
      end

      # The quality setting for the generated image.
      module Quality
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenCompletedEvent::Quality::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageGenCompletedEvent::Quality::TaggedSymbol)

        HIGH = T.let(:high, OpenAI::ImageGenCompletedEvent::Quality::TaggedSymbol)

        LOW = T.let(:low, OpenAI::ImageGenCompletedEvent::Quality::TaggedSymbol)

        MEDIUM = T.let(:medium, OpenAI::ImageGenCompletedEvent::Quality::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenCompletedEvent::Quality)
          end
      end

      # The size of the generated image.
      module Size
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenCompletedEvent::Size::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageGenCompletedEvent::Size::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        SIZE_1024X1024 = T.let(
            :"1024x1024",
            OpenAI::ImageGenCompletedEvent::Size::TaggedSymbol
          )

        SIZE_1024X1536 = T.let(
            :"1024x1536",
            OpenAI::ImageGenCompletedEvent::Size::TaggedSymbol
          )

        SIZE_1536X1024 = T.let(
            :"1536x1024",
            OpenAI::ImageGenCompletedEvent::Size::TaggedSymbol
          )

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageGenCompletedEvent::Size) }
      end

      class Usage < OpenAI::Internal::Type::BaseModel
        # The number of tokens (images and text) in the input prompt.
        sig { returns(Integer) }
        attr_accessor :input_tokens

        # The input tokens detailed information for the image generation.
        sig { returns(OpenAI::ImageGenCompletedEvent::Usage::InputTokensDetails) }
        attr_reader :input_tokens_details

        sig { params(input_tokens_details: OpenAI::ImageGenCompletedEvent::Usage::InputTokensDetails::OrHash).void }
        attr_writer :input_tokens_details

        # The number of image tokens in the output image.
        sig { returns(Integer) }
        attr_accessor :output_tokens

        # The total number of tokens (images and text) used for the image generation.
        sig { returns(Integer) }
        attr_accessor :total_tokens

        sig do
          override
            .returns({
              input_tokens: Integer,
              input_tokens_details:
                OpenAI::ImageGenCompletedEvent::Usage::InputTokensDetails,
              output_tokens: Integer,
              total_tokens: Integer
            })
        end
        def to_hash; end

        class << self
          # For `gpt-image-1` only, the token usage information for the image generation.
          sig do
            params(
              input_tokens: Integer,
              input_tokens_details: OpenAI::ImageGenCompletedEvent::Usage::InputTokensDetails::OrHash,
              output_tokens: Integer,
              total_tokens: Integer
            ).returns(T.attached_class)
          end
          def new(
            input_tokens:, # The number of tokens (images and text) in the input prompt.
            input_tokens_details:, # The input tokens detailed information for the image generation.
            output_tokens:, # The number of image tokens in the output image.
            total_tokens: # The total number of tokens (images and text) used for the image generation.
); end
        end

        class InputTokensDetails < OpenAI::Internal::Type::BaseModel
          # The number of image tokens in the input prompt.
          sig { returns(Integer) }
          attr_accessor :image_tokens

          # The number of text tokens in the input prompt.
          sig { returns(Integer) }
          attr_accessor :text_tokens

          sig { override.returns({ image_tokens: Integer, text_tokens: Integer }) }
          def to_hash; end

          class << self
            # The input tokens detailed information for the image generation.
            sig { params(image_tokens: Integer, text_tokens: Integer).returns(T.attached_class) }
            def new(
              image_tokens:, # The number of image tokens in the input prompt.
              text_tokens: # The number of text tokens in the input prompt.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::ImageGenCompletedEvent::Usage::InputTokensDetails,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::ImageGenCompletedEvent::Usage,
              OpenAI::Internal::AnyHash
            )
          end
      end
    end

    class ImageGenPartialImageEvent < OpenAI::Internal::Type::BaseModel
      # Base64-encoded partial image data, suitable for rendering as an image.
      sig { returns(String) }
      attr_accessor :b64_json

      # The background setting for the requested image.
      sig { returns(OpenAI::ImageGenPartialImageEvent::Background::TaggedSymbol) }
      attr_accessor :background

      # The Unix timestamp when the event was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The output format for the requested image.
      sig { returns(OpenAI::ImageGenPartialImageEvent::OutputFormat::TaggedSymbol) }
      attr_accessor :output_format

      # 0-based index for the partial image (streaming).
      sig { returns(Integer) }
      attr_accessor :partial_image_index

      # The quality setting for the requested image.
      sig { returns(OpenAI::ImageGenPartialImageEvent::Quality::TaggedSymbol) }
      attr_accessor :quality

      # The size of the requested image.
      sig { returns(OpenAI::ImageGenPartialImageEvent::Size::TaggedSymbol) }
      attr_accessor :size

      # The type of the event. Always `image_generation.partial_image`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig do
        override
          .returns({
            b64_json: String,
            background:
              OpenAI::ImageGenPartialImageEvent::Background::TaggedSymbol,
            created_at: Integer,
            output_format:
              OpenAI::ImageGenPartialImageEvent::OutputFormat::TaggedSymbol,
            partial_image_index: Integer,
            quality: OpenAI::ImageGenPartialImageEvent::Quality::TaggedSymbol,
            size: OpenAI::ImageGenPartialImageEvent::Size::TaggedSymbol,
            type: Symbol
          })
      end
      def to_hash; end

      class << self
        # Emitted when a partial image is available during image generation streaming.
        sig do
          params(
            b64_json: String,
            background: OpenAI::ImageGenPartialImageEvent::Background::OrSymbol,
            created_at: Integer,
            output_format: OpenAI::ImageGenPartialImageEvent::OutputFormat::OrSymbol,
            partial_image_index: Integer,
            quality: OpenAI::ImageGenPartialImageEvent::Quality::OrSymbol,
            size: OpenAI::ImageGenPartialImageEvent::Size::OrSymbol,
            type: Symbol
          ).returns(T.attached_class)
        end
        def new(
          b64_json:, # Base64-encoded partial image data, suitable for rendering as an image.
          background:, # The background setting for the requested image.
          created_at:, # The Unix timestamp when the event was created.
          output_format:, # The output format for the requested image.
          partial_image_index:, # 0-based index for the partial image (streaming).
          quality:, # The quality setting for the requested image.
          size:, # The size of the requested image.
          type: :"image_generation.partial_image" # The type of the event. Always `image_generation.partial_image`.
); end
      end

      # The background setting for the requested image.
      module Background
        extend OpenAI::Internal::Type::Enum

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::ImageGenPartialImageEvent::Background::TaggedSymbol
            ])
          end
          def values; end
        end

        AUTO = T.let(
            :auto,
            OpenAI::ImageGenPartialImageEvent::Background::TaggedSymbol
          )

        OPAQUE = T.let(
            :opaque,
            OpenAI::ImageGenPartialImageEvent::Background::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TRANSPARENT = T.let(
            :transparent,
            OpenAI::ImageGenPartialImageEvent::Background::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenPartialImageEvent::Background)
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ImageGenPartialImageEvent, OpenAI::Internal::AnyHash)
        end

      # The output format for the requested image.
      module OutputFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::ImageGenPartialImageEvent::OutputFormat::TaggedSymbol
            ])
          end
          def values; end
        end

        JPEG = T.let(
            :jpeg,
            OpenAI::ImageGenPartialImageEvent::OutputFormat::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        PNG = T.let(
            :png,
            OpenAI::ImageGenPartialImageEvent::OutputFormat::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenPartialImageEvent::OutputFormat)
          end

        WEBP = T.let(
            :webp,
            OpenAI::ImageGenPartialImageEvent::OutputFormat::TaggedSymbol
          )
      end

      # The quality setting for the requested image.
      module Quality
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenPartialImageEvent::Quality::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageGenPartialImageEvent::Quality::TaggedSymbol)

        HIGH = T.let(:high, OpenAI::ImageGenPartialImageEvent::Quality::TaggedSymbol)

        LOW = T.let(:low, OpenAI::ImageGenPartialImageEvent::Quality::TaggedSymbol)

        MEDIUM = T.let(
            :medium,
            OpenAI::ImageGenPartialImageEvent::Quality::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenPartialImageEvent::Quality)
          end
      end

      # The size of the requested image.
      module Size
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenPartialImageEvent::Size::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageGenPartialImageEvent::Size::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        SIZE_1024X1024 = T.let(
            :"1024x1024",
            OpenAI::ImageGenPartialImageEvent::Size::TaggedSymbol
          )

        SIZE_1024X1536 = T.let(
            :"1024x1536",
            OpenAI::ImageGenPartialImageEvent::Size::TaggedSymbol
          )

        SIZE_1536X1024 = T.let(
            :"1536x1024",
            OpenAI::ImageGenPartialImageEvent::Size::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenPartialImageEvent::Size)
          end
      end
    end

    # Emitted when a partial image is available during image generation streaming.
    module ImageGenStreamEvent
      extend OpenAI::Internal::Type::Union

      class << self
        sig { override.returns(T::Array[OpenAI::ImageGenStreamEvent::Variants]) }
        def variants; end
      end

      Variants = T.type_alias do
          T.any(
            OpenAI::ImageGenPartialImageEvent,
            OpenAI::ImageGenCompletedEvent
          )
        end
    end

    class ImageGenerateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # Allows to set transparency for the background of the generated image(s). This
      # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
      # `opaque` or `auto` (default value). When `auto` is used, the model will
      # automatically determine the best background for the image.
      #
      # If `transparent`, the output format needs to support transparency, so it should
      # be set to either `png` (default value) or `webp`.
      sig { returns(T.nilable(OpenAI::ImageGenerateParams::Background::OrSymbol)) }
      attr_accessor :background

      # The model to use for image generation. One of `dall-e-2`, `dall-e-3`, or
      # `gpt-image-1`. Defaults to `dall-e-2` unless a parameter specific to
      # `gpt-image-1` is used.
      sig { returns(T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol))) }
      attr_accessor :model

      # Control the content-moderation level for images generated by `gpt-image-1`. Must
      # be either `low` for less restrictive filtering or `auto` (default value).
      sig { returns(T.nilable(OpenAI::ImageGenerateParams::Moderation::OrSymbol)) }
      attr_accessor :moderation

      # The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only
      # `n=1` is supported.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :n

      # The compression level (0-100%) for the generated images. This parameter is only
      # supported for `gpt-image-1` with the `webp` or `jpeg` output formats, and
      # defaults to 100.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :output_compression

      # The format in which the generated images are returned. This parameter is only
      # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`.
      sig { returns(T.nilable(OpenAI::ImageGenerateParams::OutputFormat::OrSymbol)) }
      attr_accessor :output_format

      # The number of partial images to generate. This parameter is used for streaming
      # responses that return partial images. Value must be between 0 and 3. When set to
      # 0, the response will be a single image sent in one streaming event.
      #
      # Note that the final image may be sent before the full number of partial images
      # are generated if the full image is generated more quickly.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :partial_images

      # A text description of the desired image(s). The maximum length is 32000
      # characters for `gpt-image-1`, 1000 characters for `dall-e-2` and 4000 characters
      # for `dall-e-3`.
      sig { returns(String) }
      attr_accessor :prompt

      # The quality of the image that will be generated.
      #
      # - `auto` (default value) will automatically select the best quality for the
      #   given model.
      # - `high`, `medium` and `low` are supported for `gpt-image-1`.
      # - `hd` and `standard` are supported for `dall-e-3`.
      # - `standard` is the only option for `dall-e-2`.
      sig { returns(T.nilable(OpenAI::ImageGenerateParams::Quality::OrSymbol)) }
      attr_accessor :quality

      # The format in which generated images with `dall-e-2` and `dall-e-3` are
      # returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes
      # after the image has been generated. This parameter isn't supported for
      # `gpt-image-1` which will always return base64-encoded images.
      sig { returns(T.nilable(OpenAI::ImageGenerateParams::ResponseFormat::OrSymbol)) }
      attr_accessor :response_format

      # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
      # (landscape), `1024x1536` (portrait), or `auto` (default value) for
      # `gpt-image-1`, one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`, and
      # one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3`.
      sig { returns(T.nilable(OpenAI::ImageGenerateParams::Size::OrSymbol)) }
      attr_accessor :size

      # The style of the generated images. This parameter is only supported for
      # `dall-e-3`. Must be one of `vivid` or `natural`. Vivid causes the model to lean
      # towards generating hyper-real and dramatic images. Natural causes the model to
      # produce more natural, less hyper-real looking images.
      sig { returns(T.nilable(OpenAI::ImageGenerateParams::Style::OrSymbol)) }
      attr_accessor :style

      # A unique identifier representing your end-user, which can help OpenAI to monitor
      # and detect abuse.
      # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
      sig { returns(T.nilable(String)) }
      attr_reader :user

      sig { params(user: String).void }
      attr_writer :user

      sig do
        override
          .returns({
            prompt: String,
            background:
              T.nilable(OpenAI::ImageGenerateParams::Background::OrSymbol),
            model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
            moderation:
              T.nilable(OpenAI::ImageGenerateParams::Moderation::OrSymbol),
            n: T.nilable(Integer),
            output_compression: T.nilable(Integer),
            output_format:
              T.nilable(OpenAI::ImageGenerateParams::OutputFormat::OrSymbol),
            partial_images: T.nilable(Integer),
            quality: T.nilable(OpenAI::ImageGenerateParams::Quality::OrSymbol),
            response_format:
              T.nilable(OpenAI::ImageGenerateParams::ResponseFormat::OrSymbol),
            size: T.nilable(OpenAI::ImageGenerateParams::Size::OrSymbol),
            style: T.nilable(OpenAI::ImageGenerateParams::Style::OrSymbol),
            user: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            prompt: String,
            background: T.nilable(OpenAI::ImageGenerateParams::Background::OrSymbol),
            model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
            moderation: T.nilable(OpenAI::ImageGenerateParams::Moderation::OrSymbol),
            n: T.nilable(Integer),
            output_compression: T.nilable(Integer),
            output_format: T.nilable(OpenAI::ImageGenerateParams::OutputFormat::OrSymbol),
            partial_images: T.nilable(Integer),
            quality: T.nilable(OpenAI::ImageGenerateParams::Quality::OrSymbol),
            response_format: T.nilable(OpenAI::ImageGenerateParams::ResponseFormat::OrSymbol),
            size: T.nilable(OpenAI::ImageGenerateParams::Size::OrSymbol),
            style: T.nilable(OpenAI::ImageGenerateParams::Style::OrSymbol),
            user: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          prompt:, # A text description of the desired image(s). The maximum length is 32000
                   # characters for `gpt-image-1`, 1000 characters for `dall-e-2` and 4000 characters
                   # for `dall-e-3`.
          background: nil, # Allows to set transparency for the background of the generated image(s). This
                           # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
                           # `opaque` or `auto` (default value). When `auto` is used, the model will
                           # automatically determine the best background for the image.
                           # If `transparent`, the output format needs to support transparency, so it should
                           # be set to either `png` (default value) or `webp`.
          model: nil, # The model to use for image generation. One of `dall-e-2`, `dall-e-3`, or
                      # `gpt-image-1`. Defaults to `dall-e-2` unless a parameter specific to
                      # `gpt-image-1` is used.
          moderation: nil, # Control the content-moderation level for images generated by `gpt-image-1`. Must
                           # be either `low` for less restrictive filtering or `auto` (default value).
          n: nil, # The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only
                  # `n=1` is supported.
          output_compression: nil, # The compression level (0-100%) for the generated images. This parameter is only
                                   # supported for `gpt-image-1` with the `webp` or `jpeg` output formats, and
                                   # defaults to 100.
          output_format: nil, # The format in which the generated images are returned. This parameter is only
                              # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`.
          partial_images: nil, # The number of partial images to generate. This parameter is used for streaming
                               # responses that return partial images. Value must be between 0 and 3. When set to
                               # 0, the response will be a single image sent in one streaming event.
                               # Note that the final image may be sent before the full number of partial images
                               # are generated if the full image is generated more quickly.
          quality: nil, # The quality of the image that will be generated.
                        # - `auto` (default value) will automatically select the best quality for the
                        #   given model.
                        # - `high`, `medium` and `low` are supported for `gpt-image-1`.
                        # - `hd` and `standard` are supported for `dall-e-3`.
                        # - `standard` is the only option for `dall-e-2`.
          response_format: nil, # The format in which generated images with `dall-e-2` and `dall-e-3` are
                                # returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes
                                # after the image has been generated. This parameter isn't supported for
                                # `gpt-image-1` which will always return base64-encoded images.
          size: nil, # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
                     # (landscape), `1024x1536` (portrait), or `auto` (default value) for
                     # `gpt-image-1`, one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`, and
                     # one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3`.
          style: nil, # The style of the generated images. This parameter is only supported for
                      # `dall-e-3`. Must be one of `vivid` or `natural`. Vivid causes the model to lean
                      # towards generating hyper-real and dramatic images. Natural causes the model to
                      # produce more natural, less hyper-real looking images.
          user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                     # and detect abuse.
                     # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
          request_options: {}
); end
      end

      # Allows to set transparency for the background of the generated image(s). This
      # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
      # `opaque` or `auto` (default value). When `auto` is used, the model will
      # automatically determine the best background for the image.
      #
      # If `transparent`, the output format needs to support transparency, so it should
      # be set to either `png` (default value) or `webp`.
      module Background
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenerateParams::Background::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageGenerateParams::Background::TaggedSymbol)

        OPAQUE = T.let(:opaque, OpenAI::ImageGenerateParams::Background::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TRANSPARENT = T.let(
            :transparent,
            OpenAI::ImageGenerateParams::Background::TaggedSymbol
          )

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenerateParams::Background)
          end
      end

      # The model to use for image generation. One of `dall-e-2`, `dall-e-3`, or
      # `gpt-image-1`. Defaults to `dall-e-2` unless a parameter specific to
      # `gpt-image-1` is used.
      module Model
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenerateParams::Model::Variants]) }
          def variants; end
        end

        Variants = T.type_alias { T.any(String, OpenAI::ImageModel::TaggedSymbol) }
      end

      # Control the content-moderation level for images generated by `gpt-image-1`. Must
      # be either `low` for less restrictive filtering or `auto` (default value).
      module Moderation
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenerateParams::Moderation::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageGenerateParams::Moderation::TaggedSymbol)

        LOW = T.let(:low, OpenAI::ImageGenerateParams::Moderation::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenerateParams::Moderation)
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ImageGenerateParams, OpenAI::Internal::AnyHash)
        end

      # The format in which the generated images are returned. This parameter is only
      # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`.
      module OutputFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenerateParams::OutputFormat::TaggedSymbol]) }
          def values; end
        end

        JPEG = T.let(:jpeg, OpenAI::ImageGenerateParams::OutputFormat::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        PNG = T.let(:png, OpenAI::ImageGenerateParams::OutputFormat::TaggedSymbol)

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenerateParams::OutputFormat)
          end

        WEBP = T.let(:webp, OpenAI::ImageGenerateParams::OutputFormat::TaggedSymbol)
      end

      # The quality of the image that will be generated.
      #
      # - `auto` (default value) will automatically select the best quality for the
      #   given model.
      # - `high`, `medium` and `low` are supported for `gpt-image-1`.
      # - `hd` and `standard` are supported for `dall-e-3`.
      # - `standard` is the only option for `dall-e-2`.
      module Quality
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenerateParams::Quality::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageGenerateParams::Quality::TaggedSymbol)
        HD = T.let(:hd, OpenAI::ImageGenerateParams::Quality::TaggedSymbol)
        HIGH = T.let(:high, OpenAI::ImageGenerateParams::Quality::TaggedSymbol)
        LOW = T.let(:low, OpenAI::ImageGenerateParams::Quality::TaggedSymbol)

        MEDIUM = T.let(:medium, OpenAI::ImageGenerateParams::Quality::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        STANDARD = T.let(:standard, OpenAI::ImageGenerateParams::Quality::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageGenerateParams::Quality) }
      end

      # The format in which generated images with `dall-e-2` and `dall-e-3` are
      # returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes
      # after the image has been generated. This parameter isn't supported for
      # `gpt-image-1` which will always return base64-encoded images.
      module ResponseFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenerateParams::ResponseFormat::TaggedSymbol]) }
          def values; end
        end

        B64_JSON = T.let(
            :b64_json,
            OpenAI::ImageGenerateParams::ResponseFormat::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ImageGenerateParams::ResponseFormat)
          end

        URL = T.let(:url, OpenAI::ImageGenerateParams::ResponseFormat::TaggedSymbol)
      end

      # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
      # (landscape), `1024x1536` (portrait), or `auto` (default value) for
      # `gpt-image-1`, one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`, and
      # one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3`.
      module Size
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenerateParams::Size::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::ImageGenerateParams::Size::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        SIZE_1024X1024 = T.let(:"1024x1024", OpenAI::ImageGenerateParams::Size::TaggedSymbol)

        SIZE_1024X1536 = T.let(:"1024x1536", OpenAI::ImageGenerateParams::Size::TaggedSymbol)

        SIZE_1024X1792 = T.let(:"1024x1792", OpenAI::ImageGenerateParams::Size::TaggedSymbol)

        SIZE_1536X1024 = T.let(:"1536x1024", OpenAI::ImageGenerateParams::Size::TaggedSymbol)

        SIZE_1792X1024 = T.let(:"1792x1024", OpenAI::ImageGenerateParams::Size::TaggedSymbol)

        SIZE_256X256 = T.let(:"256x256", OpenAI::ImageGenerateParams::Size::TaggedSymbol)

        SIZE_512X512 = T.let(:"512x512", OpenAI::ImageGenerateParams::Size::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageGenerateParams::Size) }
      end

      # The style of the generated images. This parameter is only supported for
      # `dall-e-3`. Must be one of `vivid` or `natural`. Vivid causes the model to lean
      # towards generating hyper-real and dramatic images. Natural causes the model to
      # produce more natural, less hyper-real looking images.
      module Style
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImageGenerateParams::Style::TaggedSymbol]) }
          def values; end
        end

        NATURAL = T.let(:natural, OpenAI::ImageGenerateParams::Style::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageGenerateParams::Style) }

        VIVID = T.let(:vivid, OpenAI::ImageGenerateParams::Style::TaggedSymbol)
      end
    end

    module ImageModel
      extend OpenAI::Internal::Type::Enum

      class << self
        sig { override.returns(T::Array[OpenAI::ImageModel::TaggedSymbol]) }
        def values; end
      end

      DALL_E_2 = T.let(:"dall-e-2", OpenAI::ImageModel::TaggedSymbol)
      DALL_E_3 = T.let(:"dall-e-3", OpenAI::ImageModel::TaggedSymbol)
      GPT_IMAGE_1 = T.let(:"gpt-image-1", OpenAI::ImageModel::TaggedSymbol)
      OrSymbol = T.type_alias { T.any(Symbol, String) }
      TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImageModel) }
    end

    class ImagesResponse < OpenAI::Internal::Type::BaseModel
      # The background parameter used for the image generation. Either `transparent` or
      # `opaque`.
      sig { returns(T.nilable(OpenAI::ImagesResponse::Background::TaggedSymbol)) }
      attr_reader :background

      sig { params(background: OpenAI::ImagesResponse::Background::OrSymbol).void }
      attr_writer :background

      # The Unix timestamp (in seconds) of when the image was created.
      sig { returns(Integer) }
      attr_accessor :created

      # The list of generated images.
      sig { returns(T.nilable(T::Array[OpenAI::Image])) }
      attr_reader :data

      sig { params(data: T::Array[OpenAI::Image::OrHash]).void }
      attr_writer :data

      # The output format of the image generation. Either `png`, `webp`, or `jpeg`.
      sig { returns(T.nilable(OpenAI::ImagesResponse::OutputFormat::TaggedSymbol)) }
      attr_reader :output_format

      sig { params(output_format: OpenAI::ImagesResponse::OutputFormat::OrSymbol).void }
      attr_writer :output_format

      # The quality of the image generated. Either `low`, `medium`, or `high`.
      sig { returns(T.nilable(OpenAI::ImagesResponse::Quality::TaggedSymbol)) }
      attr_reader :quality

      sig { params(quality: OpenAI::ImagesResponse::Quality::OrSymbol).void }
      attr_writer :quality

      # The size of the image generated. Either `1024x1024`, `1024x1536`, or
      # `1536x1024`.
      sig { returns(T.nilable(OpenAI::ImagesResponse::Size::TaggedSymbol)) }
      attr_reader :size

      sig { params(size: OpenAI::ImagesResponse::Size::OrSymbol).void }
      attr_writer :size

      # For `gpt-image-1` only, the token usage information for the image generation.
      sig { returns(T.nilable(OpenAI::ImagesResponse::Usage)) }
      attr_reader :usage

      sig { params(usage: OpenAI::ImagesResponse::Usage::OrHash).void }
      attr_writer :usage

      sig do
        override
          .returns({
            created: Integer,
            background: OpenAI::ImagesResponse::Background::TaggedSymbol,
            data: T::Array[OpenAI::Image],
            output_format: OpenAI::ImagesResponse::OutputFormat::TaggedSymbol,
            quality: OpenAI::ImagesResponse::Quality::TaggedSymbol,
            size: OpenAI::ImagesResponse::Size::TaggedSymbol,
            usage: OpenAI::ImagesResponse::Usage
          })
      end
      def to_hash; end

      class << self
        # The response from the image generation endpoint.
        sig do
          params(
            created: Integer,
            background: OpenAI::ImagesResponse::Background::OrSymbol,
            data: T::Array[OpenAI::Image::OrHash],
            output_format: OpenAI::ImagesResponse::OutputFormat::OrSymbol,
            quality: OpenAI::ImagesResponse::Quality::OrSymbol,
            size: OpenAI::ImagesResponse::Size::OrSymbol,
            usage: OpenAI::ImagesResponse::Usage::OrHash
          ).returns(T.attached_class)
        end
        def new(
          created:, # The Unix timestamp (in seconds) of when the image was created.
          background: nil, # The background parameter used for the image generation. Either `transparent` or
                           # `opaque`.
          data: nil, # The list of generated images.
          output_format: nil, # The output format of the image generation. Either `png`, `webp`, or `jpeg`.
          quality: nil, # The quality of the image generated. Either `low`, `medium`, or `high`.
          size: nil, # The size of the image generated. Either `1024x1024`, `1024x1536`, or
                     # `1536x1024`.
          usage: nil # For `gpt-image-1` only, the token usage information for the image generation.
); end
      end

      # The background parameter used for the image generation. Either `transparent` or
      # `opaque`.
      module Background
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImagesResponse::Background::TaggedSymbol]) }
          def values; end
        end

        OPAQUE = T.let(:opaque, OpenAI::ImagesResponse::Background::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TRANSPARENT = T.let(:transparent, OpenAI::ImagesResponse::Background::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImagesResponse::Background) }
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ImagesResponse, OpenAI::Internal::AnyHash)
        end

      # The output format of the image generation. Either `png`, `webp`, or `jpeg`.
      module OutputFormat
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImagesResponse::OutputFormat::TaggedSymbol]) }
          def values; end
        end

        JPEG = T.let(:jpeg, OpenAI::ImagesResponse::OutputFormat::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }
        PNG = T.let(:png, OpenAI::ImagesResponse::OutputFormat::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImagesResponse::OutputFormat) }

        WEBP = T.let(:webp, OpenAI::ImagesResponse::OutputFormat::TaggedSymbol)
      end

      # The quality of the image generated. Either `low`, `medium`, or `high`.
      module Quality
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImagesResponse::Quality::TaggedSymbol]) }
          def values; end
        end

        HIGH = T.let(:high, OpenAI::ImagesResponse::Quality::TaggedSymbol)
        LOW = T.let(:low, OpenAI::ImagesResponse::Quality::TaggedSymbol)
        MEDIUM = T.let(:medium, OpenAI::ImagesResponse::Quality::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImagesResponse::Quality) }
      end

      # The size of the image generated. Either `1024x1024`, `1024x1536`, or
      # `1536x1024`.
      module Size
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ImagesResponse::Size::TaggedSymbol]) }
          def values; end
        end

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        SIZE_1024X1024 = T.let(:"1024x1024", OpenAI::ImagesResponse::Size::TaggedSymbol)

        SIZE_1024X1536 = T.let(:"1024x1536", OpenAI::ImagesResponse::Size::TaggedSymbol)

        SIZE_1536X1024 = T.let(:"1536x1024", OpenAI::ImagesResponse::Size::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ImagesResponse::Size) }
      end

      class Usage < OpenAI::Internal::Type::BaseModel
        # The number of tokens (images and text) in the input prompt.
        sig { returns(Integer) }
        attr_accessor :input_tokens

        # The input tokens detailed information for the image generation.
        sig { returns(OpenAI::ImagesResponse::Usage::InputTokensDetails) }
        attr_reader :input_tokens_details

        sig { params(input_tokens_details: OpenAI::ImagesResponse::Usage::InputTokensDetails::OrHash).void }
        attr_writer :input_tokens_details

        # The number of output tokens generated by the model.
        sig { returns(Integer) }
        attr_accessor :output_tokens

        # The total number of tokens (images and text) used for the image generation.
        sig { returns(Integer) }
        attr_accessor :total_tokens

        sig do
          override
            .returns({
              input_tokens: Integer,
              input_tokens_details:
                OpenAI::ImagesResponse::Usage::InputTokensDetails,
              output_tokens: Integer,
              total_tokens: Integer
            })
        end
        def to_hash; end

        class << self
          # For `gpt-image-1` only, the token usage information for the image generation.
          sig do
            params(
              input_tokens: Integer,
              input_tokens_details: OpenAI::ImagesResponse::Usage::InputTokensDetails::OrHash,
              output_tokens: Integer,
              total_tokens: Integer
            ).returns(T.attached_class)
          end
          def new(
            input_tokens:, # The number of tokens (images and text) in the input prompt.
            input_tokens_details:, # The input tokens detailed information for the image generation.
            output_tokens:, # The number of output tokens generated by the model.
            total_tokens: # The total number of tokens (images and text) used for the image generation.
); end
        end

        class InputTokensDetails < OpenAI::Internal::Type::BaseModel
          # The number of image tokens in the input prompt.
          sig { returns(Integer) }
          attr_accessor :image_tokens

          # The number of text tokens in the input prompt.
          sig { returns(Integer) }
          attr_accessor :text_tokens

          sig { override.returns({ image_tokens: Integer, text_tokens: Integer }) }
          def to_hash; end

          class << self
            # The input tokens detailed information for the image generation.
            sig { params(image_tokens: Integer, text_tokens: Integer).returns(T.attached_class) }
            def new(
              image_tokens:, # The number of image tokens in the input prompt.
              text_tokens: # The number of text tokens in the input prompt.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::ImagesResponse::Usage::InputTokensDetails,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::ImagesResponse::Usage, OpenAI::Internal::AnyHash)
          end
      end
    end

    LabelModelGrader = Graders::LabelModelGrader

    Metadata = T.let(
        OpenAI::Internal::Type::HashOf[String],
        OpenAI::Internal::Type::Converter
      )

    class Model < OpenAI::Internal::Type::BaseModel
      # The Unix timestamp (in seconds) when the model was created.
      sig { returns(Integer) }
      attr_accessor :created

      # The model identifier, which can be referenced in the API endpoints.
      sig { returns(String) }
      attr_accessor :id

      # The object type, which is always "model".
      sig { returns(Symbol) }
      attr_accessor :object

      # The organization that owns the model.
      sig { returns(String) }
      attr_accessor :owned_by

      sig { override.returns({ id: String, created: Integer, object: Symbol, owned_by: String }) }
      def to_hash; end

      class << self
        # Describes an OpenAI model offering that can be used with the API.
        sig { params(id: String, created: Integer, owned_by: String, object: Symbol).returns(T.attached_class) }
        def new(
          id:, # The model identifier, which can be referenced in the API endpoints.
          created:, # The Unix timestamp (in seconds) when the model was created.
          owned_by:, # The organization that owns the model.
          object: :model # The object type, which is always "model".
); end
      end

      OrHash = T.type_alias { T.any(OpenAI::Model, OpenAI::Internal::AnyHash) }
    end

    class ModelDeleteParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ModelDeleteParams, OpenAI::Internal::AnyHash)
        end
    end

    class ModelDeleted < OpenAI::Internal::Type::BaseModel
      sig { returns(T::Boolean) }
      attr_accessor :deleted

      sig { returns(String) }
      attr_accessor :id

      sig { returns(String) }
      attr_accessor :object

      sig { override.returns({ id: String, deleted: T::Boolean, object: String }) }
      def to_hash; end

      class << self
        sig { params(id: String, deleted: T::Boolean, object: String).returns(T.attached_class) }
        def new(id:, deleted:, object:); end
      end

      OrHash = T.type_alias { T.any(OpenAI::ModelDeleted, OpenAI::Internal::AnyHash) }
    end

    class ModelListParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ModelListParams, OpenAI::Internal::AnyHash)
        end
    end

    class ModelRetrieveParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ModelRetrieveParams, OpenAI::Internal::AnyHash)
        end
    end

    class Moderation < OpenAI::Internal::Type::BaseModel
      # A list of the categories, and whether they are flagged or not.
      sig { returns(OpenAI::Moderation::Categories) }
      attr_reader :categories

      sig { params(categories: OpenAI::Moderation::Categories::OrHash).void }
      attr_writer :categories

      # A list of the categories along with the input type(s) that the score applies to.
      sig { returns(OpenAI::Moderation::CategoryAppliedInputTypes) }
      attr_reader :category_applied_input_types

      sig { params(category_applied_input_types: OpenAI::Moderation::CategoryAppliedInputTypes::OrHash).void }
      attr_writer :category_applied_input_types

      # A list of the categories along with their scores as predicted by model.
      sig { returns(OpenAI::Moderation::CategoryScores) }
      attr_reader :category_scores

      sig { params(category_scores: OpenAI::Moderation::CategoryScores::OrHash).void }
      attr_writer :category_scores

      # Whether any of the below categories are flagged.
      sig { returns(T::Boolean) }
      attr_accessor :flagged

      sig do
        override
          .returns({
            categories: OpenAI::Moderation::Categories,
            category_applied_input_types:
              OpenAI::Moderation::CategoryAppliedInputTypes,
            category_scores: OpenAI::Moderation::CategoryScores,
            flagged: T::Boolean
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            categories: OpenAI::Moderation::Categories::OrHash,
            category_applied_input_types: OpenAI::Moderation::CategoryAppliedInputTypes::OrHash,
            category_scores: OpenAI::Moderation::CategoryScores::OrHash,
            flagged: T::Boolean
          ).returns(T.attached_class)
        end
        def new(
          categories:, # A list of the categories, and whether they are flagged or not.
          category_applied_input_types:, # A list of the categories along with the input type(s) that the score applies to.
          category_scores:, # A list of the categories along with their scores as predicted by model.
          flagged: # Whether any of the below categories are flagged.
); end
      end

      class Categories < OpenAI::Internal::Type::BaseModel
        # Content that expresses, incites, or promotes harassing language towards any
        # target.
        sig { returns(T::Boolean) }
        attr_accessor :harassment

        # Harassment content that also includes violence or serious harm towards any
        # target.
        sig { returns(T::Boolean) }
        attr_accessor :harassment_threatening

        # Content that expresses, incites, or promotes hate based on race, gender,
        # ethnicity, religion, nationality, sexual orientation, disability status, or
        # caste. Hateful content aimed at non-protected groups (e.g., chess players) is
        # harassment.
        sig { returns(T::Boolean) }
        attr_accessor :hate

        # Hateful content that also includes violence or serious harm towards the targeted
        # group based on race, gender, ethnicity, religion, nationality, sexual
        # orientation, disability status, or caste.
        sig { returns(T::Boolean) }
        attr_accessor :hate_threatening

        # Content that includes instructions or advice that facilitate the planning or
        # execution of wrongdoing, or that gives advice or instruction on how to commit
        # illicit acts. For example, "how to shoplift" would fit this category.
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :illicit

        # Content that includes instructions or advice that facilitate the planning or
        # execution of wrongdoing that also includes violence, or that gives advice or
        # instruction on the procurement of any weapon.
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :illicit_violent

        # Content that promotes, encourages, or depicts acts of self-harm, such as
        # suicide, cutting, and eating disorders.
        sig { returns(T::Boolean) }
        attr_accessor :self_harm

        # Content that encourages performing acts of self-harm, such as suicide, cutting,
        # and eating disorders, or that gives instructions or advice on how to commit such
        # acts.
        sig { returns(T::Boolean) }
        attr_accessor :self_harm_instructions

        # Content where the speaker expresses that they are engaging or intend to engage
        # in acts of self-harm, such as suicide, cutting, and eating disorders.
        sig { returns(T::Boolean) }
        attr_accessor :self_harm_intent

        # Content meant to arouse sexual excitement, such as the description of sexual
        # activity, or that promotes sexual services (excluding sex education and
        # wellness).
        sig { returns(T::Boolean) }
        attr_accessor :sexual

        # Sexual content that includes an individual who is under 18 years old.
        sig { returns(T::Boolean) }
        attr_accessor :sexual_minors

        # Content that depicts death, violence, or physical injury.
        sig { returns(T::Boolean) }
        attr_accessor :violence

        # Content that depicts death, violence, or physical injury in graphic detail.
        sig { returns(T::Boolean) }
        attr_accessor :violence_graphic

        sig do
          override
            .returns({
              harassment: T::Boolean,
              harassment_threatening: T::Boolean,
              hate: T::Boolean,
              hate_threatening: T::Boolean,
              illicit: T.nilable(T::Boolean),
              illicit_violent: T.nilable(T::Boolean),
              self_harm: T::Boolean,
              self_harm_instructions: T::Boolean,
              self_harm_intent: T::Boolean,
              sexual: T::Boolean,
              sexual_minors: T::Boolean,
              violence: T::Boolean,
              violence_graphic: T::Boolean
            })
        end
        def to_hash; end

        class << self
          # A list of the categories, and whether they are flagged or not.
          sig do
            params(
              harassment: T::Boolean,
              harassment_threatening: T::Boolean,
              hate: T::Boolean,
              hate_threatening: T::Boolean,
              illicit: T.nilable(T::Boolean),
              illicit_violent: T.nilable(T::Boolean),
              self_harm: T::Boolean,
              self_harm_instructions: T::Boolean,
              self_harm_intent: T::Boolean,
              sexual: T::Boolean,
              sexual_minors: T::Boolean,
              violence: T::Boolean,
              violence_graphic: T::Boolean
            ).returns(T.attached_class)
          end
          def new(
            harassment:, # Content that expresses, incites, or promotes harassing language towards any
                         # target.
            harassment_threatening:, # Harassment content that also includes violence or serious harm towards any
                                     # target.
            hate:, # Content that expresses, incites, or promotes hate based on race, gender,
                   # ethnicity, religion, nationality, sexual orientation, disability status, or
                   # caste. Hateful content aimed at non-protected groups (e.g., chess players) is
                   # harassment.
            hate_threatening:, # Hateful content that also includes violence or serious harm towards the targeted
                               # group based on race, gender, ethnicity, religion, nationality, sexual
                               # orientation, disability status, or caste.
            illicit:, # Content that includes instructions or advice that facilitate the planning or
                      # execution of wrongdoing, or that gives advice or instruction on how to commit
                      # illicit acts. For example, "how to shoplift" would fit this category.
            illicit_violent:, # Content that includes instructions or advice that facilitate the planning or
                              # execution of wrongdoing that also includes violence, or that gives advice or
                              # instruction on the procurement of any weapon.
            self_harm:, # Content that promotes, encourages, or depicts acts of self-harm, such as
                        # suicide, cutting, and eating disorders.
            self_harm_instructions:, # Content that encourages performing acts of self-harm, such as suicide, cutting,
                                     # and eating disorders, or that gives instructions or advice on how to commit such
                                     # acts.
            self_harm_intent:, # Content where the speaker expresses that they are engaging or intend to engage
                               # in acts of self-harm, such as suicide, cutting, and eating disorders.
            sexual:, # Content meant to arouse sexual excitement, such as the description of sexual
                     # activity, or that promotes sexual services (excluding sex education and
                     # wellness).
            sexual_minors:, # Sexual content that includes an individual who is under 18 years old.
            violence:, # Content that depicts death, violence, or physical injury.
            violence_graphic: # Content that depicts death, violence, or physical injury in graphic detail.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Moderation::Categories, OpenAI::Internal::AnyHash)
          end
      end

      class CategoryAppliedInputTypes < OpenAI::Internal::Type::BaseModel
        # The applied input type(s) for the category 'harassment'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::Harassment::TaggedSymbol
            ])
        end
        attr_accessor :harassment

        # The applied input type(s) for the category 'harassment/threatening'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::HarassmentThreatening::TaggedSymbol
            ])
        end
        attr_accessor :harassment_threatening

        # The applied input type(s) for the category 'hate'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::Hate::TaggedSymbol
            ])
        end
        attr_accessor :hate

        # The applied input type(s) for the category 'hate/threatening'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::HateThreatening::TaggedSymbol
            ])
        end
        attr_accessor :hate_threatening

        # The applied input type(s) for the category 'illicit'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::Illicit::TaggedSymbol
            ])
        end
        attr_accessor :illicit

        # The applied input type(s) for the category 'illicit/violent'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::IllicitViolent::TaggedSymbol
            ])
        end
        attr_accessor :illicit_violent

        # The applied input type(s) for the category 'self-harm'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarm::TaggedSymbol
            ])
        end
        attr_accessor :self_harm

        # The applied input type(s) for the category 'self-harm/instructions'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmInstruction::TaggedSymbol
            ])
        end
        attr_accessor :self_harm_instructions

        # The applied input type(s) for the category 'self-harm/intent'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmIntent::TaggedSymbol
            ])
        end
        attr_accessor :self_harm_intent

        # The applied input type(s) for the category 'sexual'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::Sexual::TaggedSymbol
            ])
        end
        attr_accessor :sexual

        # The applied input type(s) for the category 'sexual/minors'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::SexualMinor::TaggedSymbol
            ])
        end
        attr_accessor :sexual_minors

        # The applied input type(s) for the category 'violence'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::Violence::TaggedSymbol
            ])
        end
        attr_accessor :violence

        # The applied input type(s) for the category 'violence/graphic'.
        sig do
          returns(T::Array[
              OpenAI::Moderation::CategoryAppliedInputTypes::ViolenceGraphic::TaggedSymbol
            ])
        end
        attr_accessor :violence_graphic

        sig do
          override
            .returns({
              harassment:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::Harassment::TaggedSymbol
                ],
              harassment_threatening:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::HarassmentThreatening::TaggedSymbol
                ],
              hate:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::Hate::TaggedSymbol
                ],
              hate_threatening:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::HateThreatening::TaggedSymbol
                ],
              illicit:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::Illicit::TaggedSymbol
                ],
              illicit_violent:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::IllicitViolent::TaggedSymbol
                ],
              self_harm:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarm::TaggedSymbol
                ],
              self_harm_instructions:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmInstruction::TaggedSymbol
                ],
              self_harm_intent:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmIntent::TaggedSymbol
                ],
              sexual:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::Sexual::TaggedSymbol
                ],
              sexual_minors:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::SexualMinor::TaggedSymbol
                ],
              violence:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::Violence::TaggedSymbol
                ],
              violence_graphic:
                T::Array[
                  OpenAI::Moderation::CategoryAppliedInputTypes::ViolenceGraphic::TaggedSymbol
                ]
            })
        end
        def to_hash; end

        class << self
          # A list of the categories along with the input type(s) that the score applies to.
          sig do
            params(
              harassment: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Harassment::OrSymbol
              ],
              harassment_threatening: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::HarassmentThreatening::OrSymbol
              ],
              hate: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Hate::OrSymbol
              ],
              hate_threatening: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::HateThreatening::OrSymbol
              ],
              illicit: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Illicit::OrSymbol
              ],
              illicit_violent: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::IllicitViolent::OrSymbol
              ],
              self_harm: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarm::OrSymbol
              ],
              self_harm_instructions: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmInstruction::OrSymbol
              ],
              self_harm_intent: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmIntent::OrSymbol
              ],
              sexual: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Sexual::OrSymbol
              ],
              sexual_minors: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::SexualMinor::OrSymbol
              ],
              violence: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Violence::OrSymbol
              ],
              violence_graphic: T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::ViolenceGraphic::OrSymbol
              ]
            ).returns(T.attached_class)
          end
          def new(
            harassment:, # The applied input type(s) for the category 'harassment'.
            harassment_threatening:, # The applied input type(s) for the category 'harassment/threatening'.
            hate:, # The applied input type(s) for the category 'hate'.
            hate_threatening:, # The applied input type(s) for the category 'hate/threatening'.
            illicit:, # The applied input type(s) for the category 'illicit'.
            illicit_violent:, # The applied input type(s) for the category 'illicit/violent'.
            self_harm:, # The applied input type(s) for the category 'self-harm'.
            self_harm_instructions:, # The applied input type(s) for the category 'self-harm/instructions'.
            self_harm_intent:, # The applied input type(s) for the category 'self-harm/intent'.
            sexual:, # The applied input type(s) for the category 'sexual'.
            sexual_minors:, # The applied input type(s) for the category 'sexual/minors'.
            violence:, # The applied input type(s) for the category 'violence'.
            violence_graphic: # The applied input type(s) for the category 'violence/graphic'.
); end
        end

        module Harassment
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Harassment::TaggedSymbol
              ])
            end
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::Harassment::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::Harassment
              )
            end
        end

        module HarassmentThreatening
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::HarassmentThreatening::TaggedSymbol
              ])
            end
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::HarassmentThreatening::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::HarassmentThreatening
              )
            end
        end

        module Hate
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Hate::TaggedSymbol
              ])
            end
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::Hate::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Moderation::CategoryAppliedInputTypes::Hate)
            end
        end

        module HateThreatening
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::HateThreatening::TaggedSymbol
              ])
            end
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::HateThreatening::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::HateThreatening
              )
            end
        end

        module Illicit
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Illicit::TaggedSymbol
              ])
            end
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::Illicit::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::Illicit
              )
            end
        end

        module IllicitViolent
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::IllicitViolent::TaggedSymbol
              ])
            end
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::IllicitViolent::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::IllicitViolent
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Moderation::CategoryAppliedInputTypes,
              OpenAI::Internal::AnyHash
            )
          end

        module SelfHarm
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarm::TaggedSymbol
              ])
            end
            def values; end
          end

          IMAGE = T.let(
              :image,
              OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarm::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarm::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarm
              )
            end
        end

        module SelfHarmInstruction
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmInstruction::TaggedSymbol
              ])
            end
            def values; end
          end

          IMAGE = T.let(
              :image,
              OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmInstruction::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmInstruction::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmInstruction
              )
            end
        end

        module SelfHarmIntent
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmIntent::TaggedSymbol
              ])
            end
            def values; end
          end

          IMAGE = T.let(
              :image,
              OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmIntent::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmIntent::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::SelfHarmIntent
              )
            end
        end

        module Sexual
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Sexual::TaggedSymbol
              ])
            end
            def values; end
          end

          IMAGE = T.let(
              :image,
              OpenAI::Moderation::CategoryAppliedInputTypes::Sexual::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::Sexual::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::Sexual
              )
            end
        end

        module SexualMinor
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::SexualMinor::TaggedSymbol
              ])
            end
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::SexualMinor::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::SexualMinor
              )
            end
        end

        module Violence
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::Violence::TaggedSymbol
              ])
            end
            def values; end
          end

          IMAGE = T.let(
              :image,
              OpenAI::Moderation::CategoryAppliedInputTypes::Violence::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::Violence::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::Violence
              )
            end
        end

        module ViolenceGraphic
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Moderation::CategoryAppliedInputTypes::ViolenceGraphic::TaggedSymbol
              ])
            end
            def values; end
          end

          IMAGE = T.let(
              :image,
              OpenAI::Moderation::CategoryAppliedInputTypes::ViolenceGraphic::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Moderation::CategoryAppliedInputTypes::ViolenceGraphic::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Moderation::CategoryAppliedInputTypes::ViolenceGraphic
              )
            end
        end
      end

      class CategoryScores < OpenAI::Internal::Type::BaseModel
        # The score for the category 'harassment'.
        sig { returns(Float) }
        attr_accessor :harassment

        # The score for the category 'harassment/threatening'.
        sig { returns(Float) }
        attr_accessor :harassment_threatening

        # The score for the category 'hate'.
        sig { returns(Float) }
        attr_accessor :hate

        # The score for the category 'hate/threatening'.
        sig { returns(Float) }
        attr_accessor :hate_threatening

        # The score for the category 'illicit'.
        sig { returns(Float) }
        attr_accessor :illicit

        # The score for the category 'illicit/violent'.
        sig { returns(Float) }
        attr_accessor :illicit_violent

        # The score for the category 'self-harm'.
        sig { returns(Float) }
        attr_accessor :self_harm

        # The score for the category 'self-harm/instructions'.
        sig { returns(Float) }
        attr_accessor :self_harm_instructions

        # The score for the category 'self-harm/intent'.
        sig { returns(Float) }
        attr_accessor :self_harm_intent

        # The score for the category 'sexual'.
        sig { returns(Float) }
        attr_accessor :sexual

        # The score for the category 'sexual/minors'.
        sig { returns(Float) }
        attr_accessor :sexual_minors

        # The score for the category 'violence'.
        sig { returns(Float) }
        attr_accessor :violence

        # The score for the category 'violence/graphic'.
        sig { returns(Float) }
        attr_accessor :violence_graphic

        sig do
          override
            .returns({
              harassment: Float,
              harassment_threatening: Float,
              hate: Float,
              hate_threatening: Float,
              illicit: Float,
              illicit_violent: Float,
              self_harm: Float,
              self_harm_instructions: Float,
              self_harm_intent: Float,
              sexual: Float,
              sexual_minors: Float,
              violence: Float,
              violence_graphic: Float
            })
        end
        def to_hash; end

        class << self
          # A list of the categories along with their scores as predicted by model.
          sig do
            params(
              harassment: Float,
              harassment_threatening: Float,
              hate: Float,
              hate_threatening: Float,
              illicit: Float,
              illicit_violent: Float,
              self_harm: Float,
              self_harm_instructions: Float,
              self_harm_intent: Float,
              sexual: Float,
              sexual_minors: Float,
              violence: Float,
              violence_graphic: Float
            ).returns(T.attached_class)
          end
          def new(
            harassment:, # The score for the category 'harassment'.
            harassment_threatening:, # The score for the category 'harassment/threatening'.
            hate:, # The score for the category 'hate'.
            hate_threatening:, # The score for the category 'hate/threatening'.
            illicit:, # The score for the category 'illicit'.
            illicit_violent:, # The score for the category 'illicit/violent'.
            self_harm:, # The score for the category 'self-harm'.
            self_harm_instructions:, # The score for the category 'self-harm/instructions'.
            self_harm_intent:, # The score for the category 'self-harm/intent'.
            sexual:, # The score for the category 'sexual'.
            sexual_minors:, # The score for the category 'sexual/minors'.
            violence:, # The score for the category 'violence'.
            violence_graphic: # The score for the category 'violence/graphic'.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Moderation::CategoryScores, OpenAI::Internal::AnyHash)
          end
      end

      OrHash = T.type_alias { T.any(OpenAI::Moderation, OpenAI::Internal::AnyHash) }
    end

    class ModerationCreateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # Input (or inputs) to classify. Can be a single string, an array of strings, or
      # an array of multi-modal input objects similar to other models.
      sig { returns(OpenAI::ModerationCreateParams::Input::Variants) }
      attr_accessor :input

      # The content moderation model you would like to use. Learn more in
      # [the moderation guide](https://platform.openai.com/docs/guides/moderation), and
      # learn about available models
      # [here](https://platform.openai.com/docs/models#moderation).
      sig { returns(T.nilable(T.any(String, OpenAI::ModerationModel::OrSymbol))) }
      attr_reader :model

      sig { params(model: T.any(String, OpenAI::ModerationModel::OrSymbol)).void }
      attr_writer :model

      sig do
        override
          .returns({
            input: OpenAI::ModerationCreateParams::Input::Variants,
            model: T.any(String, OpenAI::ModerationModel::OrSymbol),
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            input: OpenAI::ModerationCreateParams::Input::Variants,
            model: T.any(String, OpenAI::ModerationModel::OrSymbol),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          input:, # Input (or inputs) to classify. Can be a single string, an array of strings, or
                  # an array of multi-modal input objects similar to other models.
          model: nil, # The content moderation model you would like to use. Learn more in
                      # [the moderation guide](https://platform.openai.com/docs/guides/moderation), and
                      # learn about available models
                      # [here](https://platform.openai.com/docs/models#moderation).
          request_options: {}
); end
      end

      # Input (or inputs) to classify. Can be a single string, an array of strings, or
      # an array of multi-modal input objects similar to other models.
      module Input
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::ModerationCreateParams::Input::Variants]) }
          def variants; end
        end

        ModerationMultiModalInputArray = T.let(
            OpenAI::Internal::Type::ArrayOf[
              union: OpenAI::ModerationMultiModalInput
            ],
            OpenAI::Internal::Type::Converter
          )

        StringArray = T.let(
            OpenAI::Internal::Type::ArrayOf[String],
            OpenAI::Internal::Type::Converter
          )

        Variants = T.type_alias do
            T.any(
              String,
              T::Array[String],
              T::Array[OpenAI::ModerationMultiModalInput::Variants]
            )
          end
      end

      # The content moderation model you would like to use. Learn more in
      # [the moderation guide](https://platform.openai.com/docs/guides/moderation), and
      # learn about available models
      # [here](https://platform.openai.com/docs/models#moderation).
      module Model
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::ModerationCreateParams::Model::Variants]) }
          def variants; end
        end

        Variants = T.type_alias { T.any(String, OpenAI::ModerationModel::TaggedSymbol) }
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ModerationCreateParams, OpenAI::Internal::AnyHash)
        end
    end

    class ModerationCreateResponse < OpenAI::Internal::Type::BaseModel
      # The unique identifier for the moderation request.
      sig { returns(String) }
      attr_accessor :id

      # The model used to generate the moderation results.
      sig { returns(String) }
      attr_accessor :model

      # A list of moderation objects.
      sig { returns(T::Array[OpenAI::Moderation]) }
      attr_accessor :results

      sig { override.returns({ id: String, model: String, results: T::Array[OpenAI::Moderation] }) }
      def to_hash; end

      class << self
        # Represents if a given text input is potentially harmful.
        sig do
          params(
            id: String,
            model: String,
            results: T::Array[OpenAI::Moderation::OrHash]
          ).returns(T.attached_class)
        end
        def new(
          id:, # The unique identifier for the moderation request.
          model:, # The model used to generate the moderation results.
          results: # A list of moderation objects.
); end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::Models::ModerationCreateResponse,
            OpenAI::Internal::AnyHash
          )
        end
    end

    class ModerationImageURLInput < OpenAI::Internal::Type::BaseModel
      # Contains either an image URL or a data URL for a base64 encoded image.
      sig { returns(OpenAI::ModerationImageURLInput::ImageURL) }
      attr_reader :image_url

      sig { params(image_url: OpenAI::ModerationImageURLInput::ImageURL::OrHash).void }
      attr_writer :image_url

      # Always `image_url`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig { override.returns({ image_url: OpenAI::ModerationImageURLInput::ImageURL, type: Symbol }) }
      def to_hash; end

      class << self
        # An object describing an image to classify.
        sig do
          params(
            image_url: OpenAI::ModerationImageURLInput::ImageURL::OrHash,
            type: Symbol
          ).returns(T.attached_class)
        end
        def new(
          image_url:, # Contains either an image URL or a data URL for a base64 encoded image.
          type: :image_url # Always `image_url`.
); end
      end

      class ImageURL < OpenAI::Internal::Type::BaseModel
        # Either a URL of the image or the base64 encoded image data.
        sig { returns(String) }
        attr_accessor :url

        sig { override.returns({ url: String }) }
        def to_hash; end

        class << self
          # Contains either an image URL or a data URL for a base64 encoded image.
          sig { params(url: String).returns(T.attached_class) }
          def new(
            url: # Either a URL of the image or the base64 encoded image data.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::ModerationImageURLInput::ImageURL,
              OpenAI::Internal::AnyHash
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ModerationImageURLInput, OpenAI::Internal::AnyHash)
        end
    end

    module ModerationModel
      extend OpenAI::Internal::Type::Enum

      class << self
        sig { override.returns(T::Array[OpenAI::ModerationModel::TaggedSymbol]) }
        def values; end
      end

      OMNI_MODERATION_2024_09_26 = T.let(
          :"omni-moderation-2024-09-26",
          OpenAI::ModerationModel::TaggedSymbol
        )

      OMNI_MODERATION_LATEST = T.let(:"omni-moderation-latest", OpenAI::ModerationModel::TaggedSymbol)

      OrSymbol = T.type_alias { T.any(Symbol, String) }

      TEXT_MODERATION_LATEST = T.let(:"text-moderation-latest", OpenAI::ModerationModel::TaggedSymbol)

      TEXT_MODERATION_STABLE = T.let(:"text-moderation-stable", OpenAI::ModerationModel::TaggedSymbol)

      TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ModerationModel) }
    end

    # An object describing an image to classify.
    module ModerationMultiModalInput
      extend OpenAI::Internal::Type::Union

      class << self
        sig { override.returns(T::Array[OpenAI::ModerationMultiModalInput::Variants]) }
        def variants; end
      end

      Variants = T.type_alias do
          T.any(OpenAI::ModerationImageURLInput, OpenAI::ModerationTextInput)
        end
    end

    class ModerationTextInput < OpenAI::Internal::Type::BaseModel
      # A string of text to classify.
      sig { returns(String) }
      attr_accessor :text

      # Always `text`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig { override.returns({ text: String, type: Symbol }) }
      def to_hash; end

      class << self
        # An object describing text to classify.
        sig { params(text: String, type: Symbol).returns(T.attached_class) }
        def new(
          text:, # A string of text to classify.
          type: :text # Always `text`.
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ModerationTextInput, OpenAI::Internal::AnyHash)
        end
    end

    MultiGrader = Graders::MultiGrader

    class OtherFileChunkingStrategyObject < OpenAI::Internal::Type::BaseModel
      # Always `other`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig { override.returns({ type: Symbol }) }
      def to_hash; end

      class << self
        # This is returned when the chunking strategy is unknown. Typically, this is
        # because the file was indexed before the `chunking_strategy` concept was
        # introduced in the API.
        sig { params(type: Symbol).returns(T.attached_class) }
        def new(
          type: :other # Always `other`.
); end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::OtherFileChunkingStrategyObject,
            OpenAI::Internal::AnyHash
          )
        end
    end

    PythonGrader = Graders::PythonGrader

    class Reasoning < OpenAI::Internal::Type::BaseModel
      # **o-series models only**
      #
      # Constrains effort on reasoning for
      # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
      # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
      # result in faster responses and fewer tokens used on reasoning in a response.
      sig { returns(T.nilable(OpenAI::ReasoningEffort::OrSymbol)) }
      attr_accessor :effort

      # **Deprecated:** use `summary` instead.
      #
      # A summary of the reasoning performed by the model. This can be useful for
      # debugging and understanding the model's reasoning process. One of `auto`,
      # `concise`, or `detailed`.
      sig { returns(T.nilable(OpenAI::Reasoning::GenerateSummary::OrSymbol)) }
      attr_accessor :generate_summary

      # A summary of the reasoning performed by the model. This can be useful for
      # debugging and understanding the model's reasoning process. One of `auto`,
      # `concise`, or `detailed`.
      sig { returns(T.nilable(OpenAI::Reasoning::Summary::OrSymbol)) }
      attr_accessor :summary

      sig do
        override
          .returns({
            effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
            generate_summary:
              T.nilable(OpenAI::Reasoning::GenerateSummary::OrSymbol),
            summary: T.nilable(OpenAI::Reasoning::Summary::OrSymbol)
          })
      end
      def to_hash; end

      class << self
        # **o-series models only**
        #
        # Configuration options for
        # [reasoning models](https://platform.openai.com/docs/guides/reasoning).
        sig do
          params(
            effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
            generate_summary: T.nilable(OpenAI::Reasoning::GenerateSummary::OrSymbol),
            summary: T.nilable(OpenAI::Reasoning::Summary::OrSymbol)
          ).returns(T.attached_class)
        end
        def new(
          effort: nil, # **o-series models only**
                       # Constrains effort on reasoning for
                       # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                       # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                       # result in faster responses and fewer tokens used on reasoning in a response.
          generate_summary: nil, # **Deprecated:** use `summary` instead.
                                 # A summary of the reasoning performed by the model. This can be useful for
                                 # debugging and understanding the model's reasoning process. One of `auto`,
                                 # `concise`, or `detailed`.
          summary: nil # A summary of the reasoning performed by the model. This can be useful for
                       # debugging and understanding the model's reasoning process. One of `auto`,
                       # `concise`, or `detailed`.
); end
      end

      # **Deprecated:** use `summary` instead.
      #
      # A summary of the reasoning performed by the model. This can be useful for
      # debugging and understanding the model's reasoning process. One of `auto`,
      # `concise`, or `detailed`.
      module GenerateSummary
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Reasoning::GenerateSummary::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::Reasoning::GenerateSummary::TaggedSymbol)

        CONCISE = T.let(:concise, OpenAI::Reasoning::GenerateSummary::TaggedSymbol)

        DETAILED = T.let(:detailed, OpenAI::Reasoning::GenerateSummary::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Reasoning::GenerateSummary) }
      end

      OrHash = T.type_alias { T.any(OpenAI::Reasoning, OpenAI::Internal::AnyHash) }

      # A summary of the reasoning performed by the model. This can be useful for
      # debugging and understanding the model's reasoning process. One of `auto`,
      # `concise`, or `detailed`.
      module Summary
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Reasoning::Summary::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::Reasoning::Summary::TaggedSymbol)
        CONCISE = T.let(:concise, OpenAI::Reasoning::Summary::TaggedSymbol)
        DETAILED = T.let(:detailed, OpenAI::Reasoning::Summary::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Reasoning::Summary) }
      end
    end

    # **o-series models only**
    #
    # Constrains effort on reasoning for
    # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
    # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
    # result in faster responses and fewer tokens used on reasoning in a response.
    module ReasoningEffort
      extend OpenAI::Internal::Type::Enum

      class << self
        sig { override.returns(T::Array[OpenAI::ReasoningEffort::TaggedSymbol]) }
        def values; end
      end

      HIGH = T.let(:high, OpenAI::ReasoningEffort::TaggedSymbol)
      LOW = T.let(:low, OpenAI::ReasoningEffort::TaggedSymbol)
      MEDIUM = T.let(:medium, OpenAI::ReasoningEffort::TaggedSymbol)
      OrSymbol = T.type_alias { T.any(Symbol, String) }
      TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::ReasoningEffort) }
    end

    class ResponseFormatJSONObject < OpenAI::Internal::Type::BaseModel
      # The type of response format being defined. Always `json_object`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig { override.returns({ type: Symbol }) }
      def to_hash; end

      class << self
        # JSON object response format. An older method of generating JSON responses. Using
        # `json_schema` is recommended for models that support it. Note that the model
        # will not generate JSON without a system or user message instructing it to do so.
        sig { params(type: Symbol).returns(T.attached_class) }
        def new(
          type: :json_object # The type of response format being defined. Always `json_object`.
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ResponseFormatJSONObject, OpenAI::Internal::AnyHash)
        end
    end

    class ResponseFormatJSONSchema < OpenAI::Internal::Type::BaseModel
      # Structured Outputs configuration options, including a JSON Schema.
      sig do
        returns(T.any(
            OpenAI::ResponseFormatJSONSchema::JSONSchema::OrHash,
            OpenAI::StructuredOutput::JsonSchemaConverter
          ))
      end
      attr_reader :json_schema

      sig do
        params(
          json_schema: T.any(
              OpenAI::ResponseFormatJSONSchema::JSONSchema::OrHash,
              OpenAI::StructuredOutput::JsonSchemaConverter
            )
        ).void
      end
      attr_writer :json_schema

      # The type of response format being defined. Always `json_schema`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig do
        override
          .returns({
            json_schema: OpenAI::ResponseFormatJSONSchema::JSONSchema,
            type: Symbol
          })
      end
      def to_hash; end

      class << self
        # JSON Schema response format. Used to generate structured JSON responses. Learn
        # more about
        # [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs).
        sig do
          params(
            json_schema: T.any(
              OpenAI::ResponseFormatJSONSchema::JSONSchema::OrHash,
              OpenAI::StructuredOutput::JsonSchemaConverter
            ),
            type: Symbol
          ).returns(T.attached_class)
        end
        def new(
          json_schema:, # Structured Outputs configuration options, including a JSON Schema.
          type: :json_schema # The type of response format being defined. Always `json_schema`.
); end
      end

      class JSONSchema < OpenAI::Internal::Type::BaseModel
        # A description of what the response format is for, used by the model to determine
        # how to respond in the format.
        sig { returns(T.nilable(String)) }
        attr_reader :description

        sig { params(description: String).void }
        attr_writer :description

        # The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores
        # and dashes, with a maximum length of 64.
        sig { returns(String) }
        attr_accessor :name

        # The schema for the response format, described as a JSON Schema object. Learn how
        # to build JSON schemas [here](https://json-schema.org/).
        sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
        attr_reader :schema

        sig { params(schema: T::Hash[Symbol, T.anything]).void }
        attr_writer :schema

        # Whether to enable strict schema adherence when generating the output. If set to
        # true, the model will always follow the exact schema defined in the `schema`
        # field. Only a subset of JSON Schema is supported when `strict` is `true`. To
        # learn more, read the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :strict

        sig do
          override
            .returns({
              name: String,
              description: String,
              schema: T::Hash[Symbol, T.anything],
              strict: T.nilable(T::Boolean)
            })
        end
        def to_hash; end

        class << self
          # Structured Outputs configuration options, including a JSON Schema.
          sig do
            params(
              name: String,
              description: String,
              schema: T::Hash[Symbol, T.anything],
              strict: T.nilable(T::Boolean)
            ).returns(T.attached_class)
          end
          def new(
            name:, # The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores
                   # and dashes, with a maximum length of 64.
            description: nil, # A description of what the response format is for, used by the model to determine
                              # how to respond in the format.
            schema: nil, # The schema for the response format, described as a JSON Schema object. Learn how
                         # to build JSON schemas [here](https://json-schema.org/).
            strict: nil # Whether to enable strict schema adherence when generating the output. If set to
                        # true, the model will always follow the exact schema defined in the `schema`
                        # field. Only a subset of JSON Schema is supported when `strict` is `true`. To
                        # learn more, read the
                        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::ResponseFormatJSONSchema::JSONSchema,
              OpenAI::Internal::AnyHash
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ResponseFormatJSONSchema, OpenAI::Internal::AnyHash)
        end
    end

    class ResponseFormatText < OpenAI::Internal::Type::BaseModel
      # The type of response format being defined. Always `text`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig { override.returns({ type: Symbol }) }
      def to_hash; end

      class << self
        # Default response format. Used to generate text responses.
        sig { params(type: Symbol).returns(T.attached_class) }
        def new(
          type: :text # The type of response format being defined. Always `text`.
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::ResponseFormatText, OpenAI::Internal::AnyHash)
        end
    end

    ResponseItemList = Responses::ResponseItemList

    module Responses
      class ComputerTool < OpenAI::Internal::Type::BaseModel
        # The height of the computer display.
        sig { returns(Integer) }
        attr_accessor :display_height

        # The width of the computer display.
        sig { returns(Integer) }
        attr_accessor :display_width

        # The type of computer environment to control.
        sig { returns(OpenAI::Responses::ComputerTool::Environment::OrSymbol) }
        attr_accessor :environment

        # The type of the computer use tool. Always `computer_use_preview`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              display_height: Integer,
              display_width: Integer,
              environment:
                OpenAI::Responses::ComputerTool::Environment::OrSymbol,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # A tool that controls a virtual computer. Learn more about the
          # [computer tool](https://platform.openai.com/docs/guides/tools-computer-use).
          sig do
            params(
              display_height: Integer,
              display_width: Integer,
              environment: OpenAI::Responses::ComputerTool::Environment::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            display_height:, # The height of the computer display.
            display_width:, # The width of the computer display.
            environment:, # The type of computer environment to control.
            type: :computer_use_preview # The type of the computer use tool. Always `computer_use_preview`.
); end
        end

        # The type of computer environment to control.
        module Environment
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ComputerTool::Environment::TaggedSymbol
              ])
            end
            def values; end
          end

          BROWSER = T.let(
              :browser,
              OpenAI::Responses::ComputerTool::Environment::TaggedSymbol
            )

          LINUX = T.let(
              :linux,
              OpenAI::Responses::ComputerTool::Environment::TaggedSymbol
            )

          MAC = T.let(
              :mac,
              OpenAI::Responses::ComputerTool::Environment::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ComputerTool::Environment)
            end

          UBUNTU = T.let(
              :ubuntu,
              OpenAI::Responses::ComputerTool::Environment::TaggedSymbol
            )

          WINDOWS = T.let(
              :windows,
              OpenAI::Responses::ComputerTool::Environment::TaggedSymbol
            )
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::ComputerTool, OpenAI::Internal::AnyHash)
          end
      end

      class EasyInputMessage < OpenAI::Internal::Type::BaseModel
        # Text, image, or audio input to the model, used to generate a response. Can also
        # contain previous assistant responses.
        sig { returns(OpenAI::Responses::EasyInputMessage::Content::Variants) }
        attr_accessor :content

        # The role of the message input. One of `user`, `assistant`, `system`, or
        # `developer`.
        sig { returns(OpenAI::Responses::EasyInputMessage::Role::OrSymbol) }
        attr_accessor :role

        # The type of the message input. Always `message`.
        sig { returns(T.nilable(OpenAI::Responses::EasyInputMessage::Type::OrSymbol)) }
        attr_reader :type

        sig { params(type: OpenAI::Responses::EasyInputMessage::Type::OrSymbol).void }
        attr_writer :type

        sig do
          override
            .returns({
              content: OpenAI::Responses::EasyInputMessage::Content::Variants,
              role: OpenAI::Responses::EasyInputMessage::Role::OrSymbol,
              type: OpenAI::Responses::EasyInputMessage::Type::OrSymbol
            })
        end
        def to_hash; end

        class << self
          # A message input to the model with a role indicating instruction following
          # hierarchy. Instructions given with the `developer` or `system` role take
          # precedence over instructions given with the `user` role. Messages with the
          # `assistant` role are presumed to have been generated by the model in previous
          # interactions.
          sig do
            params(
              content: OpenAI::Responses::EasyInputMessage::Content::Variants,
              role: OpenAI::Responses::EasyInputMessage::Role::OrSymbol,
              type: OpenAI::Responses::EasyInputMessage::Type::OrSymbol
            ).returns(T.attached_class)
          end
          def new(
            content:, # Text, image, or audio input to the model, used to generate a response. Can also
                      # contain previous assistant responses.
            role:, # The role of the message input. One of `user`, `assistant`, `system`, or
                   # `developer`.
            type: nil # The type of the message input. Always `message`.
); end
        end

        # Text, image, or audio input to the model, used to generate a response. Can also
        # contain previous assistant responses.
        module Content
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::EasyInputMessage::Content::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                String,
                T::Array[OpenAI::Responses::ResponseInputContent::Variants]
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::EasyInputMessage,
              OpenAI::Internal::AnyHash
            )
          end

        # The role of the message input. One of `user`, `assistant`, `system`, or
        # `developer`.
        module Role
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::EasyInputMessage::Role::TaggedSymbol]) }
            def values; end
          end

          ASSISTANT = T.let(
              :assistant,
              OpenAI::Responses::EasyInputMessage::Role::TaggedSymbol
            )

          DEVELOPER = T.let(
              :developer,
              OpenAI::Responses::EasyInputMessage::Role::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          SYSTEM = T.let(
              :system,
              OpenAI::Responses::EasyInputMessage::Role::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::EasyInputMessage::Role)
            end

          USER = T.let(
              :user,
              OpenAI::Responses::EasyInputMessage::Role::TaggedSymbol
            )
        end

        # The type of the message input. Always `message`.
        module Type
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::EasyInputMessage::Type::TaggedSymbol]) }
            def values; end
          end

          MESSAGE = T.let(
              :message,
              OpenAI::Responses::EasyInputMessage::Type::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::EasyInputMessage::Type)
            end
        end
      end

      class FileSearchTool < OpenAI::Internal::Type::BaseModel
        # A filter to apply.
        sig { returns(T.nilable(T.any(OpenAI::ComparisonFilter, OpenAI::CompoundFilter))) }
        attr_accessor :filters

        # The maximum number of results to return. This number should be between 1 and 50
        # inclusive.
        sig { returns(T.nilable(Integer)) }
        attr_reader :max_num_results

        sig { params(max_num_results: Integer).void }
        attr_writer :max_num_results

        # Ranking options for search.
        sig { returns(T.nilable(OpenAI::Responses::FileSearchTool::RankingOptions)) }
        attr_reader :ranking_options

        sig { params(ranking_options: OpenAI::Responses::FileSearchTool::RankingOptions::OrHash).void }
        attr_writer :ranking_options

        # The type of the file search tool. Always `file_search`.
        sig { returns(Symbol) }
        attr_accessor :type

        # The IDs of the vector stores to search.
        sig { returns(T::Array[String]) }
        attr_accessor :vector_store_ids

        sig do
          override
            .returns({
              type: Symbol,
              vector_store_ids: T::Array[String],
              filters:
                T.nilable(
                  T.any(OpenAI::ComparisonFilter, OpenAI::CompoundFilter)
                ),
              max_num_results: Integer,
              ranking_options: OpenAI::Responses::FileSearchTool::RankingOptions
            })
        end
        def to_hash; end

        class << self
          # A tool that searches for relevant content from uploaded files. Learn more about
          # the
          # [file search tool](https://platform.openai.com/docs/guides/tools-file-search).
          sig do
            params(
              vector_store_ids: T::Array[String],
              filters: T.nilable(
                T.any(
                  OpenAI::ComparisonFilter::OrHash,
                  OpenAI::CompoundFilter::OrHash
                )
              ),
              max_num_results: Integer,
              ranking_options: OpenAI::Responses::FileSearchTool::RankingOptions::OrHash,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            vector_store_ids:, # The IDs of the vector stores to search.
            filters: nil, # A filter to apply.
            max_num_results: nil, # The maximum number of results to return. This number should be between 1 and 50
                                  # inclusive.
            ranking_options: nil, # Ranking options for search.
            type: :file_search # The type of the file search tool. Always `file_search`.
); end
        end

        # A filter to apply.
        module Filters
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::FileSearchTool::Filters::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(OpenAI::ComparisonFilter, OpenAI::CompoundFilter)
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::FileSearchTool, OpenAI::Internal::AnyHash)
          end

        class RankingOptions < OpenAI::Internal::Type::BaseModel
          # The ranker to use for the file search.
          sig do
            returns(T.nilable(
                OpenAI::Responses::FileSearchTool::RankingOptions::Ranker::OrSymbol
              ))
          end
          attr_reader :ranker

          sig { params(ranker: OpenAI::Responses::FileSearchTool::RankingOptions::Ranker::OrSymbol).void }
          attr_writer :ranker

          # The score threshold for the file search, a number between 0 and 1. Numbers
          # closer to 1 will attempt to return only the most relevant results, but may
          # return fewer results.
          sig { returns(T.nilable(Float)) }
          attr_reader :score_threshold

          sig { params(score_threshold: Float).void }
          attr_writer :score_threshold

          sig do
            override
              .returns({
                ranker:
                  OpenAI::Responses::FileSearchTool::RankingOptions::Ranker::OrSymbol,
                score_threshold: Float
              })
          end
          def to_hash; end

          class << self
            # Ranking options for search.
            sig do
              params(
                ranker: OpenAI::Responses::FileSearchTool::RankingOptions::Ranker::OrSymbol,
                score_threshold: Float
              ).returns(T.attached_class)
            end
            def new(
              ranker: nil, # The ranker to use for the file search.
              score_threshold: nil # The score threshold for the file search, a number between 0 and 1. Numbers
                                   # closer to 1 will attempt to return only the most relevant results, but may
                                   # return fewer results.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::FileSearchTool::RankingOptions,
                OpenAI::Internal::AnyHash
              )
            end

          # The ranker to use for the file search.
          module Ranker
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::FileSearchTool::RankingOptions::Ranker::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Responses::FileSearchTool::RankingOptions::Ranker::TaggedSymbol
              )

            DEFAULT_2024_11_15 = T.let(
                :"default-2024-11-15",
                OpenAI::Responses::FileSearchTool::RankingOptions::Ranker::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::FileSearchTool::RankingOptions::Ranker
                )
              end
          end
        end
      end

      class FunctionTool < OpenAI::Internal::Type::BaseModel
        # A description of the function. Used by the model to determine whether or not to
        # call the function.
        sig { returns(T.nilable(String)) }
        attr_accessor :description

        # The name of the function to call.
        sig { returns(String) }
        attr_accessor :name

        # A JSON schema object describing the parameters of the function.
        sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
        attr_accessor :parameters

        # Whether to enforce strict parameter validation. Default `true`.
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :strict

        # The type of the function tool. Always `function`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              name: String,
              parameters: T.nilable(T::Hash[Symbol, T.anything]),
              strict: T.nilable(T::Boolean),
              type: Symbol,
              description: T.nilable(String)
            })
        end
        def to_hash; end

        class << self
          # Defines a function in your own code the model can choose to call. Learn more
          # about
          # [function calling](https://platform.openai.com/docs/guides/function-calling).
          sig do
            params(
              name: String,
              parameters: T.nilable(T::Hash[Symbol, T.anything]),
              strict: T.nilable(T::Boolean),
              description: T.nilable(String),
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            name:, # The name of the function to call.
            parameters:, # A JSON schema object describing the parameters of the function.
            strict:, # Whether to enforce strict parameter validation. Default `true`.
            description: nil, # A description of the function. Used by the model to determine whether or not to
                              # call the function.
            type: :function # The type of the function tool. Always `function`.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::FunctionTool, OpenAI::Internal::AnyHash)
          end
      end

      class InputItemListParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # An item ID to list items after, used in pagination.
        sig { returns(T.nilable(String)) }
        attr_reader :after

        sig { params(after: String).void }
        attr_writer :after

        # An item ID to list items before, used in pagination.
        sig { returns(T.nilable(String)) }
        attr_reader :before

        sig { params(before: String).void }
        attr_writer :before

        # Additional fields to include in the response. See the `include` parameter for
        # Response creation above for more information.
        sig { returns(T.nilable(T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol])) }
        attr_reader :include

        sig { params(include: T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol]).void }
        attr_writer :include

        # A limit on the number of objects to be returned. Limit can range between 1 and
        # 100, and the default is 20.
        sig { returns(T.nilable(Integer)) }
        attr_reader :limit

        sig { params(limit: Integer).void }
        attr_writer :limit

        # The order to return the input items in. Default is `desc`.
        #
        # - `asc`: Return the input items in ascending order.
        # - `desc`: Return the input items in descending order.
        sig { returns(T.nilable(OpenAI::Responses::InputItemListParams::Order::OrSymbol)) }
        attr_reader :order

        sig { params(order: OpenAI::Responses::InputItemListParams::Order::OrSymbol).void }
        attr_writer :order

        sig do
          override
            .returns({
              after: String,
              before: String,
              include:
                T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol],
              limit: Integer,
              order: OpenAI::Responses::InputItemListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              after: String,
              before: String,
              include: T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol],
              limit: Integer,
              order: OpenAI::Responses::InputItemListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            after: nil, # An item ID to list items after, used in pagination.
            before: nil, # An item ID to list items before, used in pagination.
            include: nil, # Additional fields to include in the response. See the `include` parameter for
                          # Response creation above for more information.
            limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                        # 100, and the default is 20.
            order: nil, # The order to return the input items in. Default is `desc`.
                        # - `asc`: Return the input items in ascending order.
                        # - `desc`: Return the input items in descending order.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::InputItemListParams,
              OpenAI::Internal::AnyHash
            )
          end

        # The order to return the input items in. Default is `desc`.
        #
        # - `asc`: Return the input items in ascending order.
        # - `desc`: Return the input items in descending order.
        module Order
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::InputItemListParams::Order::TaggedSymbol
              ])
            end
            def values; end
          end

          ASC = T.let(
              :asc,
              OpenAI::Responses::InputItemListParams::Order::TaggedSymbol
            )

          DESC = T.let(
              :desc,
              OpenAI::Responses::InputItemListParams::Order::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::InputItemListParams::Order)
            end
        end
      end

      class Response < OpenAI::Internal::Type::BaseModel
        # Whether to run the model response in the background.
        # [Learn more](https://platform.openai.com/docs/guides/background).
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :background

        # Unix timestamp (in seconds) of when this Response was created.
        sig { returns(Float) }
        attr_accessor :created_at

        # An error object returned when the model fails to generate a Response.
        sig { returns(T.nilable(OpenAI::Responses::ResponseError)) }
        attr_reader :error

        sig { params(error: T.nilable(OpenAI::Responses::ResponseError::OrHash)).void }
        attr_writer :error

        # Unique identifier for this Response.
        sig { returns(String) }
        attr_accessor :id

        # Details about why the response is incomplete.
        sig { returns(T.nilable(OpenAI::Responses::Response::IncompleteDetails)) }
        attr_reader :incomplete_details

        sig { params(incomplete_details: T.nilable(OpenAI::Responses::Response::IncompleteDetails::OrHash)).void }
        attr_writer :incomplete_details

        # A system (or developer) message inserted into the model's context.
        #
        # When using along with `previous_response_id`, the instructions from a previous
        # response will not be carried over to the next response. This makes it simple to
        # swap out system (or developer) messages in new responses.
        sig { returns(T.nilable(OpenAI::Responses::Response::Instructions::Variants)) }
        attr_accessor :instructions

        # An upper bound for the number of tokens that can be generated for a response,
        # including visible output tokens and
        # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_output_tokens

        # The maximum number of total calls to built-in tools that can be processed in a
        # response. This maximum number applies across all built-in tool calls, not per
        # individual tool. Any further attempts to call a tool by the model will be
        # ignored.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_tool_calls

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
        # wide range of models with different capabilities, performance characteristics,
        # and price points. Refer to the
        # [model guide](https://platform.openai.com/docs/models) to browse and compare
        # available models.
        sig { returns(OpenAI::ResponsesModel::Variants) }
        attr_accessor :model

        # The object type of this resource - always set to `response`.
        sig { returns(Symbol) }
        attr_accessor :object

        # An array of content items generated by the model.
        #
        # - The length and order of items in the `output` array is dependent on the
        #   model's response.
        # - Rather than accessing the first item in the `output` array and assuming it's
        #   an `assistant` message with the content generated by the model, you might
        #   consider using the `output_text` property where supported in SDKs.
        sig { returns(T::Array[OpenAI::Responses::ResponseOutputItem::Variants]) }
        attr_accessor :output

        # Whether to allow the model to run tool calls in parallel.
        sig { returns(T::Boolean) }
        attr_accessor :parallel_tool_calls

        # The unique ID of the previous response to the model. Use this to create
        # multi-turn conversations. Learn more about
        # [conversation state](https://platform.openai.com/docs/guides/conversation-state).
        sig { returns(T.nilable(String)) }
        attr_accessor :previous_response_id

        # Reference to a prompt template and its variables.
        # [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).
        sig { returns(T.nilable(OpenAI::Responses::ResponsePrompt)) }
        attr_reader :prompt

        sig { params(prompt: T.nilable(OpenAI::Responses::ResponsePrompt::OrHash)).void }
        attr_writer :prompt

        # Used by OpenAI to cache responses for similar requests to optimize your cache
        # hit rates. Replaces the `user` field.
        # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
        sig { returns(T.nilable(String)) }
        attr_reader :prompt_cache_key

        sig { params(prompt_cache_key: String).void }
        attr_writer :prompt_cache_key

        # **o-series models only**
        #
        # Configuration options for
        # [reasoning models](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(OpenAI::Reasoning)) }
        attr_reader :reasoning

        sig { params(reasoning: T.nilable(OpenAI::Reasoning::OrHash)).void }
        attr_writer :reasoning

        # A stable identifier used to help detect users of your application that may be
        # violating OpenAI's usage policies. The IDs should be a string that uniquely
        # identifies each user. We recommend hashing their username or email address, in
        # order to avoid sending us any identifying information.
        # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        sig { returns(T.nilable(String)) }
        attr_reader :safety_identifier

        sig { params(safety_identifier: String).void }
        attr_writer :safety_identifier

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        sig { returns(T.nilable(OpenAI::Responses::Response::ServiceTier::TaggedSymbol)) }
        attr_accessor :service_tier

        # The status of the response generation. One of `completed`, `failed`,
        # `in_progress`, `cancelled`, `queued`, or `incomplete`.
        sig { returns(T.nilable(OpenAI::Responses::ResponseStatus::TaggedSymbol)) }
        attr_reader :status

        sig { params(status: OpenAI::Responses::ResponseStatus::OrSymbol).void }
        attr_writer :status

        # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        # make the output more random, while lower values like 0.2 will make it more
        # focused and deterministic. We generally recommend altering this or `top_p` but
        # not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :temperature

        # Configuration options for a text response from the model. Can be plain text or
        # structured JSON data. Learn more:
        #
        # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
        # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
        sig { returns(T.nilable(OpenAI::Responses::ResponseTextConfig)) }
        attr_reader :text

        sig { params(text: OpenAI::Responses::ResponseTextConfig::OrHash).void }
        attr_writer :text

        # How the model should select which tool (or tools) to use when generating a
        # response. See the `tools` parameter to see how to specify which tools the model
        # can call.
        sig { returns(OpenAI::Responses::Response::ToolChoice::Variants) }
        attr_accessor :tool_choice

        # An array of tools the model may call while generating a response. You can
        # specify which tool to use by setting the `tool_choice` parameter.
        #
        # The two categories of tools you can provide the model are:
        #
        # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
        #   capabilities, like
        #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
        #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
        #   Learn more about
        #   [built-in tools](https://platform.openai.com/docs/guides/tools).
        # - **Function calls (custom tools)**: Functions that are defined by you, enabling
        #   the model to call your own code. Learn more about
        #   [function calling](https://platform.openai.com/docs/guides/function-calling).
        sig { returns(T::Array[OpenAI::Responses::Tool::Variants]) }
        attr_accessor :tools

        # An integer between 0 and 20 specifying the number of most likely tokens to
        # return at each token position, each with an associated log probability.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :top_logprobs

        # An alternative to sampling with temperature, called nucleus sampling, where the
        # model considers the results of the tokens with top_p probability mass. So 0.1
        # means only the tokens comprising the top 10% probability mass are considered.
        #
        # We generally recommend altering this or `temperature` but not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :top_p

        # The truncation strategy to use for the model response.
        #
        # - `auto`: If the context of this response and previous ones exceeds the model's
        #   context window size, the model will truncate the response to fit the context
        #   window by dropping input items in the middle of the conversation.
        # - `disabled` (default): If a model response will exceed the context window size
        #   for a model, the request will fail with a 400 error.
        sig { returns(T.nilable(OpenAI::Responses::Response::Truncation::TaggedSymbol)) }
        attr_accessor :truncation

        # Represents token usage details including input tokens, output tokens, a
        # breakdown of output tokens, and the total tokens used.
        sig { returns(T.nilable(OpenAI::Responses::ResponseUsage)) }
        attr_reader :usage

        sig { params(usage: OpenAI::Responses::ResponseUsage::OrHash).void }
        attr_writer :usage

        # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
        # `prompt_cache_key` instead to maintain caching optimizations. A stable
        # identifier for your end-users. Used to boost cache hit rates by better bucketing
        # similar requests and to help OpenAI detect and prevent abuse.
        # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        sig { returns(T.nilable(String)) }
        attr_reader :user

        sig { params(user: String).void }
        attr_writer :user

        sig do
          override
            .returns({
              id: String,
              created_at: Float,
              error: T.nilable(OpenAI::Responses::ResponseError),
              incomplete_details:
                T.nilable(OpenAI::Responses::Response::IncompleteDetails),
              instructions:
                T.nilable(OpenAI::Responses::Response::Instructions::Variants),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: OpenAI::ResponsesModel::Variants,
              object: Symbol,
              output: T::Array[OpenAI::Responses::ResponseOutputItem::Variants],
              parallel_tool_calls: T::Boolean,
              temperature: T.nilable(Float),
              tool_choice: OpenAI::Responses::Response::ToolChoice::Variants,
              tools: T::Array[OpenAI::Responses::Tool::Variants],
              top_p: T.nilable(Float),
              background: T.nilable(T::Boolean),
              max_output_tokens: T.nilable(Integer),
              max_tool_calls: T.nilable(Integer),
              previous_response_id: T.nilable(String),
              prompt: T.nilable(OpenAI::Responses::ResponsePrompt),
              prompt_cache_key: String,
              reasoning: T.nilable(OpenAI::Reasoning),
              safety_identifier: String,
              service_tier:
                T.nilable(
                  OpenAI::Responses::Response::ServiceTier::TaggedSymbol
                ),
              status: OpenAI::Responses::ResponseStatus::TaggedSymbol,
              text: OpenAI::Responses::ResponseTextConfig,
              top_logprobs: T.nilable(Integer),
              truncation:
                T.nilable(
                  OpenAI::Responses::Response::Truncation::TaggedSymbol
                ),
              usage: OpenAI::Responses::ResponseUsage,
              user: String
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              id: String,
              created_at: Float,
              error: T.nilable(OpenAI::Responses::ResponseError::OrHash),
              incomplete_details: T.nilable(OpenAI::Responses::Response::IncompleteDetails::OrHash),
              instructions: T.nilable(OpenAI::Responses::Response::Instructions::Variants),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: T.any(
                String,
                OpenAI::ChatModel::OrSymbol,
                OpenAI::ResponsesModel::ResponsesOnlyModel::OrSymbol
              ),
              output: T::Array[
                T.any(
                  OpenAI::Responses::ResponseOutputMessage::OrHash,
                  OpenAI::Responses::ResponseFileSearchToolCall::OrHash,
                  OpenAI::Responses::ResponseFunctionToolCall::OrHash,
                  OpenAI::Responses::ResponseFunctionWebSearch::OrHash,
                  OpenAI::Responses::ResponseComputerToolCall::OrHash,
                  OpenAI::Responses::ResponseReasoningItem::OrHash,
                  OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::OrHash,
                  OpenAI::Responses::ResponseCodeInterpreterToolCall::OrHash,
                  OpenAI::Responses::ResponseOutputItem::LocalShellCall::OrHash,
                  OpenAI::Responses::ResponseOutputItem::McpCall::OrHash,
                  OpenAI::Responses::ResponseOutputItem::McpListTools::OrHash,
                  OpenAI::Responses::ResponseOutputItem::McpApprovalRequest::OrHash
                )
              ],
              parallel_tool_calls: T::Boolean,
              temperature: T.nilable(Float),
              tool_choice: T.any(
                OpenAI::Responses::ToolChoiceOptions::OrSymbol,
                OpenAI::Responses::ToolChoiceTypes::OrHash,
                OpenAI::Responses::ToolChoiceFunction::OrHash,
                OpenAI::Responses::ToolChoiceMcp::OrHash
              ),
              tools: T::Array[
                T.any(
                  OpenAI::Responses::FunctionTool::OrHash,
                  OpenAI::Responses::FileSearchTool::OrHash,
                  OpenAI::Responses::ComputerTool::OrHash,
                  OpenAI::Responses::Tool::Mcp::OrHash,
                  OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                  OpenAI::Responses::Tool::ImageGeneration::OrHash,
                  OpenAI::Responses::Tool::LocalShell::OrHash,
                  OpenAI::Responses::WebSearchTool::OrHash
                )
              ],
              top_p: T.nilable(Float),
              background: T.nilable(T::Boolean),
              max_output_tokens: T.nilable(Integer),
              max_tool_calls: T.nilable(Integer),
              previous_response_id: T.nilable(String),
              prompt: T.nilable(OpenAI::Responses::ResponsePrompt::OrHash),
              prompt_cache_key: String,
              reasoning: T.nilable(OpenAI::Reasoning::OrHash),
              safety_identifier: String,
              service_tier: T.nilable(OpenAI::Responses::Response::ServiceTier::OrSymbol),
              status: OpenAI::Responses::ResponseStatus::OrSymbol,
              text: OpenAI::Responses::ResponseTextConfig::OrHash,
              top_logprobs: T.nilable(Integer),
              truncation: T.nilable(OpenAI::Responses::Response::Truncation::OrSymbol),
              usage: OpenAI::Responses::ResponseUsage::OrHash,
              user: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # Unique identifier for this Response.
            created_at:, # Unix timestamp (in seconds) of when this Response was created.
            error:, # An error object returned when the model fails to generate a Response.
            incomplete_details:, # Details about why the response is incomplete.
            instructions:, # A system (or developer) message inserted into the model's context.
                           # When using along with `previous_response_id`, the instructions from a previous
                           # response will not be carried over to the next response. This makes it simple to
                           # swap out system (or developer) messages in new responses.
            metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
            model:, # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
                    # wide range of models with different capabilities, performance characteristics,
                    # and price points. Refer to the
                    # [model guide](https://platform.openai.com/docs/models) to browse and compare
                    # available models.
            output:, # An array of content items generated by the model.
                     # - The length and order of items in the `output` array is dependent on the
                     #   model's response.
                     # - Rather than accessing the first item in the `output` array and assuming it's
                     #   an `assistant` message with the content generated by the model, you might
                     #   consider using the `output_text` property where supported in SDKs.
            parallel_tool_calls:, # Whether to allow the model to run tool calls in parallel.
            temperature:, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                          # make the output more random, while lower values like 0.2 will make it more
                          # focused and deterministic. We generally recommend altering this or `top_p` but
                          # not both.
            tool_choice:, # How the model should select which tool (or tools) to use when generating a
                          # response. See the `tools` parameter to see how to specify which tools the model
                          # can call.
            tools:, # An array of tools the model may call while generating a response. You can
                    # specify which tool to use by setting the `tool_choice` parameter.
                    # The two categories of tools you can provide the model are:
                    # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
                    #   capabilities, like
                    #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
                    #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
                    #   Learn more about
                    #   [built-in tools](https://platform.openai.com/docs/guides/tools).
                    # - **Function calls (custom tools)**: Functions that are defined by you, enabling
                    #   the model to call your own code. Learn more about
                    #   [function calling](https://platform.openai.com/docs/guides/function-calling).
            top_p:, # An alternative to sampling with temperature, called nucleus sampling, where the
                    # model considers the results of the tokens with top_p probability mass. So 0.1
                    # means only the tokens comprising the top 10% probability mass are considered.
                    # We generally recommend altering this or `temperature` but not both.
            background: nil, # Whether to run the model response in the background.
                             # [Learn more](https://platform.openai.com/docs/guides/background).
            max_output_tokens: nil, # An upper bound for the number of tokens that can be generated for a response,
                                    # including visible output tokens and
                                    # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
            max_tool_calls: nil, # The maximum number of total calls to built-in tools that can be processed in a
                                 # response. This maximum number applies across all built-in tool calls, not per
                                 # individual tool. Any further attempts to call a tool by the model will be
                                 # ignored.
            previous_response_id: nil, # The unique ID of the previous response to the model. Use this to create
                                       # multi-turn conversations. Learn more about
                                       # [conversation state](https://platform.openai.com/docs/guides/conversation-state).
            prompt: nil, # Reference to a prompt template and its variables.
                         # [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).
            prompt_cache_key: nil, # Used by OpenAI to cache responses for similar requests to optimize your cache
                                   # hit rates. Replaces the `user` field.
                                   # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
            reasoning: nil, # **o-series models only**
                            # Configuration options for
                            # [reasoning models](https://platform.openai.com/docs/guides/reasoning).
            safety_identifier: nil, # A stable identifier used to help detect users of your application that may be
                                    # violating OpenAI's usage policies. The IDs should be a string that uniquely
                                    # identifies each user. We recommend hashing their username or email address, in
                                    # order to avoid sending us any identifying information.
                                    # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
            service_tier: nil, # Specifies the processing type used for serving the request.
                               # - If set to 'auto', then the request will be processed with the service tier
                               #   configured in the Project settings. Unless otherwise configured, the Project
                               #   will use 'default'.
                               # - If set to 'default', then the request will be processed with the standard
                               #   pricing and performance for the selected model.
                               # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
                               #   'priority', then the request will be processed with the corresponding service
                               #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
                               #   Priority processing.
                               # - When not set, the default behavior is 'auto'.
                               # When the `service_tier` parameter is set, the response body will include the
                               # `service_tier` value based on the processing mode actually used to serve the
                               # request. This response value may be different from the value set in the
                               # parameter.
            status: nil, # The status of the response generation. One of `completed`, `failed`,
                         # `in_progress`, `cancelled`, `queued`, or `incomplete`.
            text: nil, # Configuration options for a text response from the model. Can be plain text or
                       # structured JSON data. Learn more:
                       # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                       # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
            top_logprobs: nil, # An integer between 0 and 20 specifying the number of most likely tokens to
                               # return at each token position, each with an associated log probability.
            truncation: nil, # The truncation strategy to use for the model response.
                             # - `auto`: If the context of this response and previous ones exceeds the model's
                             #   context window size, the model will truncate the response to fit the context
                             #   window by dropping input items in the middle of the conversation.
                             # - `disabled` (default): If a model response will exceed the context window size
                             #   for a model, the request will fail with a 400 error.
            usage: nil, # Represents token usage details including input tokens, output tokens, a
                        # breakdown of output tokens, and the total tokens used.
            user: nil, # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
                       # `prompt_cache_key` instead to maintain caching optimizations. A stable
                       # identifier for your end-users. Used to boost cache hit rates by better bucketing
                       # similar requests and to help OpenAI detect and prevent abuse.
                       # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
            object: :response # The object type of this resource - always set to `response`.
); end
        end

        class IncompleteDetails < OpenAI::Internal::Type::BaseModel
          # The reason why the response is incomplete.
          sig do
            returns(T.nilable(
                OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
              ))
          end
          attr_reader :reason

          sig { params(reason: OpenAI::Responses::Response::IncompleteDetails::Reason::OrSymbol).void }
          attr_writer :reason

          sig do
            override
              .returns({
                reason:
                  OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
              })
          end
          def to_hash; end

          class << self
            # Details about why the response is incomplete.
            sig do
              params(
                reason: OpenAI::Responses::Response::IncompleteDetails::Reason::OrSymbol
              ).returns(T.attached_class)
            end
            def new(
              reason: nil # The reason why the response is incomplete.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::Response::IncompleteDetails,
                OpenAI::Internal::AnyHash
              )
            end

          # The reason why the response is incomplete.
          module Reason
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
                ])
              end
              def values; end
            end

            CONTENT_FILTER = T.let(
                :content_filter,
                OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
              )

            MAX_OUTPUT_TOKENS = T.let(
                :max_output_tokens,
                OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::Response::IncompleteDetails::Reason
                )
              end
          end
        end

        # A system (or developer) message inserted into the model's context.
        #
        # When using along with `previous_response_id`, the instructions from a previous
        # response will not be carried over to the next response. This makes it simple to
        # swap out system (or developer) messages in new responses.
        module Instructions
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::Response::Instructions::Variants]) }
            def variants; end
          end

          ResponseInputItemArray = T.let(
              OpenAI::Internal::Type::ArrayOf[
                union: OpenAI::Responses::ResponseInputItem
              ],
              OpenAI::Internal::Type::Converter
            )

          Variants = T.type_alias do
              T.any(
                String,
                T::Array[OpenAI::Responses::ResponseInputItem::Variants]
              )
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::Response, OpenAI::Internal::AnyHash)
          end

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        module ServiceTier
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::Response::ServiceTier::TaggedSymbol]) }
            def values; end
          end

          AUTO = T.let(:auto, OpenAI::Responses::Response::ServiceTier::TaggedSymbol)

          DEFAULT = T.let(
              :default,
              OpenAI::Responses::Response::ServiceTier::TaggedSymbol
            )

          FLEX = T.let(:flex, OpenAI::Responses::Response::ServiceTier::TaggedSymbol)

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          PRIORITY = T.let(
              :priority,
              OpenAI::Responses::Response::ServiceTier::TaggedSymbol
            )

          SCALE = T.let(
              :scale,
              OpenAI::Responses::Response::ServiceTier::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::Response::ServiceTier)
            end
        end

        # How the model should select which tool (or tools) to use when generating a
        # response. See the `tools` parameter to see how to specify which tools the model
        # can call.
        module ToolChoice
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::Response::ToolChoice::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Responses::ToolChoiceOptions::TaggedSymbol,
                OpenAI::Responses::ToolChoiceTypes,
                OpenAI::Responses::ToolChoiceFunction,
                OpenAI::Responses::ToolChoiceMcp
              )
            end
        end

        # The truncation strategy to use for the model response.
        #
        # - `auto`: If the context of this response and previous ones exceeds the model's
        #   context window size, the model will truncate the response to fit the context
        #   window by dropping input items in the middle of the conversation.
        # - `disabled` (default): If a model response will exceed the context window size
        #   for a model, the request will fail with a 400 error.
        module Truncation
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::Response::Truncation::TaggedSymbol]) }
            def values; end
          end

          AUTO = T.let(:auto, OpenAI::Responses::Response::Truncation::TaggedSymbol)

          DISABLED = T.let(
              :disabled,
              OpenAI::Responses::Response::Truncation::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::Response::Truncation)
            end
        end
      end

      class ResponseAudioDeltaEvent < OpenAI::Internal::Type::BaseModel
        # A chunk of Base64 encoded response audio bytes.
        sig { returns(String) }
        attr_accessor :delta

        # A sequence number for this chunk of the stream response.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.audio.delta`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ delta: String, sequence_number: Integer, type: Symbol }) }
        def to_hash; end

        class << self
          # Emitted when there is a partial audio response.
          sig { params(delta: String, sequence_number: Integer, type: Symbol).returns(T.attached_class) }
          def new(
            delta:, # A chunk of Base64 encoded response audio bytes.
            sequence_number:, # A sequence number for this chunk of the stream response.
            type: :"response.audio.delta" # The type of the event. Always `response.audio.delta`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseAudioDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseAudioDoneEvent < OpenAI::Internal::Type::BaseModel
        # The sequence number of the delta.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.audio.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ sequence_number: Integer, type: Symbol }) }
        def to_hash; end

        class << self
          # Emitted when the audio response is complete.
          sig { params(sequence_number: Integer, type: Symbol).returns(T.attached_class) }
          def new(
            sequence_number:, # The sequence number of the delta.
            type: :"response.audio.done" # The type of the event. Always `response.audio.done`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseAudioDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseAudioTranscriptDeltaEvent < OpenAI::Internal::Type::BaseModel
        # The partial transcript of the audio response.
        sig { returns(String) }
        attr_accessor :delta

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.audio.transcript.delta`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ delta: String, sequence_number: Integer, type: Symbol }) }
        def to_hash; end

        class << self
          # Emitted when there is a partial transcript of audio.
          sig { params(delta: String, sequence_number: Integer, type: Symbol).returns(T.attached_class) }
          def new(
            delta:, # The partial transcript of the audio response.
            sequence_number:, # The sequence number of this event.
            type: :"response.audio.transcript.delta" # The type of the event. Always `response.audio.transcript.delta`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseAudioTranscriptDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseAudioTranscriptDoneEvent < OpenAI::Internal::Type::BaseModel
        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.audio.transcript.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ sequence_number: Integer, type: Symbol }) }
        def to_hash; end

        class << self
          # Emitted when the full audio transcript is completed.
          sig { params(sequence_number: Integer, type: Symbol).returns(T.attached_class) }
          def new(
            sequence_number:, # The sequence number of this event.
            type: :"response.audio.transcript.done" # The type of the event. Always `response.audio.transcript.done`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseAudioTranscriptDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseCancelParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCancelParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseCodeInterpreterCallCodeDeltaEvent < OpenAI::Internal::Type::BaseModel
        # The partial code snippet being streamed by the code interpreter.
        sig { returns(String) }
        attr_accessor :delta

        # The unique identifier of the code interpreter tool call item.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response for which the code is being
        # streamed.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event, used to order streaming events.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.code_interpreter_call_code.delta`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a partial code snippet is streamed by the code interpreter.
          sig do
            params(
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            delta:, # The partial code snippet being streamed by the code interpreter.
            item_id:, # The unique identifier of the code interpreter tool call item.
            output_index:, # The index of the output item in the response for which the code is being
                           # streamed.
            sequence_number:, # The sequence number of this event, used to order streaming events.
            type: :"response.code_interpreter_call_code.delta" # The type of the event. Always `response.code_interpreter_call_code.delta`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCodeInterpreterCallCodeDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseCodeInterpreterCallCodeDoneEvent < OpenAI::Internal::Type::BaseModel
        # The final code snippet output by the code interpreter.
        sig { returns(String) }
        attr_accessor :code

        # The unique identifier of the code interpreter tool call item.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response for which the code is finalized.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event, used to order streaming events.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.code_interpreter_call_code.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              code: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the code snippet is finalized by the code interpreter.
          sig do
            params(
              code: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            code:, # The final code snippet output by the code interpreter.
            item_id:, # The unique identifier of the code interpreter tool call item.
            output_index:, # The index of the output item in the response for which the code is finalized.
            sequence_number:, # The sequence number of this event, used to order streaming events.
            type: :"response.code_interpreter_call_code.done" # The type of the event. Always `response.code_interpreter_call_code.done`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCodeInterpreterCallCodeDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseCodeInterpreterCallCompletedEvent < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the code interpreter tool call item.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response for which the code interpreter call
        # is completed.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event, used to order streaming events.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.code_interpreter_call.completed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the code interpreter call is completed.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The unique identifier of the code interpreter tool call item.
            output_index:, # The index of the output item in the response for which the code interpreter call
                           # is completed.
            sequence_number:, # The sequence number of this event, used to order streaming events.
            type: :"response.code_interpreter_call.completed" # The type of the event. Always `response.code_interpreter_call.completed`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCodeInterpreterCallCompletedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseCodeInterpreterCallInProgressEvent < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the code interpreter tool call item.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response for which the code interpreter call
        # is in progress.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event, used to order streaming events.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.code_interpreter_call.in_progress`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a code interpreter call is in progress.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The unique identifier of the code interpreter tool call item.
            output_index:, # The index of the output item in the response for which the code interpreter call
                           # is in progress.
            sequence_number:, # The sequence number of this event, used to order streaming events.
            type: :"response.code_interpreter_call.in_progress" # The type of the event. Always `response.code_interpreter_call.in_progress`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCodeInterpreterCallInProgressEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseCodeInterpreterCallInterpretingEvent < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the code interpreter tool call item.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response for which the code interpreter is
        # interpreting code.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event, used to order streaming events.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.code_interpreter_call.interpreting`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the code interpreter is actively interpreting the code snippet.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The unique identifier of the code interpreter tool call item.
            output_index:, # The index of the output item in the response for which the code interpreter is
                           # interpreting code.
            sequence_number:, # The sequence number of this event, used to order streaming events.
            type: :"response.code_interpreter_call.interpreting" # The type of the event. Always `response.code_interpreter_call.interpreting`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCodeInterpreterCallInterpretingEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseCodeInterpreterToolCall < OpenAI::Internal::Type::BaseModel
        # The code to run, or null if not available.
        sig { returns(T.nilable(String)) }
        attr_accessor :code

        # The ID of the container used to run the code.
        sig { returns(String) }
        attr_accessor :container_id

        # The unique ID of the code interpreter tool call.
        sig { returns(String) }
        attr_accessor :id

        # The outputs generated by the code interpreter, such as logs or images. Can be
        # null if no outputs are available.
        sig do
          returns(T.nilable(
              T::Array[
                T.any(
                  OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Logs,
                  OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Image
                )
              ]
            ))
        end
        attr_accessor :outputs

        # The status of the code interpreter tool call. Valid values are `in_progress`,
        # `completed`, `incomplete`, `interpreting`, and `failed`.
        sig { returns(OpenAI::Responses::ResponseCodeInterpreterToolCall::Status::OrSymbol) }
        attr_accessor :status

        # The type of the code interpreter tool call. Always `code_interpreter_call`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              code: T.nilable(String),
              container_id: String,
              outputs:
                T.nilable(
                  T::Array[
                    T.any(
                      OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Logs,
                      OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Image
                    )
                  ]
                ),
              status:
                OpenAI::Responses::ResponseCodeInterpreterToolCall::Status::OrSymbol,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # A tool call to run code.
          sig do
            params(
              id: String,
              code: T.nilable(String),
              container_id: String,
              outputs: T.nilable(
                T::Array[
                  T.any(
                    OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Logs::OrHash,
                    OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Image::OrHash
                  )
                ]
              ),
              status: OpenAI::Responses::ResponseCodeInterpreterToolCall::Status::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the code interpreter tool call.
            code:, # The code to run, or null if not available.
            container_id:, # The ID of the container used to run the code.
            outputs:, # The outputs generated by the code interpreter, such as logs or images. Can be
                      # null if no outputs are available.
            status:, # The status of the code interpreter tool call. Valid values are `in_progress`,
                     # `completed`, `incomplete`, `interpreting`, and `failed`.
            type: :code_interpreter_call # The type of the code interpreter tool call. Always `code_interpreter_call`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCodeInterpreterToolCall,
              OpenAI::Internal::AnyHash
            )
          end

        # The logs output from the code interpreter.
        module Output
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Variants
              ])
            end
            def variants; end
          end

          class Image < OpenAI::Internal::Type::BaseModel
            # The type of the output. Always 'image'.
            sig { returns(Symbol) }
            attr_accessor :type

            # The URL of the image output from the code interpreter.
            sig { returns(String) }
            attr_accessor :url

            sig { override.returns({ type: Symbol, url: String }) }
            def to_hash; end

            class << self
              # The image output from the code interpreter.
              sig { params(url: String, type: Symbol).returns(T.attached_class) }
              def new(
                url:, # The URL of the image output from the code interpreter.
                type: :image # The type of the output. Always 'image'.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Image,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class Logs < OpenAI::Internal::Type::BaseModel
            # The logs output from the code interpreter.
            sig { returns(String) }
            attr_accessor :logs

            # The type of the output. Always 'logs'.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ logs: String, type: Symbol }) }
            def to_hash; end

            class << self
              # The logs output from the code interpreter.
              sig { params(logs: String, type: Symbol).returns(T.attached_class) }
              def new(
                logs:, # The logs output from the code interpreter.
                type: :logs # The type of the output. Always 'logs'.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Logs,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Logs,
                OpenAI::Responses::ResponseCodeInterpreterToolCall::Output::Image
              )
            end
        end

        # The status of the code interpreter tool call. Valid values are `in_progress`,
        # `completed`, `incomplete`, `interpreting`, and `failed`.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseCodeInterpreterToolCall::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseCodeInterpreterToolCall::Status::TaggedSymbol
            )

          FAILED = T.let(
              :failed,
              OpenAI::Responses::ResponseCodeInterpreterToolCall::Status::TaggedSymbol
            )

          INCOMPLETE = T.let(
              :incomplete,
              OpenAI::Responses::ResponseCodeInterpreterToolCall::Status::TaggedSymbol
            )

          INTERPRETING = T.let(
              :interpreting,
              OpenAI::Responses::ResponseCodeInterpreterToolCall::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseCodeInterpreterToolCall::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Responses::ResponseCodeInterpreterToolCall::Status
              )
            end
        end
      end

      class ResponseCompletedEvent < OpenAI::Internal::Type::BaseModel
        # Properties of the completed response.
        sig { returns(OpenAI::Responses::Response) }
        attr_reader :response

        sig { params(response: OpenAI::Responses::Response::OrHash).void }
        attr_writer :response

        # The sequence number for this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.completed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              response: OpenAI::Responses::Response,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the model response is complete.
          sig do
            params(
              response: OpenAI::Responses::Response::OrHash,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            response:, # Properties of the completed response.
            sequence_number:, # The sequence number for this event.
            type: :"response.completed" # The type of the event. Always `response.completed`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCompletedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseComputerToolCall < OpenAI::Internal::Type::BaseModel
        # A click action.
        sig do
          returns(T.any(
              OpenAI::Responses::ResponseComputerToolCall::Action::Click,
              OpenAI::Responses::ResponseComputerToolCall::Action::DoubleClick,
              OpenAI::Responses::ResponseComputerToolCall::Action::Drag,
              OpenAI::Responses::ResponseComputerToolCall::Action::Keypress,
              OpenAI::Responses::ResponseComputerToolCall::Action::Move,
              OpenAI::Responses::ResponseComputerToolCall::Action::Screenshot,
              OpenAI::Responses::ResponseComputerToolCall::Action::Scroll,
              OpenAI::Responses::ResponseComputerToolCall::Action::Type,
              OpenAI::Responses::ResponseComputerToolCall::Action::Wait
            ))
        end
        attr_accessor :action

        # An identifier used when responding to the tool call with output.
        sig { returns(String) }
        attr_accessor :call_id

        # The unique ID of the computer call.
        sig { returns(String) }
        attr_accessor :id

        # The pending safety checks for the computer call.
        sig do
          returns(T::Array[
              OpenAI::Responses::ResponseComputerToolCall::PendingSafetyCheck
            ])
        end
        attr_accessor :pending_safety_checks

        # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        sig { returns(OpenAI::Responses::ResponseComputerToolCall::Status::OrSymbol) }
        attr_accessor :status

        # The type of the computer call. Always `computer_call`.
        sig { returns(OpenAI::Responses::ResponseComputerToolCall::Type::OrSymbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              action:
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::Click,
                  OpenAI::Responses::ResponseComputerToolCall::Action::DoubleClick,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Drag,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Keypress,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Move,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Screenshot,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Scroll,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Type,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Wait
                ),
              call_id: String,
              pending_safety_checks:
                T::Array[
                  OpenAI::Responses::ResponseComputerToolCall::PendingSafetyCheck
                ],
              status:
                OpenAI::Responses::ResponseComputerToolCall::Status::OrSymbol,
              type: OpenAI::Responses::ResponseComputerToolCall::Type::OrSymbol
            })
        end
        def to_hash; end

        class << self
          # A tool call to a computer use tool. See the
          # [computer use guide](https://platform.openai.com/docs/guides/tools-computer-use)
          # for more information.
          sig do
            params(
              id: String,
              action: T.any(
                OpenAI::Responses::ResponseComputerToolCall::Action::Click::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::Action::DoubleClick::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::Action::Drag::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::Action::Keypress::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::Action::Move::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::Action::Screenshot::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::Action::Scroll::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::Action::Type::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::Action::Wait::OrHash
              ),
              call_id: String,
              pending_safety_checks: T::Array[
                OpenAI::Responses::ResponseComputerToolCall::PendingSafetyCheck::OrHash
              ],
              status: OpenAI::Responses::ResponseComputerToolCall::Status::OrSymbol,
              type: OpenAI::Responses::ResponseComputerToolCall::Type::OrSymbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the computer call.
            action:, # A click action.
            call_id:, # An identifier used when responding to the tool call with output.
            pending_safety_checks:, # The pending safety checks for the computer call.
            status:, # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
                     # Populated when items are returned via API.
            type: # The type of the computer call. Always `computer_call`.
); end
        end

        # A click action.
        module Action
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseComputerToolCall::Action::Variants
              ])
            end
            def variants; end
          end

          class Click < OpenAI::Internal::Type::BaseModel
            # Indicates which mouse button was pressed during the click. One of `left`,
            # `right`, `wheel`, `back`, or `forward`.
            sig { returns(OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button::OrSymbol) }
            attr_accessor :button

            # Specifies the event type. For a click action, this property is always set to
            # `click`.
            sig { returns(Symbol) }
            attr_accessor :type

            # The x-coordinate where the click occurred.
            sig { returns(Integer) }
            attr_accessor :x

            # The y-coordinate where the click occurred.
            sig { returns(Integer) }
            attr_accessor :y_

            sig do
              override
                .returns({
                  button:
                    OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button::OrSymbol,
                  type: Symbol,
                  x: Integer,
                  y_: Integer
                })
            end
            def to_hash; end

            class << self
              # A click action.
              sig do
                params(
                  button: OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button::OrSymbol,
                  x: Integer,
                  y_: Integer,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                button:, # Indicates which mouse button was pressed during the click. One of `left`,
                         # `right`, `wheel`, `back`, or `forward`.
                x:, # The x-coordinate where the click occurred.
                y_:, # The y-coordinate where the click occurred.
                type: :click # Specifies the event type. For a click action, this property is always set to
                             # `click`.
); end
            end

            # Indicates which mouse button was pressed during the click. One of `left`,
            # `right`, `wheel`, `back`, or `forward`.
            module Button
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button::TaggedSymbol
                  ])
                end
                def values; end
              end

              BACK = T.let(
                  :back,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button::TaggedSymbol
                )

              FORWARD = T.let(
                  :forward,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button::TaggedSymbol
                )

              LEFT = T.let(
                  :left,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              RIGHT = T.let(
                  :right,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button::TaggedSymbol
                )

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button
                  )
                end

              WHEEL = T.let(
                  :wheel,
                  OpenAI::Responses::ResponseComputerToolCall::Action::Click::Button::TaggedSymbol
                )
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::Click,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class DoubleClick < OpenAI::Internal::Type::BaseModel
            # Specifies the event type. For a double click action, this property is always set
            # to `double_click`.
            sig { returns(Symbol) }
            attr_accessor :type

            # The x-coordinate where the double click occurred.
            sig { returns(Integer) }
            attr_accessor :x

            # The y-coordinate where the double click occurred.
            sig { returns(Integer) }
            attr_accessor :y_

            sig { override.returns({ type: Symbol, x: Integer, y_: Integer }) }
            def to_hash; end

            class << self
              # A double click action.
              sig { params(x: Integer, y_: Integer, type: Symbol).returns(T.attached_class) }
              def new(
                x:, # The x-coordinate where the double click occurred.
                y_:, # The y-coordinate where the double click occurred.
                type: :double_click # Specifies the event type. For a double click action, this property is always set
                                    # to `double_click`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::DoubleClick,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class Drag < OpenAI::Internal::Type::BaseModel
            # An array of coordinates representing the path of the drag action. Coordinates
            # will appear as an array of objects, eg
            #
            # ```
            # [
            #   { x: 100, y: 200 },
            #   { x: 200, y: 300 }
            # ]
            # ```
            sig do
              returns(T::Array[
                  OpenAI::Responses::ResponseComputerToolCall::Action::Drag::Path
                ])
            end
            attr_accessor :path

            # Specifies the event type. For a drag action, this property is always set to
            # `drag`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  path:
                    T::Array[
                      OpenAI::Responses::ResponseComputerToolCall::Action::Drag::Path
                    ],
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              # A drag action.
              sig do
                params(
                  path: T::Array[
                    OpenAI::Responses::ResponseComputerToolCall::Action::Drag::Path::OrHash
                  ],
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                path:, # An array of coordinates representing the path of the drag action. Coordinates
                       # will appear as an array of objects, eg
                       # ```
                       # [
                       #   { x: 100, y: 200 },
                       #   { x: 200, y: 300 }
                       # ]
                       # ```
                type: :drag # Specifies the event type. For a drag action, this property is always set to
                            # `drag`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::Drag,
                  OpenAI::Internal::AnyHash
                )
              end

            class Path < OpenAI::Internal::Type::BaseModel
              # The x-coordinate.
              sig { returns(Integer) }
              attr_accessor :x

              # The y-coordinate.
              sig { returns(Integer) }
              attr_accessor :y_

              sig { override.returns({ x: Integer, y_: Integer }) }
              def to_hash; end

              class << self
                # A series of x/y coordinate pairs in the drag path.
                sig { params(x: Integer, y_: Integer).returns(T.attached_class) }
                def new(
                  x:, # The x-coordinate.
                  y_: # The y-coordinate.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Responses::ResponseComputerToolCall::Action::Drag::Path,
                    OpenAI::Internal::AnyHash
                  )
                end
            end
          end

          class Keypress < OpenAI::Internal::Type::BaseModel
            # The combination of keys the model is requesting to be pressed. This is an array
            # of strings, each representing a key.
            sig { returns(T::Array[String]) }
            attr_accessor :keys

            # Specifies the event type. For a keypress action, this property is always set to
            # `keypress`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ keys: T::Array[String], type: Symbol }) }
            def to_hash; end

            class << self
              # A collection of keypresses the model would like to perform.
              sig { params(keys: T::Array[String], type: Symbol).returns(T.attached_class) }
              def new(
                keys:, # The combination of keys the model is requesting to be pressed. This is an array
                       # of strings, each representing a key.
                type: :keypress # Specifies the event type. For a keypress action, this property is always set to
                                # `keypress`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::Keypress,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class Move < OpenAI::Internal::Type::BaseModel
            # Specifies the event type. For a move action, this property is always set to
            # `move`.
            sig { returns(Symbol) }
            attr_accessor :type

            # The x-coordinate to move to.
            sig { returns(Integer) }
            attr_accessor :x

            # The y-coordinate to move to.
            sig { returns(Integer) }
            attr_accessor :y_

            sig { override.returns({ type: Symbol, x: Integer, y_: Integer }) }
            def to_hash; end

            class << self
              # A mouse move action.
              sig { params(x: Integer, y_: Integer, type: Symbol).returns(T.attached_class) }
              def new(
                x:, # The x-coordinate to move to.
                y_:, # The y-coordinate to move to.
                type: :move # Specifies the event type. For a move action, this property is always set to
                            # `move`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::Move,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class Screenshot < OpenAI::Internal::Type::BaseModel
            # Specifies the event type. For a screenshot action, this property is always set
            # to `screenshot`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ type: Symbol }) }
            def to_hash; end

            class << self
              # A screenshot action.
              sig { params(type: Symbol).returns(T.attached_class) }
              def new(
                type: :screenshot # Specifies the event type. For a screenshot action, this property is always set
                                  # to `screenshot`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::Screenshot,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class Scroll < OpenAI::Internal::Type::BaseModel
            # The horizontal scroll distance.
            sig { returns(Integer) }
            attr_accessor :scroll_x

            # The vertical scroll distance.
            sig { returns(Integer) }
            attr_accessor :scroll_y

            # Specifies the event type. For a scroll action, this property is always set to
            # `scroll`.
            sig { returns(Symbol) }
            attr_accessor :type

            # The x-coordinate where the scroll occurred.
            sig { returns(Integer) }
            attr_accessor :x

            # The y-coordinate where the scroll occurred.
            sig { returns(Integer) }
            attr_accessor :y_

            sig do
              override
                .returns({
                  scroll_x: Integer,
                  scroll_y: Integer,
                  type: Symbol,
                  x: Integer,
                  y_: Integer
                })
            end
            def to_hash; end

            class << self
              # A scroll action.
              sig do
                params(
                  scroll_x: Integer,
                  scroll_y: Integer,
                  x: Integer,
                  y_: Integer,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                scroll_x:, # The horizontal scroll distance.
                scroll_y:, # The vertical scroll distance.
                x:, # The x-coordinate where the scroll occurred.
                y_:, # The y-coordinate where the scroll occurred.
                type: :scroll # Specifies the event type. For a scroll action, this property is always set to
                              # `scroll`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::Scroll,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class Type < OpenAI::Internal::Type::BaseModel
            # The text to type.
            sig { returns(String) }
            attr_accessor :text

            # Specifies the event type. For a type action, this property is always set to
            # `type`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ text: String, type: Symbol }) }
            def to_hash; end

            class << self
              # An action to type in text.
              sig { params(text: String, type: Symbol).returns(T.attached_class) }
              def new(
                text:, # The text to type.
                type: :type # Specifies the event type. For a type action, this property is always set to
                            # `type`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::Type,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseComputerToolCall::Action::Click,
                OpenAI::Responses::ResponseComputerToolCall::Action::DoubleClick,
                OpenAI::Responses::ResponseComputerToolCall::Action::Drag,
                OpenAI::Responses::ResponseComputerToolCall::Action::Keypress,
                OpenAI::Responses::ResponseComputerToolCall::Action::Move,
                OpenAI::Responses::ResponseComputerToolCall::Action::Screenshot,
                OpenAI::Responses::ResponseComputerToolCall::Action::Scroll,
                OpenAI::Responses::ResponseComputerToolCall::Action::Type,
                OpenAI::Responses::ResponseComputerToolCall::Action::Wait
              )
            end

          class Wait < OpenAI::Internal::Type::BaseModel
            # Specifies the event type. For a wait action, this property is always set to
            # `wait`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ type: Symbol }) }
            def to_hash; end

            class << self
              # A wait action.
              sig { params(type: Symbol).returns(T.attached_class) }
              def new(
                type: :wait # Specifies the event type. For a wait action, this property is always set to
                            # `wait`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseComputerToolCall::Action::Wait,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseComputerToolCall,
              OpenAI::Internal::AnyHash
            )
          end

        class PendingSafetyCheck < OpenAI::Internal::Type::BaseModel
          # The type of the pending safety check.
          sig { returns(String) }
          attr_accessor :code

          # The ID of the pending safety check.
          sig { returns(String) }
          attr_accessor :id

          # Details about the pending safety check.
          sig { returns(String) }
          attr_accessor :message

          sig { override.returns({ id: String, code: String, message: String }) }
          def to_hash; end

          class << self
            # A pending safety check for the computer call.
            sig { params(id: String, code: String, message: String).returns(T.attached_class) }
            def new(
              id:, # The ID of the pending safety check.
              code:, # The type of the pending safety check.
              message: # Details about the pending safety check.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseComputerToolCall::PendingSafetyCheck,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseComputerToolCall::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseComputerToolCall::Status::TaggedSymbol
            )

          INCOMPLETE = T.let(
              :incomplete,
              OpenAI::Responses::ResponseComputerToolCall::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseComputerToolCall::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseComputerToolCall::Status)
            end
        end

        # The type of the computer call. Always `computer_call`.
        module Type
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseComputerToolCall::Type::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPUTER_CALL = T.let(
              :computer_call,
              OpenAI::Responses::ResponseComputerToolCall::Type::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseComputerToolCall::Type)
            end
        end
      end

      class ResponseComputerToolCallOutputItem < OpenAI::Internal::Type::BaseModel
        # The safety checks reported by the API that have been acknowledged by the
        # developer.
        sig do
          returns(T.nilable(
              T::Array[
                OpenAI::Responses::ResponseComputerToolCallOutputItem::AcknowledgedSafetyCheck
              ]
            ))
        end
        attr_reader :acknowledged_safety_checks

        sig do
          params(
            acknowledged_safety_checks: T::Array[
                OpenAI::Responses::ResponseComputerToolCallOutputItem::AcknowledgedSafetyCheck::OrHash
              ]
          ).void
        end
        attr_writer :acknowledged_safety_checks

        # The ID of the computer tool call that produced the output.
        sig { returns(String) }
        attr_accessor :call_id

        # The unique ID of the computer call tool output.
        sig { returns(String) }
        attr_accessor :id

        # A computer screenshot image used with the computer use tool.
        sig { returns(OpenAI::Responses::ResponseComputerToolCallOutputScreenshot) }
        attr_reader :output

        sig { params(output: OpenAI::Responses::ResponseComputerToolCallOutputScreenshot::OrHash).void }
        attr_writer :output

        # The status of the message input. One of `in_progress`, `completed`, or
        # `incomplete`. Populated when input items are returned via API.
        sig do
          returns(T.nilable(
              OpenAI::Responses::ResponseComputerToolCallOutputItem::Status::TaggedSymbol
            ))
        end
        attr_reader :status

        sig { params(status: OpenAI::Responses::ResponseComputerToolCallOutputItem::Status::OrSymbol).void }
        attr_writer :status

        # The type of the computer tool call output. Always `computer_call_output`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              call_id: String,
              output:
                OpenAI::Responses::ResponseComputerToolCallOutputScreenshot,
              type: Symbol,
              acknowledged_safety_checks:
                T::Array[
                  OpenAI::Responses::ResponseComputerToolCallOutputItem::AcknowledgedSafetyCheck
                ],
              status:
                OpenAI::Responses::ResponseComputerToolCallOutputItem::Status::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              id: String,
              call_id: String,
              output: OpenAI::Responses::ResponseComputerToolCallOutputScreenshot::OrHash,
              acknowledged_safety_checks: T::Array[
                OpenAI::Responses::ResponseComputerToolCallOutputItem::AcknowledgedSafetyCheck::OrHash
              ],
              status: OpenAI::Responses::ResponseComputerToolCallOutputItem::Status::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the computer call tool output.
            call_id:, # The ID of the computer tool call that produced the output.
            output:, # A computer screenshot image used with the computer use tool.
            acknowledged_safety_checks: nil, # The safety checks reported by the API that have been acknowledged by the
                                             # developer.
            status: nil, # The status of the message input. One of `in_progress`, `completed`, or
                         # `incomplete`. Populated when input items are returned via API.
            type: :computer_call_output # The type of the computer tool call output. Always `computer_call_output`.
); end
        end

        class AcknowledgedSafetyCheck < OpenAI::Internal::Type::BaseModel
          # The type of the pending safety check.
          sig { returns(String) }
          attr_accessor :code

          # The ID of the pending safety check.
          sig { returns(String) }
          attr_accessor :id

          # Details about the pending safety check.
          sig { returns(String) }
          attr_accessor :message

          sig { override.returns({ id: String, code: String, message: String }) }
          def to_hash; end

          class << self
            # A pending safety check for the computer call.
            sig { params(id: String, code: String, message: String).returns(T.attached_class) }
            def new(
              id:, # The ID of the pending safety check.
              code:, # The type of the pending safety check.
              message: # Details about the pending safety check.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseComputerToolCallOutputItem::AcknowledgedSafetyCheck,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseComputerToolCallOutputItem,
              OpenAI::Internal::AnyHash
            )
          end

        # The status of the message input. One of `in_progress`, `completed`, or
        # `incomplete`. Populated when input items are returned via API.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseComputerToolCallOutputItem::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseComputerToolCallOutputItem::Status::TaggedSymbol
            )

          INCOMPLETE = T.let(
              :incomplete,
              OpenAI::Responses::ResponseComputerToolCallOutputItem::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseComputerToolCallOutputItem::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Responses::ResponseComputerToolCallOutputItem::Status
              )
            end
        end
      end

      class ResponseComputerToolCallOutputScreenshot < OpenAI::Internal::Type::BaseModel
        # The identifier of an uploaded file that contains the screenshot.
        sig { returns(T.nilable(String)) }
        attr_reader :file_id

        sig { params(file_id: String).void }
        attr_writer :file_id

        # The URL of the screenshot image.
        sig { returns(T.nilable(String)) }
        attr_reader :image_url

        sig { params(image_url: String).void }
        attr_writer :image_url

        # Specifies the event type. For a computer screenshot, this property is always set
        # to `computer_screenshot`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ type: Symbol, file_id: String, image_url: String }) }
        def to_hash; end

        class << self
          # A computer screenshot image used with the computer use tool.
          sig { params(file_id: String, image_url: String, type: Symbol).returns(T.attached_class) }
          def new(
            file_id: nil, # The identifier of an uploaded file that contains the screenshot.
            image_url: nil, # The URL of the screenshot image.
            type: :computer_screenshot # Specifies the event type. For a computer screenshot, this property is always set
                                       # to `computer_screenshot`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseComputerToolCallOutputScreenshot,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # Multi-modal input and output contents.
      module ResponseContent
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ResponseContent::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseInputText,
              OpenAI::Responses::ResponseInputImage,
              OpenAI::Responses::ResponseInputFile,
              OpenAI::Responses::ResponseOutputText,
              OpenAI::Responses::ResponseOutputRefusal
            )
          end
      end

      class ResponseContentPartAddedEvent < OpenAI::Internal::Type::BaseModel
        # The index of the content part that was added.
        sig { returns(Integer) }
        attr_accessor :content_index

        # The ID of the output item that the content part was added to.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the content part was added to.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The content part that was added.
        sig { returns(OpenAI::Responses::ResponseContentPartAddedEvent::Part::Variants) }
        attr_accessor :part

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.content_part.added`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              content_index: Integer,
              item_id: String,
              output_index: Integer,
              part:
                OpenAI::Responses::ResponseContentPartAddedEvent::Part::Variants,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a new content part is added.
          sig do
            params(
              content_index: Integer,
              item_id: String,
              output_index: Integer,
              part: T.any(
                OpenAI::Responses::ResponseOutputText::OrHash,
                OpenAI::Responses::ResponseOutputRefusal::OrHash
              ),
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content_index:, # The index of the content part that was added.
            item_id:, # The ID of the output item that the content part was added to.
            output_index:, # The index of the output item that the content part was added to.
            part:, # The content part that was added.
            sequence_number:, # The sequence number of this event.
            type: :"response.content_part.added" # The type of the event. Always `response.content_part.added`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseContentPartAddedEvent,
              OpenAI::Internal::AnyHash
            )
          end

        # The content part that was added.
        module Part
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseContentPartAddedEvent::Part::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputText,
                OpenAI::Responses::ResponseOutputRefusal
              )
            end
        end
      end

      class ResponseContentPartDoneEvent < OpenAI::Internal::Type::BaseModel
        # The index of the content part that is done.
        sig { returns(Integer) }
        attr_accessor :content_index

        # The ID of the output item that the content part was added to.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the content part was added to.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The content part that is done.
        sig { returns(OpenAI::Responses::ResponseContentPartDoneEvent::Part::Variants) }
        attr_accessor :part

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.content_part.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              content_index: Integer,
              item_id: String,
              output_index: Integer,
              part:
                OpenAI::Responses::ResponseContentPartDoneEvent::Part::Variants,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a content part is done.
          sig do
            params(
              content_index: Integer,
              item_id: String,
              output_index: Integer,
              part: T.any(
                OpenAI::Responses::ResponseOutputText::OrHash,
                OpenAI::Responses::ResponseOutputRefusal::OrHash
              ),
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content_index:, # The index of the content part that is done.
            item_id:, # The ID of the output item that the content part was added to.
            output_index:, # The index of the output item that the content part was added to.
            part:, # The content part that is done.
            sequence_number:, # The sequence number of this event.
            type: :"response.content_part.done" # The type of the event. Always `response.content_part.done`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseContentPartDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end

        # The content part that is done.
        module Part
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseContentPartDoneEvent::Part::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputText,
                OpenAI::Responses::ResponseOutputRefusal
              )
            end
        end
      end

      class ResponseCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Whether to run the model response in the background.
        # [Learn more](https://platform.openai.com/docs/guides/background).
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :background

        # Specify additional output data to include in the model response. Currently
        # supported values are:
        #
        # - `code_interpreter_call.outputs`: Includes the outputs of python code execution
        #   in code interpreter tool call items.
        # - `computer_call_output.output.image_url`: Include image urls from the computer
        #   call output.
        # - `file_search_call.results`: Include the search results of the file search tool
        #   call.
        # - `message.input_image.image_url`: Include image urls from the input message.
        # - `message.output_text.logprobs`: Include logprobs with assistant messages.
        # - `reasoning.encrypted_content`: Includes an encrypted version of reasoning
        #   tokens in reasoning item outputs. This enables reasoning items to be used in
        #   multi-turn conversations when using the Responses API statelessly (like when
        #   the `store` parameter is set to `false`, or when an organization is enrolled
        #   in the zero data retention program).
        sig { returns(T.nilable(T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol])) }
        attr_accessor :include

        # Text, image, or file inputs to the model, used to generate a response.
        #
        # Learn more:
        #
        # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
        # - [Image inputs](https://platform.openai.com/docs/guides/images)
        # - [File inputs](https://platform.openai.com/docs/guides/pdf-files)
        # - [Conversation state](https://platform.openai.com/docs/guides/conversation-state)
        # - [Function calling](https://platform.openai.com/docs/guides/function-calling)
        sig { returns(T.nilable(OpenAI::Responses::ResponseCreateParams::Input::Variants)) }
        attr_reader :input

        sig { params(input: OpenAI::Responses::ResponseCreateParams::Input::Variants).void }
        attr_writer :input

        # A system (or developer) message inserted into the model's context.
        #
        # When using along with `previous_response_id`, the instructions from a previous
        # response will not be carried over to the next response. This makes it simple to
        # swap out system (or developer) messages in new responses.
        sig { returns(T.nilable(String)) }
        attr_accessor :instructions

        # An upper bound for the number of tokens that can be generated for a response,
        # including visible output tokens and
        # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_output_tokens

        # The maximum number of total calls to built-in tools that can be processed in a
        # response. This maximum number applies across all built-in tool calls, not per
        # individual tool. Any further attempts to call a tool by the model will be
        # ignored.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_tool_calls

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
        # wide range of models with different capabilities, performance characteristics,
        # and price points. Refer to the
        # [model guide](https://platform.openai.com/docs/models) to browse and compare
        # available models.
        sig do
          returns(T.nilable(
              T.any(
                String,
                OpenAI::ChatModel::OrSymbol,
                OpenAI::ResponsesModel::ResponsesOnlyModel::OrSymbol
              )
            ))
        end
        attr_reader :model

        sig do
          params(
            model: T.any(
                String,
                OpenAI::ChatModel::OrSymbol,
                OpenAI::ResponsesModel::ResponsesOnlyModel::OrSymbol
              )
          ).void
        end
        attr_writer :model

        # Whether to allow the model to run tool calls in parallel.
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :parallel_tool_calls

        # The unique ID of the previous response to the model. Use this to create
        # multi-turn conversations. Learn more about
        # [conversation state](https://platform.openai.com/docs/guides/conversation-state).
        sig { returns(T.nilable(String)) }
        attr_accessor :previous_response_id

        # Reference to a prompt template and its variables.
        # [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).
        sig { returns(T.nilable(OpenAI::Responses::ResponsePrompt)) }
        attr_reader :prompt

        sig { params(prompt: T.nilable(OpenAI::Responses::ResponsePrompt::OrHash)).void }
        attr_writer :prompt

        # Used by OpenAI to cache responses for similar requests to optimize your cache
        # hit rates. Replaces the `user` field.
        # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
        sig { returns(T.nilable(String)) }
        attr_reader :prompt_cache_key

        sig { params(prompt_cache_key: String).void }
        attr_writer :prompt_cache_key

        # **o-series models only**
        #
        # Configuration options for
        # [reasoning models](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(OpenAI::Reasoning)) }
        attr_reader :reasoning

        sig { params(reasoning: T.nilable(OpenAI::Reasoning::OrHash)).void }
        attr_writer :reasoning

        # A stable identifier used to help detect users of your application that may be
        # violating OpenAI's usage policies. The IDs should be a string that uniquely
        # identifies each user. We recommend hashing their username or email address, in
        # order to avoid sending us any identifying information.
        # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        sig { returns(T.nilable(String)) }
        attr_reader :safety_identifier

        sig { params(safety_identifier: String).void }
        attr_writer :safety_identifier

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        sig do
          returns(T.nilable(
              OpenAI::Responses::ResponseCreateParams::ServiceTier::OrSymbol
            ))
        end
        attr_accessor :service_tier

        # Whether to store the generated model response for later retrieval via API.
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :store

        # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        # make the output more random, while lower values like 0.2 will make it more
        # focused and deterministic. We generally recommend altering this or `top_p` but
        # not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :temperature

        # Configuration options for a text response from the model. Can be plain text or
        # structured JSON data. Learn more:
        #
        # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
        # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
        sig { returns(T.nilable(OpenAI::Responses::ResponseTextConfig)) }
        attr_reader :text

        sig do
          params(
            text: T.any(
                OpenAI::Responses::ResponseTextConfig::OrHash,
                OpenAI::StructuredOutput::JsonSchemaConverter
              )
          ).void
        end
        attr_writer :text

        # How the model should select which tool (or tools) to use when generating a
        # response. See the `tools` parameter to see how to specify which tools the model
        # can call.
        sig do
          returns(T.nilable(
              T.any(
                OpenAI::Responses::ToolChoiceOptions::OrSymbol,
                OpenAI::Responses::ToolChoiceTypes,
                OpenAI::Responses::ToolChoiceFunction,
                OpenAI::Responses::ToolChoiceMcp
              )
            ))
        end
        attr_reader :tool_choice

        sig do
          params(
            tool_choice: T.any(
                OpenAI::Responses::ToolChoiceOptions::OrSymbol,
                OpenAI::Responses::ToolChoiceTypes::OrHash,
                OpenAI::Responses::ToolChoiceFunction::OrHash,
                OpenAI::Responses::ToolChoiceMcp::OrHash
              )
          ).void
        end
        attr_writer :tool_choice

        # An array of tools the model may call while generating a response. You can
        # specify which tool to use by setting the `tool_choice` parameter.
        #
        # The two categories of tools you can provide the model are:
        #
        # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
        #   capabilities, like
        #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
        #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
        #   Learn more about
        #   [built-in tools](https://platform.openai.com/docs/guides/tools).
        # - **Function calls (custom tools)**: Functions that are defined by you, enabling
        #   the model to call your own code. Learn more about
        #   [function calling](https://platform.openai.com/docs/guides/function-calling).
        sig do
          returns(T.nilable(
              T::Array[
                T.any(
                  OpenAI::Responses::FunctionTool,
                  OpenAI::Responses::FileSearchTool,
                  OpenAI::Responses::ComputerTool,
                  OpenAI::Responses::Tool::Mcp,
                  OpenAI::Responses::Tool::CodeInterpreter,
                  OpenAI::Responses::Tool::ImageGeneration,
                  OpenAI::Responses::Tool::LocalShell,
                  OpenAI::Responses::WebSearchTool
                )
              ]
            ))
        end
        attr_reader :tools

        sig do
          params(
            tools: T::Array[
                T.any(
                  OpenAI::Responses::FunctionTool::OrHash,
                  OpenAI::Responses::FileSearchTool::OrHash,
                  OpenAI::Responses::ComputerTool::OrHash,
                  OpenAI::Responses::Tool::Mcp::OrHash,
                  OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                  OpenAI::Responses::Tool::ImageGeneration::OrHash,
                  OpenAI::Responses::Tool::LocalShell::OrHash,
                  OpenAI::Responses::WebSearchTool::OrHash
                )
              ]
          ).void
        end
        attr_writer :tools

        # An integer between 0 and 20 specifying the number of most likely tokens to
        # return at each token position, each with an associated log probability.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :top_logprobs

        # An alternative to sampling with temperature, called nucleus sampling, where the
        # model considers the results of the tokens with top_p probability mass. So 0.1
        # means only the tokens comprising the top 10% probability mass are considered.
        #
        # We generally recommend altering this or `temperature` but not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :top_p

        # The truncation strategy to use for the model response.
        #
        # - `auto`: If the context of this response and previous ones exceeds the model's
        #   context window size, the model will truncate the response to fit the context
        #   window by dropping input items in the middle of the conversation.
        # - `disabled` (default): If a model response will exceed the context window size
        #   for a model, the request will fail with a 400 error.
        sig do
          returns(T.nilable(
              OpenAI::Responses::ResponseCreateParams::Truncation::OrSymbol
            ))
        end
        attr_accessor :truncation

        # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
        # `prompt_cache_key` instead to maintain caching optimizations. A stable
        # identifier for your end-users. Used to boost cache hit rates by better bucketing
        # similar requests and to help OpenAI detect and prevent abuse.
        # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        sig { returns(T.nilable(String)) }
        attr_reader :user

        sig { params(user: String).void }
        attr_writer :user

        sig do
          override
            .returns({
              background: T.nilable(T::Boolean),
              include:
                T.nilable(
                  T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol]
                ),
              input: OpenAI::Responses::ResponseCreateParams::Input::Variants,
              instructions: T.nilable(String),
              max_output_tokens: T.nilable(Integer),
              max_tool_calls: T.nilable(Integer),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model:
                T.any(
                  String,
                  OpenAI::ChatModel::OrSymbol,
                  OpenAI::ResponsesModel::ResponsesOnlyModel::OrSymbol
                ),
              parallel_tool_calls: T.nilable(T::Boolean),
              previous_response_id: T.nilable(String),
              prompt: T.nilable(OpenAI::Responses::ResponsePrompt),
              prompt_cache_key: String,
              reasoning: T.nilable(OpenAI::Reasoning),
              safety_identifier: String,
              service_tier:
                T.nilable(
                  OpenAI::Responses::ResponseCreateParams::ServiceTier::OrSymbol
                ),
              store: T.nilable(T::Boolean),
              temperature: T.nilable(Float),
              text: OpenAI::Responses::ResponseTextConfig,
              tool_choice:
                T.any(
                  OpenAI::Responses::ToolChoiceOptions::OrSymbol,
                  OpenAI::Responses::ToolChoiceTypes,
                  OpenAI::Responses::ToolChoiceFunction,
                  OpenAI::Responses::ToolChoiceMcp
                ),
              tools:
                T::Array[
                  T.any(
                    OpenAI::Responses::FunctionTool,
                    OpenAI::Responses::FileSearchTool,
                    OpenAI::Responses::ComputerTool,
                    OpenAI::Responses::Tool::Mcp,
                    OpenAI::Responses::Tool::CodeInterpreter,
                    OpenAI::Responses::Tool::ImageGeneration,
                    OpenAI::Responses::Tool::LocalShell,
                    OpenAI::Responses::WebSearchTool
                  )
                ],
              top_logprobs: T.nilable(Integer),
              top_p: T.nilable(Float),
              truncation:
                T.nilable(
                  OpenAI::Responses::ResponseCreateParams::Truncation::OrSymbol
                ),
              user: String,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              background: T.nilable(T::Boolean),
              include: T.nilable(
                T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol]
              ),
              input: OpenAI::Responses::ResponseCreateParams::Input::Variants,
              instructions: T.nilable(String),
              max_output_tokens: T.nilable(Integer),
              max_tool_calls: T.nilable(Integer),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: T.any(
                String,
                OpenAI::ChatModel::OrSymbol,
                OpenAI::ResponsesModel::ResponsesOnlyModel::OrSymbol
              ),
              parallel_tool_calls: T.nilable(T::Boolean),
              previous_response_id: T.nilable(String),
              prompt: T.nilable(OpenAI::Responses::ResponsePrompt::OrHash),
              prompt_cache_key: String,
              reasoning: T.nilable(OpenAI::Reasoning::OrHash),
              safety_identifier: String,
              service_tier: T.nilable(
                OpenAI::Responses::ResponseCreateParams::ServiceTier::OrSymbol
              ),
              store: T.nilable(T::Boolean),
              temperature: T.nilable(Float),
              text: OpenAI::Responses::ResponseTextConfig::OrHash,
              tool_choice: T.any(
                OpenAI::Responses::ToolChoiceOptions::OrSymbol,
                OpenAI::Responses::ToolChoiceTypes::OrHash,
                OpenAI::Responses::ToolChoiceFunction::OrHash,
                OpenAI::Responses::ToolChoiceMcp::OrHash
              ),
              tools: T::Array[
                T.any(
                  OpenAI::Responses::FunctionTool::OrHash,
                  OpenAI::Responses::FileSearchTool::OrHash,
                  OpenAI::Responses::ComputerTool::OrHash,
                  OpenAI::Responses::Tool::Mcp::OrHash,
                  OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                  OpenAI::Responses::Tool::ImageGeneration::OrHash,
                  OpenAI::Responses::Tool::LocalShell::OrHash,
                  OpenAI::Responses::WebSearchTool::OrHash
                )
              ],
              top_logprobs: T.nilable(Integer),
              top_p: T.nilable(Float),
              truncation: T.nilable(
                OpenAI::Responses::ResponseCreateParams::Truncation::OrSymbol
              ),
              user: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            background: nil, # Whether to run the model response in the background.
                             # [Learn more](https://platform.openai.com/docs/guides/background).
            include: nil, # Specify additional output data to include in the model response. Currently
                          # supported values are:
                          # - `code_interpreter_call.outputs`: Includes the outputs of python code execution
                          #   in code interpreter tool call items.
                          # - `computer_call_output.output.image_url`: Include image urls from the computer
                          #   call output.
                          # - `file_search_call.results`: Include the search results of the file search tool
                          #   call.
                          # - `message.input_image.image_url`: Include image urls from the input message.
                          # - `message.output_text.logprobs`: Include logprobs with assistant messages.
                          # - `reasoning.encrypted_content`: Includes an encrypted version of reasoning
                          #   tokens in reasoning item outputs. This enables reasoning items to be used in
                          #   multi-turn conversations when using the Responses API statelessly (like when
                          #   the `store` parameter is set to `false`, or when an organization is enrolled
                          #   in the zero data retention program).
            input: nil, # Text, image, or file inputs to the model, used to generate a response.
                        # Learn more:
                        # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                        # - [Image inputs](https://platform.openai.com/docs/guides/images)
                        # - [File inputs](https://platform.openai.com/docs/guides/pdf-files)
                        # - [Conversation state](https://platform.openai.com/docs/guides/conversation-state)
                        # - [Function calling](https://platform.openai.com/docs/guides/function-calling)
            instructions: nil, # A system (or developer) message inserted into the model's context.
                               # When using along with `previous_response_id`, the instructions from a previous
                               # response will not be carried over to the next response. This makes it simple to
                               # swap out system (or developer) messages in new responses.
            max_output_tokens: nil, # An upper bound for the number of tokens that can be generated for a response,
                                    # including visible output tokens and
                                    # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
            max_tool_calls: nil, # The maximum number of total calls to built-in tools that can be processed in a
                                 # response. This maximum number applies across all built-in tool calls, not per
                                 # individual tool. Any further attempts to call a tool by the model will be
                                 # ignored.
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            model: nil, # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
                        # wide range of models with different capabilities, performance characteristics,
                        # and price points. Refer to the
                        # [model guide](https://platform.openai.com/docs/models) to browse and compare
                        # available models.
            parallel_tool_calls: nil, # Whether to allow the model to run tool calls in parallel.
            previous_response_id: nil, # The unique ID of the previous response to the model. Use this to create
                                       # multi-turn conversations. Learn more about
                                       # [conversation state](https://platform.openai.com/docs/guides/conversation-state).
            prompt: nil, # Reference to a prompt template and its variables.
                         # [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).
            prompt_cache_key: nil, # Used by OpenAI to cache responses for similar requests to optimize your cache
                                   # hit rates. Replaces the `user` field.
                                   # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
            reasoning: nil, # **o-series models only**
                            # Configuration options for
                            # [reasoning models](https://platform.openai.com/docs/guides/reasoning).
            safety_identifier: nil, # A stable identifier used to help detect users of your application that may be
                                    # violating OpenAI's usage policies. The IDs should be a string that uniquely
                                    # identifies each user. We recommend hashing their username or email address, in
                                    # order to avoid sending us any identifying information.
                                    # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
            service_tier: nil, # Specifies the processing type used for serving the request.
                               # - If set to 'auto', then the request will be processed with the service tier
                               #   configured in the Project settings. Unless otherwise configured, the Project
                               #   will use 'default'.
                               # - If set to 'default', then the request will be processed with the standard
                               #   pricing and performance for the selected model.
                               # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
                               #   'priority', then the request will be processed with the corresponding service
                               #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
                               #   Priority processing.
                               # - When not set, the default behavior is 'auto'.
                               # When the `service_tier` parameter is set, the response body will include the
                               # `service_tier` value based on the processing mode actually used to serve the
                               # request. This response value may be different from the value set in the
                               # parameter.
            store: nil, # Whether to store the generated model response for later retrieval via API.
            temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                              # make the output more random, while lower values like 0.2 will make it more
                              # focused and deterministic. We generally recommend altering this or `top_p` but
                              # not both.
            text: nil, # Configuration options for a text response from the model. Can be plain text or
                       # structured JSON data. Learn more:
                       # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                       # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
            tool_choice: nil, # How the model should select which tool (or tools) to use when generating a
                              # response. See the `tools` parameter to see how to specify which tools the model
                              # can call.
            tools: nil, # An array of tools the model may call while generating a response. You can
                        # specify which tool to use by setting the `tool_choice` parameter.
                        # The two categories of tools you can provide the model are:
                        # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
                        #   capabilities, like
                        #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
                        #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
                        #   Learn more about
                        #   [built-in tools](https://platform.openai.com/docs/guides/tools).
                        # - **Function calls (custom tools)**: Functions that are defined by you, enabling
                        #   the model to call your own code. Learn more about
                        #   [function calling](https://platform.openai.com/docs/guides/function-calling).
            top_logprobs: nil, # An integer between 0 and 20 specifying the number of most likely tokens to
                               # return at each token position, each with an associated log probability.
            top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                        # model considers the results of the tokens with top_p probability mass. So 0.1
                        # means only the tokens comprising the top 10% probability mass are considered.
                        # We generally recommend altering this or `temperature` but not both.
            truncation: nil, # The truncation strategy to use for the model response.
                             # - `auto`: If the context of this response and previous ones exceeds the model's
                             #   context window size, the model will truncate the response to fit the context
                             #   window by dropping input items in the middle of the conversation.
                             # - `disabled` (default): If a model response will exceed the context window size
                             #   for a model, the request will fail with a 400 error.
            user: nil, # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
                       # `prompt_cache_key` instead to maintain caching optimizations. A stable
                       # identifier for your end-users. Used to boost cache hit rates by better bucketing
                       # similar requests and to help OpenAI detect and prevent abuse.
                       # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
            request_options: {}
); end
        end

        # Text, image, or file inputs to the model, used to generate a response.
        #
        # Learn more:
        #
        # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
        # - [Image inputs](https://platform.openai.com/docs/guides/images)
        # - [File inputs](https://platform.openai.com/docs/guides/pdf-files)
        # - [Conversation state](https://platform.openai.com/docs/guides/conversation-state)
        # - [Function calling](https://platform.openai.com/docs/guides/function-calling)
        module Input
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::ResponseCreateParams::Input::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                String,
                T::Array[OpenAI::Responses::ResponseInputItem::Variants]
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCreateParams,
              OpenAI::Internal::AnyHash
            )
          end

        # Specifies the processing type used for serving the request.
        #
        # - If set to 'auto', then the request will be processed with the service tier
        #   configured in the Project settings. Unless otherwise configured, the Project
        #   will use 'default'.
        # - If set to 'default', then the request will be processed with the standard
        #   pricing and performance for the selected model.
        # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
        #   'priority', then the request will be processed with the corresponding service
        #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
        #   Priority processing.
        # - When not set, the default behavior is 'auto'.
        #
        # When the `service_tier` parameter is set, the response body will include the
        # `service_tier` value based on the processing mode actually used to serve the
        # request. This response value may be different from the value set in the
        # parameter.
        module ServiceTier
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseCreateParams::ServiceTier::TaggedSymbol
              ])
            end
            def values; end
          end

          AUTO = T.let(
              :auto,
              OpenAI::Responses::ResponseCreateParams::ServiceTier::TaggedSymbol
            )

          DEFAULT = T.let(
              :default,
              OpenAI::Responses::ResponseCreateParams::ServiceTier::TaggedSymbol
            )

          FLEX = T.let(
              :flex,
              OpenAI::Responses::ResponseCreateParams::ServiceTier::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          PRIORITY = T.let(
              :priority,
              OpenAI::Responses::ResponseCreateParams::ServiceTier::TaggedSymbol
            )

          SCALE = T.let(
              :scale,
              OpenAI::Responses::ResponseCreateParams::ServiceTier::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Responses::ResponseCreateParams::ServiceTier
              )
            end
        end

        # How the model should select which tool (or tools) to use when generating a
        # response. See the `tools` parameter to see how to specify which tools the model
        # can call.
        module ToolChoice
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseCreateParams::ToolChoice::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Responses::ToolChoiceOptions::TaggedSymbol,
                OpenAI::Responses::ToolChoiceTypes,
                OpenAI::Responses::ToolChoiceFunction,
                OpenAI::Responses::ToolChoiceMcp
              )
            end
        end

        # The truncation strategy to use for the model response.
        #
        # - `auto`: If the context of this response and previous ones exceeds the model's
        #   context window size, the model will truncate the response to fit the context
        #   window by dropping input items in the middle of the conversation.
        # - `disabled` (default): If a model response will exceed the context window size
        #   for a model, the request will fail with a 400 error.
        module Truncation
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseCreateParams::Truncation::TaggedSymbol
              ])
            end
            def values; end
          end

          AUTO = T.let(
              :auto,
              OpenAI::Responses::ResponseCreateParams::Truncation::TaggedSymbol
            )

          DISABLED = T.let(
              :disabled,
              OpenAI::Responses::ResponseCreateParams::Truncation::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseCreateParams::Truncation)
            end
        end
      end

      class ResponseCreatedEvent < OpenAI::Internal::Type::BaseModel
        # The response that was created.
        sig { returns(OpenAI::Responses::Response) }
        attr_reader :response

        sig { params(response: OpenAI::Responses::Response::OrHash).void }
        attr_writer :response

        # The sequence number for this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.created`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              response: OpenAI::Responses::Response,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # An event that is emitted when a response is created.
          sig do
            params(
              response: OpenAI::Responses::Response::OrHash,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            response:, # The response that was created.
            sequence_number:, # The sequence number for this event.
            type: :"response.created" # The type of the event. Always `response.created`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseCreatedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseDeleteParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseDeleteParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseError < OpenAI::Internal::Type::BaseModel
        # The error code for the response.
        sig { returns(OpenAI::Responses::ResponseError::Code::TaggedSymbol) }
        attr_accessor :code

        # A human-readable description of the error.
        sig { returns(String) }
        attr_accessor :message

        sig do
          override
            .returns({
              code: OpenAI::Responses::ResponseError::Code::TaggedSymbol,
              message: String
            })
        end
        def to_hash; end

        class << self
          # An error object returned when the model fails to generate a Response.
          sig do
            params(
              code: OpenAI::Responses::ResponseError::Code::OrSymbol,
              message: String
            ).returns(T.attached_class)
          end
          def new(
            code:, # The error code for the response.
            message: # A human-readable description of the error.
); end
        end

        # The error code for the response.
        module Code
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::ResponseError::Code::TaggedSymbol]) }
            def values; end
          end

          EMPTY_IMAGE_FILE = T.let(
              :empty_image_file,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          FAILED_TO_DOWNLOAD_IMAGE = T.let(
              :failed_to_download_image,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          IMAGE_CONTENT_POLICY_VIOLATION = T.let(
              :image_content_policy_violation,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          IMAGE_FILE_NOT_FOUND = T.let(
              :image_file_not_found,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          IMAGE_FILE_TOO_LARGE = T.let(
              :image_file_too_large,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          IMAGE_PARSE_ERROR = T.let(
              :image_parse_error,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          IMAGE_TOO_LARGE = T.let(
              :image_too_large,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          IMAGE_TOO_SMALL = T.let(
              :image_too_small,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          INVALID_BASE64_IMAGE = T.let(
              :invalid_base64_image,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          INVALID_IMAGE = T.let(
              :invalid_image,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          INVALID_IMAGE_FORMAT = T.let(
              :invalid_image_format,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          INVALID_IMAGE_MODE = T.let(
              :invalid_image_mode,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          INVALID_IMAGE_URL = T.let(
              :invalid_image_url,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          INVALID_PROMPT = T.let(
              :invalid_prompt,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          RATE_LIMIT_EXCEEDED = T.let(
              :rate_limit_exceeded,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          SERVER_ERROR = T.let(
              :server_error,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseError::Code)
            end

          UNSUPPORTED_IMAGE_MEDIA_TYPE = T.let(
              :unsupported_image_media_type,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )

          VECTOR_STORE_TIMEOUT = T.let(
              :vector_store_timeout,
              OpenAI::Responses::ResponseError::Code::TaggedSymbol
            )
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::ResponseError, OpenAI::Internal::AnyHash)
          end
      end

      class ResponseErrorEvent < OpenAI::Internal::Type::BaseModel
        # The error code.
        sig { returns(T.nilable(String)) }
        attr_accessor :code

        # The error message.
        sig { returns(String) }
        attr_accessor :message

        # The error parameter.
        sig { returns(T.nilable(String)) }
        attr_accessor :param

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `error`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              code: T.nilable(String),
              message: String,
              param: T.nilable(String),
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when an error occurs.
          sig do
            params(
              code: T.nilable(String),
              message: String,
              param: T.nilable(String),
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            code:, # The error code.
            message:, # The error message.
            param:, # The error parameter.
            sequence_number:, # The sequence number of this event.
            type: :error # The type of the event. Always `error`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseErrorEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFailedEvent < OpenAI::Internal::Type::BaseModel
        # The response that failed.
        sig { returns(OpenAI::Responses::Response) }
        attr_reader :response

        sig { params(response: OpenAI::Responses::Response::OrHash).void }
        attr_writer :response

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.failed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              response: OpenAI::Responses::Response,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # An event that is emitted when a response fails.
          sig do
            params(
              response: OpenAI::Responses::Response::OrHash,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            response:, # The response that failed.
            sequence_number:, # The sequence number of this event.
            type: :"response.failed" # The type of the event. Always `response.failed`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFailedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFileSearchCallCompletedEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the output item that the file search call is initiated.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the file search call is initiated.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.file_search_call.completed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a file search call is completed (results found).
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the output item that the file search call is initiated.
            output_index:, # The index of the output item that the file search call is initiated.
            sequence_number:, # The sequence number of this event.
            type: :"response.file_search_call.completed" # The type of the event. Always `response.file_search_call.completed`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFileSearchCallCompletedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFileSearchCallInProgressEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the output item that the file search call is initiated.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the file search call is initiated.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.file_search_call.in_progress`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a file search call is initiated.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the output item that the file search call is initiated.
            output_index:, # The index of the output item that the file search call is initiated.
            sequence_number:, # The sequence number of this event.
            type: :"response.file_search_call.in_progress" # The type of the event. Always `response.file_search_call.in_progress`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFileSearchCallInProgressEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFileSearchCallSearchingEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the output item that the file search call is initiated.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the file search call is searching.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.file_search_call.searching`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a file search is currently searching.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the output item that the file search call is initiated.
            output_index:, # The index of the output item that the file search call is searching.
            sequence_number:, # The sequence number of this event.
            type: :"response.file_search_call.searching" # The type of the event. Always `response.file_search_call.searching`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFileSearchCallSearchingEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFileSearchToolCall < OpenAI::Internal::Type::BaseModel
        # The unique ID of the file search tool call.
        sig { returns(String) }
        attr_accessor :id

        # The queries used to search for files.
        sig { returns(T::Array[String]) }
        attr_accessor :queries

        # The results of the file search tool call.
        sig do
          returns(T.nilable(
              T::Array[OpenAI::Responses::ResponseFileSearchToolCall::Result]
            ))
        end
        attr_accessor :results

        # The status of the file search tool call. One of `in_progress`, `searching`,
        # `incomplete` or `failed`,
        sig { returns(OpenAI::Responses::ResponseFileSearchToolCall::Status::OrSymbol) }
        attr_accessor :status

        # The type of the file search tool call. Always `file_search_call`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              queries: T::Array[String],
              status:
                OpenAI::Responses::ResponseFileSearchToolCall::Status::OrSymbol,
              type: Symbol,
              results:
                T.nilable(
                  T::Array[
                    OpenAI::Responses::ResponseFileSearchToolCall::Result
                  ]
                )
            })
        end
        def to_hash; end

        class << self
          # The results of a file search tool call. See the
          # [file search guide](https://platform.openai.com/docs/guides/tools-file-search)
          # for more information.
          sig do
            params(
              id: String,
              queries: T::Array[String],
              status: OpenAI::Responses::ResponseFileSearchToolCall::Status::OrSymbol,
              results: T.nilable(
                T::Array[
                  OpenAI::Responses::ResponseFileSearchToolCall::Result::OrHash
                ]
              ),
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the file search tool call.
            queries:, # The queries used to search for files.
            status:, # The status of the file search tool call. One of `in_progress`, `searching`,
                     # `incomplete` or `failed`,
            results: nil, # The results of the file search tool call.
            type: :file_search_call # The type of the file search tool call. Always `file_search_call`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFileSearchToolCall,
              OpenAI::Internal::AnyHash
            )
          end

        class Result < OpenAI::Internal::Type::BaseModel
          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard. Keys are strings with a maximum
          # length of 64 characters. Values are strings with a maximum length of 512
          # characters, booleans, or numbers.
          sig do
            returns(T.nilable(
                T::Hash[
                  Symbol,
                  OpenAI::Responses::ResponseFileSearchToolCall::Result::Attribute::Variants
                ]
              ))
          end
          attr_accessor :attributes

          # The unique ID of the file.
          sig { returns(T.nilable(String)) }
          attr_reader :file_id

          sig { params(file_id: String).void }
          attr_writer :file_id

          # The name of the file.
          sig { returns(T.nilable(String)) }
          attr_reader :filename

          sig { params(filename: String).void }
          attr_writer :filename

          # The relevance score of the file - a value between 0 and 1.
          sig { returns(T.nilable(Float)) }
          attr_reader :score

          sig { params(score: Float).void }
          attr_writer :score

          # The text that was retrieved from the file.
          sig { returns(T.nilable(String)) }
          attr_reader :text

          sig { params(text: String).void }
          attr_writer :text

          sig do
            override
              .returns({
                attributes:
                  T.nilable(
                    T::Hash[
                      Symbol,
                      OpenAI::Responses::ResponseFileSearchToolCall::Result::Attribute::Variants
                    ]
                  ),
                file_id: String,
                filename: String,
                score: Float,
                text: String
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                attributes: T.nilable(
                  T::Hash[
                    Symbol,
                    OpenAI::Responses::ResponseFileSearchToolCall::Result::Attribute::Variants
                  ]
                ),
                file_id: String,
                filename: String,
                score: Float,
                text: String
              ).returns(T.attached_class)
            end
            def new(
              attributes: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                               # for storing additional information about the object in a structured format, and
                               # querying for objects via API or the dashboard. Keys are strings with a maximum
                               # length of 64 characters. Values are strings with a maximum length of 512
                               # characters, booleans, or numbers.
              file_id: nil, # The unique ID of the file.
              filename: nil, # The name of the file.
              score: nil, # The relevance score of the file - a value between 0 and 1.
              text: nil # The text that was retrieved from the file.
); end
          end

          module Attribute
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseFileSearchToolCall::Result::Attribute::Variants
                ])
              end
              def variants; end
            end

            Variants = T.type_alias { T.any(String, Float, T::Boolean) }
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseFileSearchToolCall::Result,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The status of the file search tool call. One of `in_progress`, `searching`,
        # `incomplete` or `failed`,
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseFileSearchToolCall::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseFileSearchToolCall::Status::TaggedSymbol
            )

          FAILED = T.let(
              :failed,
              OpenAI::Responses::ResponseFileSearchToolCall::Status::TaggedSymbol
            )

          INCOMPLETE = T.let(
              :incomplete,
              OpenAI::Responses::ResponseFileSearchToolCall::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseFileSearchToolCall::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          SEARCHING = T.let(
              :searching,
              OpenAI::Responses::ResponseFileSearchToolCall::Status::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Responses::ResponseFileSearchToolCall::Status
              )
            end
        end
      end

      # An object specifying the format that the model must output.
      #
      # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
      # ensures the model will match your supplied JSON schema. Learn more in the
      # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
      #
      # The default format is `{ "type": "text" }` with no additional options.
      #
      # **Not recommended for gpt-4o and newer models:**
      #
      # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
      # ensures the message the model generates is valid JSON. Using `json_schema` is
      # preferred for models that support it.
      module ResponseFormatTextConfig
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ResponseFormatTextConfig::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::ResponseFormatText,
              OpenAI::Responses::ResponseFormatTextJSONSchemaConfig,
              OpenAI::ResponseFormatJSONObject
            )
          end
      end

      class ResponseFormatTextJSONSchemaConfig < OpenAI::Internal::Type::BaseModel
        # A description of what the response format is for, used by the model to determine
        # how to respond in the format.
        sig { returns(T.nilable(String)) }
        attr_reader :description

        sig { params(description: String).void }
        attr_writer :description

        # The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores
        # and dashes, with a maximum length of 64.
        sig { returns(String) }
        attr_accessor :name

        # The schema for the response format, described as a JSON Schema object. Learn how
        # to build JSON schemas [here](https://json-schema.org/).
        sig { returns(T::Hash[Symbol, T.anything]) }
        attr_accessor :schema

        # Whether to enable strict schema adherence when generating the output. If set to
        # true, the model will always follow the exact schema defined in the `schema`
        # field. Only a subset of JSON Schema is supported when `strict` is `true`. To
        # learn more, read the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :strict

        # The type of response format being defined. Always `json_schema`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              name: String,
              schema: T::Hash[Symbol, T.anything],
              type: Symbol,
              description: String,
              strict: T.nilable(T::Boolean)
            })
        end
        def to_hash; end

        class << self
          # JSON Schema response format. Used to generate structured JSON responses. Learn
          # more about
          # [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs).
          sig do
            params(
              name: String,
              schema: T::Hash[Symbol, T.anything],
              description: String,
              strict: T.nilable(T::Boolean),
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            name:, # The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores
                   # and dashes, with a maximum length of 64.
            schema:, # The schema for the response format, described as a JSON Schema object. Learn how
                     # to build JSON schemas [here](https://json-schema.org/).
            description: nil, # A description of what the response format is for, used by the model to determine
                              # how to respond in the format.
            strict: nil, # Whether to enable strict schema adherence when generating the output. If set to
                         # true, the model will always follow the exact schema defined in the `schema`
                         # field. Only a subset of JSON Schema is supported when `strict` is `true`. To
                         # learn more, read the
                         # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
            type: :json_schema # The type of response format being defined. Always `json_schema`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFormatTextJSONSchemaConfig,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFunctionCallArgumentsDeltaEvent < OpenAI::Internal::Type::BaseModel
        # The function-call arguments delta that is added.
        sig { returns(String) }
        attr_accessor :delta

        # The ID of the output item that the function-call arguments delta is added to.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the function-call arguments delta is added to.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.function_call_arguments.delta`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when there is a partial function-call arguments delta.
          sig do
            params(
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            delta:, # The function-call arguments delta that is added.
            item_id:, # The ID of the output item that the function-call arguments delta is added to.
            output_index:, # The index of the output item that the function-call arguments delta is added to.
            sequence_number:, # The sequence number of this event.
            type: :"response.function_call_arguments.delta" # The type of the event. Always `response.function_call_arguments.delta`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFunctionCallArgumentsDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFunctionCallArgumentsDoneEvent < OpenAI::Internal::Type::BaseModel
        # The function-call arguments.
        sig { returns(String) }
        attr_accessor :arguments

        # The ID of the item.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              arguments: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when function-call arguments are finalized.
          sig do
            params(
              arguments: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            arguments:, # The function-call arguments.
            item_id:, # The ID of the item.
            output_index:, # The index of the output item.
            sequence_number:, # The sequence number of this event.
            type: :"response.function_call_arguments.done"
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFunctionCallArgumentsDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFunctionToolCall < OpenAI::Internal::Type::BaseModel
        # A JSON string of the arguments to pass to the function.
        sig { returns(String) }
        attr_accessor :arguments

        # The unique ID of the function tool call generated by the model.
        sig { returns(String) }
        attr_accessor :call_id

        # The unique ID of the function tool call.
        sig { returns(T.nilable(String)) }
        attr_reader :id

        sig { params(id: String).void }
        attr_writer :id

        # The name of the function to run.
        sig { returns(String) }
        attr_accessor :name

        # The parsed contents of the arguments.
        sig { returns(T.anything) }
        attr_accessor :parsed

        # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        sig do
          returns(T.nilable(
              OpenAI::Responses::ResponseFunctionToolCall::Status::OrSymbol
            ))
        end
        attr_reader :status

        sig { params(status: OpenAI::Responses::ResponseFunctionToolCall::Status::OrSymbol).void }
        attr_writer :status

        # The type of the function tool call. Always `function_call`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              arguments: String,
              call_id: String,
              name: String,
              type: Symbol,
              id: String,
              status:
                OpenAI::Responses::ResponseFunctionToolCall::Status::OrSymbol
            })
        end
        def to_hash; end

        class << self
          # A tool call to run a function. See the
          # [function calling guide](https://platform.openai.com/docs/guides/function-calling)
          # for more information.
          sig do
            params(
              arguments: String,
              call_id: String,
              name: String,
              id: String,
              status: OpenAI::Responses::ResponseFunctionToolCall::Status::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            arguments:, # A JSON string of the arguments to pass to the function.
            call_id:, # The unique ID of the function tool call generated by the model.
            name:, # The name of the function to run.
            id: nil, # The unique ID of the function tool call.
            status: nil, # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
                         # Populated when items are returned via API.
            type: :function_call # The type of the function tool call. Always `function_call`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFunctionToolCall,
              OpenAI::Internal::AnyHash
            )
          end

        # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseFunctionToolCall::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseFunctionToolCall::Status::TaggedSymbol
            )

          INCOMPLETE = T.let(
              :incomplete,
              OpenAI::Responses::ResponseFunctionToolCall::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseFunctionToolCall::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseFunctionToolCall::Status)
            end
        end
      end

      class ResponseFunctionToolCallItem < OpenAI::Models::Responses::ResponseFunctionToolCall
        # The unique ID of the function tool call.
        sig { returns(String) }
        attr_accessor :id

        sig { override.returns({ id: String }) }
        def to_hash; end

        class << self
          # A tool call to run a function. See the
          # [function calling guide](https://platform.openai.com/docs/guides/function-calling)
          # for more information.
          sig { params(id: String).returns(T.attached_class) }
          def new(
            id: # The unique ID of the function tool call.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFunctionToolCallItem,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFunctionToolCallOutputItem < OpenAI::Internal::Type::BaseModel
        # The unique ID of the function tool call generated by the model.
        sig { returns(String) }
        attr_accessor :call_id

        # The unique ID of the function call tool output.
        sig { returns(String) }
        attr_accessor :id

        # A JSON string of the output of the function tool call.
        sig { returns(String) }
        attr_accessor :output

        # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        sig do
          returns(T.nilable(
              OpenAI::Responses::ResponseFunctionToolCallOutputItem::Status::TaggedSymbol
            ))
        end
        attr_reader :status

        sig { params(status: OpenAI::Responses::ResponseFunctionToolCallOutputItem::Status::OrSymbol).void }
        attr_writer :status

        # The type of the function tool call output. Always `function_call_output`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              call_id: String,
              output: String,
              type: Symbol,
              status:
                OpenAI::Responses::ResponseFunctionToolCallOutputItem::Status::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              id: String,
              call_id: String,
              output: String,
              status: OpenAI::Responses::ResponseFunctionToolCallOutputItem::Status::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the function call tool output.
            call_id:, # The unique ID of the function tool call generated by the model.
            output:, # A JSON string of the output of the function tool call.
            status: nil, # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
                         # Populated when items are returned via API.
            type: :function_call_output # The type of the function tool call output. Always `function_call_output`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFunctionToolCallOutputItem,
              OpenAI::Internal::AnyHash
            )
          end

        # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseFunctionToolCallOutputItem::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseFunctionToolCallOutputItem::Status::TaggedSymbol
            )

          INCOMPLETE = T.let(
              :incomplete,
              OpenAI::Responses::ResponseFunctionToolCallOutputItem::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseFunctionToolCallOutputItem::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Responses::ResponseFunctionToolCallOutputItem::Status
              )
            end
        end
      end

      class ResponseFunctionWebSearch < OpenAI::Internal::Type::BaseModel
        # An object describing the specific action taken in this web search call. Includes
        # details on how the model used the web (search, open_page, find).
        sig do
          returns(T.any(
              OpenAI::Responses::ResponseFunctionWebSearch::Action::Search,
              OpenAI::Responses::ResponseFunctionWebSearch::Action::OpenPage,
              OpenAI::Responses::ResponseFunctionWebSearch::Action::Find
            ))
        end
        attr_accessor :action

        # The unique ID of the web search tool call.
        sig { returns(String) }
        attr_accessor :id

        # The status of the web search tool call.
        sig { returns(OpenAI::Responses::ResponseFunctionWebSearch::Status::OrSymbol) }
        attr_accessor :status

        # The type of the web search tool call. Always `web_search_call`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              action:
                T.any(
                  OpenAI::Responses::ResponseFunctionWebSearch::Action::Search,
                  OpenAI::Responses::ResponseFunctionWebSearch::Action::OpenPage,
                  OpenAI::Responses::ResponseFunctionWebSearch::Action::Find
                ),
              status:
                OpenAI::Responses::ResponseFunctionWebSearch::Status::OrSymbol,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # The results of a web search tool call. See the
          # [web search guide](https://platform.openai.com/docs/guides/tools-web-search) for
          # more information.
          sig do
            params(
              id: String,
              action: T.any(
                OpenAI::Responses::ResponseFunctionWebSearch::Action::Search::OrHash,
                OpenAI::Responses::ResponseFunctionWebSearch::Action::OpenPage::OrHash,
                OpenAI::Responses::ResponseFunctionWebSearch::Action::Find::OrHash
              ),
              status: OpenAI::Responses::ResponseFunctionWebSearch::Status::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the web search tool call.
            action:, # An object describing the specific action taken in this web search call. Includes
                     # details on how the model used the web (search, open_page, find).
            status:, # The status of the web search tool call.
            type: :web_search_call # The type of the web search tool call. Always `web_search_call`.
); end
        end

        # An object describing the specific action taken in this web search call. Includes
        # details on how the model used the web (search, open_page, find).
        module Action
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseFunctionWebSearch::Action::Variants
              ])
            end
            def variants; end
          end

          class Find < OpenAI::Internal::Type::BaseModel
            # The pattern or text to search for within the page.
            sig { returns(String) }
            attr_accessor :pattern

            # The action type.
            sig { returns(Symbol) }
            attr_accessor :type

            # The URL of the page searched for the pattern.
            sig { returns(String) }
            attr_accessor :url

            sig { override.returns({ pattern: String, type: Symbol, url: String }) }
            def to_hash; end

            class << self
              # Action type "find": Searches for a pattern within a loaded page.
              sig { params(pattern: String, url: String, type: Symbol).returns(T.attached_class) }
              def new(
                pattern:, # The pattern or text to search for within the page.
                url:, # The URL of the page searched for the pattern.
                type: :find # The action type.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseFunctionWebSearch::Action::Find,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class OpenPage < OpenAI::Internal::Type::BaseModel
            # The action type.
            sig { returns(Symbol) }
            attr_accessor :type

            # The URL opened by the model.
            sig { returns(String) }
            attr_accessor :url

            sig { override.returns({ type: Symbol, url: String }) }
            def to_hash; end

            class << self
              # Action type "open_page" - Opens a specific URL from search results.
              sig { params(url: String, type: Symbol).returns(T.attached_class) }
              def new(
                url:, # The URL opened by the model.
                type: :open_page # The action type.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseFunctionWebSearch::Action::OpenPage,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class Search < OpenAI::Internal::Type::BaseModel
            # The search query.
            sig { returns(String) }
            attr_accessor :query

            # The action type.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ query: String, type: Symbol }) }
            def to_hash; end

            class << self
              # Action type "search" - Performs a web search query.
              sig { params(query: String, type: Symbol).returns(T.attached_class) }
              def new(
                query:, # The search query.
                type: :search # The action type.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseFunctionWebSearch::Action::Search,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseFunctionWebSearch::Action::Search,
                OpenAI::Responses::ResponseFunctionWebSearch::Action::OpenPage,
                OpenAI::Responses::ResponseFunctionWebSearch::Action::Find
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseFunctionWebSearch,
              OpenAI::Internal::AnyHash
            )
          end

        # The status of the web search tool call.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseFunctionWebSearch::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseFunctionWebSearch::Status::TaggedSymbol
            )

          FAILED = T.let(
              :failed,
              OpenAI::Responses::ResponseFunctionWebSearch::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseFunctionWebSearch::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          SEARCHING = T.let(
              :searching,
              OpenAI::Responses::ResponseFunctionWebSearch::Status::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Responses::ResponseFunctionWebSearch::Status
              )
            end
        end
      end

      class ResponseImageGenCallCompletedEvent < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the image generation item being processed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.image_generation_call.completed'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when an image generation tool call has completed and the final image is
          # available.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The unique identifier of the image generation item being processed.
            output_index:, # The index of the output item in the response's output array.
            sequence_number:, # The sequence number of this event.
            type: :"response.image_generation_call.completed" # The type of the event. Always 'response.image_generation_call.completed'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseImageGenCallCompletedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseImageGenCallGeneratingEvent < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the image generation item being processed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of the image generation item being processed.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.image_generation_call.generating'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when an image generation tool call is actively generating an image
          # (intermediate state).
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The unique identifier of the image generation item being processed.
            output_index:, # The index of the output item in the response's output array.
            sequence_number:, # The sequence number of the image generation item being processed.
            type: :"response.image_generation_call.generating" # The type of the event. Always 'response.image_generation_call.generating'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseImageGenCallGeneratingEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseImageGenCallInProgressEvent < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the image generation item being processed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of the image generation item being processed.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.image_generation_call.in_progress'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when an image generation tool call is in progress.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The unique identifier of the image generation item being processed.
            output_index:, # The index of the output item in the response's output array.
            sequence_number:, # The sequence number of the image generation item being processed.
            type: :"response.image_generation_call.in_progress" # The type of the event. Always 'response.image_generation_call.in_progress'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseImageGenCallInProgressEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseImageGenCallPartialImageEvent < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the image generation item being processed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # Base64-encoded partial image data, suitable for rendering as an image.
        sig { returns(String) }
        attr_accessor :partial_image_b64

        # 0-based index for the partial image (backend is 1-based, but this is 0-based for
        # the user).
        sig { returns(Integer) }
        attr_accessor :partial_image_index

        # The sequence number of the image generation item being processed.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.image_generation_call.partial_image'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              partial_image_b64: String,
              partial_image_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a partial image is available during image generation streaming.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              partial_image_b64: String,
              partial_image_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The unique identifier of the image generation item being processed.
            output_index:, # The index of the output item in the response's output array.
            partial_image_b64:, # Base64-encoded partial image data, suitable for rendering as an image.
            partial_image_index:, # 0-based index for the partial image (backend is 1-based, but this is 0-based for
                                  # the user).
            sequence_number:, # The sequence number of the image generation item being processed.
            type: :"response.image_generation_call.partial_image" # The type of the event. Always 'response.image_generation_call.partial_image'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseImageGenCallPartialImageEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseInProgressEvent < OpenAI::Internal::Type::BaseModel
        # The response that is in progress.
        sig { returns(OpenAI::Responses::Response) }
        attr_reader :response

        sig { params(response: OpenAI::Responses::Response::OrHash).void }
        attr_writer :response

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.in_progress`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              response: OpenAI::Responses::Response,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the response is in progress.
          sig do
            params(
              response: OpenAI::Responses::Response::OrHash,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            response:, # The response that is in progress.
            sequence_number:, # The sequence number of this event.
            type: :"response.in_progress" # The type of the event. Always `response.in_progress`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseInProgressEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # Specify additional output data to include in the model response. Currently
      # supported values are:
      #
      # - `code_interpreter_call.outputs`: Includes the outputs of python code execution
      #   in code interpreter tool call items.
      # - `computer_call_output.output.image_url`: Include image urls from the computer
      #   call output.
      # - `file_search_call.results`: Include the search results of the file search tool
      #   call.
      # - `message.input_image.image_url`: Include image urls from the input message.
      # - `message.output_text.logprobs`: Include logprobs with assistant messages.
      # - `reasoning.encrypted_content`: Includes an encrypted version of reasoning
      #   tokens in reasoning item outputs. This enables reasoning items to be used in
      #   multi-turn conversations when using the Responses API statelessly (like when
      #   the `store` parameter is set to `false`, or when an organization is enrolled
      #   in the zero data retention program).
      module ResponseIncludable
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ResponseIncludable::TaggedSymbol]) }
          def values; end
        end

        CODE_INTERPRETER_CALL_OUTPUTS = T.let(
            :"code_interpreter_call.outputs",
            OpenAI::Responses::ResponseIncludable::TaggedSymbol
          )

        COMPUTER_CALL_OUTPUT_OUTPUT_IMAGE_URL = T.let(
            :"computer_call_output.output.image_url",
            OpenAI::Responses::ResponseIncludable::TaggedSymbol
          )

        FILE_SEARCH_CALL_RESULTS = T.let(
            :"file_search_call.results",
            OpenAI::Responses::ResponseIncludable::TaggedSymbol
          )

        MESSAGE_INPUT_IMAGE_IMAGE_URL = T.let(
            :"message.input_image.image_url",
            OpenAI::Responses::ResponseIncludable::TaggedSymbol
          )

        MESSAGE_OUTPUT_TEXT_LOGPROBS = T.let(
            :"message.output_text.logprobs",
            OpenAI::Responses::ResponseIncludable::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        REASONING_ENCRYPTED_CONTENT = T.let(
            :"reasoning.encrypted_content",
            OpenAI::Responses::ResponseIncludable::TaggedSymbol
          )

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Responses::ResponseIncludable) }
      end

      class ResponseIncompleteEvent < OpenAI::Internal::Type::BaseModel
        # The response that was incomplete.
        sig { returns(OpenAI::Responses::Response) }
        attr_reader :response

        sig { params(response: OpenAI::Responses::Response::OrHash).void }
        attr_writer :response

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.incomplete`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              response: OpenAI::Responses::Response,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # An event that is emitted when a response finishes as incomplete.
          sig do
            params(
              response: OpenAI::Responses::Response::OrHash,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            response:, # The response that was incomplete.
            sequence_number:, # The sequence number of this event.
            type: :"response.incomplete" # The type of the event. Always `response.incomplete`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseIncompleteEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      ResponseInput = T.let(
          OpenAI::Internal::Type::ArrayOf[
            union: OpenAI::Responses::ResponseInputItem
          ],
          OpenAI::Internal::Type::Converter
        )

      class ResponseInputAudio < OpenAI::Internal::Type::BaseModel
        # Base64-encoded audio data.
        sig { returns(String) }
        attr_accessor :data

        # The format of the audio data. Currently supported formats are `mp3` and `wav`.
        sig { returns(OpenAI::Responses::ResponseInputAudio::Format::OrSymbol) }
        attr_accessor :format_

        # The type of the input item. Always `input_audio`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              data: String,
              format_: OpenAI::Responses::ResponseInputAudio::Format::OrSymbol,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # An audio input to the model.
          sig do
            params(
              data: String,
              format_: OpenAI::Responses::ResponseInputAudio::Format::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            data:, # Base64-encoded audio data.
            format_:, # The format of the audio data. Currently supported formats are `mp3` and `wav`.
            type: :input_audio # The type of the input item. Always `input_audio`.
); end
        end

        # The format of the audio data. Currently supported formats are `mp3` and `wav`.
        module Format
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseInputAudio::Format::TaggedSymbol
              ])
            end
            def values; end
          end

          MP3 = T.let(
              :mp3,
              OpenAI::Responses::ResponseInputAudio::Format::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseInputAudio::Format)
            end

          WAV = T.let(
              :wav,
              OpenAI::Responses::ResponseInputAudio::Format::TaggedSymbol
            )
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseInputAudio,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # A text input to the model.
      module ResponseInputContent
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ResponseInputContent::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseInputText,
              OpenAI::Responses::ResponseInputImage,
              OpenAI::Responses::ResponseInputFile
            )
          end
      end

      class ResponseInputFile < OpenAI::Internal::Type::BaseModel
        # The content of the file to be sent to the model.
        sig { returns(T.nilable(String)) }
        attr_reader :file_data

        sig { params(file_data: String).void }
        attr_writer :file_data

        # The ID of the file to be sent to the model.
        sig { returns(T.nilable(String)) }
        attr_accessor :file_id

        # The URL of the file to be sent to the model.
        sig { returns(T.nilable(String)) }
        attr_reader :file_url

        sig { params(file_url: String).void }
        attr_writer :file_url

        # The name of the file to be sent to the model.
        sig { returns(T.nilable(String)) }
        attr_reader :filename

        sig { params(filename: String).void }
        attr_writer :filename

        # The type of the input item. Always `input_file`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              type: Symbol,
              file_data: String,
              file_id: T.nilable(String),
              file_url: String,
              filename: String
            })
        end
        def to_hash; end

        class << self
          # A file input to the model.
          sig do
            params(
              file_data: String,
              file_id: T.nilable(String),
              file_url: String,
              filename: String,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            file_data: nil, # The content of the file to be sent to the model.
            file_id: nil, # The ID of the file to be sent to the model.
            file_url: nil, # The URL of the file to be sent to the model.
            filename: nil, # The name of the file to be sent to the model.
            type: :input_file # The type of the input item. Always `input_file`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseInputFile,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseInputImage < OpenAI::Internal::Type::BaseModel
        # The detail level of the image to be sent to the model. One of `high`, `low`, or
        # `auto`. Defaults to `auto`.
        sig { returns(OpenAI::Responses::ResponseInputImage::Detail::OrSymbol) }
        attr_accessor :detail

        # The ID of the file to be sent to the model.
        sig { returns(T.nilable(String)) }
        attr_accessor :file_id

        # The URL of the image to be sent to the model. A fully qualified URL or base64
        # encoded image in a data URL.
        sig { returns(T.nilable(String)) }
        attr_accessor :image_url

        # The type of the input item. Always `input_image`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              detail: OpenAI::Responses::ResponseInputImage::Detail::OrSymbol,
              type: Symbol,
              file_id: T.nilable(String),
              image_url: T.nilable(String)
            })
        end
        def to_hash; end

        class << self
          # An image input to the model. Learn about
          # [image inputs](https://platform.openai.com/docs/guides/vision).
          sig do
            params(
              detail: OpenAI::Responses::ResponseInputImage::Detail::OrSymbol,
              file_id: T.nilable(String),
              image_url: T.nilable(String),
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            detail:, # The detail level of the image to be sent to the model. One of `high`, `low`, or
                     # `auto`. Defaults to `auto`.
            file_id: nil, # The ID of the file to be sent to the model.
            image_url: nil, # The URL of the image to be sent to the model. A fully qualified URL or base64
                            # encoded image in a data URL.
            type: :input_image # The type of the input item. Always `input_image`.
); end
        end

        # The detail level of the image to be sent to the model. One of `high`, `low`, or
        # `auto`. Defaults to `auto`.
        module Detail
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseInputImage::Detail::TaggedSymbol
              ])
            end
            def values; end
          end

          AUTO = T.let(
              :auto,
              OpenAI::Responses::ResponseInputImage::Detail::TaggedSymbol
            )

          HIGH = T.let(
              :high,
              OpenAI::Responses::ResponseInputImage::Detail::TaggedSymbol
            )

          LOW = T.let(
              :low,
              OpenAI::Responses::ResponseInputImage::Detail::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseInputImage::Detail)
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseInputImage,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # A message input to the model with a role indicating instruction following
      # hierarchy. Instructions given with the `developer` or `system` role take
      # precedence over instructions given with the `user` role. Messages with the
      # `assistant` role are presumed to have been generated by the model in previous
      # interactions.
      module ResponseInputItem
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ResponseInputItem::Variants]) }
          def variants; end
        end

        class ComputerCallOutput < OpenAI::Internal::Type::BaseModel
          # The safety checks reported by the API that have been acknowledged by the
          # developer.
          sig do
            returns(T.nilable(
                T::Array[
                  OpenAI::Responses::ResponseInputItem::ComputerCallOutput::AcknowledgedSafetyCheck
                ]
              ))
          end
          attr_accessor :acknowledged_safety_checks

          # The ID of the computer tool call that produced the output.
          sig { returns(String) }
          attr_accessor :call_id

          # The ID of the computer tool call output.
          sig { returns(T.nilable(String)) }
          attr_accessor :id

          # A computer screenshot image used with the computer use tool.
          sig { returns(OpenAI::Responses::ResponseComputerToolCallOutputScreenshot) }
          attr_reader :output

          sig { params(output: OpenAI::Responses::ResponseComputerToolCallOutputScreenshot::OrHash).void }
          attr_writer :output

          # The status of the message input. One of `in_progress`, `completed`, or
          # `incomplete`. Populated when input items are returned via API.
          sig do
            returns(T.nilable(
                OpenAI::Responses::ResponseInputItem::ComputerCallOutput::Status::OrSymbol
              ))
          end
          attr_accessor :status

          # The type of the computer tool call output. Always `computer_call_output`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                call_id: String,
                output:
                  OpenAI::Responses::ResponseComputerToolCallOutputScreenshot,
                type: Symbol,
                id: T.nilable(String),
                acknowledged_safety_checks:
                  T.nilable(
                    T::Array[
                      OpenAI::Responses::ResponseInputItem::ComputerCallOutput::AcknowledgedSafetyCheck
                    ]
                  ),
                status:
                  T.nilable(
                    OpenAI::Responses::ResponseInputItem::ComputerCallOutput::Status::OrSymbol
                  )
              })
          end
          def to_hash; end

          class << self
            # The output of a computer tool call.
            sig do
              params(
                call_id: String,
                output: OpenAI::Responses::ResponseComputerToolCallOutputScreenshot::OrHash,
                id: T.nilable(String),
                acknowledged_safety_checks: T.nilable(
                  T::Array[
                    OpenAI::Responses::ResponseInputItem::ComputerCallOutput::AcknowledgedSafetyCheck::OrHash
                  ]
                ),
                status: T.nilable(
                  OpenAI::Responses::ResponseInputItem::ComputerCallOutput::Status::OrSymbol
                ),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              call_id:, # The ID of the computer tool call that produced the output.
              output:, # A computer screenshot image used with the computer use tool.
              id: nil, # The ID of the computer tool call output.
              acknowledged_safety_checks: nil, # The safety checks reported by the API that have been acknowledged by the
                                               # developer.
              status: nil, # The status of the message input. One of `in_progress`, `completed`, or
                           # `incomplete`. Populated when input items are returned via API.
              type: :computer_call_output # The type of the computer tool call output. Always `computer_call_output`.
); end
          end

          class AcknowledgedSafetyCheck < OpenAI::Internal::Type::BaseModel
            # The type of the pending safety check.
            sig { returns(T.nilable(String)) }
            attr_accessor :code

            # The ID of the pending safety check.
            sig { returns(String) }
            attr_accessor :id

            # Details about the pending safety check.
            sig { returns(T.nilable(String)) }
            attr_accessor :message

            sig do
              override
                .returns({
                  id: String,
                  code: T.nilable(String),
                  message: T.nilable(String)
                })
            end
            def to_hash; end

            class << self
              # A pending safety check for the computer call.
              sig { params(id: String, code: T.nilable(String), message: T.nilable(String)).returns(T.attached_class) }
              def new(
                id:, # The ID of the pending safety check.
                code: nil, # The type of the pending safety check.
                message: nil # Details about the pending safety check.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseInputItem::ComputerCallOutput::AcknowledgedSafetyCheck,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::ComputerCallOutput,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the message input. One of `in_progress`, `completed`, or
          # `incomplete`. Populated when input items are returned via API.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseInputItem::ComputerCallOutput::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseInputItem::ComputerCallOutput::Status::TaggedSymbol
              )

            INCOMPLETE = T.let(
                :incomplete,
                OpenAI::Responses::ResponseInputItem::ComputerCallOutput::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseInputItem::ComputerCallOutput::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseInputItem::ComputerCallOutput::Status
                )
              end
          end
        end

        class FunctionCallOutput < OpenAI::Internal::Type::BaseModel
          # The unique ID of the function tool call generated by the model.
          sig { returns(String) }
          attr_accessor :call_id

          # The unique ID of the function tool call output. Populated when this item is
          # returned via API.
          sig { returns(T.nilable(String)) }
          attr_accessor :id

          # A JSON string of the output of the function tool call.
          sig { returns(String) }
          attr_accessor :output

          # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
          # Populated when items are returned via API.
          sig do
            returns(T.nilable(
                OpenAI::Responses::ResponseInputItem::FunctionCallOutput::Status::OrSymbol
              ))
          end
          attr_accessor :status

          # The type of the function tool call output. Always `function_call_output`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                call_id: String,
                output: String,
                type: Symbol,
                id: T.nilable(String),
                status:
                  T.nilable(
                    OpenAI::Responses::ResponseInputItem::FunctionCallOutput::Status::OrSymbol
                  )
              })
          end
          def to_hash; end

          class << self
            # The output of a function tool call.
            sig do
              params(
                call_id: String,
                output: String,
                id: T.nilable(String),
                status: T.nilable(
                  OpenAI::Responses::ResponseInputItem::FunctionCallOutput::Status::OrSymbol
                ),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              call_id:, # The unique ID of the function tool call generated by the model.
              output:, # A JSON string of the output of the function tool call.
              id: nil, # The unique ID of the function tool call output. Populated when this item is
                       # returned via API.
              status: nil, # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
                           # Populated when items are returned via API.
              type: :function_call_output # The type of the function tool call output. Always `function_call_output`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::FunctionCallOutput,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
          # Populated when items are returned via API.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseInputItem::FunctionCallOutput::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseInputItem::FunctionCallOutput::Status::TaggedSymbol
              )

            INCOMPLETE = T.let(
                :incomplete,
                OpenAI::Responses::ResponseInputItem::FunctionCallOutput::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseInputItem::FunctionCallOutput::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseInputItem::FunctionCallOutput::Status
                )
              end
          end
        end

        class ImageGenerationCall < OpenAI::Internal::Type::BaseModel
          # The unique ID of the image generation call.
          sig { returns(String) }
          attr_accessor :id

          # The generated image encoded in base64.
          sig { returns(T.nilable(String)) }
          attr_accessor :result

          # The status of the image generation call.
          sig { returns(OpenAI::Responses::ResponseInputItem::ImageGenerationCall::Status::OrSymbol) }
          attr_accessor :status

          # The type of the image generation call. Always `image_generation_call`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                result: T.nilable(String),
                status:
                  OpenAI::Responses::ResponseInputItem::ImageGenerationCall::Status::OrSymbol,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # An image generation request made by the model.
            sig do
              params(
                id: String,
                result: T.nilable(String),
                status: OpenAI::Responses::ResponseInputItem::ImageGenerationCall::Status::OrSymbol,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the image generation call.
              result:, # The generated image encoded in base64.
              status:, # The status of the image generation call.
              type: :image_generation_call # The type of the image generation call. Always `image_generation_call`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::ImageGenerationCall,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the image generation call.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseInputItem::ImageGenerationCall::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseInputItem::ImageGenerationCall::Status::TaggedSymbol
              )

            FAILED = T.let(
                :failed,
                OpenAI::Responses::ResponseInputItem::ImageGenerationCall::Status::TaggedSymbol
              )

            GENERATING = T.let(
                :generating,
                OpenAI::Responses::ResponseInputItem::ImageGenerationCall::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseInputItem::ImageGenerationCall::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseInputItem::ImageGenerationCall::Status
                )
              end
          end
        end

        class ItemReference < OpenAI::Internal::Type::BaseModel
          # The ID of the item to reference.
          sig { returns(String) }
          attr_accessor :id

          # The type of item to reference. Always `item_reference`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::ResponseInputItem::ItemReference::Type::OrSymbol
              ))
          end
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                type:
                  T.nilable(
                    OpenAI::Responses::ResponseInputItem::ItemReference::Type::OrSymbol
                  )
              })
          end
          def to_hash; end

          class << self
            # An internal identifier for an item to reference.
            sig do
              params(
                id: String,
                type: T.nilable(
                  OpenAI::Responses::ResponseInputItem::ItemReference::Type::OrSymbol
                )
              ).returns(T.attached_class)
            end
            def new(
              id:, # The ID of the item to reference.
              type: nil # The type of item to reference. Always `item_reference`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::ItemReference,
                OpenAI::Internal::AnyHash
              )
            end

          # The type of item to reference. Always `item_reference`.
          module Type
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseInputItem::ItemReference::Type::TaggedSymbol
                ])
              end
              def values; end
            end

            ITEM_REFERENCE = T.let(
                :item_reference,
                OpenAI::Responses::ResponseInputItem::ItemReference::Type::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseInputItem::ItemReference::Type
                )
              end
          end
        end

        class LocalShellCall < OpenAI::Internal::Type::BaseModel
          # Execute a shell command on the server.
          sig { returns(OpenAI::Responses::ResponseInputItem::LocalShellCall::Action) }
          attr_reader :action

          sig { params(action: OpenAI::Responses::ResponseInputItem::LocalShellCall::Action::OrHash).void }
          attr_writer :action

          # The unique ID of the local shell tool call generated by the model.
          sig { returns(String) }
          attr_accessor :call_id

          # The unique ID of the local shell call.
          sig { returns(String) }
          attr_accessor :id

          # The status of the local shell call.
          sig { returns(OpenAI::Responses::ResponseInputItem::LocalShellCall::Status::OrSymbol) }
          attr_accessor :status

          # The type of the local shell call. Always `local_shell_call`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                action:
                  OpenAI::Responses::ResponseInputItem::LocalShellCall::Action,
                call_id: String,
                status:
                  OpenAI::Responses::ResponseInputItem::LocalShellCall::Status::OrSymbol,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A tool call to run a command on the local shell.
            sig do
              params(
                id: String,
                action: OpenAI::Responses::ResponseInputItem::LocalShellCall::Action::OrHash,
                call_id: String,
                status: OpenAI::Responses::ResponseInputItem::LocalShellCall::Status::OrSymbol,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the local shell call.
              action:, # Execute a shell command on the server.
              call_id:, # The unique ID of the local shell tool call generated by the model.
              status:, # The status of the local shell call.
              type: :local_shell_call # The type of the local shell call. Always `local_shell_call`.
); end
          end

          class Action < OpenAI::Internal::Type::BaseModel
            # The command to run.
            sig { returns(T::Array[String]) }
            attr_accessor :command

            # Environment variables to set for the command.
            sig { returns(T::Hash[Symbol, String]) }
            attr_accessor :env

            # Optional timeout in milliseconds for the command.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :timeout_ms

            # The type of the local shell action. Always `exec`.
            sig { returns(Symbol) }
            attr_accessor :type

            # Optional user to run the command as.
            sig { returns(T.nilable(String)) }
            attr_accessor :user

            # Optional working directory to run the command in.
            sig { returns(T.nilable(String)) }
            attr_accessor :working_directory

            sig do
              override
                .returns({
                  command: T::Array[String],
                  env: T::Hash[Symbol, String],
                  type: Symbol,
                  timeout_ms: T.nilable(Integer),
                  user: T.nilable(String),
                  working_directory: T.nilable(String)
                })
            end
            def to_hash; end

            class << self
              # Execute a shell command on the server.
              sig do
                params(
                  command: T::Array[String],
                  env: T::Hash[Symbol, String],
                  timeout_ms: T.nilable(Integer),
                  user: T.nilable(String),
                  working_directory: T.nilable(String),
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                command:, # The command to run.
                env:, # Environment variables to set for the command.
                timeout_ms: nil, # Optional timeout in milliseconds for the command.
                user: nil, # Optional user to run the command as.
                working_directory: nil, # Optional working directory to run the command in.
                type: :exec # The type of the local shell action. Always `exec`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseInputItem::LocalShellCall::Action,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::LocalShellCall,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the local shell call.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseInputItem::LocalShellCall::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseInputItem::LocalShellCall::Status::TaggedSymbol
              )

            INCOMPLETE = T.let(
                :incomplete,
                OpenAI::Responses::ResponseInputItem::LocalShellCall::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseInputItem::LocalShellCall::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseInputItem::LocalShellCall::Status
                )
              end
          end
        end

        class LocalShellCallOutput < OpenAI::Internal::Type::BaseModel
          # The unique ID of the local shell tool call generated by the model.
          sig { returns(String) }
          attr_accessor :id

          # A JSON string of the output of the local shell tool call.
          sig { returns(String) }
          attr_accessor :output

          # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::ResponseInputItem::LocalShellCallOutput::Status::OrSymbol
              ))
          end
          attr_accessor :status

          # The type of the local shell tool call output. Always `local_shell_call_output`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                output: String,
                type: Symbol,
                status:
                  T.nilable(
                    OpenAI::Responses::ResponseInputItem::LocalShellCallOutput::Status::OrSymbol
                  )
              })
          end
          def to_hash; end

          class << self
            # The output of a local shell tool call.
            sig do
              params(
                id: String,
                output: String,
                status: T.nilable(
                  OpenAI::Responses::ResponseInputItem::LocalShellCallOutput::Status::OrSymbol
                ),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the local shell tool call generated by the model.
              output:, # A JSON string of the output of the local shell tool call.
              status: nil, # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
              type: :local_shell_call_output # The type of the local shell tool call output. Always `local_shell_call_output`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::LocalShellCallOutput,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseInputItem::LocalShellCallOutput::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseInputItem::LocalShellCallOutput::Status::TaggedSymbol
              )

            INCOMPLETE = T.let(
                :incomplete,
                OpenAI::Responses::ResponseInputItem::LocalShellCallOutput::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseInputItem::LocalShellCallOutput::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseInputItem::LocalShellCallOutput::Status
                )
              end
          end
        end

        class McpApprovalRequest < OpenAI::Internal::Type::BaseModel
          # A JSON string of arguments for the tool.
          sig { returns(String) }
          attr_accessor :arguments

          # The unique ID of the approval request.
          sig { returns(String) }
          attr_accessor :id

          # The name of the tool to run.
          sig { returns(String) }
          attr_accessor :name

          # The label of the MCP server making the request.
          sig { returns(String) }
          attr_accessor :server_label

          # The type of the item. Always `mcp_approval_request`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A request for human approval of a tool invocation.
            sig do
              params(
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the approval request.
              arguments:, # A JSON string of arguments for the tool.
              name:, # The name of the tool to run.
              server_label:, # The label of the MCP server making the request.
              type: :mcp_approval_request # The type of the item. Always `mcp_approval_request`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::McpApprovalRequest,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class McpApprovalResponse < OpenAI::Internal::Type::BaseModel
          # The ID of the approval request being answered.
          sig { returns(String) }
          attr_accessor :approval_request_id

          # Whether the request was approved.
          sig { returns(T::Boolean) }
          attr_accessor :approve

          # The unique ID of the approval response
          sig { returns(T.nilable(String)) }
          attr_accessor :id

          # Optional reason for the decision.
          sig { returns(T.nilable(String)) }
          attr_accessor :reason

          # The type of the item. Always `mcp_approval_response`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                approval_request_id: String,
                approve: T::Boolean,
                type: Symbol,
                id: T.nilable(String),
                reason: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            # A response to an MCP approval request.
            sig do
              params(
                approval_request_id: String,
                approve: T::Boolean,
                id: T.nilable(String),
                reason: T.nilable(String),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              approval_request_id:, # The ID of the approval request being answered.
              approve:, # Whether the request was approved.
              id: nil, # The unique ID of the approval response
              reason: nil, # Optional reason for the decision.
              type: :mcp_approval_response # The type of the item. Always `mcp_approval_response`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::McpApprovalResponse,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class McpCall < OpenAI::Internal::Type::BaseModel
          # A JSON string of the arguments passed to the tool.
          sig { returns(String) }
          attr_accessor :arguments

          # The error from the tool call, if any.
          sig { returns(T.nilable(String)) }
          attr_accessor :error

          # The unique ID of the tool call.
          sig { returns(String) }
          attr_accessor :id

          # The name of the tool that was run.
          sig { returns(String) }
          attr_accessor :name

          # The output from the tool call.
          sig { returns(T.nilable(String)) }
          attr_accessor :output

          # The label of the MCP server running the tool.
          sig { returns(String) }
          attr_accessor :server_label

          # The type of the item. Always `mcp_call`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                type: Symbol,
                error: T.nilable(String),
                output: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            # An invocation of a tool on an MCP server.
            sig do
              params(
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                error: T.nilable(String),
                output: T.nilable(String),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the tool call.
              arguments:, # A JSON string of the arguments passed to the tool.
              name:, # The name of the tool that was run.
              server_label:, # The label of the MCP server running the tool.
              error: nil, # The error from the tool call, if any.
              output: nil, # The output from the tool call.
              type: :mcp_call # The type of the item. Always `mcp_call`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::McpCall,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class McpListTools < OpenAI::Internal::Type::BaseModel
          # Error message if the server could not list tools.
          sig { returns(T.nilable(String)) }
          attr_accessor :error

          # The unique ID of the list.
          sig { returns(String) }
          attr_accessor :id

          # The label of the MCP server.
          sig { returns(String) }
          attr_accessor :server_label

          # The tools available on the server.
          sig { returns(T::Array[OpenAI::Responses::ResponseInputItem::McpListTools::Tool]) }
          attr_accessor :tools

          # The type of the item. Always `mcp_list_tools`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                server_label: String,
                tools:
                  T::Array[
                    OpenAI::Responses::ResponseInputItem::McpListTools::Tool
                  ],
                type: Symbol,
                error: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            # A list of tools available on an MCP server.
            sig do
              params(
                id: String,
                server_label: String,
                tools: T::Array[
                  OpenAI::Responses::ResponseInputItem::McpListTools::Tool::OrHash
                ],
                error: T.nilable(String),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the list.
              server_label:, # The label of the MCP server.
              tools:, # The tools available on the server.
              error: nil, # Error message if the server could not list tools.
              type: :mcp_list_tools # The type of the item. Always `mcp_list_tools`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::McpListTools,
                OpenAI::Internal::AnyHash
              )
            end

          class Tool < OpenAI::Internal::Type::BaseModel
            # Additional annotations about the tool.
            sig { returns(T.nilable(T.anything)) }
            attr_accessor :annotations

            # The description of the tool.
            sig { returns(T.nilable(String)) }
            attr_accessor :description

            # The JSON schema describing the tool's input.
            sig { returns(T.anything) }
            attr_accessor :input_schema

            # The name of the tool.
            sig { returns(String) }
            attr_accessor :name

            sig do
              override
                .returns({
                  input_schema: T.anything,
                  name: String,
                  annotations: T.nilable(T.anything),
                  description: T.nilable(String)
                })
            end
            def to_hash; end

            class << self
              # A tool available on an MCP server.
              sig do
                params(
                  input_schema: T.anything,
                  name: String,
                  annotations: T.nilable(T.anything),
                  description: T.nilable(String)
                ).returns(T.attached_class)
              end
              def new(
                input_schema:, # The JSON schema describing the tool's input.
                name:, # The name of the tool.
                annotations: nil, # Additional annotations about the tool.
                description: nil # The description of the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseInputItem::McpListTools::Tool,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        class Message < OpenAI::Internal::Type::BaseModel
          # A list of one or many input items to the model, containing different content
          # types.
          sig do
            returns(T::Array[
                T.any(
                  OpenAI::Responses::ResponseInputText,
                  OpenAI::Responses::ResponseInputImage,
                  OpenAI::Responses::ResponseInputFile
                )
              ])
          end
          attr_accessor :content

          # The role of the message input. One of `user`, `system`, or `developer`.
          sig { returns(OpenAI::Responses::ResponseInputItem::Message::Role::OrSymbol) }
          attr_accessor :role

          # The status of item. One of `in_progress`, `completed`, or `incomplete`.
          # Populated when items are returned via API.
          sig do
            returns(T.nilable(
                OpenAI::Responses::ResponseInputItem::Message::Status::OrSymbol
              ))
          end
          attr_reader :status

          sig { params(status: OpenAI::Responses::ResponseInputItem::Message::Status::OrSymbol).void }
          attr_writer :status

          # The type of the message input. Always set to `message`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::ResponseInputItem::Message::Type::OrSymbol
              ))
          end
          attr_reader :type

          sig { params(type: OpenAI::Responses::ResponseInputItem::Message::Type::OrSymbol).void }
          attr_writer :type

          sig do
            override
              .returns({
                content:
                  T::Array[
                    T.any(
                      OpenAI::Responses::ResponseInputText,
                      OpenAI::Responses::ResponseInputImage,
                      OpenAI::Responses::ResponseInputFile
                    )
                  ],
                role:
                  OpenAI::Responses::ResponseInputItem::Message::Role::OrSymbol,
                status:
                  OpenAI::Responses::ResponseInputItem::Message::Status::OrSymbol,
                type:
                  OpenAI::Responses::ResponseInputItem::Message::Type::OrSymbol
              })
          end
          def to_hash; end

          class << self
            # A message input to the model with a role indicating instruction following
            # hierarchy. Instructions given with the `developer` or `system` role take
            # precedence over instructions given with the `user` role.
            sig do
              params(
                content: T::Array[
                  T.any(
                    OpenAI::Responses::ResponseInputText::OrHash,
                    OpenAI::Responses::ResponseInputImage::OrHash,
                    OpenAI::Responses::ResponseInputFile::OrHash
                  )
                ],
                role: OpenAI::Responses::ResponseInputItem::Message::Role::OrSymbol,
                status: OpenAI::Responses::ResponseInputItem::Message::Status::OrSymbol,
                type: OpenAI::Responses::ResponseInputItem::Message::Type::OrSymbol
              ).returns(T.attached_class)
            end
            def new(
              content:, # A list of one or many input items to the model, containing different content
                        # types.
              role:, # The role of the message input. One of `user`, `system`, or `developer`.
              status: nil, # The status of item. One of `in_progress`, `completed`, or `incomplete`.
                           # Populated when items are returned via API.
              type: nil # The type of the message input. Always set to `message`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseInputItem::Message,
                OpenAI::Internal::AnyHash
              )
            end

          # The role of the message input. One of `user`, `system`, or `developer`.
          module Role
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseInputItem::Message::Role::TaggedSymbol
                ])
              end
              def values; end
            end

            DEVELOPER = T.let(
                :developer,
                OpenAI::Responses::ResponseInputItem::Message::Role::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            SYSTEM = T.let(
                :system,
                OpenAI::Responses::ResponseInputItem::Message::Role::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseInputItem::Message::Role
                )
              end

            USER = T.let(
                :user,
                OpenAI::Responses::ResponseInputItem::Message::Role::TaggedSymbol
              )
          end

          # The status of item. One of `in_progress`, `completed`, or `incomplete`.
          # Populated when items are returned via API.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseInputItem::Message::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseInputItem::Message::Status::TaggedSymbol
              )

            INCOMPLETE = T.let(
                :incomplete,
                OpenAI::Responses::ResponseInputItem::Message::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseInputItem::Message::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseInputItem::Message::Status
                )
              end
          end

          # The type of the message input. Always set to `message`.
          module Type
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseInputItem::Message::Type::TaggedSymbol
                ])
              end
              def values; end
            end

            MESSAGE = T.let(
                :message,
                OpenAI::Responses::ResponseInputItem::Message::Type::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseInputItem::Message::Type
                )
              end
          end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Responses::EasyInputMessage,
              OpenAI::Responses::ResponseInputItem::Message,
              OpenAI::Responses::ResponseOutputMessage,
              OpenAI::Responses::ResponseFileSearchToolCall,
              OpenAI::Responses::ResponseComputerToolCall,
              OpenAI::Responses::ResponseInputItem::ComputerCallOutput,
              OpenAI::Responses::ResponseFunctionWebSearch,
              OpenAI::Responses::ResponseFunctionToolCall,
              OpenAI::Responses::ResponseInputItem::FunctionCallOutput,
              OpenAI::Responses::ResponseReasoningItem,
              OpenAI::Responses::ResponseInputItem::ImageGenerationCall,
              OpenAI::Responses::ResponseCodeInterpreterToolCall,
              OpenAI::Responses::ResponseInputItem::LocalShellCall,
              OpenAI::Responses::ResponseInputItem::LocalShellCallOutput,
              OpenAI::Responses::ResponseInputItem::McpListTools,
              OpenAI::Responses::ResponseInputItem::McpApprovalRequest,
              OpenAI::Responses::ResponseInputItem::McpApprovalResponse,
              OpenAI::Responses::ResponseInputItem::McpCall,
              OpenAI::Responses::ResponseInputItem::ItemReference
            )
          end
      end

      ResponseInputMessageContentList = T.let(
          OpenAI::Internal::Type::ArrayOf[
            union: OpenAI::Responses::ResponseInputContent
          ],
          OpenAI::Internal::Type::Converter
        )

      class ResponseInputMessageItem < OpenAI::Internal::Type::BaseModel
        # A list of one or many input items to the model, containing different content
        # types.
        sig { returns(T::Array[OpenAI::Responses::ResponseInputContent::Variants]) }
        attr_accessor :content

        # The unique ID of the message input.
        sig { returns(String) }
        attr_accessor :id

        # The role of the message input. One of `user`, `system`, or `developer`.
        sig { returns(OpenAI::Responses::ResponseInputMessageItem::Role::TaggedSymbol) }
        attr_accessor :role

        # The status of item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        sig do
          returns(T.nilable(
              OpenAI::Responses::ResponseInputMessageItem::Status::TaggedSymbol
            ))
        end
        attr_reader :status

        sig { params(status: OpenAI::Responses::ResponseInputMessageItem::Status::OrSymbol).void }
        attr_writer :status

        # The type of the message input. Always set to `message`.
        sig do
          returns(T.nilable(
              OpenAI::Responses::ResponseInputMessageItem::Type::TaggedSymbol
            ))
        end
        attr_reader :type

        sig { params(type: OpenAI::Responses::ResponseInputMessageItem::Type::OrSymbol).void }
        attr_writer :type

        sig do
          override
            .returns({
              id: String,
              content:
                T::Array[OpenAI::Responses::ResponseInputContent::Variants],
              role:
                OpenAI::Responses::ResponseInputMessageItem::Role::TaggedSymbol,
              status:
                OpenAI::Responses::ResponseInputMessageItem::Status::TaggedSymbol,
              type:
                OpenAI::Responses::ResponseInputMessageItem::Type::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              id: String,
              content: T::Array[
                T.any(
                  OpenAI::Responses::ResponseInputText::OrHash,
                  OpenAI::Responses::ResponseInputImage::OrHash,
                  OpenAI::Responses::ResponseInputFile::OrHash
                )
              ],
              role: OpenAI::Responses::ResponseInputMessageItem::Role::OrSymbol,
              status: OpenAI::Responses::ResponseInputMessageItem::Status::OrSymbol,
              type: OpenAI::Responses::ResponseInputMessageItem::Type::OrSymbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the message input.
            content:, # A list of one or many input items to the model, containing different content
                      # types.
            role:, # The role of the message input. One of `user`, `system`, or `developer`.
            status: nil, # The status of item. One of `in_progress`, `completed`, or `incomplete`.
                         # Populated when items are returned via API.
            type: nil # The type of the message input. Always set to `message`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseInputMessageItem,
              OpenAI::Internal::AnyHash
            )
          end

        # The role of the message input. One of `user`, `system`, or `developer`.
        module Role
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseInputMessageItem::Role::TaggedSymbol
              ])
            end
            def values; end
          end

          DEVELOPER = T.let(
              :developer,
              OpenAI::Responses::ResponseInputMessageItem::Role::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          SYSTEM = T.let(
              :system,
              OpenAI::Responses::ResponseInputMessageItem::Role::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseInputMessageItem::Role)
            end

          USER = T.let(
              :user,
              OpenAI::Responses::ResponseInputMessageItem::Role::TaggedSymbol
            )
        end

        # The status of item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseInputMessageItem::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseInputMessageItem::Status::TaggedSymbol
            )

          INCOMPLETE = T.let(
              :incomplete,
              OpenAI::Responses::ResponseInputMessageItem::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseInputMessageItem::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseInputMessageItem::Status)
            end
        end

        # The type of the message input. Always set to `message`.
        module Type
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseInputMessageItem::Type::TaggedSymbol
              ])
            end
            def values; end
          end

          MESSAGE = T.let(
              :message,
              OpenAI::Responses::ResponseInputMessageItem::Type::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseInputMessageItem::Type)
            end
        end
      end

      class ResponseInputText < OpenAI::Internal::Type::BaseModel
        # The text input to the model.
        sig { returns(String) }
        attr_accessor :text

        # The type of the input item. Always `input_text`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ text: String, type: Symbol }) }
        def to_hash; end

        class << self
          # A text input to the model.
          sig { params(text: String, type: Symbol).returns(T.attached_class) }
          def new(
            text:, # The text input to the model.
            type: :input_text # The type of the input item. Always `input_text`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseInputText,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # Content item used to generate a response.
      module ResponseItem
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ResponseItem::Variants]) }
          def variants; end
        end

        class ImageGenerationCall < OpenAI::Internal::Type::BaseModel
          # The unique ID of the image generation call.
          sig { returns(String) }
          attr_accessor :id

          # The generated image encoded in base64.
          sig { returns(T.nilable(String)) }
          attr_accessor :result

          # The status of the image generation call.
          sig { returns(OpenAI::Responses::ResponseItem::ImageGenerationCall::Status::TaggedSymbol) }
          attr_accessor :status

          # The type of the image generation call. Always `image_generation_call`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                result: T.nilable(String),
                status:
                  OpenAI::Responses::ResponseItem::ImageGenerationCall::Status::TaggedSymbol,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # An image generation request made by the model.
            sig do
              params(
                id: String,
                result: T.nilable(String),
                status: OpenAI::Responses::ResponseItem::ImageGenerationCall::Status::OrSymbol,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the image generation call.
              result:, # The generated image encoded in base64.
              status:, # The status of the image generation call.
              type: :image_generation_call # The type of the image generation call. Always `image_generation_call`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseItem::ImageGenerationCall,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the image generation call.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseItem::ImageGenerationCall::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseItem::ImageGenerationCall::Status::TaggedSymbol
              )

            FAILED = T.let(
                :failed,
                OpenAI::Responses::ResponseItem::ImageGenerationCall::Status::TaggedSymbol
              )

            GENERATING = T.let(
                :generating,
                OpenAI::Responses::ResponseItem::ImageGenerationCall::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseItem::ImageGenerationCall::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseItem::ImageGenerationCall::Status
                )
              end
          end
        end

        class LocalShellCall < OpenAI::Internal::Type::BaseModel
          # Execute a shell command on the server.
          sig { returns(OpenAI::Responses::ResponseItem::LocalShellCall::Action) }
          attr_reader :action

          sig { params(action: OpenAI::Responses::ResponseItem::LocalShellCall::Action::OrHash).void }
          attr_writer :action

          # The unique ID of the local shell tool call generated by the model.
          sig { returns(String) }
          attr_accessor :call_id

          # The unique ID of the local shell call.
          sig { returns(String) }
          attr_accessor :id

          # The status of the local shell call.
          sig { returns(OpenAI::Responses::ResponseItem::LocalShellCall::Status::TaggedSymbol) }
          attr_accessor :status

          # The type of the local shell call. Always `local_shell_call`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                action: OpenAI::Responses::ResponseItem::LocalShellCall::Action,
                call_id: String,
                status:
                  OpenAI::Responses::ResponseItem::LocalShellCall::Status::TaggedSymbol,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A tool call to run a command on the local shell.
            sig do
              params(
                id: String,
                action: OpenAI::Responses::ResponseItem::LocalShellCall::Action::OrHash,
                call_id: String,
                status: OpenAI::Responses::ResponseItem::LocalShellCall::Status::OrSymbol,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the local shell call.
              action:, # Execute a shell command on the server.
              call_id:, # The unique ID of the local shell tool call generated by the model.
              status:, # The status of the local shell call.
              type: :local_shell_call # The type of the local shell call. Always `local_shell_call`.
); end
          end

          class Action < OpenAI::Internal::Type::BaseModel
            # The command to run.
            sig { returns(T::Array[String]) }
            attr_accessor :command

            # Environment variables to set for the command.
            sig { returns(T::Hash[Symbol, String]) }
            attr_accessor :env

            # Optional timeout in milliseconds for the command.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :timeout_ms

            # The type of the local shell action. Always `exec`.
            sig { returns(Symbol) }
            attr_accessor :type

            # Optional user to run the command as.
            sig { returns(T.nilable(String)) }
            attr_accessor :user

            # Optional working directory to run the command in.
            sig { returns(T.nilable(String)) }
            attr_accessor :working_directory

            sig do
              override
                .returns({
                  command: T::Array[String],
                  env: T::Hash[Symbol, String],
                  type: Symbol,
                  timeout_ms: T.nilable(Integer),
                  user: T.nilable(String),
                  working_directory: T.nilable(String)
                })
            end
            def to_hash; end

            class << self
              # Execute a shell command on the server.
              sig do
                params(
                  command: T::Array[String],
                  env: T::Hash[Symbol, String],
                  timeout_ms: T.nilable(Integer),
                  user: T.nilable(String),
                  working_directory: T.nilable(String),
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                command:, # The command to run.
                env:, # Environment variables to set for the command.
                timeout_ms: nil, # Optional timeout in milliseconds for the command.
                user: nil, # Optional user to run the command as.
                working_directory: nil, # Optional working directory to run the command in.
                type: :exec # The type of the local shell action. Always `exec`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseItem::LocalShellCall::Action,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseItem::LocalShellCall,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the local shell call.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseItem::LocalShellCall::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseItem::LocalShellCall::Status::TaggedSymbol
              )

            INCOMPLETE = T.let(
                :incomplete,
                OpenAI::Responses::ResponseItem::LocalShellCall::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseItem::LocalShellCall::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseItem::LocalShellCall::Status
                )
              end
          end
        end

        class LocalShellCallOutput < OpenAI::Internal::Type::BaseModel
          # The unique ID of the local shell tool call generated by the model.
          sig { returns(String) }
          attr_accessor :id

          # A JSON string of the output of the local shell tool call.
          sig { returns(String) }
          attr_accessor :output

          # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::ResponseItem::LocalShellCallOutput::Status::TaggedSymbol
              ))
          end
          attr_accessor :status

          # The type of the local shell tool call output. Always `local_shell_call_output`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                output: String,
                type: Symbol,
                status:
                  T.nilable(
                    OpenAI::Responses::ResponseItem::LocalShellCallOutput::Status::TaggedSymbol
                  )
              })
          end
          def to_hash; end

          class << self
            # The output of a local shell tool call.
            sig do
              params(
                id: String,
                output: String,
                status: T.nilable(
                  OpenAI::Responses::ResponseItem::LocalShellCallOutput::Status::OrSymbol
                ),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the local shell tool call generated by the model.
              output:, # A JSON string of the output of the local shell tool call.
              status: nil, # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
              type: :local_shell_call_output # The type of the local shell tool call output. Always `local_shell_call_output`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseItem::LocalShellCallOutput,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseItem::LocalShellCallOutput::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseItem::LocalShellCallOutput::Status::TaggedSymbol
              )

            INCOMPLETE = T.let(
                :incomplete,
                OpenAI::Responses::ResponseItem::LocalShellCallOutput::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseItem::LocalShellCallOutput::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseItem::LocalShellCallOutput::Status
                )
              end
          end
        end

        class McpApprovalRequest < OpenAI::Internal::Type::BaseModel
          # A JSON string of arguments for the tool.
          sig { returns(String) }
          attr_accessor :arguments

          # The unique ID of the approval request.
          sig { returns(String) }
          attr_accessor :id

          # The name of the tool to run.
          sig { returns(String) }
          attr_accessor :name

          # The label of the MCP server making the request.
          sig { returns(String) }
          attr_accessor :server_label

          # The type of the item. Always `mcp_approval_request`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A request for human approval of a tool invocation.
            sig do
              params(
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the approval request.
              arguments:, # A JSON string of arguments for the tool.
              name:, # The name of the tool to run.
              server_label:, # The label of the MCP server making the request.
              type: :mcp_approval_request # The type of the item. Always `mcp_approval_request`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseItem::McpApprovalRequest,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class McpApprovalResponse < OpenAI::Internal::Type::BaseModel
          # The ID of the approval request being answered.
          sig { returns(String) }
          attr_accessor :approval_request_id

          # Whether the request was approved.
          sig { returns(T::Boolean) }
          attr_accessor :approve

          # The unique ID of the approval response
          sig { returns(String) }
          attr_accessor :id

          # Optional reason for the decision.
          sig { returns(T.nilable(String)) }
          attr_accessor :reason

          # The type of the item. Always `mcp_approval_response`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                approval_request_id: String,
                approve: T::Boolean,
                type: Symbol,
                reason: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            # A response to an MCP approval request.
            sig do
              params(
                id: String,
                approval_request_id: String,
                approve: T::Boolean,
                reason: T.nilable(String),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the approval response
              approval_request_id:, # The ID of the approval request being answered.
              approve:, # Whether the request was approved.
              reason: nil, # Optional reason for the decision.
              type: :mcp_approval_response # The type of the item. Always `mcp_approval_response`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseItem::McpApprovalResponse,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class McpCall < OpenAI::Internal::Type::BaseModel
          # A JSON string of the arguments passed to the tool.
          sig { returns(String) }
          attr_accessor :arguments

          # The error from the tool call, if any.
          sig { returns(T.nilable(String)) }
          attr_accessor :error

          # The unique ID of the tool call.
          sig { returns(String) }
          attr_accessor :id

          # The name of the tool that was run.
          sig { returns(String) }
          attr_accessor :name

          # The output from the tool call.
          sig { returns(T.nilable(String)) }
          attr_accessor :output

          # The label of the MCP server running the tool.
          sig { returns(String) }
          attr_accessor :server_label

          # The type of the item. Always `mcp_call`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                type: Symbol,
                error: T.nilable(String),
                output: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            # An invocation of a tool on an MCP server.
            sig do
              params(
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                error: T.nilable(String),
                output: T.nilable(String),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the tool call.
              arguments:, # A JSON string of the arguments passed to the tool.
              name:, # The name of the tool that was run.
              server_label:, # The label of the MCP server running the tool.
              error: nil, # The error from the tool call, if any.
              output: nil, # The output from the tool call.
              type: :mcp_call # The type of the item. Always `mcp_call`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseItem::McpCall,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class McpListTools < OpenAI::Internal::Type::BaseModel
          # Error message if the server could not list tools.
          sig { returns(T.nilable(String)) }
          attr_accessor :error

          # The unique ID of the list.
          sig { returns(String) }
          attr_accessor :id

          # The label of the MCP server.
          sig { returns(String) }
          attr_accessor :server_label

          # The tools available on the server.
          sig { returns(T::Array[OpenAI::Responses::ResponseItem::McpListTools::Tool]) }
          attr_accessor :tools

          # The type of the item. Always `mcp_list_tools`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                server_label: String,
                tools:
                  T::Array[OpenAI::Responses::ResponseItem::McpListTools::Tool],
                type: Symbol,
                error: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            # A list of tools available on an MCP server.
            sig do
              params(
                id: String,
                server_label: String,
                tools: T::Array[
                  OpenAI::Responses::ResponseItem::McpListTools::Tool::OrHash
                ],
                error: T.nilable(String),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the list.
              server_label:, # The label of the MCP server.
              tools:, # The tools available on the server.
              error: nil, # Error message if the server could not list tools.
              type: :mcp_list_tools # The type of the item. Always `mcp_list_tools`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseItem::McpListTools,
                OpenAI::Internal::AnyHash
              )
            end

          class Tool < OpenAI::Internal::Type::BaseModel
            # Additional annotations about the tool.
            sig { returns(T.nilable(T.anything)) }
            attr_accessor :annotations

            # The description of the tool.
            sig { returns(T.nilable(String)) }
            attr_accessor :description

            # The JSON schema describing the tool's input.
            sig { returns(T.anything) }
            attr_accessor :input_schema

            # The name of the tool.
            sig { returns(String) }
            attr_accessor :name

            sig do
              override
                .returns({
                  input_schema: T.anything,
                  name: String,
                  annotations: T.nilable(T.anything),
                  description: T.nilable(String)
                })
            end
            def to_hash; end

            class << self
              # A tool available on an MCP server.
              sig do
                params(
                  input_schema: T.anything,
                  name: String,
                  annotations: T.nilable(T.anything),
                  description: T.nilable(String)
                ).returns(T.attached_class)
              end
              def new(
                input_schema:, # The JSON schema describing the tool's input.
                name:, # The name of the tool.
                annotations: nil, # Additional annotations about the tool.
                description: nil # The description of the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseItem::McpListTools::Tool,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseInputMessageItem,
              OpenAI::Responses::ResponseOutputMessage,
              OpenAI::Responses::ResponseFileSearchToolCall,
              OpenAI::Responses::ResponseComputerToolCall,
              OpenAI::Responses::ResponseComputerToolCallOutputItem,
              OpenAI::Responses::ResponseFunctionWebSearch,
              OpenAI::Responses::ResponseFunctionToolCallItem,
              OpenAI::Responses::ResponseFunctionToolCallOutputItem,
              OpenAI::Responses::ResponseItem::ImageGenerationCall,
              OpenAI::Responses::ResponseCodeInterpreterToolCall,
              OpenAI::Responses::ResponseItem::LocalShellCall,
              OpenAI::Responses::ResponseItem::LocalShellCallOutput,
              OpenAI::Responses::ResponseItem::McpListTools,
              OpenAI::Responses::ResponseItem::McpApprovalRequest,
              OpenAI::Responses::ResponseItem::McpApprovalResponse,
              OpenAI::Responses::ResponseItem::McpCall
            )
          end
      end

      class ResponseItemList < OpenAI::Internal::Type::BaseModel
        # A list of items used to generate this response.
        sig { returns(T::Array[OpenAI::Responses::ResponseItem::Variants]) }
        attr_accessor :data

        # The ID of the first item in the list.
        sig { returns(String) }
        attr_accessor :first_id

        # Whether there are more items available.
        sig { returns(T::Boolean) }
        attr_accessor :has_more

        # The ID of the last item in the list.
        sig { returns(String) }
        attr_accessor :last_id

        # The type of object returned, must be `list`.
        sig { returns(Symbol) }
        attr_accessor :object

        sig do
          override
            .returns({
              data: T::Array[OpenAI::Responses::ResponseItem::Variants],
              first_id: String,
              has_more: T::Boolean,
              last_id: String,
              object: Symbol
            })
        end
        def to_hash; end

        class << self
          # A list of Response items.
          sig do
            params(
              data: T::Array[
                T.any(
                  OpenAI::Responses::ResponseInputMessageItem::OrHash,
                  OpenAI::Responses::ResponseOutputMessage::OrHash,
                  OpenAI::Responses::ResponseFileSearchToolCall::OrHash,
                  OpenAI::Responses::ResponseComputerToolCall::OrHash,
                  OpenAI::Responses::ResponseComputerToolCallOutputItem::OrHash,
                  OpenAI::Responses::ResponseFunctionWebSearch::OrHash,
                  OpenAI::Responses::ResponseFunctionToolCallItem::OrHash,
                  OpenAI::Responses::ResponseFunctionToolCallOutputItem::OrHash,
                  OpenAI::Responses::ResponseItem::ImageGenerationCall::OrHash,
                  OpenAI::Responses::ResponseCodeInterpreterToolCall::OrHash,
                  OpenAI::Responses::ResponseItem::LocalShellCall::OrHash,
                  OpenAI::Responses::ResponseItem::LocalShellCallOutput::OrHash,
                  OpenAI::Responses::ResponseItem::McpListTools::OrHash,
                  OpenAI::Responses::ResponseItem::McpApprovalRequest::OrHash,
                  OpenAI::Responses::ResponseItem::McpApprovalResponse::OrHash,
                  OpenAI::Responses::ResponseItem::McpCall::OrHash
                )
              ],
              first_id: String,
              has_more: T::Boolean,
              last_id: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            data:, # A list of items used to generate this response.
            first_id:, # The ID of the first item in the list.
            has_more:, # Whether there are more items available.
            last_id:, # The ID of the last item in the list.
            object: :list # The type of object returned, must be `list`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseItemList,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseMcpCallArgumentsDeltaEvent < OpenAI::Internal::Type::BaseModel
        # A JSON string containing the partial update to the arguments for the MCP tool
        # call.
        sig { returns(String) }
        attr_accessor :delta

        # The unique identifier of the MCP tool call item being processed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.mcp_call_arguments.delta'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when there is a delta (partial update) to the arguments of an MCP tool
          # call.
          sig do
            params(
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            delta:, # A JSON string containing the partial update to the arguments for the MCP tool
                    # call.
            item_id:, # The unique identifier of the MCP tool call item being processed.
            output_index:, # The index of the output item in the response's output array.
            sequence_number:, # The sequence number of this event.
            type: :"response.mcp_call_arguments.delta" # The type of the event. Always 'response.mcp_call_arguments.delta'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseMcpCallArgumentsDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseMcpCallArgumentsDoneEvent < OpenAI::Internal::Type::BaseModel
        # A JSON string containing the finalized arguments for the MCP tool call.
        sig { returns(String) }
        attr_accessor :arguments

        # The unique identifier of the MCP tool call item being processed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.mcp_call_arguments.done'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              arguments: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the arguments for an MCP tool call are finalized.
          sig do
            params(
              arguments: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            arguments:, # A JSON string containing the finalized arguments for the MCP tool call.
            item_id:, # The unique identifier of the MCP tool call item being processed.
            output_index:, # The index of the output item in the response's output array.
            sequence_number:, # The sequence number of this event.
            type: :"response.mcp_call_arguments.done" # The type of the event. Always 'response.mcp_call_arguments.done'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseMcpCallArgumentsDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseMcpCallCompletedEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the MCP tool call item that completed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that completed.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.mcp_call.completed'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when an MCP tool call has completed successfully.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the MCP tool call item that completed.
            output_index:, # The index of the output item that completed.
            sequence_number:, # The sequence number of this event.
            type: :"response.mcp_call.completed" # The type of the event. Always 'response.mcp_call.completed'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseMcpCallCompletedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseMcpCallFailedEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the MCP tool call item that failed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that failed.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.mcp_call.failed'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when an MCP tool call has failed.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the MCP tool call item that failed.
            output_index:, # The index of the output item that failed.
            sequence_number:, # The sequence number of this event.
            type: :"response.mcp_call.failed" # The type of the event. Always 'response.mcp_call.failed'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseMcpCallFailedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseMcpCallInProgressEvent < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the MCP tool call item being processed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.mcp_call.in_progress'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when an MCP tool call is in progress.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The unique identifier of the MCP tool call item being processed.
            output_index:, # The index of the output item in the response's output array.
            sequence_number:, # The sequence number of this event.
            type: :"response.mcp_call.in_progress" # The type of the event. Always 'response.mcp_call.in_progress'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseMcpCallInProgressEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseMcpListToolsCompletedEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the MCP tool call item that produced this output.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that was processed.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.mcp_list_tools.completed'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the list of available MCP tools has been successfully retrieved.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the MCP tool call item that produced this output.
            output_index:, # The index of the output item that was processed.
            sequence_number:, # The sequence number of this event.
            type: :"response.mcp_list_tools.completed" # The type of the event. Always 'response.mcp_list_tools.completed'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseMcpListToolsCompletedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseMcpListToolsFailedEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the MCP tool call item that failed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that failed.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.mcp_list_tools.failed'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the attempt to list available MCP tools has failed.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the MCP tool call item that failed.
            output_index:, # The index of the output item that failed.
            sequence_number:, # The sequence number of this event.
            type: :"response.mcp_list_tools.failed" # The type of the event. Always 'response.mcp_list_tools.failed'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseMcpListToolsFailedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseMcpListToolsInProgressEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the MCP tool call item that is being processed.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that is being processed.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.mcp_list_tools.in_progress'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the system is in the process of retrieving the list of available
          # MCP tools.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the MCP tool call item that is being processed.
            output_index:, # The index of the output item that is being processed.
            sequence_number:, # The sequence number of this event.
            type: :"response.mcp_list_tools.in_progress" # The type of the event. Always 'response.mcp_list_tools.in_progress'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseMcpListToolsInProgressEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseOutputAudio < OpenAI::Internal::Type::BaseModel
        # Base64-encoded audio data from the model.
        sig { returns(String) }
        attr_accessor :data

        # The transcript of the audio data from the model.
        sig { returns(String) }
        attr_accessor :transcript

        # The type of the output audio. Always `output_audio`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ data: String, transcript: String, type: Symbol }) }
        def to_hash; end

        class << self
          # An audio output from the model.
          sig { params(data: String, transcript: String, type: Symbol).returns(T.attached_class) }
          def new(
            data:, # Base64-encoded audio data from the model.
            transcript:, # The transcript of the audio data from the model.
            type: :output_audio # The type of the output audio. Always `output_audio`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseOutputAudio,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # An output message from the model.
      module ResponseOutputItem
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ResponseOutputItem::Variants]) }
          def variants; end
        end

        class ImageGenerationCall < OpenAI::Internal::Type::BaseModel
          # The unique ID of the image generation call.
          sig { returns(String) }
          attr_accessor :id

          # The generated image encoded in base64.
          sig { returns(T.nilable(String)) }
          attr_accessor :result

          # The status of the image generation call.
          sig { returns(OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::Status::TaggedSymbol) }
          attr_accessor :status

          # The type of the image generation call. Always `image_generation_call`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                result: T.nilable(String),
                status:
                  OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::Status::TaggedSymbol,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # An image generation request made by the model.
            sig do
              params(
                id: String,
                result: T.nilable(String),
                status: OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::Status::OrSymbol,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the image generation call.
              result:, # The generated image encoded in base64.
              status:, # The status of the image generation call.
              type: :image_generation_call # The type of the image generation call. Always `image_generation_call`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputItem::ImageGenerationCall,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the image generation call.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::Status::TaggedSymbol
              )

            FAILED = T.let(
                :failed,
                OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::Status::TaggedSymbol
              )

            GENERATING = T.let(
                :generating,
                OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::Status
                )
              end
          end
        end

        class LocalShellCall < OpenAI::Internal::Type::BaseModel
          # Execute a shell command on the server.
          sig { returns(OpenAI::Responses::ResponseOutputItem::LocalShellCall::Action) }
          attr_reader :action

          sig { params(action: OpenAI::Responses::ResponseOutputItem::LocalShellCall::Action::OrHash).void }
          attr_writer :action

          # The unique ID of the local shell tool call generated by the model.
          sig { returns(String) }
          attr_accessor :call_id

          # The unique ID of the local shell call.
          sig { returns(String) }
          attr_accessor :id

          # The status of the local shell call.
          sig { returns(OpenAI::Responses::ResponseOutputItem::LocalShellCall::Status::TaggedSymbol) }
          attr_accessor :status

          # The type of the local shell call. Always `local_shell_call`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                action:
                  OpenAI::Responses::ResponseOutputItem::LocalShellCall::Action,
                call_id: String,
                status:
                  OpenAI::Responses::ResponseOutputItem::LocalShellCall::Status::TaggedSymbol,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A tool call to run a command on the local shell.
            sig do
              params(
                id: String,
                action: OpenAI::Responses::ResponseOutputItem::LocalShellCall::Action::OrHash,
                call_id: String,
                status: OpenAI::Responses::ResponseOutputItem::LocalShellCall::Status::OrSymbol,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the local shell call.
              action:, # Execute a shell command on the server.
              call_id:, # The unique ID of the local shell tool call generated by the model.
              status:, # The status of the local shell call.
              type: :local_shell_call # The type of the local shell call. Always `local_shell_call`.
); end
          end

          class Action < OpenAI::Internal::Type::BaseModel
            # The command to run.
            sig { returns(T::Array[String]) }
            attr_accessor :command

            # Environment variables to set for the command.
            sig { returns(T::Hash[Symbol, String]) }
            attr_accessor :env

            # Optional timeout in milliseconds for the command.
            sig { returns(T.nilable(Integer)) }
            attr_accessor :timeout_ms

            # The type of the local shell action. Always `exec`.
            sig { returns(Symbol) }
            attr_accessor :type

            # Optional user to run the command as.
            sig { returns(T.nilable(String)) }
            attr_accessor :user

            # Optional working directory to run the command in.
            sig { returns(T.nilable(String)) }
            attr_accessor :working_directory

            sig do
              override
                .returns({
                  command: T::Array[String],
                  env: T::Hash[Symbol, String],
                  type: Symbol,
                  timeout_ms: T.nilable(Integer),
                  user: T.nilable(String),
                  working_directory: T.nilable(String)
                })
            end
            def to_hash; end

            class << self
              # Execute a shell command on the server.
              sig do
                params(
                  command: T::Array[String],
                  env: T::Hash[Symbol, String],
                  timeout_ms: T.nilable(Integer),
                  user: T.nilable(String),
                  working_directory: T.nilable(String),
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                command:, # The command to run.
                env:, # Environment variables to set for the command.
                timeout_ms: nil, # Optional timeout in milliseconds for the command.
                user: nil, # Optional user to run the command as.
                working_directory: nil, # Optional working directory to run the command in.
                type: :exec # The type of the local shell action. Always `exec`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseOutputItem::LocalShellCall::Action,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputItem::LocalShellCall,
                OpenAI::Internal::AnyHash
              )
            end

          # The status of the local shell call.
          module Status
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::ResponseOutputItem::LocalShellCall::Status::TaggedSymbol
                ])
              end
              def values; end
            end

            COMPLETED = T.let(
                :completed,
                OpenAI::Responses::ResponseOutputItem::LocalShellCall::Status::TaggedSymbol
              )

            INCOMPLETE = T.let(
                :incomplete,
                OpenAI::Responses::ResponseOutputItem::LocalShellCall::Status::TaggedSymbol
              )

            IN_PROGRESS = T.let(
                :in_progress,
                OpenAI::Responses::ResponseOutputItem::LocalShellCall::Status::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::ResponseOutputItem::LocalShellCall::Status
                )
              end
          end
        end

        class McpApprovalRequest < OpenAI::Internal::Type::BaseModel
          # A JSON string of arguments for the tool.
          sig { returns(String) }
          attr_accessor :arguments

          # The unique ID of the approval request.
          sig { returns(String) }
          attr_accessor :id

          # The name of the tool to run.
          sig { returns(String) }
          attr_accessor :name

          # The label of the MCP server making the request.
          sig { returns(String) }
          attr_accessor :server_label

          # The type of the item. Always `mcp_approval_request`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A request for human approval of a tool invocation.
            sig do
              params(
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the approval request.
              arguments:, # A JSON string of arguments for the tool.
              name:, # The name of the tool to run.
              server_label:, # The label of the MCP server making the request.
              type: :mcp_approval_request # The type of the item. Always `mcp_approval_request`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputItem::McpApprovalRequest,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class McpCall < OpenAI::Internal::Type::BaseModel
          # A JSON string of the arguments passed to the tool.
          sig { returns(String) }
          attr_accessor :arguments

          # The error from the tool call, if any.
          sig { returns(T.nilable(String)) }
          attr_accessor :error

          # The unique ID of the tool call.
          sig { returns(String) }
          attr_accessor :id

          # The name of the tool that was run.
          sig { returns(String) }
          attr_accessor :name

          # The output from the tool call.
          sig { returns(T.nilable(String)) }
          attr_accessor :output

          # The label of the MCP server running the tool.
          sig { returns(String) }
          attr_accessor :server_label

          # The type of the item. Always `mcp_call`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                type: Symbol,
                error: T.nilable(String),
                output: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            # An invocation of a tool on an MCP server.
            sig do
              params(
                id: String,
                arguments: String,
                name: String,
                server_label: String,
                error: T.nilable(String),
                output: T.nilable(String),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the tool call.
              arguments:, # A JSON string of the arguments passed to the tool.
              name:, # The name of the tool that was run.
              server_label:, # The label of the MCP server running the tool.
              error: nil, # The error from the tool call, if any.
              output: nil, # The output from the tool call.
              type: :mcp_call # The type of the item. Always `mcp_call`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputItem::McpCall,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class McpListTools < OpenAI::Internal::Type::BaseModel
          # Error message if the server could not list tools.
          sig { returns(T.nilable(String)) }
          attr_accessor :error

          # The unique ID of the list.
          sig { returns(String) }
          attr_accessor :id

          # The label of the MCP server.
          sig { returns(String) }
          attr_accessor :server_label

          # The tools available on the server.
          sig do
            returns(T::Array[
                OpenAI::Responses::ResponseOutputItem::McpListTools::Tool
              ])
          end
          attr_accessor :tools

          # The type of the item. Always `mcp_list_tools`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                id: String,
                server_label: String,
                tools:
                  T::Array[
                    OpenAI::Responses::ResponseOutputItem::McpListTools::Tool
                  ],
                type: Symbol,
                error: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            # A list of tools available on an MCP server.
            sig do
              params(
                id: String,
                server_label: String,
                tools: T::Array[
                  OpenAI::Responses::ResponseOutputItem::McpListTools::Tool::OrHash
                ],
                error: T.nilable(String),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              id:, # The unique ID of the list.
              server_label:, # The label of the MCP server.
              tools:, # The tools available on the server.
              error: nil, # Error message if the server could not list tools.
              type: :mcp_list_tools # The type of the item. Always `mcp_list_tools`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputItem::McpListTools,
                OpenAI::Internal::AnyHash
              )
            end

          class Tool < OpenAI::Internal::Type::BaseModel
            # Additional annotations about the tool.
            sig { returns(T.nilable(T.anything)) }
            attr_accessor :annotations

            # The description of the tool.
            sig { returns(T.nilable(String)) }
            attr_accessor :description

            # The JSON schema describing the tool's input.
            sig { returns(T.anything) }
            attr_accessor :input_schema

            # The name of the tool.
            sig { returns(String) }
            attr_accessor :name

            sig do
              override
                .returns({
                  input_schema: T.anything,
                  name: String,
                  annotations: T.nilable(T.anything),
                  description: T.nilable(String)
                })
            end
            def to_hash; end

            class << self
              # A tool available on an MCP server.
              sig do
                params(
                  input_schema: T.anything,
                  name: String,
                  annotations: T.nilable(T.anything),
                  description: T.nilable(String)
                ).returns(T.attached_class)
              end
              def new(
                input_schema:, # The JSON schema describing the tool's input.
                name:, # The name of the tool.
                annotations: nil, # Additional annotations about the tool.
                description: nil # The description of the tool.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseOutputItem::McpListTools::Tool,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseOutputMessage,
              OpenAI::Responses::ResponseFileSearchToolCall,
              OpenAI::Responses::ResponseFunctionToolCall,
              OpenAI::Responses::ResponseFunctionWebSearch,
              OpenAI::Responses::ResponseComputerToolCall,
              OpenAI::Responses::ResponseReasoningItem,
              OpenAI::Responses::ResponseOutputItem::ImageGenerationCall,
              OpenAI::Responses::ResponseCodeInterpreterToolCall,
              OpenAI::Responses::ResponseOutputItem::LocalShellCall,
              OpenAI::Responses::ResponseOutputItem::McpCall,
              OpenAI::Responses::ResponseOutputItem::McpListTools,
              OpenAI::Responses::ResponseOutputItem::McpApprovalRequest
            )
          end
      end

      class ResponseOutputItemAddedEvent < OpenAI::Internal::Type::BaseModel
        # The output item that was added.
        sig { returns(OpenAI::Responses::ResponseOutputItem::Variants) }
        attr_accessor :item

        # The index of the output item that was added.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.output_item.added`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item: OpenAI::Responses::ResponseOutputItem::Variants,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a new output item is added.
          sig do
            params(
              item: T.any(
                OpenAI::Responses::ResponseOutputMessage::OrHash,
                OpenAI::Responses::ResponseFileSearchToolCall::OrHash,
                OpenAI::Responses::ResponseFunctionToolCall::OrHash,
                OpenAI::Responses::ResponseFunctionWebSearch::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::OrHash,
                OpenAI::Responses::ResponseReasoningItem::OrHash,
                OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::OrHash,
                OpenAI::Responses::ResponseCodeInterpreterToolCall::OrHash,
                OpenAI::Responses::ResponseOutputItem::LocalShellCall::OrHash,
                OpenAI::Responses::ResponseOutputItem::McpCall::OrHash,
                OpenAI::Responses::ResponseOutputItem::McpListTools::OrHash,
                OpenAI::Responses::ResponseOutputItem::McpApprovalRequest::OrHash
              ),
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item:, # The output item that was added.
            output_index:, # The index of the output item that was added.
            sequence_number:, # The sequence number of this event.
            type: :"response.output_item.added" # The type of the event. Always `response.output_item.added`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseOutputItemAddedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseOutputItemDoneEvent < OpenAI::Internal::Type::BaseModel
        # The output item that was marked done.
        sig { returns(OpenAI::Responses::ResponseOutputItem::Variants) }
        attr_accessor :item

        # The index of the output item that was marked done.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.output_item.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item: OpenAI::Responses::ResponseOutputItem::Variants,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when an output item is marked done.
          sig do
            params(
              item: T.any(
                OpenAI::Responses::ResponseOutputMessage::OrHash,
                OpenAI::Responses::ResponseFileSearchToolCall::OrHash,
                OpenAI::Responses::ResponseFunctionToolCall::OrHash,
                OpenAI::Responses::ResponseFunctionWebSearch::OrHash,
                OpenAI::Responses::ResponseComputerToolCall::OrHash,
                OpenAI::Responses::ResponseReasoningItem::OrHash,
                OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::OrHash,
                OpenAI::Responses::ResponseCodeInterpreterToolCall::OrHash,
                OpenAI::Responses::ResponseOutputItem::LocalShellCall::OrHash,
                OpenAI::Responses::ResponseOutputItem::McpCall::OrHash,
                OpenAI::Responses::ResponseOutputItem::McpListTools::OrHash,
                OpenAI::Responses::ResponseOutputItem::McpApprovalRequest::OrHash
              ),
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item:, # The output item that was marked done.
            output_index:, # The index of the output item that was marked done.
            sequence_number:, # The sequence number of this event.
            type: :"response.output_item.done" # The type of the event. Always `response.output_item.done`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseOutputItemDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseOutputMessage < OpenAI::Internal::Type::BaseModel
        # The content of the output message.
        sig do
          returns(T::Array[
              T.any(
                OpenAI::Responses::ResponseOutputText,
                OpenAI::Responses::ResponseOutputRefusal
              )
            ])
        end
        attr_accessor :content

        # The unique ID of the output message.
        sig { returns(String) }
        attr_accessor :id

        # The role of the output message. Always `assistant`.
        sig { returns(Symbol) }
        attr_accessor :role

        # The status of the message input. One of `in_progress`, `completed`, or
        # `incomplete`. Populated when input items are returned via API.
        sig { returns(OpenAI::Responses::ResponseOutputMessage::Status::OrSymbol) }
        attr_accessor :status

        # The type of the output message. Always `message`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              content:
                T::Array[
                  T.any(
                    OpenAI::Responses::ResponseOutputText,
                    OpenAI::Responses::ResponseOutputRefusal
                  )
                ],
              role: Symbol,
              status:
                OpenAI::Responses::ResponseOutputMessage::Status::OrSymbol,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # An output message from the model.
          sig do
            params(
              id: String,
              content: T::Array[
                T.any(
                  OpenAI::Responses::ResponseOutputText::OrHash,
                  OpenAI::Responses::ResponseOutputRefusal::OrHash
                )
              ],
              status: OpenAI::Responses::ResponseOutputMessage::Status::OrSymbol,
              role: Symbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the output message.
            content:, # The content of the output message.
            status:, # The status of the message input. One of `in_progress`, `completed`, or
                     # `incomplete`. Populated when input items are returned via API.
            role: :assistant, # The role of the output message. Always `assistant`.
            type: :message # The type of the output message. Always `message`.
); end
        end

        # A text output from the model.
        module Content
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseOutputMessage::Content::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputText,
                OpenAI::Responses::ResponseOutputRefusal
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseOutputMessage,
              OpenAI::Internal::AnyHash
            )
          end

        # The status of the message input. One of `in_progress`, `completed`, or
        # `incomplete`. Populated when input items are returned via API.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseOutputMessage::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseOutputMessage::Status::TaggedSymbol
            )

          INCOMPLETE = T.let(
              :incomplete,
              OpenAI::Responses::ResponseOutputMessage::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseOutputMessage::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseOutputMessage::Status)
            end
        end
      end

      class ResponseOutputRefusal < OpenAI::Internal::Type::BaseModel
        # The refusal explanation from the model.
        sig { returns(String) }
        attr_accessor :refusal

        # The type of the refusal. Always `refusal`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ refusal: String, type: Symbol }) }
        def to_hash; end

        class << self
          # A refusal from the model.
          sig { params(refusal: String, type: Symbol).returns(T.attached_class) }
          def new(
            refusal:, # The refusal explanation from the model.
            type: :refusal # The type of the refusal. Always `refusal`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseOutputRefusal,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseOutputText < OpenAI::Internal::Type::BaseModel
        # The annotations of the text output.
        sig do
          returns(T::Array[
              T.any(
                OpenAI::Responses::ResponseOutputText::Annotation::FileCitation,
                OpenAI::Responses::ResponseOutputText::Annotation::URLCitation,
                OpenAI::Responses::ResponseOutputText::Annotation::ContainerFileCitation,
                OpenAI::Responses::ResponseOutputText::Annotation::FilePath
              )
            ])
        end
        attr_accessor :annotations

        sig { returns(T.nilable(T::Array[OpenAI::Responses::ResponseOutputText::Logprob])) }
        attr_reader :logprobs

        sig { params(logprobs: T::Array[OpenAI::Responses::ResponseOutputText::Logprob::OrHash]).void }
        attr_writer :logprobs

        # The parsed contents of the output, if JSON schema is specified.
        sig { returns(T.anything) }
        attr_accessor :parsed

        # The text output from the model.
        sig { returns(String) }
        attr_accessor :text

        # The type of the output text. Always `output_text`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              annotations:
                T::Array[
                  T.any(
                    OpenAI::Responses::ResponseOutputText::Annotation::FileCitation,
                    OpenAI::Responses::ResponseOutputText::Annotation::URLCitation,
                    OpenAI::Responses::ResponseOutputText::Annotation::ContainerFileCitation,
                    OpenAI::Responses::ResponseOutputText::Annotation::FilePath
                  )
                ],
              text: String,
              type: Symbol,
              logprobs: T::Array[OpenAI::Responses::ResponseOutputText::Logprob]
            })
        end
        def to_hash; end

        class << self
          # A text output from the model.
          sig do
            params(
              annotations: T::Array[
                T.any(
                  OpenAI::Responses::ResponseOutputText::Annotation::FileCitation::OrHash,
                  OpenAI::Responses::ResponseOutputText::Annotation::URLCitation::OrHash,
                  OpenAI::Responses::ResponseOutputText::Annotation::ContainerFileCitation::OrHash,
                  OpenAI::Responses::ResponseOutputText::Annotation::FilePath::OrHash
                )
              ],
              text: String,
              logprobs: T::Array[OpenAI::Responses::ResponseOutputText::Logprob::OrHash],
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            annotations:, # The annotations of the text output.
            text:, # The text output from the model.
            logprobs: nil,
            type: :output_text # The type of the output text. Always `output_text`.
); end
        end

        # A citation to a file.
        module Annotation
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseOutputText::Annotation::Variants
              ])
            end
            def variants; end
          end

          class ContainerFileCitation < OpenAI::Internal::Type::BaseModel
            # The ID of the container file.
            sig { returns(String) }
            attr_accessor :container_id

            # The index of the last character of the container file citation in the message.
            sig { returns(Integer) }
            attr_accessor :end_index

            # The ID of the file.
            sig { returns(String) }
            attr_accessor :file_id

            # The filename of the container file cited.
            sig { returns(String) }
            attr_accessor :filename

            # The index of the first character of the container file citation in the message.
            sig { returns(Integer) }
            attr_accessor :start_index

            # The type of the container file citation. Always `container_file_citation`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  container_id: String,
                  end_index: Integer,
                  file_id: String,
                  filename: String,
                  start_index: Integer,
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              # A citation for a container file used to generate a model response.
              sig do
                params(
                  container_id: String,
                  end_index: Integer,
                  file_id: String,
                  filename: String,
                  start_index: Integer,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                container_id:, # The ID of the container file.
                end_index:, # The index of the last character of the container file citation in the message.
                file_id:, # The ID of the file.
                filename:, # The filename of the container file cited.
                start_index:, # The index of the first character of the container file citation in the message.
                type: :container_file_citation # The type of the container file citation. Always `container_file_citation`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseOutputText::Annotation::ContainerFileCitation,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FileCitation < OpenAI::Internal::Type::BaseModel
            # The ID of the file.
            sig { returns(String) }
            attr_accessor :file_id

            # The filename of the file cited.
            sig { returns(String) }
            attr_accessor :filename

            # The index of the file in the list of files.
            sig { returns(Integer) }
            attr_accessor :index

            # The type of the file citation. Always `file_citation`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig do
              override
                .returns({
                  file_id: String,
                  filename: String,
                  index: Integer,
                  type: Symbol
                })
            end
            def to_hash; end

            class << self
              # A citation to a file.
              sig { params(file_id: String, filename: String, index: Integer, type: Symbol).returns(T.attached_class) }
              def new(
                file_id:, # The ID of the file.
                filename:, # The filename of the file cited.
                index:, # The index of the file in the list of files.
                type: :file_citation # The type of the file citation. Always `file_citation`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseOutputText::Annotation::FileCitation,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class FilePath < OpenAI::Internal::Type::BaseModel
            # The ID of the file.
            sig { returns(String) }
            attr_accessor :file_id

            # The index of the file in the list of files.
            sig { returns(Integer) }
            attr_accessor :index

            # The type of the file path. Always `file_path`.
            sig { returns(Symbol) }
            attr_accessor :type

            sig { override.returns({ file_id: String, index: Integer, type: Symbol }) }
            def to_hash; end

            class << self
              # A path to a file.
              sig { params(file_id: String, index: Integer, type: Symbol).returns(T.attached_class) }
              def new(
                file_id:, # The ID of the file.
                index:, # The index of the file in the list of files.
                type: :file_path # The type of the file path. Always `file_path`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseOutputText::Annotation::FilePath,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          class URLCitation < OpenAI::Internal::Type::BaseModel
            # The index of the last character of the URL citation in the message.
            sig { returns(Integer) }
            attr_accessor :end_index

            # The index of the first character of the URL citation in the message.
            sig { returns(Integer) }
            attr_accessor :start_index

            # The title of the web resource.
            sig { returns(String) }
            attr_accessor :title

            # The type of the URL citation. Always `url_citation`.
            sig { returns(Symbol) }
            attr_accessor :type

            # The URL of the web resource.
            sig { returns(String) }
            attr_accessor :url

            sig do
              override
                .returns({
                  end_index: Integer,
                  start_index: Integer,
                  title: String,
                  type: Symbol,
                  url: String
                })
            end
            def to_hash; end

            class << self
              # A citation for a web resource used to generate a model response.
              sig do
                params(
                  end_index: Integer,
                  start_index: Integer,
                  title: String,
                  url: String,
                  type: Symbol
                ).returns(T.attached_class)
              end
              def new(
                end_index:, # The index of the last character of the URL citation in the message.
                start_index:, # The index of the first character of the URL citation in the message.
                title:, # The title of the web resource.
                url:, # The URL of the web resource.
                type: :url_citation # The type of the URL citation. Always `url_citation`.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseOutputText::Annotation::URLCitation,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          Variants = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputText::Annotation::FileCitation,
                OpenAI::Responses::ResponseOutputText::Annotation::URLCitation,
                OpenAI::Responses::ResponseOutputText::Annotation::ContainerFileCitation,
                OpenAI::Responses::ResponseOutputText::Annotation::FilePath
              )
            end
        end

        class Logprob < OpenAI::Internal::Type::BaseModel
          sig { returns(T::Array[Integer]) }
          attr_accessor :bytes

          sig { returns(Float) }
          attr_accessor :logprob

          sig { returns(String) }
          attr_accessor :token

          sig do
            returns(T::Array[
                OpenAI::Responses::ResponseOutputText::Logprob::TopLogprob
              ])
          end
          attr_accessor :top_logprobs

          sig do
            override
              .returns({
                token: String,
                bytes: T::Array[Integer],
                logprob: Float,
                top_logprobs:
                  T::Array[
                    OpenAI::Responses::ResponseOutputText::Logprob::TopLogprob
                  ]
              })
          end
          def to_hash; end

          class << self
            # The log probability of a token.
            sig do
              params(
                token: String,
                bytes: T::Array[Integer],
                logprob: Float,
                top_logprobs: T::Array[
                  OpenAI::Responses::ResponseOutputText::Logprob::TopLogprob::OrHash
                ]
              ).returns(T.attached_class)
            end
            def new(token:, bytes:, logprob:, top_logprobs:); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseOutputText::Logprob,
                OpenAI::Internal::AnyHash
              )
            end

          class TopLogprob < OpenAI::Internal::Type::BaseModel
            sig { returns(T::Array[Integer]) }
            attr_accessor :bytes

            sig { returns(Float) }
            attr_accessor :logprob

            sig { returns(String) }
            attr_accessor :token

            sig { override.returns({ token: String, bytes: T::Array[Integer], logprob: Float }) }
            def to_hash; end

            class << self
              # The top log probability of a token.
              sig { params(token: String, bytes: T::Array[Integer], logprob: Float).returns(T.attached_class) }
              def new(token:, bytes:, logprob:); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseOutputText::Logprob::TopLogprob,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseOutputText,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseOutputTextAnnotationAddedEvent < OpenAI::Internal::Type::BaseModel
        # The annotation object being added. (See annotation schema for details.)
        sig { returns(T.anything) }
        attr_accessor :annotation

        # The index of the annotation within the content part.
        sig { returns(Integer) }
        attr_accessor :annotation_index

        # The index of the content part within the output item.
        sig { returns(Integer) }
        attr_accessor :content_index

        # The unique identifier of the item to which the annotation is being added.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.output_text.annotation.added'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              annotation: T.anything,
              annotation_index: Integer,
              content_index: Integer,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when an annotation is added to output text content.
          sig do
            params(
              annotation: T.anything,
              annotation_index: Integer,
              content_index: Integer,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            annotation:, # The annotation object being added. (See annotation schema for details.)
            annotation_index:, # The index of the annotation within the content part.
            content_index:, # The index of the content part within the output item.
            item_id:, # The unique identifier of the item to which the annotation is being added.
            output_index:, # The index of the output item in the response's output array.
            sequence_number:, # The sequence number of this event.
            type: :"response.output_text.annotation.added" # The type of the event. Always 'response.output_text.annotation.added'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseOutputTextAnnotationAddedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponsePrompt < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the prompt template to use.
        sig { returns(String) }
        attr_accessor :id

        # Optional map of values to substitute in for variables in your prompt. The
        # substitution values can either be strings, or other Response input types like
        # images or files.
        sig do
          returns(T.nilable(
              T::Hash[
                Symbol,
                T.any(
                  String,
                  OpenAI::Responses::ResponseInputText,
                  OpenAI::Responses::ResponseInputImage,
                  OpenAI::Responses::ResponseInputFile
                )
              ]
            ))
        end
        attr_accessor :variables

        # Optional version of the prompt template.
        sig { returns(T.nilable(String)) }
        attr_accessor :version

        sig do
          override
            .returns({
              id: String,
              variables:
                T.nilable(
                  T::Hash[
                    Symbol,
                    T.any(
                      String,
                      OpenAI::Responses::ResponseInputText,
                      OpenAI::Responses::ResponseInputImage,
                      OpenAI::Responses::ResponseInputFile
                    )
                  ]
                ),
              version: T.nilable(String)
            })
        end
        def to_hash; end

        class << self
          # Reference to a prompt template and its variables.
          # [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).
          sig do
            params(
              id: String,
              variables: T.nilable(
                T::Hash[
                  Symbol,
                  T.any(
                    String,
                    OpenAI::Responses::ResponseInputText::OrHash,
                    OpenAI::Responses::ResponseInputImage::OrHash,
                    OpenAI::Responses::ResponseInputFile::OrHash
                  )
                ]
              ),
              version: T.nilable(String)
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique identifier of the prompt template to use.
            variables: nil, # Optional map of values to substitute in for variables in your prompt. The
                            # substitution values can either be strings, or other Response input types like
                            # images or files.
            version: nil # Optional version of the prompt template.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::ResponsePrompt, OpenAI::Internal::AnyHash)
          end

        # A text input to the model.
        module Variable
          extend OpenAI::Internal::Type::Union

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::ResponsePrompt::Variable::Variants]) }
            def variants; end
          end

          Variants = T.type_alias do
              T.any(
                String,
                OpenAI::Responses::ResponseInputText,
                OpenAI::Responses::ResponseInputImage,
                OpenAI::Responses::ResponseInputFile
              )
            end
        end
      end

      class ResponseQueuedEvent < OpenAI::Internal::Type::BaseModel
        # The full response object that is queued.
        sig { returns(OpenAI::Responses::Response) }
        attr_reader :response

        sig { params(response: OpenAI::Responses::Response::OrHash).void }
        attr_writer :response

        # The sequence number for this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always 'response.queued'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              response: OpenAI::Responses::Response,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a response is queued and waiting to be processed.
          sig do
            params(
              response: OpenAI::Responses::Response::OrHash,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            response:, # The full response object that is queued.
            sequence_number:, # The sequence number for this event.
            type: :"response.queued" # The type of the event. Always 'response.queued'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseQueuedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseReasoningItem < OpenAI::Internal::Type::BaseModel
        # The encrypted content of the reasoning item - populated when a response is
        # generated with `reasoning.encrypted_content` in the `include` parameter.
        sig { returns(T.nilable(String)) }
        attr_accessor :encrypted_content

        # The unique identifier of the reasoning content.
        sig { returns(String) }
        attr_accessor :id

        # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        sig do
          returns(T.nilable(
              OpenAI::Responses::ResponseReasoningItem::Status::OrSymbol
            ))
        end
        attr_reader :status

        sig { params(status: OpenAI::Responses::ResponseReasoningItem::Status::OrSymbol).void }
        attr_writer :status

        # Reasoning text contents.
        sig { returns(T::Array[OpenAI::Responses::ResponseReasoningItem::Summary]) }
        attr_accessor :summary

        # The type of the object. Always `reasoning`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              summary:
                T::Array[OpenAI::Responses::ResponseReasoningItem::Summary],
              type: Symbol,
              encrypted_content: T.nilable(String),
              status: OpenAI::Responses::ResponseReasoningItem::Status::OrSymbol
            })
        end
        def to_hash; end

        class << self
          # A description of the chain of thought used by a reasoning model while generating
          # a response. Be sure to include these items in your `input` to the Responses API
          # for subsequent turns of a conversation if you are manually
          # [managing context](https://platform.openai.com/docs/guides/conversation-state).
          sig do
            params(
              id: String,
              summary: T::Array[
                OpenAI::Responses::ResponseReasoningItem::Summary::OrHash
              ],
              encrypted_content: T.nilable(String),
              status: OpenAI::Responses::ResponseReasoningItem::Status::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique identifier of the reasoning content.
            summary:, # Reasoning text contents.
            encrypted_content: nil, # The encrypted content of the reasoning item - populated when a response is
                                    # generated with `reasoning.encrypted_content` in the `include` parameter.
            status: nil, # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
                         # Populated when items are returned via API.
            type: :reasoning # The type of the object. Always `reasoning`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseReasoningItem,
              OpenAI::Internal::AnyHash
            )
          end

        # The status of the item. One of `in_progress`, `completed`, or `incomplete`.
        # Populated when items are returned via API.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::ResponseReasoningItem::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          COMPLETED = T.let(
              :completed,
              OpenAI::Responses::ResponseReasoningItem::Status::TaggedSymbol
            )

          INCOMPLETE = T.let(
              :incomplete,
              OpenAI::Responses::ResponseReasoningItem::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::Responses::ResponseReasoningItem::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ResponseReasoningItem::Status)
            end
        end

        class Summary < OpenAI::Internal::Type::BaseModel
          # A short summary of the reasoning used by the model when generating the response.
          sig { returns(String) }
          attr_accessor :text

          # The type of the object. Always `summary_text`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ text: String, type: Symbol }) }
          def to_hash; end

          class << self
            sig { params(text: String, type: Symbol).returns(T.attached_class) }
            def new(
              text:, # A short summary of the reasoning used by the model when generating the response.
              type: :summary_text # The type of the object. Always `summary_text`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseReasoningItem::Summary,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class ResponseReasoningSummaryDeltaEvent < OpenAI::Internal::Type::BaseModel
        # The partial update to the reasoning summary content.
        sig { returns(T.anything) }
        attr_accessor :delta

        # The unique identifier of the item for which the reasoning summary is being
        # updated.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The index of the summary part within the output item.
        sig { returns(Integer) }
        attr_accessor :summary_index

        # The type of the event. Always 'response.reasoning_summary.delta'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              delta: T.anything,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              summary_index: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when there is a delta (partial update) to the reasoning summary content.
          sig do
            params(
              delta: T.anything,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              summary_index: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            delta:, # The partial update to the reasoning summary content.
            item_id:, # The unique identifier of the item for which the reasoning summary is being
                      # updated.
            output_index:, # The index of the output item in the response's output array.
            sequence_number:, # The sequence number of this event.
            summary_index:, # The index of the summary part within the output item.
            type: :"response.reasoning_summary.delta" # The type of the event. Always 'response.reasoning_summary.delta'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseReasoningSummaryDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseReasoningSummaryDoneEvent < OpenAI::Internal::Type::BaseModel
        # The unique identifier of the item for which the reasoning summary is finalized.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item in the response's output array.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The index of the summary part within the output item.
        sig { returns(Integer) }
        attr_accessor :summary_index

        # The finalized reasoning summary text.
        sig { returns(String) }
        attr_accessor :text

        # The type of the event. Always 'response.reasoning_summary.done'.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              summary_index: Integer,
              text: String,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when the reasoning summary content is finalized for an item.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              summary_index: Integer,
              text: String,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The unique identifier of the item for which the reasoning summary is finalized.
            output_index:, # The index of the output item in the response's output array.
            sequence_number:, # The sequence number of this event.
            summary_index:, # The index of the summary part within the output item.
            text:, # The finalized reasoning summary text.
            type: :"response.reasoning_summary.done" # The type of the event. Always 'response.reasoning_summary.done'.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseReasoningSummaryDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseReasoningSummaryPartAddedEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the item this summary part is associated with.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item this summary part is associated with.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The summary part that was added.
        sig { returns(OpenAI::Responses::ResponseReasoningSummaryPartAddedEvent::Part) }
        attr_reader :part

        sig { params(part: OpenAI::Responses::ResponseReasoningSummaryPartAddedEvent::Part::OrHash).void }
        attr_writer :part

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The index of the summary part within the reasoning summary.
        sig { returns(Integer) }
        attr_accessor :summary_index

        # The type of the event. Always `response.reasoning_summary_part.added`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              part:
                OpenAI::Responses::ResponseReasoningSummaryPartAddedEvent::Part,
              sequence_number: Integer,
              summary_index: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a new reasoning summary part is added.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              part: OpenAI::Responses::ResponseReasoningSummaryPartAddedEvent::Part::OrHash,
              sequence_number: Integer,
              summary_index: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the item this summary part is associated with.
            output_index:, # The index of the output item this summary part is associated with.
            part:, # The summary part that was added.
            sequence_number:, # The sequence number of this event.
            summary_index:, # The index of the summary part within the reasoning summary.
            type: :"response.reasoning_summary_part.added" # The type of the event. Always `response.reasoning_summary_part.added`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseReasoningSummaryPartAddedEvent,
              OpenAI::Internal::AnyHash
            )
          end

        class Part < OpenAI::Internal::Type::BaseModel
          # The text of the summary part.
          sig { returns(String) }
          attr_accessor :text

          # The type of the summary part. Always `summary_text`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ text: String, type: Symbol }) }
          def to_hash; end

          class << self
            # The summary part that was added.
            sig { params(text: String, type: Symbol).returns(T.attached_class) }
            def new(
              text:, # The text of the summary part.
              type: :summary_text # The type of the summary part. Always `summary_text`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseReasoningSummaryPartAddedEvent::Part,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class ResponseReasoningSummaryPartDoneEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the item this summary part is associated with.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item this summary part is associated with.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The completed summary part.
        sig { returns(OpenAI::Responses::ResponseReasoningSummaryPartDoneEvent::Part) }
        attr_reader :part

        sig { params(part: OpenAI::Responses::ResponseReasoningSummaryPartDoneEvent::Part::OrHash).void }
        attr_writer :part

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The index of the summary part within the reasoning summary.
        sig { returns(Integer) }
        attr_accessor :summary_index

        # The type of the event. Always `response.reasoning_summary_part.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              part:
                OpenAI::Responses::ResponseReasoningSummaryPartDoneEvent::Part,
              sequence_number: Integer,
              summary_index: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a reasoning summary part is completed.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              part: OpenAI::Responses::ResponseReasoningSummaryPartDoneEvent::Part::OrHash,
              sequence_number: Integer,
              summary_index: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the item this summary part is associated with.
            output_index:, # The index of the output item this summary part is associated with.
            part:, # The completed summary part.
            sequence_number:, # The sequence number of this event.
            summary_index:, # The index of the summary part within the reasoning summary.
            type: :"response.reasoning_summary_part.done" # The type of the event. Always `response.reasoning_summary_part.done`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseReasoningSummaryPartDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end

        class Part < OpenAI::Internal::Type::BaseModel
          # The text of the summary part.
          sig { returns(String) }
          attr_accessor :text

          # The type of the summary part. Always `summary_text`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ text: String, type: Symbol }) }
          def to_hash; end

          class << self
            # The completed summary part.
            sig { params(text: String, type: Symbol).returns(T.attached_class) }
            def new(
              text:, # The text of the summary part.
              type: :summary_text # The type of the summary part. Always `summary_text`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseReasoningSummaryPartDoneEvent::Part,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class ResponseReasoningSummaryTextDeltaEvent < OpenAI::Internal::Type::BaseModel
        # The text delta that was added to the summary.
        sig { returns(String) }
        attr_accessor :delta

        # The ID of the item this summary text delta is associated with.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item this summary text delta is associated with.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The index of the summary part within the reasoning summary.
        sig { returns(Integer) }
        attr_accessor :summary_index

        # The type of the event. Always `response.reasoning_summary_text.delta`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              summary_index: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a delta is added to a reasoning summary text.
          sig do
            params(
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              summary_index: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            delta:, # The text delta that was added to the summary.
            item_id:, # The ID of the item this summary text delta is associated with.
            output_index:, # The index of the output item this summary text delta is associated with.
            sequence_number:, # The sequence number of this event.
            summary_index:, # The index of the summary part within the reasoning summary.
            type: :"response.reasoning_summary_text.delta" # The type of the event. Always `response.reasoning_summary_text.delta`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseReasoningSummaryTextDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseReasoningSummaryTextDoneEvent < OpenAI::Internal::Type::BaseModel
        # The ID of the item this summary text is associated with.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item this summary text is associated with.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The index of the summary part within the reasoning summary.
        sig { returns(Integer) }
        attr_accessor :summary_index

        # The full text of the completed reasoning summary.
        sig { returns(String) }
        attr_accessor :text

        # The type of the event. Always `response.reasoning_summary_text.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              summary_index: Integer,
              text: String,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a reasoning summary text is completed.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              summary_index: Integer,
              text: String,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # The ID of the item this summary text is associated with.
            output_index:, # The index of the output item this summary text is associated with.
            sequence_number:, # The sequence number of this event.
            summary_index:, # The index of the summary part within the reasoning summary.
            text:, # The full text of the completed reasoning summary.
            type: :"response.reasoning_summary_text.done" # The type of the event. Always `response.reasoning_summary_text.done`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseReasoningSummaryTextDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseRefusalDeltaEvent < OpenAI::Internal::Type::BaseModel
        # The index of the content part that the refusal text is added to.
        sig { returns(Integer) }
        attr_accessor :content_index

        # The refusal text that is added.
        sig { returns(String) }
        attr_accessor :delta

        # The ID of the output item that the refusal text is added to.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the refusal text is added to.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.refusal.delta`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              content_index: Integer,
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when there is a partial refusal text.
          sig do
            params(
              content_index: Integer,
              delta: String,
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content_index:, # The index of the content part that the refusal text is added to.
            delta:, # The refusal text that is added.
            item_id:, # The ID of the output item that the refusal text is added to.
            output_index:, # The index of the output item that the refusal text is added to.
            sequence_number:, # The sequence number of this event.
            type: :"response.refusal.delta" # The type of the event. Always `response.refusal.delta`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseRefusalDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseRefusalDoneEvent < OpenAI::Internal::Type::BaseModel
        # The index of the content part that the refusal text is finalized.
        sig { returns(Integer) }
        attr_accessor :content_index

        # The ID of the output item that the refusal text is finalized.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the refusal text is finalized.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The refusal text that is finalized.
        sig { returns(String) }
        attr_accessor :refusal

        # The sequence number of this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.refusal.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              content_index: Integer,
              item_id: String,
              output_index: Integer,
              refusal: String,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when refusal text is finalized.
          sig do
            params(
              content_index: Integer,
              item_id: String,
              output_index: Integer,
              refusal: String,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content_index:, # The index of the content part that the refusal text is finalized.
            item_id:, # The ID of the output item that the refusal text is finalized.
            output_index:, # The index of the output item that the refusal text is finalized.
            refusal:, # The refusal text that is finalized.
            sequence_number:, # The sequence number of this event.
            type: :"response.refusal.done" # The type of the event. Always `response.refusal.done`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseRefusalDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseRetrieveParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Additional fields to include in the response. See the `include` parameter for
        # Response creation above for more information.
        sig { returns(T.nilable(T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol])) }
        attr_reader :include

        sig { params(include: T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol]).void }
        attr_writer :include

        # The sequence number of the event after which to start streaming.
        sig { returns(T.nilable(Integer)) }
        attr_reader :starting_after

        sig { params(starting_after: Integer).void }
        attr_writer :starting_after

        sig do
          override
            .returns({
              include:
                T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol],
              starting_after: Integer,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              include: T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol],
              starting_after: Integer,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            include: nil, # Additional fields to include in the response. See the `include` parameter for
                          # Response creation above for more information.
            starting_after: nil, # The sequence number of the event after which to start streaming.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseRetrieveParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # The status of the response generation. One of `completed`, `failed`,
      # `in_progress`, `cancelled`, `queued`, or `incomplete`.
      module ResponseStatus
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ResponseStatus::TaggedSymbol]) }
          def values; end
        end

        CANCELLED = T.let(:cancelled, OpenAI::Responses::ResponseStatus::TaggedSymbol)

        COMPLETED = T.let(:completed, OpenAI::Responses::ResponseStatus::TaggedSymbol)

        FAILED = T.let(:failed, OpenAI::Responses::ResponseStatus::TaggedSymbol)

        INCOMPLETE = T.let(:incomplete, OpenAI::Responses::ResponseStatus::TaggedSymbol)

        IN_PROGRESS = T.let(:in_progress, OpenAI::Responses::ResponseStatus::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }
        QUEUED = T.let(:queued, OpenAI::Responses::ResponseStatus::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Responses::ResponseStatus) }
      end

      # Emitted when there is a partial audio response.
      module ResponseStreamEvent
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ResponseStreamEvent::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseAudioDeltaEvent,
              OpenAI::Responses::ResponseAudioDoneEvent,
              OpenAI::Responses::ResponseAudioTranscriptDeltaEvent,
              OpenAI::Responses::ResponseAudioTranscriptDoneEvent,
              OpenAI::Responses::ResponseCodeInterpreterCallCodeDeltaEvent,
              OpenAI::Responses::ResponseCodeInterpreterCallCodeDoneEvent,
              OpenAI::Responses::ResponseCodeInterpreterCallCompletedEvent,
              OpenAI::Responses::ResponseCodeInterpreterCallInProgressEvent,
              OpenAI::Responses::ResponseCodeInterpreterCallInterpretingEvent,
              OpenAI::Responses::ResponseCompletedEvent,
              OpenAI::Responses::ResponseContentPartAddedEvent,
              OpenAI::Responses::ResponseContentPartDoneEvent,
              OpenAI::Responses::ResponseCreatedEvent,
              OpenAI::Responses::ResponseErrorEvent,
              OpenAI::Responses::ResponseFileSearchCallCompletedEvent,
              OpenAI::Responses::ResponseFileSearchCallInProgressEvent,
              OpenAI::Responses::ResponseFileSearchCallSearchingEvent,
              OpenAI::Responses::ResponseFunctionCallArgumentsDeltaEvent,
              OpenAI::Responses::ResponseFunctionCallArgumentsDoneEvent,
              OpenAI::Responses::ResponseInProgressEvent,
              OpenAI::Responses::ResponseFailedEvent,
              OpenAI::Responses::ResponseIncompleteEvent,
              OpenAI::Responses::ResponseOutputItemAddedEvent,
              OpenAI::Responses::ResponseOutputItemDoneEvent,
              OpenAI::Responses::ResponseReasoningSummaryPartAddedEvent,
              OpenAI::Responses::ResponseReasoningSummaryPartDoneEvent,
              OpenAI::Responses::ResponseReasoningSummaryTextDeltaEvent,
              OpenAI::Responses::ResponseReasoningSummaryTextDoneEvent,
              OpenAI::Responses::ResponseRefusalDeltaEvent,
              OpenAI::Responses::ResponseRefusalDoneEvent,
              OpenAI::Responses::ResponseTextDeltaEvent,
              OpenAI::Responses::ResponseTextDoneEvent,
              OpenAI::Responses::ResponseWebSearchCallCompletedEvent,
              OpenAI::Responses::ResponseWebSearchCallInProgressEvent,
              OpenAI::Responses::ResponseWebSearchCallSearchingEvent,
              OpenAI::Responses::ResponseImageGenCallCompletedEvent,
              OpenAI::Responses::ResponseImageGenCallGeneratingEvent,
              OpenAI::Responses::ResponseImageGenCallInProgressEvent,
              OpenAI::Responses::ResponseImageGenCallPartialImageEvent,
              OpenAI::Responses::ResponseMcpCallArgumentsDeltaEvent,
              OpenAI::Responses::ResponseMcpCallArgumentsDoneEvent,
              OpenAI::Responses::ResponseMcpCallCompletedEvent,
              OpenAI::Responses::ResponseMcpCallFailedEvent,
              OpenAI::Responses::ResponseMcpCallInProgressEvent,
              OpenAI::Responses::ResponseMcpListToolsCompletedEvent,
              OpenAI::Responses::ResponseMcpListToolsFailedEvent,
              OpenAI::Responses::ResponseMcpListToolsInProgressEvent,
              OpenAI::Responses::ResponseOutputTextAnnotationAddedEvent,
              OpenAI::Responses::ResponseQueuedEvent,
              OpenAI::Responses::ResponseReasoningSummaryDeltaEvent,
              OpenAI::Responses::ResponseReasoningSummaryDoneEvent
            )
          end
      end

      class ResponseTextConfig < OpenAI::Internal::Type::BaseModel
        # An object specifying the format that the model must output.
        #
        # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
        # ensures the model will match your supplied JSON schema. Learn more in the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        # The default format is `{ "type": "text" }` with no additional options.
        #
        # **Not recommended for gpt-4o and newer models:**
        #
        # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
        # ensures the message the model generates is valid JSON. Using `json_schema` is
        # preferred for models that support it.
        sig do
          returns(T.nilable(
              T.any(
                OpenAI::ResponseFormatText,
                OpenAI::Responses::ResponseFormatTextJSONSchemaConfig,
                OpenAI::ResponseFormatJSONObject
              )
            ))
        end
        attr_reader :format_

        sig do
          params(
            format_: T.any(
                OpenAI::ResponseFormatText::OrHash,
                OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                OpenAI::ResponseFormatJSONObject::OrHash
              )
          ).void
        end
        attr_writer :format_

        sig do
          override
            .returns({
              format_:
                T.any(
                  OpenAI::ResponseFormatText,
                  OpenAI::Responses::ResponseFormatTextJSONSchemaConfig,
                  OpenAI::ResponseFormatJSONObject
                )
            })
        end
        def to_hash; end

        class << self
          # Configuration options for a text response from the model. Can be plain text or
          # structured JSON data. Learn more:
          #
          # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
          # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
          sig do
            params(
              format_: T.any(
                OpenAI::ResponseFormatText::OrHash,
                OpenAI::Responses::ResponseFormatTextJSONSchemaConfig::OrHash,
                OpenAI::ResponseFormatJSONObject::OrHash
              )
            ).returns(T.attached_class)
          end
          def new(
            format_: nil # An object specifying the format that the model must output.
                         # Configuring `{ "type": "json_schema" }` enables Structured Outputs, which
                         # ensures the model will match your supplied JSON schema. Learn more in the
                         # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                         # The default format is `{ "type": "text" }` with no additional options.
                         # **Not recommended for gpt-4o and newer models:**
                         # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                         # ensures the message the model generates is valid JSON. Using `json_schema` is
                         # preferred for models that support it.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseTextConfig,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseTextDeltaEvent < OpenAI::Internal::Type::BaseModel
        # The index of the content part that the text delta was added to.
        sig { returns(Integer) }
        attr_accessor :content_index

        # The text delta that was added.
        sig { returns(String) }
        attr_accessor :delta

        # The ID of the output item that the text delta was added to.
        sig { returns(String) }
        attr_accessor :item_id

        # The log probabilities of the tokens in the delta.
        sig { returns(T::Array[OpenAI::Responses::ResponseTextDeltaEvent::Logprob]) }
        attr_accessor :logprobs

        # The index of the output item that the text delta was added to.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number for this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.output_text.delta`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              content_index: Integer,
              delta: String,
              item_id: String,
              logprobs:
                T::Array[OpenAI::Responses::ResponseTextDeltaEvent::Logprob],
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when there is an additional text delta.
          sig do
            params(
              content_index: Integer,
              delta: String,
              item_id: String,
              logprobs: T::Array[
                OpenAI::Responses::ResponseTextDeltaEvent::Logprob::OrHash
              ],
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content_index:, # The index of the content part that the text delta was added to.
            delta:, # The text delta that was added.
            item_id:, # The ID of the output item that the text delta was added to.
            logprobs:, # The log probabilities of the tokens in the delta.
            output_index:, # The index of the output item that the text delta was added to.
            sequence_number:, # The sequence number for this event.
            type: :"response.output_text.delta" # The type of the event. Always `response.output_text.delta`.
); end
        end

        class Logprob < OpenAI::Internal::Type::BaseModel
          # The log probability of this token.
          sig { returns(Float) }
          attr_accessor :logprob

          # A possible text token.
          sig { returns(String) }
          attr_accessor :token

          # The log probability of the top 20 most likely tokens.
          sig do
            returns(T.nilable(
                T::Array[
                  OpenAI::Responses::ResponseTextDeltaEvent::Logprob::TopLogprob
                ]
              ))
          end
          attr_reader :top_logprobs

          sig do
            params(
              top_logprobs: T::Array[
                  OpenAI::Responses::ResponseTextDeltaEvent::Logprob::TopLogprob::OrHash
                ]
            ).void
          end
          attr_writer :top_logprobs

          sig do
            override
              .returns({
                token: String,
                logprob: Float,
                top_logprobs:
                  T::Array[
                    OpenAI::Responses::ResponseTextDeltaEvent::Logprob::TopLogprob
                  ]
              })
          end
          def to_hash; end

          class << self
            # A logprob is the logarithmic probability that the model assigns to producing a
            # particular token at a given position in the sequence. Less-negative (higher)
            # logprob values indicate greater model confidence in that token choice.
            sig do
              params(
                token: String,
                logprob: Float,
                top_logprobs: T::Array[
                  OpenAI::Responses::ResponseTextDeltaEvent::Logprob::TopLogprob::OrHash
                ]
              ).returns(T.attached_class)
            end
            def new(
              token:, # A possible text token.
              logprob:, # The log probability of this token.
              top_logprobs: nil # The log probability of the top 20 most likely tokens.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseTextDeltaEvent::Logprob,
                OpenAI::Internal::AnyHash
              )
            end

          class TopLogprob < OpenAI::Internal::Type::BaseModel
            # The log probability of this token.
            sig { returns(T.nilable(Float)) }
            attr_reader :logprob

            sig { params(logprob: Float).void }
            attr_writer :logprob

            # A possible text token.
            sig { returns(T.nilable(String)) }
            attr_reader :token

            sig { params(token: String).void }
            attr_writer :token

            sig { override.returns({ token: String, logprob: Float }) }
            def to_hash; end

            class << self
              sig { params(token: String, logprob: Float).returns(T.attached_class) }
              def new(
                token: nil, # A possible text token.
                logprob: nil # The log probability of this token.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseTextDeltaEvent::Logprob::TopLogprob,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseTextDeltaEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseTextDoneEvent < OpenAI::Internal::Type::BaseModel
        # The index of the content part that the text content is finalized.
        sig { returns(Integer) }
        attr_accessor :content_index

        # The ID of the output item that the text content is finalized.
        sig { returns(String) }
        attr_accessor :item_id

        # The log probabilities of the tokens in the delta.
        sig { returns(T::Array[OpenAI::Responses::ResponseTextDoneEvent::Logprob]) }
        attr_accessor :logprobs

        # The index of the output item that the text content is finalized.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number for this event.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The text content that is finalized.
        sig { returns(String) }
        attr_accessor :text

        # The type of the event. Always `response.output_text.done`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              content_index: Integer,
              item_id: String,
              logprobs:
                T::Array[OpenAI::Responses::ResponseTextDoneEvent::Logprob],
              output_index: Integer,
              sequence_number: Integer,
              text: String,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when text content is finalized.
          sig do
            params(
              content_index: Integer,
              item_id: String,
              logprobs: T::Array[
                OpenAI::Responses::ResponseTextDoneEvent::Logprob::OrHash
              ],
              output_index: Integer,
              sequence_number: Integer,
              text: String,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            content_index:, # The index of the content part that the text content is finalized.
            item_id:, # The ID of the output item that the text content is finalized.
            logprobs:, # The log probabilities of the tokens in the delta.
            output_index:, # The index of the output item that the text content is finalized.
            sequence_number:, # The sequence number for this event.
            text:, # The text content that is finalized.
            type: :"response.output_text.done" # The type of the event. Always `response.output_text.done`.
); end
        end

        class Logprob < OpenAI::Internal::Type::BaseModel
          # The log probability of this token.
          sig { returns(Float) }
          attr_accessor :logprob

          # A possible text token.
          sig { returns(String) }
          attr_accessor :token

          # The log probability of the top 20 most likely tokens.
          sig do
            returns(T.nilable(
                T::Array[
                  OpenAI::Responses::ResponseTextDoneEvent::Logprob::TopLogprob
                ]
              ))
          end
          attr_reader :top_logprobs

          sig do
            params(
              top_logprobs: T::Array[
                  OpenAI::Responses::ResponseTextDoneEvent::Logprob::TopLogprob::OrHash
                ]
            ).void
          end
          attr_writer :top_logprobs

          sig do
            override
              .returns({
                token: String,
                logprob: Float,
                top_logprobs:
                  T::Array[
                    OpenAI::Responses::ResponseTextDoneEvent::Logprob::TopLogprob
                  ]
              })
          end
          def to_hash; end

          class << self
            # A logprob is the logarithmic probability that the model assigns to producing a
            # particular token at a given position in the sequence. Less-negative (higher)
            # logprob values indicate greater model confidence in that token choice.
            sig do
              params(
                token: String,
                logprob: Float,
                top_logprobs: T::Array[
                  OpenAI::Responses::ResponseTextDoneEvent::Logprob::TopLogprob::OrHash
                ]
              ).returns(T.attached_class)
            end
            def new(
              token:, # A possible text token.
              logprob:, # The log probability of this token.
              top_logprobs: nil # The log probability of the top 20 most likely tokens.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseTextDoneEvent::Logprob,
                OpenAI::Internal::AnyHash
              )
            end

          class TopLogprob < OpenAI::Internal::Type::BaseModel
            # The log probability of this token.
            sig { returns(T.nilable(Float)) }
            attr_reader :logprob

            sig { params(logprob: Float).void }
            attr_writer :logprob

            # A possible text token.
            sig { returns(T.nilable(String)) }
            attr_reader :token

            sig { params(token: String).void }
            attr_writer :token

            sig { override.returns({ token: String, logprob: Float }) }
            def to_hash; end

            class << self
              sig { params(token: String, logprob: Float).returns(T.attached_class) }
              def new(
                token: nil, # A possible text token.
                logprob: nil # The log probability of this token.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::ResponseTextDoneEvent::Logprob::TopLogprob,
                  OpenAI::Internal::AnyHash
                )
              end
          end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseTextDoneEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseUsage < OpenAI::Internal::Type::BaseModel
        # The number of input tokens.
        sig { returns(Integer) }
        attr_accessor :input_tokens

        # A detailed breakdown of the input tokens.
        sig { returns(OpenAI::Responses::ResponseUsage::InputTokensDetails) }
        attr_reader :input_tokens_details

        sig { params(input_tokens_details: OpenAI::Responses::ResponseUsage::InputTokensDetails::OrHash).void }
        attr_writer :input_tokens_details

        # The number of output tokens.
        sig { returns(Integer) }
        attr_accessor :output_tokens

        # A detailed breakdown of the output tokens.
        sig { returns(OpenAI::Responses::ResponseUsage::OutputTokensDetails) }
        attr_reader :output_tokens_details

        sig { params(output_tokens_details: OpenAI::Responses::ResponseUsage::OutputTokensDetails::OrHash).void }
        attr_writer :output_tokens_details

        # The total number of tokens used.
        sig { returns(Integer) }
        attr_accessor :total_tokens

        sig do
          override
            .returns({
              input_tokens: Integer,
              input_tokens_details:
                OpenAI::Responses::ResponseUsage::InputTokensDetails,
              output_tokens: Integer,
              output_tokens_details:
                OpenAI::Responses::ResponseUsage::OutputTokensDetails,
              total_tokens: Integer
            })
        end
        def to_hash; end

        class << self
          # Represents token usage details including input tokens, output tokens, a
          # breakdown of output tokens, and the total tokens used.
          sig do
            params(
              input_tokens: Integer,
              input_tokens_details: OpenAI::Responses::ResponseUsage::InputTokensDetails::OrHash,
              output_tokens: Integer,
              output_tokens_details: OpenAI::Responses::ResponseUsage::OutputTokensDetails::OrHash,
              total_tokens: Integer
            ).returns(T.attached_class)
          end
          def new(
            input_tokens:, # The number of input tokens.
            input_tokens_details:, # A detailed breakdown of the input tokens.
            output_tokens:, # The number of output tokens.
            output_tokens_details:, # A detailed breakdown of the output tokens.
            total_tokens: # The total number of tokens used.
); end
        end

        class InputTokensDetails < OpenAI::Internal::Type::BaseModel
          # The number of tokens that were retrieved from the cache.
          # [More on prompt caching](https://platform.openai.com/docs/guides/prompt-caching).
          sig { returns(Integer) }
          attr_accessor :cached_tokens

          sig { override.returns({ cached_tokens: Integer }) }
          def to_hash; end

          class << self
            # A detailed breakdown of the input tokens.
            sig { params(cached_tokens: Integer).returns(T.attached_class) }
            def new(
              cached_tokens: # The number of tokens that were retrieved from the cache.
                             # [More on prompt caching](https://platform.openai.com/docs/guides/prompt-caching).
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseUsage::InputTokensDetails,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::ResponseUsage, OpenAI::Internal::AnyHash)
          end

        class OutputTokensDetails < OpenAI::Internal::Type::BaseModel
          # The number of reasoning tokens.
          sig { returns(Integer) }
          attr_accessor :reasoning_tokens

          sig { override.returns({ reasoning_tokens: Integer }) }
          def to_hash; end

          class << self
            # A detailed breakdown of the output tokens.
            sig { params(reasoning_tokens: Integer).returns(T.attached_class) }
            def new(
              reasoning_tokens: # The number of reasoning tokens.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::ResponseUsage::OutputTokensDetails,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end

      class ResponseWebSearchCallCompletedEvent < OpenAI::Internal::Type::BaseModel
        # Unique ID for the output item associated with the web search call.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the web search call is associated with.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of the web search call being processed.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.web_search_call.completed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a web search call is completed.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # Unique ID for the output item associated with the web search call.
            output_index:, # The index of the output item that the web search call is associated with.
            sequence_number:, # The sequence number of the web search call being processed.
            type: :"response.web_search_call.completed" # The type of the event. Always `response.web_search_call.completed`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseWebSearchCallCompletedEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseWebSearchCallInProgressEvent < OpenAI::Internal::Type::BaseModel
        # Unique ID for the output item associated with the web search call.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the web search call is associated with.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of the web search call being processed.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.web_search_call.in_progress`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a web search call is initiated.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # Unique ID for the output item associated with the web search call.
            output_index:, # The index of the output item that the web search call is associated with.
            sequence_number:, # The sequence number of the web search call being processed.
            type: :"response.web_search_call.in_progress" # The type of the event. Always `response.web_search_call.in_progress`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseWebSearchCallInProgressEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseWebSearchCallSearchingEvent < OpenAI::Internal::Type::BaseModel
        # Unique ID for the output item associated with the web search call.
        sig { returns(String) }
        attr_accessor :item_id

        # The index of the output item that the web search call is associated with.
        sig { returns(Integer) }
        attr_accessor :output_index

        # The sequence number of the web search call being processed.
        sig { returns(Integer) }
        attr_accessor :sequence_number

        # The type of the event. Always `response.web_search_call.searching`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            })
        end
        def to_hash; end

        class << self
          # Emitted when a web search call is executing.
          sig do
            params(
              item_id: String,
              output_index: Integer,
              sequence_number: Integer,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            item_id:, # Unique ID for the output item associated with the web search call.
            output_index:, # The index of the output item that the web search call is associated with.
            sequence_number:, # The sequence number of the web search call being processed.
            type: :"response.web_search_call.searching" # The type of the event. Always `response.web_search_call.searching`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ResponseWebSearchCallSearchingEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # A tool that can be used to generate a response.
      module Tool
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::Tool::Variants]) }
          def variants; end
        end

        class CodeInterpreter < OpenAI::Internal::Type::BaseModel
          # The code interpreter container. Can be a container ID or an object that
          # specifies uploaded file IDs to make available to your code.
          sig do
            returns(T.any(
                String,
                OpenAI::Responses::Tool::CodeInterpreter::Container::CodeInterpreterToolAuto
              ))
          end
          attr_accessor :container

          # The type of the code interpreter tool. Always `code_interpreter`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                container:
                  T.any(
                    String,
                    OpenAI::Responses::Tool::CodeInterpreter::Container::CodeInterpreterToolAuto
                  ),
                type: Symbol
              })
          end
          def to_hash; end

          class << self
            # A tool that runs Python code to help generate a response to a prompt.
            sig do
              params(
                container: T.any(
                  String,
                  OpenAI::Responses::Tool::CodeInterpreter::Container::CodeInterpreterToolAuto::OrHash
                ),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              container:, # The code interpreter container. Can be a container ID or an object that
                          # specifies uploaded file IDs to make available to your code.
              type: :code_interpreter # The type of the code interpreter tool. Always `code_interpreter`.
); end
          end

          # The code interpreter container. Can be a container ID or an object that
          # specifies uploaded file IDs to make available to your code.
          module Container
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Tool::CodeInterpreter::Container::Variants
                ])
              end
              def variants; end
            end

            class CodeInterpreterToolAuto < OpenAI::Internal::Type::BaseModel
              # An optional list of uploaded files to make available to your code.
              sig { returns(T.nilable(T::Array[String])) }
              attr_reader :file_ids

              sig { params(file_ids: T::Array[String]).void }
              attr_writer :file_ids

              # Always `auto`.
              sig { returns(Symbol) }
              attr_accessor :type

              sig { override.returns({ type: Symbol, file_ids: T::Array[String] }) }
              def to_hash; end

              class << self
                # Configuration for a code interpreter container. Optionally specify the IDs of
                # the files to run the code on.
                sig { params(file_ids: T::Array[String], type: Symbol).returns(T.attached_class) }
                def new(
                  file_ids: nil, # An optional list of uploaded files to make available to your code.
                  type: :auto # Always `auto`.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Responses::Tool::CodeInterpreter::Container::CodeInterpreterToolAuto,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            Variants = T.type_alias do
                T.any(
                  String,
                  OpenAI::Responses::Tool::CodeInterpreter::Container::CodeInterpreterToolAuto
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::Tool::CodeInterpreter,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class ImageGeneration < OpenAI::Internal::Type::BaseModel
          # Background type for the generated image. One of `transparent`, `opaque`, or
          # `auto`. Default: `auto`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::Tool::ImageGeneration::Background::OrSymbol
              ))
          end
          attr_reader :background

          sig { params(background: OpenAI::Responses::Tool::ImageGeneration::Background::OrSymbol).void }
          attr_writer :background

          # Control how much effort the model will exert to match the style and features,
          # especially facial features, of input images. This parameter is only supported
          # for `gpt-image-1`. Supports `high` and `low`. Defaults to `low`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::Tool::ImageGeneration::InputFidelity::OrSymbol
              ))
          end
          attr_accessor :input_fidelity

          # Optional mask for inpainting. Contains `image_url` (string, optional) and
          # `file_id` (string, optional).
          sig do
            returns(T.nilable(
                OpenAI::Responses::Tool::ImageGeneration::InputImageMask
              ))
          end
          attr_reader :input_image_mask

          sig { params(input_image_mask: OpenAI::Responses::Tool::ImageGeneration::InputImageMask::OrHash).void }
          attr_writer :input_image_mask

          # The image generation model to use. Default: `gpt-image-1`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::Tool::ImageGeneration::Model::OrSymbol
              ))
          end
          attr_reader :model

          sig { params(model: OpenAI::Responses::Tool::ImageGeneration::Model::OrSymbol).void }
          attr_writer :model

          # Moderation level for the generated image. Default: `auto`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::Tool::ImageGeneration::Moderation::OrSymbol
              ))
          end
          attr_reader :moderation

          sig { params(moderation: OpenAI::Responses::Tool::ImageGeneration::Moderation::OrSymbol).void }
          attr_writer :moderation

          # Compression level for the output image. Default: 100.
          sig { returns(T.nilable(Integer)) }
          attr_reader :output_compression

          sig { params(output_compression: Integer).void }
          attr_writer :output_compression

          # The output format of the generated image. One of `png`, `webp`, or `jpeg`.
          # Default: `png`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::Tool::ImageGeneration::OutputFormat::OrSymbol
              ))
          end
          attr_reader :output_format

          sig { params(output_format: OpenAI::Responses::Tool::ImageGeneration::OutputFormat::OrSymbol).void }
          attr_writer :output_format

          # Number of partial images to generate in streaming mode, from 0 (default value)
          # to 3.
          sig { returns(T.nilable(Integer)) }
          attr_reader :partial_images

          sig { params(partial_images: Integer).void }
          attr_writer :partial_images

          # The quality of the generated image. One of `low`, `medium`, `high`, or `auto`.
          # Default: `auto`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::Tool::ImageGeneration::Quality::OrSymbol
              ))
          end
          attr_reader :quality

          sig { params(quality: OpenAI::Responses::Tool::ImageGeneration::Quality::OrSymbol).void }
          attr_writer :quality

          # The size of the generated image. One of `1024x1024`, `1024x1536`, `1536x1024`,
          # or `auto`. Default: `auto`.
          sig do
            returns(T.nilable(
                OpenAI::Responses::Tool::ImageGeneration::Size::OrSymbol
              ))
          end
          attr_reader :size

          sig { params(size: OpenAI::Responses::Tool::ImageGeneration::Size::OrSymbol).void }
          attr_writer :size

          # The type of the image generation tool. Always `image_generation`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                type: Symbol,
                background:
                  OpenAI::Responses::Tool::ImageGeneration::Background::OrSymbol,
                input_fidelity:
                  T.nilable(
                    OpenAI::Responses::Tool::ImageGeneration::InputFidelity::OrSymbol
                  ),
                input_image_mask:
                  OpenAI::Responses::Tool::ImageGeneration::InputImageMask,
                model:
                  OpenAI::Responses::Tool::ImageGeneration::Model::OrSymbol,
                moderation:
                  OpenAI::Responses::Tool::ImageGeneration::Moderation::OrSymbol,
                output_compression: Integer,
                output_format:
                  OpenAI::Responses::Tool::ImageGeneration::OutputFormat::OrSymbol,
                partial_images: Integer,
                quality:
                  OpenAI::Responses::Tool::ImageGeneration::Quality::OrSymbol,
                size: OpenAI::Responses::Tool::ImageGeneration::Size::OrSymbol
              })
          end
          def to_hash; end

          class << self
            # A tool that generates images using a model like `gpt-image-1`.
            sig do
              params(
                background: OpenAI::Responses::Tool::ImageGeneration::Background::OrSymbol,
                input_fidelity: T.nilable(
                  OpenAI::Responses::Tool::ImageGeneration::InputFidelity::OrSymbol
                ),
                input_image_mask: OpenAI::Responses::Tool::ImageGeneration::InputImageMask::OrHash,
                model: OpenAI::Responses::Tool::ImageGeneration::Model::OrSymbol,
                moderation: OpenAI::Responses::Tool::ImageGeneration::Moderation::OrSymbol,
                output_compression: Integer,
                output_format: OpenAI::Responses::Tool::ImageGeneration::OutputFormat::OrSymbol,
                partial_images: Integer,
                quality: OpenAI::Responses::Tool::ImageGeneration::Quality::OrSymbol,
                size: OpenAI::Responses::Tool::ImageGeneration::Size::OrSymbol,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              background: nil, # Background type for the generated image. One of `transparent`, `opaque`, or
                               # `auto`. Default: `auto`.
              input_fidelity: nil, # Control how much effort the model will exert to match the style and features,
                                   # especially facial features, of input images. This parameter is only supported
                                   # for `gpt-image-1`. Supports `high` and `low`. Defaults to `low`.
              input_image_mask: nil, # Optional mask for inpainting. Contains `image_url` (string, optional) and
                                     # `file_id` (string, optional).
              model: nil, # The image generation model to use. Default: `gpt-image-1`.
              moderation: nil, # Moderation level for the generated image. Default: `auto`.
              output_compression: nil, # Compression level for the output image. Default: 100.
              output_format: nil, # The output format of the generated image. One of `png`, `webp`, or `jpeg`.
                                  # Default: `png`.
              partial_images: nil, # Number of partial images to generate in streaming mode, from 0 (default value)
                                   # to 3.
              quality: nil, # The quality of the generated image. One of `low`, `medium`, `high`, or `auto`.
                            # Default: `auto`.
              size: nil, # The size of the generated image. One of `1024x1024`, `1024x1536`, `1536x1024`,
                         # or `auto`. Default: `auto`.
              type: :image_generation # The type of the image generation tool. Always `image_generation`.
); end
          end

          # Background type for the generated image. One of `transparent`, `opaque`, or
          # `auto`. Default: `auto`.
          module Background
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Tool::ImageGeneration::Background::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Responses::Tool::ImageGeneration::Background::TaggedSymbol
              )

            OPAQUE = T.let(
                :opaque,
                OpenAI::Responses::Tool::ImageGeneration::Background::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TRANSPARENT = T.let(
                :transparent,
                OpenAI::Responses::Tool::ImageGeneration::Background::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::Tool::ImageGeneration::Background
                )
              end
          end

          # Control how much effort the model will exert to match the style and features,
          # especially facial features, of input images. This parameter is only supported
          # for `gpt-image-1`. Supports `high` and `low`. Defaults to `low`.
          module InputFidelity
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Tool::ImageGeneration::InputFidelity::TaggedSymbol
                ])
              end
              def values; end
            end

            HIGH = T.let(
                :high,
                OpenAI::Responses::Tool::ImageGeneration::InputFidelity::TaggedSymbol
              )

            LOW = T.let(
                :low,
                OpenAI::Responses::Tool::ImageGeneration::InputFidelity::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::Tool::ImageGeneration::InputFidelity
                )
              end
          end

          class InputImageMask < OpenAI::Internal::Type::BaseModel
            # File ID for the mask image.
            sig { returns(T.nilable(String)) }
            attr_reader :file_id

            sig { params(file_id: String).void }
            attr_writer :file_id

            # Base64-encoded mask image.
            sig { returns(T.nilable(String)) }
            attr_reader :image_url

            sig { params(image_url: String).void }
            attr_writer :image_url

            sig { override.returns({ file_id: String, image_url: String }) }
            def to_hash; end

            class << self
              # Optional mask for inpainting. Contains `image_url` (string, optional) and
              # `file_id` (string, optional).
              sig { params(file_id: String, image_url: String).returns(T.attached_class) }
              def new(
                file_id: nil, # File ID for the mask image.
                image_url: nil # Base64-encoded mask image.
); end
            end

            OrHash = T.type_alias do
                T.any(
                  OpenAI::Responses::Tool::ImageGeneration::InputImageMask,
                  OpenAI::Internal::AnyHash
                )
              end
          end

          # The image generation model to use. Default: `gpt-image-1`.
          module Model
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Tool::ImageGeneration::Model::TaggedSymbol
                ])
              end
              def values; end
            end

            GPT_IMAGE_1 = T.let(
                :"gpt-image-1",
                OpenAI::Responses::Tool::ImageGeneration::Model::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Responses::Tool::ImageGeneration::Model)
              end
          end

          # Moderation level for the generated image. Default: `auto`.
          module Moderation
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Tool::ImageGeneration::Moderation::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Responses::Tool::ImageGeneration::Moderation::TaggedSymbol
              )

            LOW = T.let(
                :low,
                OpenAI::Responses::Tool::ImageGeneration::Moderation::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::Tool::ImageGeneration::Moderation
                )
              end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::Tool::ImageGeneration,
                OpenAI::Internal::AnyHash
              )
            end

          # The output format of the generated image. One of `png`, `webp`, or `jpeg`.
          # Default: `png`.
          module OutputFormat
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Tool::ImageGeneration::OutputFormat::TaggedSymbol
                ])
              end
              def values; end
            end

            JPEG = T.let(
                :jpeg,
                OpenAI::Responses::Tool::ImageGeneration::OutputFormat::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            PNG = T.let(
                :png,
                OpenAI::Responses::Tool::ImageGeneration::OutputFormat::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::Tool::ImageGeneration::OutputFormat
                )
              end

            WEBP = T.let(
                :webp,
                OpenAI::Responses::Tool::ImageGeneration::OutputFormat::TaggedSymbol
              )
          end

          # The quality of the generated image. One of `low`, `medium`, `high`, or `auto`.
          # Default: `auto`.
          module Quality
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Tool::ImageGeneration::Quality::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Responses::Tool::ImageGeneration::Quality::TaggedSymbol
              )

            HIGH = T.let(
                :high,
                OpenAI::Responses::Tool::ImageGeneration::Quality::TaggedSymbol
              )

            LOW = T.let(
                :low,
                OpenAI::Responses::Tool::ImageGeneration::Quality::TaggedSymbol
              )

            MEDIUM = T.let(
                :medium,
                OpenAI::Responses::Tool::ImageGeneration::Quality::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Responses::Tool::ImageGeneration::Quality)
              end
          end

          # The size of the generated image. One of `1024x1024`, `1024x1536`, `1536x1024`,
          # or `auto`. Default: `auto`.
          module Size
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Tool::ImageGeneration::Size::TaggedSymbol
                ])
              end
              def values; end
            end

            AUTO = T.let(
                :auto,
                OpenAI::Responses::Tool::ImageGeneration::Size::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            SIZE_1024X1024 = T.let(
                :"1024x1024",
                OpenAI::Responses::Tool::ImageGeneration::Size::TaggedSymbol
              )

            SIZE_1024X1536 = T.let(
                :"1024x1536",
                OpenAI::Responses::Tool::ImageGeneration::Size::TaggedSymbol
              )

            SIZE_1536X1024 = T.let(
                :"1536x1024",
                OpenAI::Responses::Tool::ImageGeneration::Size::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(Symbol, OpenAI::Responses::Tool::ImageGeneration::Size)
              end
          end
        end

        class LocalShell < OpenAI::Internal::Type::BaseModel
          # The type of the local shell tool. Always `local_shell`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig { override.returns({ type: Symbol }) }
          def to_hash; end

          class << self
            # A tool that allows the model to execute shell commands in a local environment.
            sig { params(type: Symbol).returns(T.attached_class) }
            def new(
              type: :local_shell # The type of the local shell tool. Always `local_shell`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::Tool::LocalShell,
                OpenAI::Internal::AnyHash
              )
            end
        end

        class Mcp < OpenAI::Internal::Type::BaseModel
          # List of allowed tool names or a filter object.
          sig do
            returns(T.nilable(
                T.any(
                  T::Array[String],
                  OpenAI::Responses::Tool::Mcp::AllowedTools::McpAllowedToolsFilter
                )
              ))
          end
          attr_accessor :allowed_tools

          # Optional HTTP headers to send to the MCP server. Use for authentication or other
          # purposes.
          sig { returns(T.nilable(T::Hash[Symbol, String])) }
          attr_accessor :headers

          # Specify which of the MCP server's tools require approval.
          sig do
            returns(T.nilable(
                T.any(
                  OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter,
                  OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalSetting::OrSymbol
                )
              ))
          end
          attr_accessor :require_approval

          # Optional description of the MCP server, used to provide more context.
          sig { returns(T.nilable(String)) }
          attr_reader :server_description

          sig { params(server_description: String).void }
          attr_writer :server_description

          # A label for this MCP server, used to identify it in tool calls.
          sig { returns(String) }
          attr_accessor :server_label

          # The URL for the MCP server.
          sig { returns(String) }
          attr_accessor :server_url

          # The type of the MCP tool. Always `mcp`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                server_label: String,
                server_url: String,
                type: Symbol,
                allowed_tools:
                  T.nilable(
                    T.any(
                      T::Array[String],
                      OpenAI::Responses::Tool::Mcp::AllowedTools::McpAllowedToolsFilter
                    )
                  ),
                headers: T.nilable(T::Hash[Symbol, String]),
                require_approval:
                  T.nilable(
                    T.any(
                      OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter,
                      OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalSetting::OrSymbol
                    )
                  ),
                server_description: String
              })
          end
          def to_hash; end

          class << self
            # Give the model access to additional tools via remote Model Context Protocol
            # (MCP) servers.
            # [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).
            sig do
              params(
                server_label: String,
                server_url: String,
                allowed_tools: T.nilable(
                  T.any(
                    T::Array[String],
                    OpenAI::Responses::Tool::Mcp::AllowedTools::McpAllowedToolsFilter::OrHash
                  )
                ),
                headers: T.nilable(T::Hash[Symbol, String]),
                require_approval: T.nilable(
                  T.any(
                    OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::OrHash,
                    OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalSetting::OrSymbol
                  )
                ),
                server_description: String,
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              server_label:, # A label for this MCP server, used to identify it in tool calls.
              server_url:, # The URL for the MCP server.
              allowed_tools: nil, # List of allowed tool names or a filter object.
              headers: nil, # Optional HTTP headers to send to the MCP server. Use for authentication or other
                            # purposes.
              require_approval: nil, # Specify which of the MCP server's tools require approval.
              server_description: nil, # Optional description of the MCP server, used to provide more context.
              type: :mcp # The type of the MCP tool. Always `mcp`.
); end
          end

          # List of allowed tool names or a filter object.
          module AllowedTools
            extend OpenAI::Internal::Type::Union

            class << self
              sig { override.returns(T::Array[OpenAI::Responses::Tool::Mcp::AllowedTools::Variants]) }
              def variants; end
            end

            class McpAllowedToolsFilter < OpenAI::Internal::Type::BaseModel
              # List of allowed tool names.
              sig { returns(T.nilable(T::Array[String])) }
              attr_reader :tool_names

              sig { params(tool_names: T::Array[String]).void }
              attr_writer :tool_names

              sig { override.returns({ tool_names: T::Array[String] }) }
              def to_hash; end

              class << self
                # A filter object to specify which tools are allowed.
                sig { params(tool_names: T::Array[String]).returns(T.attached_class) }
                def new(
                  tool_names: nil # List of allowed tool names.
); end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Responses::Tool::Mcp::AllowedTools::McpAllowedToolsFilter,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            StringArray = T.let(
                OpenAI::Internal::Type::ArrayOf[String],
                OpenAI::Internal::Type::Converter
              )

            Variants = T.type_alias do
                T.any(
                  T::Array[String],
                  OpenAI::Responses::Tool::Mcp::AllowedTools::McpAllowedToolsFilter
                )
              end
          end

          OrHash = T.type_alias do
              T.any(OpenAI::Responses::Tool::Mcp, OpenAI::Internal::AnyHash)
            end

          # Specify which of the MCP server's tools require approval.
          module RequireApproval
            extend OpenAI::Internal::Type::Union

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::Responses::Tool::Mcp::RequireApproval::Variants
                ])
              end
              def variants; end
            end

            class McpToolApprovalFilter < OpenAI::Internal::Type::BaseModel
              # A list of tools that always require approval.
              sig do
                returns(T.nilable(
                    OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Always
                  ))
              end
              attr_reader :always

              sig do
                params(
                  always: OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Always::OrHash
                ).void
              end
              attr_writer :always

              # A list of tools that never require approval.
              sig do
                returns(T.nilable(
                    OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Never
                  ))
              end
              attr_reader :never

              sig do
                params(
                  never: OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Never::OrHash
                ).void
              end
              attr_writer :never

              sig do
                override
                  .returns({
                    always:
                      OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Always,
                    never:
                      OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Never
                  })
              end
              def to_hash; end

              class << self
                sig do
                  params(
                    always: OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Always::OrHash,
                    never: OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Never::OrHash
                  ).returns(T.attached_class)
                end
                def new(
                  always: nil, # A list of tools that always require approval.
                  never: nil # A list of tools that never require approval.
); end
              end

              class Always < OpenAI::Internal::Type::BaseModel
                # List of tools that require approval.
                sig { returns(T.nilable(T::Array[String])) }
                attr_reader :tool_names

                sig { params(tool_names: T::Array[String]).void }
                attr_writer :tool_names

                sig { override.returns({ tool_names: T::Array[String] }) }
                def to_hash; end

                class << self
                  # A list of tools that always require approval.
                  sig { params(tool_names: T::Array[String]).returns(T.attached_class) }
                  def new(
                    tool_names: nil # List of tools that require approval.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Always,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              class Never < OpenAI::Internal::Type::BaseModel
                # List of tools that do not require approval.
                sig { returns(T.nilable(T::Array[String])) }
                attr_reader :tool_names

                sig { params(tool_names: T::Array[String]).void }
                attr_writer :tool_names

                sig { override.returns({ tool_names: T::Array[String] }) }
                def to_hash; end

                class << self
                  # A list of tools that never require approval.
                  sig { params(tool_names: T::Array[String]).returns(T.attached_class) }
                  def new(
                    tool_names: nil # List of tools that do not require approval.
); end
                end

                OrHash = T.type_alias do
                    T.any(
                      OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter::Never,
                      OpenAI::Internal::AnyHash
                    )
                  end
              end

              OrHash = T.type_alias do
                  T.any(
                    OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter,
                    OpenAI::Internal::AnyHash
                  )
                end
            end

            # Specify a single approval policy for all tools. One of `always` or `never`. When
            # set to `always`, all tools will require approval. When set to `never`, all tools
            # will not require approval.
            module McpToolApprovalSetting
              extend OpenAI::Internal::Type::Enum

              class << self
                sig do
                  override
                    .returns(T::Array[
                    OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalSetting::TaggedSymbol
                  ])
                end
                def values; end
              end

              ALWAYS = T.let(
                  :always,
                  OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalSetting::TaggedSymbol
                )

              NEVER = T.let(
                  :never,
                  OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalSetting::TaggedSymbol
                )

              OrSymbol = T.type_alias { T.any(Symbol, String) }

              TaggedSymbol = T.type_alias do
                  T.all(
                    Symbol,
                    OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalSetting
                  )
                end
            end

            Variants = T.type_alias do
                T.any(
                  OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalFilter,
                  OpenAI::Responses::Tool::Mcp::RequireApproval::McpToolApprovalSetting::TaggedSymbol
                )
              end
          end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Responses::FunctionTool,
              OpenAI::Responses::FileSearchTool,
              OpenAI::Responses::ComputerTool,
              OpenAI::Responses::Tool::Mcp,
              OpenAI::Responses::Tool::CodeInterpreter,
              OpenAI::Responses::Tool::ImageGeneration,
              OpenAI::Responses::Tool::LocalShell,
              OpenAI::Responses::WebSearchTool
            )
          end
      end

      class ToolChoiceFunction < OpenAI::Internal::Type::BaseModel
        # The name of the function to call.
        sig { returns(String) }
        attr_accessor :name

        # For function calling, the type is always `function`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ name: String, type: Symbol }) }
        def to_hash; end

        class << self
          # Use this option to force the model to call a specific function.
          sig { params(name: String, type: Symbol).returns(T.attached_class) }
          def new(
            name:, # The name of the function to call.
            type: :function # For function calling, the type is always `function`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Responses::ToolChoiceFunction,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ToolChoiceMcp < OpenAI::Internal::Type::BaseModel
        # The name of the tool to call on the server.
        sig { returns(T.nilable(String)) }
        attr_accessor :name

        # The label of the MCP server to use.
        sig { returns(String) }
        attr_accessor :server_label

        # For MCP tools, the type is always `mcp`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig { override.returns({ server_label: String, type: Symbol, name: T.nilable(String) }) }
        def to_hash; end

        class << self
          # Use this option to force the model to call a specific tool on a remote MCP
          # server.
          sig { params(server_label: String, name: T.nilable(String), type: Symbol).returns(T.attached_class) }
          def new(
            server_label:, # The label of the MCP server to use.
            name: nil, # The name of the tool to call on the server.
            type: :mcp # For MCP tools, the type is always `mcp`.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::ToolChoiceMcp, OpenAI::Internal::AnyHash)
          end
      end

      # Controls which (if any) tool is called by the model.
      #
      # `none` means the model will not call any tool and instead generates a message.
      #
      # `auto` means the model can pick between generating a message or calling one or
      # more tools.
      #
      # `required` means the model must call one or more tools.
      module ToolChoiceOptions
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Responses::ToolChoiceOptions::TaggedSymbol]) }
          def values; end
        end

        AUTO = T.let(:auto, OpenAI::Responses::ToolChoiceOptions::TaggedSymbol)
        NONE = T.let(:none, OpenAI::Responses::ToolChoiceOptions::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        REQUIRED = T.let(:required, OpenAI::Responses::ToolChoiceOptions::TaggedSymbol)

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Responses::ToolChoiceOptions) }
      end

      class ToolChoiceTypes < OpenAI::Internal::Type::BaseModel
        # The type of hosted tool the model should to use. Learn more about
        # [built-in tools](https://platform.openai.com/docs/guides/tools).
        #
        # Allowed values are:
        #
        # - `file_search`
        # - `web_search_preview`
        # - `computer_use_preview`
        # - `code_interpreter`
        # - `image_generation`
        sig { returns(OpenAI::Responses::ToolChoiceTypes::Type::OrSymbol) }
        attr_accessor :type

        sig { override.returns({ type: OpenAI::Responses::ToolChoiceTypes::Type::OrSymbol }) }
        def to_hash; end

        class << self
          # Indicates that the model should use a built-in tool to generate a response.
          # [Learn more about built-in tools](https://platform.openai.com/docs/guides/tools).
          sig { params(type: OpenAI::Responses::ToolChoiceTypes::Type::OrSymbol).returns(T.attached_class) }
          def new(
            type: # The type of hosted tool the model should to use. Learn more about
                  # [built-in tools](https://platform.openai.com/docs/guides/tools).
                  # Allowed values are:
                  # - `file_search`
                  # - `web_search_preview`
                  # - `computer_use_preview`
                  # - `code_interpreter`
                  # - `image_generation`
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::ToolChoiceTypes, OpenAI::Internal::AnyHash)
          end

        # The type of hosted tool the model should to use. Learn more about
        # [built-in tools](https://platform.openai.com/docs/guides/tools).
        #
        # Allowed values are:
        #
        # - `file_search`
        # - `web_search_preview`
        # - `computer_use_preview`
        # - `code_interpreter`
        # - `image_generation`
        module Type
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::ToolChoiceTypes::Type::TaggedSymbol]) }
            def values; end
          end

          CODE_INTERPRETER = T.let(
              :code_interpreter,
              OpenAI::Responses::ToolChoiceTypes::Type::TaggedSymbol
            )

          COMPUTER_USE_PREVIEW = T.let(
              :computer_use_preview,
              OpenAI::Responses::ToolChoiceTypes::Type::TaggedSymbol
            )

          FILE_SEARCH = T.let(
              :file_search,
              OpenAI::Responses::ToolChoiceTypes::Type::TaggedSymbol
            )

          IMAGE_GENERATION = T.let(
              :image_generation,
              OpenAI::Responses::ToolChoiceTypes::Type::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::ToolChoiceTypes::Type)
            end

          WEB_SEARCH_PREVIEW = T.let(
              :web_search_preview,
              OpenAI::Responses::ToolChoiceTypes::Type::TaggedSymbol
            )

          WEB_SEARCH_PREVIEW_2025_03_11 = T.let(
              :web_search_preview_2025_03_11,
              OpenAI::Responses::ToolChoiceTypes::Type::TaggedSymbol
            )
        end
      end

      class WebSearchTool < OpenAI::Internal::Type::BaseModel
        # High level guidance for the amount of context window space to use for the
        # search. One of `low`, `medium`, or `high`. `medium` is the default.
        sig do
          returns(T.nilable(
              OpenAI::Responses::WebSearchTool::SearchContextSize::OrSymbol
            ))
        end
        attr_reader :search_context_size

        sig { params(search_context_size: OpenAI::Responses::WebSearchTool::SearchContextSize::OrSymbol).void }
        attr_writer :search_context_size

        # The type of the web search tool. One of `web_search_preview` or
        # `web_search_preview_2025_03_11`.
        sig { returns(OpenAI::Responses::WebSearchTool::Type::OrSymbol) }
        attr_accessor :type

        # The user's location.
        sig { returns(T.nilable(OpenAI::Responses::WebSearchTool::UserLocation)) }
        attr_reader :user_location

        sig { params(user_location: T.nilable(OpenAI::Responses::WebSearchTool::UserLocation::OrHash)).void }
        attr_writer :user_location

        sig do
          override
            .returns({
              type: OpenAI::Responses::WebSearchTool::Type::OrSymbol,
              search_context_size:
                OpenAI::Responses::WebSearchTool::SearchContextSize::OrSymbol,
              user_location:
                T.nilable(OpenAI::Responses::WebSearchTool::UserLocation)
            })
        end
        def to_hash; end

        class << self
          # This tool searches the web for relevant results to use in a response. Learn more
          # about the
          # [web search tool](https://platform.openai.com/docs/guides/tools-web-search).
          sig do
            params(
              type: OpenAI::Responses::WebSearchTool::Type::OrSymbol,
              search_context_size: OpenAI::Responses::WebSearchTool::SearchContextSize::OrSymbol,
              user_location: T.nilable(OpenAI::Responses::WebSearchTool::UserLocation::OrHash)
            ).returns(T.attached_class)
          end
          def new(
            type:, # The type of the web search tool. One of `web_search_preview` or
                   # `web_search_preview_2025_03_11`.
            search_context_size: nil, # High level guidance for the amount of context window space to use for the
                                      # search. One of `low`, `medium`, or `high`. `medium` is the default.
            user_location: nil # The user's location.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Responses::WebSearchTool, OpenAI::Internal::AnyHash)
          end

        # High level guidance for the amount of context window space to use for the
        # search. One of `low`, `medium`, or `high`. `medium` is the default.
        module SearchContextSize
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Responses::WebSearchTool::SearchContextSize::TaggedSymbol
              ])
            end
            def values; end
          end

          HIGH = T.let(
              :high,
              OpenAI::Responses::WebSearchTool::SearchContextSize::TaggedSymbol
            )

          LOW = T.let(
              :low,
              OpenAI::Responses::WebSearchTool::SearchContextSize::TaggedSymbol
            )

          MEDIUM = T.let(
              :medium,
              OpenAI::Responses::WebSearchTool::SearchContextSize::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::WebSearchTool::SearchContextSize)
            end
        end

        # The type of the web search tool. One of `web_search_preview` or
        # `web_search_preview_2025_03_11`.
        module Type
          extend OpenAI::Internal::Type::Enum

          class << self
            sig { override.returns(T::Array[OpenAI::Responses::WebSearchTool::Type::TaggedSymbol]) }
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Responses::WebSearchTool::Type)
            end

          WEB_SEARCH_PREVIEW = T.let(
              :web_search_preview,
              OpenAI::Responses::WebSearchTool::Type::TaggedSymbol
            )

          WEB_SEARCH_PREVIEW_2025_03_11 = T.let(
              :web_search_preview_2025_03_11,
              OpenAI::Responses::WebSearchTool::Type::TaggedSymbol
            )
        end

        class UserLocation < OpenAI::Internal::Type::BaseModel
          # Free text input for the city of the user, e.g. `San Francisco`.
          sig { returns(T.nilable(String)) }
          attr_accessor :city

          # The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of
          # the user, e.g. `US`.
          sig { returns(T.nilable(String)) }
          attr_accessor :country

          # Free text input for the region of the user, e.g. `California`.
          sig { returns(T.nilable(String)) }
          attr_accessor :region

          # The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the
          # user, e.g. `America/Los_Angeles`.
          sig { returns(T.nilable(String)) }
          attr_accessor :timezone

          # The type of location approximation. Always `approximate`.
          sig { returns(Symbol) }
          attr_accessor :type

          sig do
            override
              .returns({
                type: Symbol,
                city: T.nilable(String),
                country: T.nilable(String),
                region: T.nilable(String),
                timezone: T.nilable(String)
              })
          end
          def to_hash; end

          class << self
            # The user's location.
            sig do
              params(
                city: T.nilable(String),
                country: T.nilable(String),
                region: T.nilable(String),
                timezone: T.nilable(String),
                type: Symbol
              ).returns(T.attached_class)
            end
            def new(
              city: nil, # Free text input for the city of the user, e.g. `San Francisco`.
              country: nil, # The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of
                            # the user, e.g. `US`.
              region: nil, # Free text input for the region of the user, e.g. `California`.
              timezone: nil, # The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the
                             # user, e.g. `America/Los_Angeles`.
              type: :approximate # The type of location approximation. Always `approximate`.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Responses::WebSearchTool::UserLocation,
                OpenAI::Internal::AnyHash
              )
            end
        end
      end
    end

    module ResponsesModel
      extend OpenAI::Internal::Type::Union

      class << self
        sig { override.returns(T::Array[OpenAI::ResponsesModel::Variants]) }
        def variants; end
      end

      module ResponsesOnlyModel
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol]) }
          def values; end
        end

        COMPUTER_USE_PREVIEW = T.let(
            :"computer-use-preview",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        COMPUTER_USE_PREVIEW_2025_03_11 = T.let(
            :"computer-use-preview-2025-03-11",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        O1_PRO = T.let(
            :"o1-pro",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        O1_PRO_2025_03_19 = T.let(
            :"o1-pro-2025-03-19",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        O3_DEEP_RESEARCH = T.let(
            :"o3-deep-research",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        O3_DEEP_RESEARCH_2025_06_26 = T.let(
            :"o3-deep-research-2025-06-26",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        O3_PRO = T.let(
            :"o3-pro",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        O3_PRO_2025_06_10 = T.let(
            :"o3-pro-2025-06-10",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        O4_MINI_DEEP_RESEARCH = T.let(
            :"o4-mini-deep-research",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        O4_MINI_DEEP_RESEARCH_2025_06_26 = T.let(
            :"o4-mini-deep-research-2025-06-26",
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias do
            T.all(Symbol, OpenAI::ResponsesModel::ResponsesOnlyModel)
          end
      end

      Variants = T.type_alias do
          T.any(
            String,
            OpenAI::ChatModel::TaggedSymbol,
            OpenAI::ResponsesModel::ResponsesOnlyModel::TaggedSymbol
          )
        end
    end

    ScoreModelGrader = Graders::ScoreModelGrader

    class StaticFileChunkingStrategy < OpenAI::Internal::Type::BaseModel
      # The number of tokens that overlap between chunks. The default value is `400`.
      #
      # Note that the overlap must not exceed half of `max_chunk_size_tokens`.
      sig { returns(Integer) }
      attr_accessor :chunk_overlap_tokens

      # The maximum number of tokens in each chunk. The default value is `800`. The
      # minimum value is `100` and the maximum value is `4096`.
      sig { returns(Integer) }
      attr_accessor :max_chunk_size_tokens

      sig { override.returns({ chunk_overlap_tokens: Integer, max_chunk_size_tokens: Integer }) }
      def to_hash; end

      class << self
        sig { params(chunk_overlap_tokens: Integer, max_chunk_size_tokens: Integer).returns(T.attached_class) }
        def new(
          chunk_overlap_tokens:, # The number of tokens that overlap between chunks. The default value is `400`.
                                 # Note that the overlap must not exceed half of `max_chunk_size_tokens`.
          max_chunk_size_tokens: # The maximum number of tokens in each chunk. The default value is `800`. The
                                 # minimum value is `100` and the maximum value is `4096`.
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::StaticFileChunkingStrategy, OpenAI::Internal::AnyHash)
        end
    end

    class StaticFileChunkingStrategyObject < OpenAI::Internal::Type::BaseModel
      sig { returns(OpenAI::StaticFileChunkingStrategy) }
      attr_reader :static

      sig { params(static: OpenAI::StaticFileChunkingStrategy::OrHash).void }
      attr_writer :static

      # Always `static`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig { override.returns({ static: OpenAI::StaticFileChunkingStrategy, type: Symbol }) }
      def to_hash; end

      class << self
        sig { params(static: OpenAI::StaticFileChunkingStrategy::OrHash, type: Symbol).returns(T.attached_class) }
        def new(
          static:,
          type: :static # Always `static`.
); end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::StaticFileChunkingStrategyObject,
            OpenAI::Internal::AnyHash
          )
        end
    end

    class StaticFileChunkingStrategyObjectParam < OpenAI::Internal::Type::BaseModel
      sig { returns(OpenAI::StaticFileChunkingStrategy) }
      attr_reader :static

      sig { params(static: OpenAI::StaticFileChunkingStrategy::OrHash).void }
      attr_writer :static

      # Always `static`.
      sig { returns(Symbol) }
      attr_accessor :type

      sig { override.returns({ static: OpenAI::StaticFileChunkingStrategy, type: Symbol }) }
      def to_hash; end

      class << self
        # Customize your own chunking strategy by setting chunk size and chunk overlap.
        sig { params(static: OpenAI::StaticFileChunkingStrategy::OrHash, type: Symbol).returns(T.attached_class) }
        def new(
          static:,
          type: :static # Always `static`.
); end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::StaticFileChunkingStrategyObjectParam,
            OpenAI::Internal::AnyHash
          )
        end
    end

    StringCheckGrader = Graders::StringCheckGrader
    TextSimilarityGrader = Graders::TextSimilarityGrader

    class Upload < OpenAI::Internal::Type::BaseModel
      # The intended number of bytes to be uploaded.
      sig { returns(Integer) }
      attr_accessor :bytes

      # The Unix timestamp (in seconds) for when the Upload was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The Unix timestamp (in seconds) for when the Upload will expire.
      sig { returns(Integer) }
      attr_accessor :expires_at

      # The `File` object represents a document that has been uploaded to OpenAI.
      sig { returns(T.nilable(OpenAI::FileObject)) }
      attr_reader :file

      sig { params(file: T.nilable(OpenAI::FileObject::OrHash)).void }
      attr_writer :file

      # The name of the file to be uploaded.
      sig { returns(String) }
      attr_accessor :filename

      # The Upload unique identifier, which can be referenced in API endpoints.
      sig { returns(String) }
      attr_accessor :id

      # The object type, which is always "upload".
      sig { returns(Symbol) }
      attr_accessor :object

      # The intended purpose of the file.
      # [Please refer here](https://platform.openai.com/docs/api-reference/files/object#files/object-purpose)
      # for acceptable values.
      sig { returns(String) }
      attr_accessor :purpose

      # The status of the Upload.
      sig { returns(OpenAI::Upload::Status::TaggedSymbol) }
      attr_accessor :status

      sig do
        override
          .returns({
            id: String,
            bytes: Integer,
            created_at: Integer,
            expires_at: Integer,
            filename: String,
            object: Symbol,
            purpose: String,
            status: OpenAI::Upload::Status::TaggedSymbol,
            file: T.nilable(OpenAI::FileObject)
          })
      end
      def to_hash; end

      class << self
        # The Upload object can accept byte chunks in the form of Parts.
        sig do
          params(
            id: String,
            bytes: Integer,
            created_at: Integer,
            expires_at: Integer,
            filename: String,
            purpose: String,
            status: OpenAI::Upload::Status::OrSymbol,
            file: T.nilable(OpenAI::FileObject::OrHash),
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          id:, # The Upload unique identifier, which can be referenced in API endpoints.
          bytes:, # The intended number of bytes to be uploaded.
          created_at:, # The Unix timestamp (in seconds) for when the Upload was created.
          expires_at:, # The Unix timestamp (in seconds) for when the Upload will expire.
          filename:, # The name of the file to be uploaded.
          purpose:, # The intended purpose of the file.
                    # [Please refer here](https://platform.openai.com/docs/api-reference/files/object#files/object-purpose)
                    # for acceptable values.
          status:, # The status of the Upload.
          file: nil, # The `File` object represents a document that has been uploaded to OpenAI.
          object: :upload # The object type, which is always "upload".
); end
      end

      OrHash = T.type_alias { T.any(OpenAI::Upload, OpenAI::Internal::AnyHash) }

      # The status of the Upload.
      module Status
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::Upload::Status::TaggedSymbol]) }
          def values; end
        end

        CANCELLED = T.let(:cancelled, OpenAI::Upload::Status::TaggedSymbol)
        COMPLETED = T.let(:completed, OpenAI::Upload::Status::TaggedSymbol)
        EXPIRED = T.let(:expired, OpenAI::Upload::Status::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }
        PENDING = T.let(:pending, OpenAI::Upload::Status::TaggedSymbol)
        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::Upload::Status) }
      end
    end

    class UploadCancelParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::UploadCancelParams, OpenAI::Internal::AnyHash)
        end
    end

    class UploadCompleteParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # The optional md5 checksum for the file contents to verify if the bytes uploaded
      # matches what you expect.
      sig { returns(T.nilable(String)) }
      attr_reader :md5

      sig { params(md5: String).void }
      attr_writer :md5

      # The ordered list of Part IDs.
      sig { returns(T::Array[String]) }
      attr_accessor :part_ids

      sig do
        override
          .returns({
            part_ids: T::Array[String],
            md5: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            part_ids: T::Array[String],
            md5: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          part_ids:, # The ordered list of Part IDs.
          md5: nil, # The optional md5 checksum for the file contents to verify if the bytes uploaded
                    # matches what you expect.
          request_options: {}
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::UploadCompleteParams, OpenAI::Internal::AnyHash)
        end
    end

    class UploadCreateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # The number of bytes in the file you are uploading.
      sig { returns(Integer) }
      attr_accessor :bytes

      # The name of the file to upload.
      sig { returns(String) }
      attr_accessor :filename

      # The MIME type of the file.
      #
      # This must fall within the supported MIME types for your file purpose. See the
      # supported MIME types for assistants and vision.
      sig { returns(String) }
      attr_accessor :mime_type

      # The intended purpose of the uploaded file.
      #
      # See the
      # [documentation on File purposes](https://platform.openai.com/docs/api-reference/files/create#files-create-purpose).
      sig { returns(OpenAI::FilePurpose::OrSymbol) }
      attr_accessor :purpose

      sig do
        override
          .returns({
            bytes: Integer,
            filename: String,
            mime_type: String,
            purpose: OpenAI::FilePurpose::OrSymbol,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            bytes: Integer,
            filename: String,
            mime_type: String,
            purpose: OpenAI::FilePurpose::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          bytes:, # The number of bytes in the file you are uploading.
          filename:, # The name of the file to upload.
          mime_type:, # The MIME type of the file.
                      # This must fall within the supported MIME types for your file purpose. See the
                      # supported MIME types for assistants and vision.
          purpose:, # The intended purpose of the uploaded file.
                    # See the
                    # [documentation on File purposes](https://platform.openai.com/docs/api-reference/files/create#files-create-purpose).
          request_options: {}
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::UploadCreateParams, OpenAI::Internal::AnyHash)
        end
    end

    UploadPart = Uploads::UploadPart

    module Uploads
      class PartCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # The chunk of bytes for this Part.
        sig { returns(OpenAI::Internal::FileInput) }
        attr_accessor :data

        sig do
          override
            .returns({
              data: OpenAI::Internal::FileInput,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              data: OpenAI::Internal::FileInput,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            data:, # The chunk of bytes for this Part.
            request_options: {}
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Uploads::PartCreateParams, OpenAI::Internal::AnyHash)
          end
      end

      class UploadPart < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) for when the Part was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # The upload Part unique identifier, which can be referenced in API endpoints.
        sig { returns(String) }
        attr_accessor :id

        # The object type, which is always `upload.part`.
        sig { returns(Symbol) }
        attr_accessor :object

        # The ID of the Upload object that this Part was added to.
        sig { returns(String) }
        attr_accessor :upload_id

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              object: Symbol,
              upload_id: String
            })
        end
        def to_hash; end

        class << self
          # The upload Part represents a chunk of bytes we can add to an Upload object.
          sig { params(id: String, created_at: Integer, upload_id: String, object: Symbol).returns(T.attached_class) }
          def new(
            id:, # The upload Part unique identifier, which can be referenced in API endpoints.
            created_at:, # The Unix timestamp (in seconds) for when the Part was created.
            upload_id:, # The ID of the Upload object that this Part was added to.
            object: :"upload.part" # The object type, which is always `upload.part`.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::Uploads::UploadPart, OpenAI::Internal::AnyHash)
          end
      end
    end

    class VectorStore < OpenAI::Internal::Type::BaseModel
      # The Unix timestamp (in seconds) for when the vector store was created.
      sig { returns(Integer) }
      attr_accessor :created_at

      # The expiration policy for a vector store.
      sig { returns(T.nilable(OpenAI::VectorStore::ExpiresAfter)) }
      attr_reader :expires_after

      sig { params(expires_after: OpenAI::VectorStore::ExpiresAfter::OrHash).void }
      attr_writer :expires_after

      # The Unix timestamp (in seconds) for when the vector store will expire.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :expires_at

      sig { returns(OpenAI::VectorStore::FileCounts) }
      attr_reader :file_counts

      sig { params(file_counts: OpenAI::VectorStore::FileCounts::OrHash).void }
      attr_writer :file_counts

      # The identifier, which can be referenced in API endpoints.
      sig { returns(String) }
      attr_accessor :id

      # The Unix timestamp (in seconds) for when the vector store was last active.
      sig { returns(T.nilable(Integer)) }
      attr_accessor :last_active_at

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The name of the vector store.
      sig { returns(String) }
      attr_accessor :name

      # The object type, which is always `vector_store`.
      sig { returns(Symbol) }
      attr_accessor :object

      # The status of the vector store, which can be either `expired`, `in_progress`, or
      # `completed`. A status of `completed` indicates that the vector store is ready
      # for use.
      sig { returns(OpenAI::VectorStore::Status::TaggedSymbol) }
      attr_accessor :status

      # The total number of bytes used by the files in the vector store.
      sig { returns(Integer) }
      attr_accessor :usage_bytes

      sig do
        override
          .returns({
            id: String,
            created_at: Integer,
            file_counts: OpenAI::VectorStore::FileCounts,
            last_active_at: T.nilable(Integer),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            object: Symbol,
            status: OpenAI::VectorStore::Status::TaggedSymbol,
            usage_bytes: Integer,
            expires_after: OpenAI::VectorStore::ExpiresAfter,
            expires_at: T.nilable(Integer)
          })
      end
      def to_hash; end

      class << self
        # A vector store is a collection of processed files can be used by the
        # `file_search` tool.
        sig do
          params(
            id: String,
            created_at: Integer,
            file_counts: OpenAI::VectorStore::FileCounts::OrHash,
            last_active_at: T.nilable(Integer),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            status: OpenAI::VectorStore::Status::OrSymbol,
            usage_bytes: Integer,
            expires_after: OpenAI::VectorStore::ExpiresAfter::OrHash,
            expires_at: T.nilable(Integer),
            object: Symbol
          ).returns(T.attached_class)
        end
        def new(
          id:, # The identifier, which can be referenced in API endpoints.
          created_at:, # The Unix timestamp (in seconds) for when the vector store was created.
          file_counts:,
          last_active_at:, # The Unix timestamp (in seconds) for when the vector store was last active.
          metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                     # for storing additional information about the object in a structured format, and
                     # querying for objects via API or the dashboard.
                     # Keys are strings with a maximum length of 64 characters. Values are strings with
                     # a maximum length of 512 characters.
          name:, # The name of the vector store.
          status:, # The status of the vector store, which can be either `expired`, `in_progress`, or
                   # `completed`. A status of `completed` indicates that the vector store is ready
                   # for use.
          usage_bytes:, # The total number of bytes used by the files in the vector store.
          expires_after: nil, # The expiration policy for a vector store.
          expires_at: nil, # The Unix timestamp (in seconds) for when the vector store will expire.
          object: :vector_store # The object type, which is always `vector_store`.
); end
      end

      class ExpiresAfter < OpenAI::Internal::Type::BaseModel
        # Anchor timestamp after which the expiration policy applies. Supported anchors:
        # `last_active_at`.
        sig { returns(Symbol) }
        attr_accessor :anchor

        # The number of days after the anchor time that the vector store will expire.
        sig { returns(Integer) }
        attr_accessor :days

        sig { override.returns({ anchor: Symbol, days: Integer }) }
        def to_hash; end

        class << self
          # The expiration policy for a vector store.
          sig { params(days: Integer, anchor: Symbol).returns(T.attached_class) }
          def new(
            days:, # The number of days after the anchor time that the vector store will expire.
            anchor: :last_active_at # Anchor timestamp after which the expiration policy applies. Supported anchors:
                                    # `last_active_at`.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::VectorStore::ExpiresAfter, OpenAI::Internal::AnyHash)
          end
      end

      class FileCounts < OpenAI::Internal::Type::BaseModel
        # The number of files that were cancelled.
        sig { returns(Integer) }
        attr_accessor :cancelled

        # The number of files that have been successfully processed.
        sig { returns(Integer) }
        attr_accessor :completed

        # The number of files that have failed to process.
        sig { returns(Integer) }
        attr_accessor :failed

        # The number of files that are currently being processed.
        sig { returns(Integer) }
        attr_accessor :in_progress

        # The total number of files.
        sig { returns(Integer) }
        attr_accessor :total

        sig do
          override
            .returns({
              cancelled: Integer,
              completed: Integer,
              failed: Integer,
              in_progress: Integer,
              total: Integer
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              cancelled: Integer,
              completed: Integer,
              failed: Integer,
              in_progress: Integer,
              total: Integer
            ).returns(T.attached_class)
          end
          def new(
            cancelled:, # The number of files that were cancelled.
            completed:, # The number of files that have been successfully processed.
            failed:, # The number of files that have failed to process.
            in_progress:, # The number of files that are currently being processed.
            total: # The total number of files.
); end
        end

        OrHash = T.type_alias do
            T.any(OpenAI::VectorStore::FileCounts, OpenAI::Internal::AnyHash)
          end
      end

      OrHash = T.type_alias { T.any(OpenAI::VectorStore, OpenAI::Internal::AnyHash) }

      # The status of the vector store, which can be either `expired`, `in_progress`, or
      # `completed`. A status of `completed` indicates that the vector store is ready
      # for use.
      module Status
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::VectorStore::Status::TaggedSymbol]) }
          def values; end
        end

        COMPLETED = T.let(:completed, OpenAI::VectorStore::Status::TaggedSymbol)
        EXPIRED = T.let(:expired, OpenAI::VectorStore::Status::TaggedSymbol)

        IN_PROGRESS = T.let(:in_progress, OpenAI::VectorStore::Status::TaggedSymbol)

        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::VectorStore::Status) }
      end
    end

    class VectorStoreCreateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
      # strategy. Only applicable if `file_ids` is non-empty.
      sig do
        returns(T.nilable(
            T.any(
              OpenAI::AutoFileChunkingStrategyParam,
              OpenAI::StaticFileChunkingStrategyObjectParam
            )
          ))
      end
      attr_reader :chunking_strategy

      sig do
        params(
          chunking_strategy: T.any(
              OpenAI::AutoFileChunkingStrategyParam::OrHash,
              OpenAI::StaticFileChunkingStrategyObjectParam::OrHash
            )
        ).void
      end
      attr_writer :chunking_strategy

      # The expiration policy for a vector store.
      sig { returns(T.nilable(OpenAI::VectorStoreCreateParams::ExpiresAfter)) }
      attr_reader :expires_after

      sig { params(expires_after: OpenAI::VectorStoreCreateParams::ExpiresAfter::OrHash).void }
      attr_writer :expires_after

      # A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that
      # the vector store should use. Useful for tools like `file_search` that can access
      # files.
      sig { returns(T.nilable(T::Array[String])) }
      attr_reader :file_ids

      sig { params(file_ids: T::Array[String]).void }
      attr_writer :file_ids

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The name of the vector store.
      sig { returns(T.nilable(String)) }
      attr_reader :name

      sig { params(name: String).void }
      attr_writer :name

      sig do
        override
          .returns({
            chunking_strategy:
              T.any(
                OpenAI::AutoFileChunkingStrategyParam,
                OpenAI::StaticFileChunkingStrategyObjectParam
              ),
            expires_after: OpenAI::VectorStoreCreateParams::ExpiresAfter,
            file_ids: T::Array[String],
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            chunking_strategy: T.any(
              OpenAI::AutoFileChunkingStrategyParam::OrHash,
              OpenAI::StaticFileChunkingStrategyObjectParam::OrHash
            ),
            expires_after: OpenAI::VectorStoreCreateParams::ExpiresAfter::OrHash,
            file_ids: T::Array[String],
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          chunking_strategy: nil, # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                                  # strategy. Only applicable if `file_ids` is non-empty.
          expires_after: nil, # The expiration policy for a vector store.
          file_ids: nil, # A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that
                         # the vector store should use. Useful for tools like `file_search` that can access
                         # files.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          name: nil, # The name of the vector store.
          request_options: {}
); end
      end

      class ExpiresAfter < OpenAI::Internal::Type::BaseModel
        # Anchor timestamp after which the expiration policy applies. Supported anchors:
        # `last_active_at`.
        sig { returns(Symbol) }
        attr_accessor :anchor

        # The number of days after the anchor time that the vector store will expire.
        sig { returns(Integer) }
        attr_accessor :days

        sig { override.returns({ anchor: Symbol, days: Integer }) }
        def to_hash; end

        class << self
          # The expiration policy for a vector store.
          sig { params(days: Integer, anchor: Symbol).returns(T.attached_class) }
          def new(
            days:, # The number of days after the anchor time that the vector store will expire.
            anchor: :last_active_at # Anchor timestamp after which the expiration policy applies. Supported anchors:
                                    # `last_active_at`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStoreCreateParams::ExpiresAfter,
              OpenAI::Internal::AnyHash
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::VectorStoreCreateParams, OpenAI::Internal::AnyHash)
        end
    end

    class VectorStoreDeleteParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::VectorStoreDeleteParams, OpenAI::Internal::AnyHash)
        end
    end

    class VectorStoreDeleted < OpenAI::Internal::Type::BaseModel
      sig { returns(T::Boolean) }
      attr_accessor :deleted

      sig { returns(String) }
      attr_accessor :id

      sig { returns(Symbol) }
      attr_accessor :object

      sig { override.returns({ id: String, deleted: T::Boolean, object: Symbol }) }
      def to_hash; end

      class << self
        sig { params(id: String, deleted: T::Boolean, object: Symbol).returns(T.attached_class) }
        def new(id:, deleted:, object: :"vector_store.deleted"); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::VectorStoreDeleted, OpenAI::Internal::AnyHash)
        end
    end

    VectorStoreFile = VectorStores::VectorStoreFile
    VectorStoreFileBatch = VectorStores::VectorStoreFileBatch
    VectorStoreFileDeleted = VectorStores::VectorStoreFileDeleted

    class VectorStoreListParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # A cursor for use in pagination. `after` is an object ID that defines your place
      # in the list. For instance, if you make a list request and receive 100 objects,
      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
      # fetch the next page of the list.
      sig { returns(T.nilable(String)) }
      attr_reader :after

      sig { params(after: String).void }
      attr_writer :after

      # A cursor for use in pagination. `before` is an object ID that defines your place
      # in the list. For instance, if you make a list request and receive 100 objects,
      # starting with obj_foo, your subsequent call can include before=obj_foo in order
      # to fetch the previous page of the list.
      sig { returns(T.nilable(String)) }
      attr_reader :before

      sig { params(before: String).void }
      attr_writer :before

      # A limit on the number of objects to be returned. Limit can range between 1 and
      # 100, and the default is 20.
      sig { returns(T.nilable(Integer)) }
      attr_reader :limit

      sig { params(limit: Integer).void }
      attr_writer :limit

      # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
      # order and `desc` for descending order.
      sig { returns(T.nilable(OpenAI::VectorStoreListParams::Order::OrSymbol)) }
      attr_reader :order

      sig { params(order: OpenAI::VectorStoreListParams::Order::OrSymbol).void }
      attr_writer :order

      sig do
        override
          .returns({
            after: String,
            before: String,
            limit: Integer,
            order: OpenAI::VectorStoreListParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            after: String,
            before: String,
            limit: Integer,
            order: OpenAI::VectorStoreListParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                      # in the list. For instance, if you make a list request and receive 100 objects,
                      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                      # fetch the next page of the list.
          before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                       # in the list. For instance, if you make a list request and receive 100 objects,
                       # starting with obj_foo, your subsequent call can include before=obj_foo in order
                       # to fetch the previous page of the list.
          limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                      # 100, and the default is 20.
          order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                      # order and `desc` for descending order.
          request_options: {}
); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::VectorStoreListParams, OpenAI::Internal::AnyHash)
        end

      # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
      # order and `desc` for descending order.
      module Order
        extend OpenAI::Internal::Type::Enum

        class << self
          sig { override.returns(T::Array[OpenAI::VectorStoreListParams::Order::TaggedSymbol]) }
          def values; end
        end

        ASC = T.let(:asc, OpenAI::VectorStoreListParams::Order::TaggedSymbol)
        DESC = T.let(:desc, OpenAI::VectorStoreListParams::Order::TaggedSymbol)
        OrSymbol = T.type_alias { T.any(Symbol, String) }

        TaggedSymbol = T.type_alias { T.all(Symbol, OpenAI::VectorStoreListParams::Order) }
      end
    end

    class VectorStoreRetrieveParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      sig { override.returns({ request_options: OpenAI::RequestOptions }) }
      def to_hash; end

      class << self
        sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
        def new(request_options: {}); end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::VectorStoreRetrieveParams, OpenAI::Internal::AnyHash)
        end
    end

    class VectorStoreSearchParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # A filter to apply based on file attributes.
      sig { returns(T.nilable(T.any(OpenAI::ComparisonFilter, OpenAI::CompoundFilter))) }
      attr_reader :filters

      sig do
        params(
          filters: T.any(
              OpenAI::ComparisonFilter::OrHash,
              OpenAI::CompoundFilter::OrHash
            )
        ).void
      end
      attr_writer :filters

      # The maximum number of results to return. This number should be between 1 and 50
      # inclusive.
      sig { returns(T.nilable(Integer)) }
      attr_reader :max_num_results

      sig { params(max_num_results: Integer).void }
      attr_writer :max_num_results

      # A query string for a search
      sig { returns(OpenAI::VectorStoreSearchParams::Query::Variants) }
      attr_accessor :query

      # Ranking options for search.
      sig { returns(T.nilable(OpenAI::VectorStoreSearchParams::RankingOptions)) }
      attr_reader :ranking_options

      sig { params(ranking_options: OpenAI::VectorStoreSearchParams::RankingOptions::OrHash).void }
      attr_writer :ranking_options

      # Whether to rewrite the natural language query for vector search.
      sig { returns(T.nilable(T::Boolean)) }
      attr_reader :rewrite_query

      sig { params(rewrite_query: T::Boolean).void }
      attr_writer :rewrite_query

      sig do
        override
          .returns({
            query: OpenAI::VectorStoreSearchParams::Query::Variants,
            filters: T.any(OpenAI::ComparisonFilter, OpenAI::CompoundFilter),
            max_num_results: Integer,
            ranking_options: OpenAI::VectorStoreSearchParams::RankingOptions,
            rewrite_query: T::Boolean,
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            query: OpenAI::VectorStoreSearchParams::Query::Variants,
            filters: T.any(
              OpenAI::ComparisonFilter::OrHash,
              OpenAI::CompoundFilter::OrHash
            ),
            max_num_results: Integer,
            ranking_options: OpenAI::VectorStoreSearchParams::RankingOptions::OrHash,
            rewrite_query: T::Boolean,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          query:, # A query string for a search
          filters: nil, # A filter to apply based on file attributes.
          max_num_results: nil, # The maximum number of results to return. This number should be between 1 and 50
                                # inclusive.
          ranking_options: nil, # Ranking options for search.
          rewrite_query: nil, # Whether to rewrite the natural language query for vector search.
          request_options: {}
); end
      end

      # A filter to apply based on file attributes.
      module Filters
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::VectorStoreSearchParams::Filters::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(OpenAI::ComparisonFilter, OpenAI::CompoundFilter)
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::VectorStoreSearchParams, OpenAI::Internal::AnyHash)
        end

      # A query string for a search
      module Query
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::VectorStoreSearchParams::Query::Variants]) }
          def variants; end
        end

        StringArray = T.let(
            OpenAI::Internal::Type::ArrayOf[String],
            OpenAI::Internal::Type::Converter
          )

        Variants = T.type_alias { T.any(String, T::Array[String]) }
      end

      class RankingOptions < OpenAI::Internal::Type::BaseModel
        sig do
          returns(T.nilable(
              OpenAI::VectorStoreSearchParams::RankingOptions::Ranker::OrSymbol
            ))
        end
        attr_reader :ranker

        sig { params(ranker: OpenAI::VectorStoreSearchParams::RankingOptions::Ranker::OrSymbol).void }
        attr_writer :ranker

        sig { returns(T.nilable(Float)) }
        attr_reader :score_threshold

        sig { params(score_threshold: Float).void }
        attr_writer :score_threshold

        sig do
          override
            .returns({
              ranker:
                OpenAI::VectorStoreSearchParams::RankingOptions::Ranker::OrSymbol,
              score_threshold: Float
            })
        end
        def to_hash; end

        class << self
          # Ranking options for search.
          sig do
            params(
              ranker: OpenAI::VectorStoreSearchParams::RankingOptions::Ranker::OrSymbol,
              score_threshold: Float
            ).returns(T.attached_class)
          end
          def new(ranker: nil, score_threshold: nil); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStoreSearchParams::RankingOptions,
              OpenAI::Internal::AnyHash
            )
          end

        module Ranker
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStoreSearchParams::RankingOptions::Ranker::TaggedSymbol
              ])
            end
            def values; end
          end

          AUTO = T.let(
              :auto,
              OpenAI::VectorStoreSearchParams::RankingOptions::Ranker::TaggedSymbol
            )

          DEFAULT_2024_11_15 = T.let(
              :"default-2024-11-15",
              OpenAI::VectorStoreSearchParams::RankingOptions::Ranker::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::VectorStoreSearchParams::RankingOptions::Ranker
              )
            end
        end
      end
    end

    class VectorStoreSearchResponse < OpenAI::Internal::Type::BaseModel
      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard. Keys are strings with a maximum
      # length of 64 characters. Values are strings with a maximum length of 512
      # characters, booleans, or numbers.
      sig do
        returns(T.nilable(
            T::Hash[
              Symbol,
              OpenAI::Models::VectorStoreSearchResponse::Attribute::Variants
            ]
          ))
      end
      attr_accessor :attributes

      # Content chunks from the file.
      sig { returns(T::Array[OpenAI::Models::VectorStoreSearchResponse::Content]) }
      attr_accessor :content

      # The ID of the vector store file.
      sig { returns(String) }
      attr_accessor :file_id

      # The name of the vector store file.
      sig { returns(String) }
      attr_accessor :filename

      # The similarity score for the result.
      sig { returns(Float) }
      attr_accessor :score

      sig do
        override
          .returns({
            attributes:
              T.nilable(
                T::Hash[
                  Symbol,
                  OpenAI::Models::VectorStoreSearchResponse::Attribute::Variants
                ]
              ),
            content:
              T::Array[OpenAI::Models::VectorStoreSearchResponse::Content],
            file_id: String,
            filename: String,
            score: Float
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            attributes: T.nilable(
              T::Hash[
                Symbol,
                OpenAI::Models::VectorStoreSearchResponse::Attribute::Variants
              ]
            ),
            content: T::Array[
              OpenAI::Models::VectorStoreSearchResponse::Content::OrHash
            ],
            file_id: String,
            filename: String,
            score: Float
          ).returns(T.attached_class)
        end
        def new(
          attributes:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard. Keys are strings with a maximum
                       # length of 64 characters. Values are strings with a maximum length of 512
                       # characters, booleans, or numbers.
          content:, # Content chunks from the file.
          file_id:, # The ID of the vector store file.
          filename:, # The name of the vector store file.
          score: # The similarity score for the result.
); end
      end

      module Attribute
        extend OpenAI::Internal::Type::Union

        class << self
          sig do
            override
              .returns(T::Array[
              OpenAI::Models::VectorStoreSearchResponse::Attribute::Variants
            ])
          end
          def variants; end
        end

        Variants = T.type_alias { T.any(String, Float, T::Boolean) }
      end

      class Content < OpenAI::Internal::Type::BaseModel
        # The text content returned from search.
        sig { returns(String) }
        attr_accessor :text

        # The type of content.
        sig { returns(OpenAI::Models::VectorStoreSearchResponse::Content::Type::TaggedSymbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              text: String,
              type:
                OpenAI::Models::VectorStoreSearchResponse::Content::Type::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              text: String,
              type: OpenAI::Models::VectorStoreSearchResponse::Content::Type::OrSymbol
            ).returns(T.attached_class)
          end
          def new(
            text:, # The text content returned from search.
            type: # The type of content.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::VectorStoreSearchResponse::Content,
              OpenAI::Internal::AnyHash
            )
          end

        # The type of content.
        module Type
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Models::VectorStoreSearchResponse::Content::Type::TaggedSymbol
              ])
            end
            def values; end
          end

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT = T.let(
              :text,
              OpenAI::Models::VectorStoreSearchResponse::Content::Type::TaggedSymbol
            )

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Models::VectorStoreSearchResponse::Content::Type
              )
            end
        end
      end

      OrHash = T.type_alias do
          T.any(
            OpenAI::Models::VectorStoreSearchResponse,
            OpenAI::Internal::AnyHash
          )
        end
    end

    class VectorStoreUpdateParams < OpenAI::Internal::Type::BaseModel
      extend OpenAI::Internal::Type::RequestParameters::Converter
      include OpenAI::Internal::Type::RequestParameters

      # The expiration policy for a vector store.
      sig { returns(T.nilable(OpenAI::VectorStoreUpdateParams::ExpiresAfter)) }
      attr_reader :expires_after

      sig { params(expires_after: T.nilable(OpenAI::VectorStoreUpdateParams::ExpiresAfter::OrHash)).void }
      attr_writer :expires_after

      # Set of 16 key-value pairs that can be attached to an object. This can be useful
      # for storing additional information about the object in a structured format, and
      # querying for objects via API or the dashboard.
      #
      # Keys are strings with a maximum length of 64 characters. Values are strings with
      # a maximum length of 512 characters.
      sig { returns(T.nilable(T::Hash[Symbol, String])) }
      attr_accessor :metadata

      # The name of the vector store.
      sig { returns(T.nilable(String)) }
      attr_accessor :name

      sig do
        override
          .returns({
            expires_after:
              T.nilable(OpenAI::VectorStoreUpdateParams::ExpiresAfter),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: T.nilable(String),
            request_options: OpenAI::RequestOptions
          })
      end
      def to_hash; end

      class << self
        sig do
          params(
            expires_after: T.nilable(OpenAI::VectorStoreUpdateParams::ExpiresAfter::OrHash),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: T.nilable(String),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def new(
          expires_after: nil, # The expiration policy for a vector store.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          name: nil, # The name of the vector store.
          request_options: {}
); end
      end

      class ExpiresAfter < OpenAI::Internal::Type::BaseModel
        # Anchor timestamp after which the expiration policy applies. Supported anchors:
        # `last_active_at`.
        sig { returns(Symbol) }
        attr_accessor :anchor

        # The number of days after the anchor time that the vector store will expire.
        sig { returns(Integer) }
        attr_accessor :days

        sig { override.returns({ anchor: Symbol, days: Integer }) }
        def to_hash; end

        class << self
          # The expiration policy for a vector store.
          sig { params(days: Integer, anchor: Symbol).returns(T.attached_class) }
          def new(
            days:, # The number of days after the anchor time that the vector store will expire.
            anchor: :last_active_at # Anchor timestamp after which the expiration policy applies. Supported anchors:
                                    # `last_active_at`.
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStoreUpdateParams::ExpiresAfter,
              OpenAI::Internal::AnyHash
            )
          end
      end

      OrHash = T.type_alias do
          T.any(OpenAI::VectorStoreUpdateParams, OpenAI::Internal::AnyHash)
        end
    end

    module VectorStores
      class FileBatchCancelParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :vector_store_id

        sig { override.returns({ vector_store_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig do
            params(
              vector_store_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(vector_store_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileBatchCancelParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileBatchCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard. Keys are strings with a maximum
        # length of 64 characters. Values are strings with a maximum length of 512
        # characters, booleans, or numbers.
        sig do
          returns(T.nilable(
              T::Hash[
                Symbol,
                OpenAI::VectorStores::FileBatchCreateParams::Attribute::Variants
              ]
            ))
        end
        attr_accessor :attributes

        # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
        # strategy. Only applicable if `file_ids` is non-empty.
        sig do
          returns(T.nilable(
              T.any(
                OpenAI::AutoFileChunkingStrategyParam,
                OpenAI::StaticFileChunkingStrategyObjectParam
              )
            ))
        end
        attr_reader :chunking_strategy

        sig do
          params(
            chunking_strategy: T.any(
                OpenAI::AutoFileChunkingStrategyParam::OrHash,
                OpenAI::StaticFileChunkingStrategyObjectParam::OrHash
              )
          ).void
        end
        attr_writer :chunking_strategy

        # A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that
        # the vector store should use. Useful for tools like `file_search` that can access
        # files.
        sig { returns(T::Array[String]) }
        attr_accessor :file_ids

        sig do
          override
            .returns({
              file_ids: T::Array[String],
              attributes:
                T.nilable(
                  T::Hash[
                    Symbol,
                    OpenAI::VectorStores::FileBatchCreateParams::Attribute::Variants
                  ]
                ),
              chunking_strategy:
                T.any(
                  OpenAI::AutoFileChunkingStrategyParam,
                  OpenAI::StaticFileChunkingStrategyObjectParam
                ),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              file_ids: T::Array[String],
              attributes: T.nilable(
                T::Hash[
                  Symbol,
                  OpenAI::VectorStores::FileBatchCreateParams::Attribute::Variants
                ]
              ),
              chunking_strategy: T.any(
                OpenAI::AutoFileChunkingStrategyParam::OrHash,
                OpenAI::StaticFileChunkingStrategyObjectParam::OrHash
              ),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            file_ids:, # A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that
                       # the vector store should use. Useful for tools like `file_search` that can access
                       # files.
            attributes: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard. Keys are strings with a maximum
                             # length of 64 characters. Values are strings with a maximum length of 512
                             # characters, booleans, or numbers.
            chunking_strategy: nil, # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                                    # strategy. Only applicable if `file_ids` is non-empty.
            request_options: {}
); end
        end

        module Attribute
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::FileBatchCreateParams::Attribute::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(String, Float, T::Boolean) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileBatchCreateParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileBatchListFilesParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # A cursor for use in pagination. `after` is an object ID that defines your place
        # in the list. For instance, if you make a list request and receive 100 objects,
        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
        # fetch the next page of the list.
        sig { returns(T.nilable(String)) }
        attr_reader :after

        sig { params(after: String).void }
        attr_writer :after

        # A cursor for use in pagination. `before` is an object ID that defines your place
        # in the list. For instance, if you make a list request and receive 100 objects,
        # starting with obj_foo, your subsequent call can include before=obj_foo in order
        # to fetch the previous page of the list.
        sig { returns(T.nilable(String)) }
        attr_reader :before

        sig { params(before: String).void }
        attr_writer :before

        # Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.
        sig do
          returns(T.nilable(
              OpenAI::VectorStores::FileBatchListFilesParams::Filter::OrSymbol
            ))
        end
        attr_reader :filter

        sig { params(filter: OpenAI::VectorStores::FileBatchListFilesParams::Filter::OrSymbol).void }
        attr_writer :filter

        # A limit on the number of objects to be returned. Limit can range between 1 and
        # 100, and the default is 20.
        sig { returns(T.nilable(Integer)) }
        attr_reader :limit

        sig { params(limit: Integer).void }
        attr_writer :limit

        # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
        # order and `desc` for descending order.
        sig do
          returns(T.nilable(
              OpenAI::VectorStores::FileBatchListFilesParams::Order::OrSymbol
            ))
        end
        attr_reader :order

        sig { params(order: OpenAI::VectorStores::FileBatchListFilesParams::Order::OrSymbol).void }
        attr_writer :order

        sig { returns(String) }
        attr_accessor :vector_store_id

        sig do
          override
            .returns({
              vector_store_id: String,
              after: String,
              before: String,
              filter:
                OpenAI::VectorStores::FileBatchListFilesParams::Filter::OrSymbol,
              limit: Integer,
              order:
                OpenAI::VectorStores::FileBatchListFilesParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              vector_store_id: String,
              after: String,
              before: String,
              filter: OpenAI::VectorStores::FileBatchListFilesParams::Filter::OrSymbol,
              limit: Integer,
              order: OpenAI::VectorStores::FileBatchListFilesParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            vector_store_id:,
            after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                        # in the list. For instance, if you make a list request and receive 100 objects,
                        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                        # fetch the next page of the list.
            before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                         # in the list. For instance, if you make a list request and receive 100 objects,
                         # starting with obj_foo, your subsequent call can include before=obj_foo in order
                         # to fetch the previous page of the list.
            filter: nil, # Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.
            limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                        # 100, and the default is 20.
            order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                        # order and `desc` for descending order.
            request_options: {}
); end
        end

        # Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.
        module Filter
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::FileBatchListFilesParams::Filter::TaggedSymbol
              ])
            end
            def values; end
          end

          CANCELLED = T.let(
              :cancelled,
              OpenAI::VectorStores::FileBatchListFilesParams::Filter::TaggedSymbol
            )

          COMPLETED = T.let(
              :completed,
              OpenAI::VectorStores::FileBatchListFilesParams::Filter::TaggedSymbol
            )

          FAILED = T.let(
              :failed,
              OpenAI::VectorStores::FileBatchListFilesParams::Filter::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::VectorStores::FileBatchListFilesParams::Filter::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::VectorStores::FileBatchListFilesParams::Filter
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileBatchListFilesParams,
              OpenAI::Internal::AnyHash
            )
          end

        # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
        # order and `desc` for descending order.
        module Order
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::FileBatchListFilesParams::Order::TaggedSymbol
              ])
            end
            def values; end
          end

          ASC = T.let(
              :asc,
              OpenAI::VectorStores::FileBatchListFilesParams::Order::TaggedSymbol
            )

          DESC = T.let(
              :desc,
              OpenAI::VectorStores::FileBatchListFilesParams::Order::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::VectorStores::FileBatchListFilesParams::Order
              )
            end
        end
      end

      class FileBatchRetrieveParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :vector_store_id

        sig { override.returns({ vector_store_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig do
            params(
              vector_store_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(vector_store_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileBatchRetrieveParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileContentParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :vector_store_id

        sig { override.returns({ vector_store_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig do
            params(
              vector_store_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(vector_store_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileContentParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileContentResponse < OpenAI::Internal::Type::BaseModel
        # The text content
        sig { returns(T.nilable(String)) }
        attr_reader :text

        sig { params(text: String).void }
        attr_writer :text

        # The content type (currently only `"text"`)
        sig { returns(T.nilable(String)) }
        attr_reader :type

        sig { params(type: String).void }
        attr_writer :type

        sig { override.returns({ text: String, type: String }) }
        def to_hash; end

        class << self
          sig { params(text: String, type: String).returns(T.attached_class) }
          def new(
            text: nil, # The text content
            type: nil # The content type (currently only `"text"`)
); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Models::VectorStores::FileContentResponse,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard. Keys are strings with a maximum
        # length of 64 characters. Values are strings with a maximum length of 512
        # characters, booleans, or numbers.
        sig do
          returns(T.nilable(
              T::Hash[
                Symbol,
                OpenAI::VectorStores::FileCreateParams::Attribute::Variants
              ]
            ))
        end
        attr_accessor :attributes

        # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
        # strategy. Only applicable if `file_ids` is non-empty.
        sig do
          returns(T.nilable(
              T.any(
                OpenAI::AutoFileChunkingStrategyParam,
                OpenAI::StaticFileChunkingStrategyObjectParam
              )
            ))
        end
        attr_reader :chunking_strategy

        sig do
          params(
            chunking_strategy: T.any(
                OpenAI::AutoFileChunkingStrategyParam::OrHash,
                OpenAI::StaticFileChunkingStrategyObjectParam::OrHash
              )
          ).void
        end
        attr_writer :chunking_strategy

        # A [File](https://platform.openai.com/docs/api-reference/files) ID that the
        # vector store should use. Useful for tools like `file_search` that can access
        # files.
        sig { returns(String) }
        attr_accessor :file_id

        sig do
          override
            .returns({
              file_id: String,
              attributes:
                T.nilable(
                  T::Hash[
                    Symbol,
                    OpenAI::VectorStores::FileCreateParams::Attribute::Variants
                  ]
                ),
              chunking_strategy:
                T.any(
                  OpenAI::AutoFileChunkingStrategyParam,
                  OpenAI::StaticFileChunkingStrategyObjectParam
                ),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              file_id: String,
              attributes: T.nilable(
                T::Hash[
                  Symbol,
                  OpenAI::VectorStores::FileCreateParams::Attribute::Variants
                ]
              ),
              chunking_strategy: T.any(
                OpenAI::AutoFileChunkingStrategyParam::OrHash,
                OpenAI::StaticFileChunkingStrategyObjectParam::OrHash
              ),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            file_id:, # A [File](https://platform.openai.com/docs/api-reference/files) ID that the
                      # vector store should use. Useful for tools like `file_search` that can access
                      # files.
            attributes: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard. Keys are strings with a maximum
                             # length of 64 characters. Values are strings with a maximum length of 512
                             # characters, booleans, or numbers.
            chunking_strategy: nil, # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                                    # strategy. Only applicable if `file_ids` is non-empty.
            request_options: {}
); end
        end

        module Attribute
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::FileCreateParams::Attribute::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(String, Float, T::Boolean) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileCreateParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileDeleteParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :vector_store_id

        sig { override.returns({ vector_store_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig do
            params(
              vector_store_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(vector_store_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileDeleteParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileListParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # A cursor for use in pagination. `after` is an object ID that defines your place
        # in the list. For instance, if you make a list request and receive 100 objects,
        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
        # fetch the next page of the list.
        sig { returns(T.nilable(String)) }
        attr_reader :after

        sig { params(after: String).void }
        attr_writer :after

        # A cursor for use in pagination. `before` is an object ID that defines your place
        # in the list. For instance, if you make a list request and receive 100 objects,
        # starting with obj_foo, your subsequent call can include before=obj_foo in order
        # to fetch the previous page of the list.
        sig { returns(T.nilable(String)) }
        attr_reader :before

        sig { params(before: String).void }
        attr_writer :before

        # Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.
        sig { returns(T.nilable(OpenAI::VectorStores::FileListParams::Filter::OrSymbol)) }
        attr_reader :filter

        sig { params(filter: OpenAI::VectorStores::FileListParams::Filter::OrSymbol).void }
        attr_writer :filter

        # A limit on the number of objects to be returned. Limit can range between 1 and
        # 100, and the default is 20.
        sig { returns(T.nilable(Integer)) }
        attr_reader :limit

        sig { params(limit: Integer).void }
        attr_writer :limit

        # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
        # order and `desc` for descending order.
        sig { returns(T.nilable(OpenAI::VectorStores::FileListParams::Order::OrSymbol)) }
        attr_reader :order

        sig { params(order: OpenAI::VectorStores::FileListParams::Order::OrSymbol).void }
        attr_writer :order

        sig do
          override
            .returns({
              after: String,
              before: String,
              filter: OpenAI::VectorStores::FileListParams::Filter::OrSymbol,
              limit: Integer,
              order: OpenAI::VectorStores::FileListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              after: String,
              before: String,
              filter: OpenAI::VectorStores::FileListParams::Filter::OrSymbol,
              limit: Integer,
              order: OpenAI::VectorStores::FileListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                        # in the list. For instance, if you make a list request and receive 100 objects,
                        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                        # fetch the next page of the list.
            before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                         # in the list. For instance, if you make a list request and receive 100 objects,
                         # starting with obj_foo, your subsequent call can include before=obj_foo in order
                         # to fetch the previous page of the list.
            filter: nil, # Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.
            limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                        # 100, and the default is 20.
            order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                        # order and `desc` for descending order.
            request_options: {}
); end
        end

        # Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.
        module Filter
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::FileListParams::Filter::TaggedSymbol
              ])
            end
            def values; end
          end

          CANCELLED = T.let(
              :cancelled,
              OpenAI::VectorStores::FileListParams::Filter::TaggedSymbol
            )

          COMPLETED = T.let(
              :completed,
              OpenAI::VectorStores::FileListParams::Filter::TaggedSymbol
            )

          FAILED = T.let(
              :failed,
              OpenAI::VectorStores::FileListParams::Filter::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::VectorStores::FileListParams::Filter::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::VectorStores::FileListParams::Filter)
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileListParams,
              OpenAI::Internal::AnyHash
            )
          end

        # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
        # order and `desc` for descending order.
        module Order
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::FileListParams::Order::TaggedSymbol
              ])
            end
            def values; end
          end

          ASC = T.let(
              :asc,
              OpenAI::VectorStores::FileListParams::Order::TaggedSymbol
            )

          DESC = T.let(
              :desc,
              OpenAI::VectorStores::FileListParams::Order::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::VectorStores::FileListParams::Order)
            end
        end
      end

      class FileRetrieveParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { returns(String) }
        attr_accessor :vector_store_id

        sig { override.returns({ vector_store_id: String, request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig do
            params(
              vector_store_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(vector_store_id:, request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileRetrieveParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FileUpdateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard. Keys are strings with a maximum
        # length of 64 characters. Values are strings with a maximum length of 512
        # characters, booleans, or numbers.
        sig do
          returns(T.nilable(
              T::Hash[
                Symbol,
                OpenAI::VectorStores::FileUpdateParams::Attribute::Variants
              ]
            ))
        end
        attr_accessor :attributes

        sig { returns(String) }
        attr_accessor :vector_store_id

        sig do
          override
            .returns({
              vector_store_id: String,
              attributes:
                T.nilable(
                  T::Hash[
                    Symbol,
                    OpenAI::VectorStores::FileUpdateParams::Attribute::Variants
                  ]
                ),
              request_options: OpenAI::RequestOptions
            })
        end
        def to_hash; end

        class << self
          sig do
            params(
              vector_store_id: String,
              attributes: T.nilable(
                T::Hash[
                  Symbol,
                  OpenAI::VectorStores::FileUpdateParams::Attribute::Variants
                ]
              ),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(T.attached_class)
          end
          def new(
            vector_store_id:,
            attributes:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard. Keys are strings with a maximum
                         # length of 64 characters. Values are strings with a maximum length of 512
                         # characters, booleans, or numbers.
            request_options: {}
); end
        end

        module Attribute
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::FileUpdateParams::Attribute::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(String, Float, T::Boolean) }
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::FileUpdateParams,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class VectorStoreFile < OpenAI::Internal::Type::BaseModel
        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard. Keys are strings with a maximum
        # length of 64 characters. Values are strings with a maximum length of 512
        # characters, booleans, or numbers.
        sig do
          returns(T.nilable(
              T::Hash[
                Symbol,
                OpenAI::VectorStores::VectorStoreFile::Attribute::Variants
              ]
            ))
        end
        attr_accessor :attributes

        # The strategy used to chunk the file.
        sig { returns(T.nilable(OpenAI::FileChunkingStrategy::Variants)) }
        attr_reader :chunking_strategy

        sig do
          params(
            chunking_strategy: T.any(
                OpenAI::StaticFileChunkingStrategyObject::OrHash,
                OpenAI::OtherFileChunkingStrategyObject::OrHash
              )
          ).void
        end
        attr_writer :chunking_strategy

        # The Unix timestamp (in seconds) for when the vector store file was created.
        sig { returns(Integer) }
        attr_accessor :created_at

        # The identifier, which can be referenced in API endpoints.
        sig { returns(String) }
        attr_accessor :id

        # The last error associated with this vector store file. Will be `null` if there
        # are no errors.
        sig { returns(T.nilable(OpenAI::VectorStores::VectorStoreFile::LastError)) }
        attr_reader :last_error

        sig do
          params(
            last_error: T.nilable(
                OpenAI::VectorStores::VectorStoreFile::LastError::OrHash
              )
          ).void
        end
        attr_writer :last_error

        # The object type, which is always `vector_store.file`.
        sig { returns(Symbol) }
        attr_accessor :object

        # The status of the vector store file, which can be either `in_progress`,
        # `completed`, `cancelled`, or `failed`. The status `completed` indicates that the
        # vector store file is ready for use.
        sig { returns(OpenAI::VectorStores::VectorStoreFile::Status::TaggedSymbol) }
        attr_accessor :status

        # The total vector store usage in bytes. Note that this may be different from the
        # original file size.
        sig { returns(Integer) }
        attr_accessor :usage_bytes

        # The ID of the
        # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
        # that the [File](https://platform.openai.com/docs/api-reference/files) is
        # attached to.
        sig { returns(String) }
        attr_accessor :vector_store_id

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              last_error:
                T.nilable(OpenAI::VectorStores::VectorStoreFile::LastError),
              object: Symbol,
              status:
                OpenAI::VectorStores::VectorStoreFile::Status::TaggedSymbol,
              usage_bytes: Integer,
              vector_store_id: String,
              attributes:
                T.nilable(
                  T::Hash[
                    Symbol,
                    OpenAI::VectorStores::VectorStoreFile::Attribute::Variants
                  ]
                ),
              chunking_strategy: OpenAI::FileChunkingStrategy::Variants
            })
        end
        def to_hash; end

        class << self
          # A list of files attached to a vector store.
          sig do
            params(
              id: String,
              created_at: Integer,
              last_error: T.nilable(
                OpenAI::VectorStores::VectorStoreFile::LastError::OrHash
              ),
              status: OpenAI::VectorStores::VectorStoreFile::Status::OrSymbol,
              usage_bytes: Integer,
              vector_store_id: String,
              attributes: T.nilable(
                T::Hash[
                  Symbol,
                  OpenAI::VectorStores::VectorStoreFile::Attribute::Variants
                ]
              ),
              chunking_strategy: T.any(
                OpenAI::StaticFileChunkingStrategyObject::OrHash,
                OpenAI::OtherFileChunkingStrategyObject::OrHash
              ),
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The identifier, which can be referenced in API endpoints.
            created_at:, # The Unix timestamp (in seconds) for when the vector store file was created.
            last_error:, # The last error associated with this vector store file. Will be `null` if there
                         # are no errors.
            status:, # The status of the vector store file, which can be either `in_progress`,
                     # `completed`, `cancelled`, or `failed`. The status `completed` indicates that the
                     # vector store file is ready for use.
            usage_bytes:, # The total vector store usage in bytes. Note that this may be different from the
                          # original file size.
            vector_store_id:, # The ID of the
                              # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                              # that the [File](https://platform.openai.com/docs/api-reference/files) is
                              # attached to.
            attributes: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                             # for storing additional information about the object in a structured format, and
                             # querying for objects via API or the dashboard. Keys are strings with a maximum
                             # length of 64 characters. Values are strings with a maximum length of 512
                             # characters, booleans, or numbers.
            chunking_strategy: nil, # The strategy used to chunk the file.
            object: :"vector_store.file" # The object type, which is always `vector_store.file`.
); end
        end

        module Attribute
          extend OpenAI::Internal::Type::Union

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::VectorStoreFile::Attribute::Variants
              ])
            end
            def variants; end
          end

          Variants = T.type_alias { T.any(String, Float, T::Boolean) }
        end

        class LastError < OpenAI::Internal::Type::BaseModel
          # One of `server_error` or `rate_limit_exceeded`.
          sig { returns(OpenAI::VectorStores::VectorStoreFile::LastError::Code::TaggedSymbol) }
          attr_accessor :code

          # A human-readable description of the error.
          sig { returns(String) }
          attr_accessor :message

          sig do
            override
              .returns({
                code:
                  OpenAI::VectorStores::VectorStoreFile::LastError::Code::TaggedSymbol,
                message: String
              })
          end
          def to_hash; end

          class << self
            # The last error associated with this vector store file. Will be `null` if there
            # are no errors.
            sig do
              params(
                code: OpenAI::VectorStores::VectorStoreFile::LastError::Code::OrSymbol,
                message: String
              ).returns(T.attached_class)
            end
            def new(
              code:, # One of `server_error` or `rate_limit_exceeded`.
              message: # A human-readable description of the error.
); end
          end

          # One of `server_error` or `rate_limit_exceeded`.
          module Code
            extend OpenAI::Internal::Type::Enum

            class << self
              sig do
                override
                  .returns(T::Array[
                  OpenAI::VectorStores::VectorStoreFile::LastError::Code::TaggedSymbol
                ])
              end
              def values; end
            end

            INVALID_FILE = T.let(
                :invalid_file,
                OpenAI::VectorStores::VectorStoreFile::LastError::Code::TaggedSymbol
              )

            OrSymbol = T.type_alias { T.any(Symbol, String) }

            SERVER_ERROR = T.let(
                :server_error,
                OpenAI::VectorStores::VectorStoreFile::LastError::Code::TaggedSymbol
              )

            TaggedSymbol = T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::VectorStores::VectorStoreFile::LastError::Code
                )
              end

            UNSUPPORTED_FILE = T.let(
                :unsupported_file,
                OpenAI::VectorStores::VectorStoreFile::LastError::Code::TaggedSymbol
              )
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::VectorStores::VectorStoreFile::LastError,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::VectorStoreFile,
              OpenAI::Internal::AnyHash
            )
          end

        # The status of the vector store file, which can be either `in_progress`,
        # `completed`, `cancelled`, or `failed`. The status `completed` indicates that the
        # vector store file is ready for use.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::VectorStoreFile::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          CANCELLED = T.let(
              :cancelled,
              OpenAI::VectorStores::VectorStoreFile::Status::TaggedSymbol
            )

          COMPLETED = T.let(
              :completed,
              OpenAI::VectorStores::VectorStoreFile::Status::TaggedSymbol
            )

          FAILED = T.let(
              :failed,
              OpenAI::VectorStores::VectorStoreFile::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::VectorStores::VectorStoreFile::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::VectorStores::VectorStoreFile::Status)
            end
        end
      end

      class VectorStoreFileBatch < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) for when the vector store files batch was
        # created.
        sig { returns(Integer) }
        attr_accessor :created_at

        sig { returns(OpenAI::VectorStores::VectorStoreFileBatch::FileCounts) }
        attr_reader :file_counts

        sig { params(file_counts: OpenAI::VectorStores::VectorStoreFileBatch::FileCounts::OrHash).void }
        attr_writer :file_counts

        # The identifier, which can be referenced in API endpoints.
        sig { returns(String) }
        attr_accessor :id

        # The object type, which is always `vector_store.file_batch`.
        sig { returns(Symbol) }
        attr_accessor :object

        # The status of the vector store files batch, which can be either `in_progress`,
        # `completed`, `cancelled` or `failed`.
        sig { returns(OpenAI::VectorStores::VectorStoreFileBatch::Status::TaggedSymbol) }
        attr_accessor :status

        # The ID of the
        # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
        # that the [File](https://platform.openai.com/docs/api-reference/files) is
        # attached to.
        sig { returns(String) }
        attr_accessor :vector_store_id

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              file_counts:
                OpenAI::VectorStores::VectorStoreFileBatch::FileCounts,
              object: Symbol,
              status:
                OpenAI::VectorStores::VectorStoreFileBatch::Status::TaggedSymbol,
              vector_store_id: String
            })
        end
        def to_hash; end

        class << self
          # A batch of files attached to a vector store.
          sig do
            params(
              id: String,
              created_at: Integer,
              file_counts: OpenAI::VectorStores::VectorStoreFileBatch::FileCounts::OrHash,
              status: OpenAI::VectorStores::VectorStoreFileBatch::Status::OrSymbol,
              vector_store_id: String,
              object: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The identifier, which can be referenced in API endpoints.
            created_at:, # The Unix timestamp (in seconds) for when the vector store files batch was
                         # created.
            file_counts:,
            status:, # The status of the vector store files batch, which can be either `in_progress`,
                     # `completed`, `cancelled` or `failed`.
            vector_store_id:, # The ID of the
                              # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)
                              # that the [File](https://platform.openai.com/docs/api-reference/files) is
                              # attached to.
            object: :"vector_store.files_batch" # The object type, which is always `vector_store.file_batch`.
); end
        end

        class FileCounts < OpenAI::Internal::Type::BaseModel
          # The number of files that where cancelled.
          sig { returns(Integer) }
          attr_accessor :cancelled

          # The number of files that have been processed.
          sig { returns(Integer) }
          attr_accessor :completed

          # The number of files that have failed to process.
          sig { returns(Integer) }
          attr_accessor :failed

          # The number of files that are currently being processed.
          sig { returns(Integer) }
          attr_accessor :in_progress

          # The total number of files.
          sig { returns(Integer) }
          attr_accessor :total

          sig do
            override
              .returns({
                cancelled: Integer,
                completed: Integer,
                failed: Integer,
                in_progress: Integer,
                total: Integer
              })
          end
          def to_hash; end

          class << self
            sig do
              params(
                cancelled: Integer,
                completed: Integer,
                failed: Integer,
                in_progress: Integer,
                total: Integer
              ).returns(T.attached_class)
            end
            def new(
              cancelled:, # The number of files that where cancelled.
              completed:, # The number of files that have been processed.
              failed:, # The number of files that have failed to process.
              in_progress:, # The number of files that are currently being processed.
              total: # The total number of files.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::VectorStores::VectorStoreFileBatch::FileCounts,
                OpenAI::Internal::AnyHash
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::VectorStoreFileBatch,
              OpenAI::Internal::AnyHash
            )
          end

        # The status of the vector store files batch, which can be either `in_progress`,
        # `completed`, `cancelled` or `failed`.
        module Status
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::VectorStores::VectorStoreFileBatch::Status::TaggedSymbol
              ])
            end
            def values; end
          end

          CANCELLED = T.let(
              :cancelled,
              OpenAI::VectorStores::VectorStoreFileBatch::Status::TaggedSymbol
            )

          COMPLETED = T.let(
              :completed,
              OpenAI::VectorStores::VectorStoreFileBatch::Status::TaggedSymbol
            )

          FAILED = T.let(
              :failed,
              OpenAI::VectorStores::VectorStoreFileBatch::Status::TaggedSymbol
            )

          IN_PROGRESS = T.let(
              :in_progress,
              OpenAI::VectorStores::VectorStoreFileBatch::Status::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::VectorStores::VectorStoreFileBatch::Status)
            end
        end
      end

      class VectorStoreFileDeleted < OpenAI::Internal::Type::BaseModel
        sig { returns(T::Boolean) }
        attr_accessor :deleted

        sig { returns(String) }
        attr_accessor :id

        sig { returns(Symbol) }
        attr_accessor :object

        sig { override.returns({ id: String, deleted: T::Boolean, object: Symbol }) }
        def to_hash; end

        class << self
          sig { params(id: String, deleted: T::Boolean, object: Symbol).returns(T.attached_class) }
          def new(id:, deleted:, object: :"vector_store.file.deleted"); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::VectorStores::VectorStoreFileDeleted,
              OpenAI::Internal::AnyHash
            )
          end
      end
    end

    module Webhooks
      class BatchCancelledWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the batch API request was cancelled.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::BatchCancelledWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::BatchCancelledWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::BatchCancelledWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::BatchCancelledWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `batch.cancelled`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::BatchCancelledWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::BatchCancelledWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a batch API request has been cancelled.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::BatchCancelledWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::BatchCancelledWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the batch API request was cancelled.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"batch.cancelled" # The type of the event. Always `batch.cancelled`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the batch API request.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the batch API request.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::BatchCancelledWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::BatchCancelledWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::BatchCancelledWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::BatchCancelledWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::BatchCancelledWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class BatchCompletedWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the batch API request was completed.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::BatchCompletedWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::BatchCompletedWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::BatchCompletedWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::BatchCompletedWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `batch.completed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::BatchCompletedWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::BatchCompletedWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a batch API request has been completed.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::BatchCompletedWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::BatchCompletedWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the batch API request was completed.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"batch.completed" # The type of the event. Always `batch.completed`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the batch API request.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the batch API request.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::BatchCompletedWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::BatchCompletedWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::BatchCompletedWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::BatchCompletedWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::BatchCompletedWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class BatchExpiredWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the batch API request expired.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::BatchExpiredWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::BatchExpiredWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::BatchExpiredWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::BatchExpiredWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `batch.expired`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::BatchExpiredWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::BatchExpiredWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a batch API request has expired.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::BatchExpiredWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::BatchExpiredWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the batch API request expired.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"batch.expired" # The type of the event. Always `batch.expired`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the batch API request.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the batch API request.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::BatchExpiredWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::BatchExpiredWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::BatchExpiredWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Webhooks::BatchExpiredWebhookEvent::Object)
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::BatchExpiredWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class BatchFailedWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the batch API request failed.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::BatchFailedWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::BatchFailedWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::BatchFailedWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::BatchFailedWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `batch.failed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::BatchFailedWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::BatchFailedWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a batch API request has failed.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::BatchFailedWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::BatchFailedWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the batch API request failed.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"batch.failed" # The type of the event. Always `batch.failed`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the batch API request.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the batch API request.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::BatchFailedWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::BatchFailedWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::BatchFailedWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Webhooks::BatchFailedWebhookEvent::Object)
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::BatchFailedWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class EvalRunCanceledWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the eval run was canceled.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `eval.run.canceled`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when an eval run has been canceled.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the eval run was canceled.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"eval.run.canceled" # The type of the event. Always `eval.run.canceled`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the eval run.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the eval run.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::EvalRunCanceledWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::EvalRunCanceledWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class EvalRunFailedWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the eval run failed.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::EvalRunFailedWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::EvalRunFailedWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::EvalRunFailedWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::EvalRunFailedWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `eval.run.failed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::EvalRunFailedWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::EvalRunFailedWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when an eval run has failed.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::EvalRunFailedWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::EvalRunFailedWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the eval run failed.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"eval.run.failed" # The type of the event. Always `eval.run.failed`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the eval run.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the eval run.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::EvalRunFailedWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::EvalRunFailedWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::EvalRunFailedWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(Symbol, OpenAI::Webhooks::EvalRunFailedWebhookEvent::Object)
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::EvalRunFailedWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class EvalRunSucceededWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the eval run succeeded.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `eval.run.succeeded`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when an eval run has succeeded.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the eval run succeeded.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"eval.run.succeeded" # The type of the event. Always `eval.run.succeeded`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the eval run.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the eval run.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::EvalRunSucceededWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::EvalRunSucceededWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FineTuningJobCancelledWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the fine-tuning job was cancelled.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `fine_tuning.job.cancelled`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a fine-tuning job has been cancelled.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the fine-tuning job was cancelled.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"fine_tuning.job.cancelled" # The type of the event. Always `fine_tuning.job.cancelled`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the fine-tuning job.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the fine-tuning job.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FineTuningJobFailedWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the fine-tuning job failed.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `fine_tuning.job.failed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a fine-tuning job has failed.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the fine-tuning job failed.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"fine_tuning.job.failed" # The type of the event. Always `fine_tuning.job.failed`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the fine-tuning job.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the fine-tuning job.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::FineTuningJobFailedWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::FineTuningJobFailedWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class FineTuningJobSucceededWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the fine-tuning job succeeded.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `fine_tuning.job.succeeded`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a fine-tuning job has succeeded.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the fine-tuning job succeeded.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"fine_tuning.job.succeeded" # The type of the event. Always `fine_tuning.job.succeeded`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the fine-tuning job.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the fine-tuning job.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseCancelledWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the model response was cancelled.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::ResponseCancelledWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::ResponseCancelledWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::ResponseCancelledWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::ResponseCancelledWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `response.cancelled`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::ResponseCancelledWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::ResponseCancelledWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a background response has been cancelled.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::ResponseCancelledWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::ResponseCancelledWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the model response was cancelled.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"response.cancelled" # The type of the event. Always `response.cancelled`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the model response.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the model response.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::ResponseCancelledWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::ResponseCancelledWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::ResponseCancelledWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::ResponseCancelledWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::ResponseCancelledWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseCompletedWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the model response was completed.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::ResponseCompletedWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::ResponseCompletedWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::ResponseCompletedWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::ResponseCompletedWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `response.completed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::ResponseCompletedWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::ResponseCompletedWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a background response has been completed.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::ResponseCompletedWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::ResponseCompletedWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the model response was completed.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"response.completed" # The type of the event. Always `response.completed`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the model response.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the model response.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::ResponseCompletedWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::ResponseCompletedWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::ResponseCompletedWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::ResponseCompletedWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::ResponseCompletedWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseFailedWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the model response failed.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::ResponseFailedWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::ResponseFailedWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::ResponseFailedWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::ResponseFailedWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `response.failed`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::ResponseFailedWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::ResponseFailedWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a background response has failed.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::ResponseFailedWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::ResponseFailedWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the model response failed.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"response.failed" # The type of the event. Always `response.failed`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the model response.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the model response.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::ResponseFailedWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::ResponseFailedWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::ResponseFailedWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::ResponseFailedWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::ResponseFailedWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      class ResponseIncompleteWebhookEvent < OpenAI::Internal::Type::BaseModel
        # The Unix timestamp (in seconds) of when the model response was interrupted.
        sig { returns(Integer) }
        attr_accessor :created_at

        # Event data payload.
        sig { returns(OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Data) }
        attr_reader :data

        sig { params(data: OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Data::OrHash).void }
        attr_writer :data

        # The unique ID of the event.
        sig { returns(String) }
        attr_accessor :id

        # The object of the event. Always `event`.
        sig do
          returns(T.nilable(
              OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Object::TaggedSymbol
            ))
        end
        attr_reader :object

        sig { params(object: OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Object::OrSymbol).void }
        attr_writer :object

        # The type of the event. Always `response.incomplete`.
        sig { returns(Symbol) }
        attr_accessor :type

        sig do
          override
            .returns({
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Data,
              type: Symbol,
              object:
                OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Object::TaggedSymbol
            })
        end
        def to_hash; end

        class << self
          # Sent when a background response has been interrupted.
          sig do
            params(
              id: String,
              created_at: Integer,
              data: OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Data::OrHash,
              object: OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Object::OrSymbol,
              type: Symbol
            ).returns(T.attached_class)
          end
          def new(
            id:, # The unique ID of the event.
            created_at:, # The Unix timestamp (in seconds) of when the model response was interrupted.
            data:, # Event data payload.
            object: nil, # The object of the event. Always `event`.
            type: :"response.incomplete" # The type of the event. Always `response.incomplete`.
); end
        end

        class Data < OpenAI::Internal::Type::BaseModel
          # The unique ID of the model response.
          sig { returns(String) }
          attr_accessor :id

          sig { override.returns({ id: String }) }
          def to_hash; end

          class << self
            # Event data payload.
            sig { params(id: String).returns(T.attached_class) }
            def new(
              id: # The unique ID of the model response.
); end
          end

          OrHash = T.type_alias do
              T.any(
                OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Data,
                OpenAI::Internal::AnyHash
              )
            end
        end

        # The object of the event. Always `event`.
        module Object
          extend OpenAI::Internal::Type::Enum

          class << self
            sig do
              override
                .returns(T::Array[
                OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Object::TaggedSymbol
              ])
            end
            def values; end
          end

          EVENT = T.let(
              :event,
              OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Object::TaggedSymbol
            )

          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TaggedSymbol = T.type_alias do
              T.all(
                Symbol,
                OpenAI::Webhooks::ResponseIncompleteWebhookEvent::Object
              )
            end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::ResponseIncompleteWebhookEvent,
              OpenAI::Internal::AnyHash
            )
          end
      end

      # Sent when a batch API request has been cancelled.
      module UnwrapWebhookEvent
        extend OpenAI::Internal::Type::Union

        class << self
          sig { override.returns(T::Array[OpenAI::Webhooks::UnwrapWebhookEvent::Variants]) }
          def variants; end
        end

        Variants = T.type_alias do
            T.any(
              OpenAI::Webhooks::BatchCancelledWebhookEvent,
              OpenAI::Webhooks::BatchCompletedWebhookEvent,
              OpenAI::Webhooks::BatchExpiredWebhookEvent,
              OpenAI::Webhooks::BatchFailedWebhookEvent,
              OpenAI::Webhooks::EvalRunCanceledWebhookEvent,
              OpenAI::Webhooks::EvalRunFailedWebhookEvent,
              OpenAI::Webhooks::EvalRunSucceededWebhookEvent,
              OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent,
              OpenAI::Webhooks::FineTuningJobFailedWebhookEvent,
              OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent,
              OpenAI::Webhooks::ResponseCancelledWebhookEvent,
              OpenAI::Webhooks::ResponseCompletedWebhookEvent,
              OpenAI::Webhooks::ResponseFailedWebhookEvent,
              OpenAI::Webhooks::ResponseIncompleteWebhookEvent
            )
          end
      end

      class WebhookUnwrapParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        sig { override.returns({ request_options: OpenAI::RequestOptions }) }
        def to_hash; end

        class << self
          sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(T.attached_class) }
          def new(request_options: {}); end
        end

        OrHash = T.type_alias do
            T.any(
              OpenAI::Webhooks::WebhookUnwrapParams,
              OpenAI::Internal::AnyHash
            )
          end
      end
    end
  end

  Moderation = OpenAI::Models::Moderation
  ModerationCreateParams = OpenAI::Models::ModerationCreateParams
  ModerationImageURLInput = OpenAI::Models::ModerationImageURLInput
  ModerationModel = OpenAI::Models::ModerationModel
  ModerationMultiModalInput = OpenAI::Models::ModerationMultiModalInput
  ModerationTextInput = OpenAI::Models::ModerationTextInput

  OtherFileChunkingStrategyObject = OpenAI::Models::OtherFileChunkingStrategyObject

  Reasoning = OpenAI::Models::Reasoning
  ReasoningEffort = OpenAI::Models::ReasoningEffort

  # Specify HTTP behaviour to use for a specific request. These options supplement
  # or override those provided at the client level.
  #
  # When making a request, you can pass an actual {RequestOptions} instance, or
  # simply pass a Hash with symbol keys matching the attributes on this class.
  class RequestOptions < OpenAI::Internal::Type::BaseModel
    # Extra data to send with the request. These are deep merged into any data
    # generated as part of the normal request.
    sig { returns(T.nilable(T.anything)) }
    attr_accessor :extra_body

    # Extra headers to send with the request. These are `.merged`’d into any
    # `extra_headers` given at the client level.
    sig { returns(T.nilable(T::Hash[String, T.nilable(String)])) }
    attr_accessor :extra_headers

    # Extra query params to send with the request. These are `.merge`’d into any
    # `query` given at the client level.
    sig { returns(T.nilable(T::Hash[String, T.nilable(T.any(T::Array[String], String))])) }
    attr_accessor :extra_query

    # Idempotency key to send with request and all associated retries. Will only be
    # sent for write requests.
    sig { returns(T.nilable(String)) }
    attr_accessor :idempotency_key

    # Maximum number of retries to attempt after a failed initial request.
    sig { returns(T.nilable(Integer)) }
    attr_accessor :max_retries

    # Request timeout in seconds.
    sig { returns(T.nilable(Float)) }
    attr_accessor :timeout

    class << self
      # Returns a new instance of RequestOptions.
      sig { params(values: OpenAI::Internal::AnyHash).returns(T.attached_class) }
      def new(values = {}); end

      # @api private
      sig { params(opts: OpenAI::RequestOptions::OrHash).void }
      def validate!(opts); end
    end

    OrHash = T.type_alias { T.any(OpenAI::RequestOptions, OpenAI::Internal::AnyHash) }
  end

  module Resources
    class Audio
      sig { returns(OpenAI::Resources::Audio::Speech) }
      attr_reader :speech

      sig { returns(OpenAI::Resources::Audio::Transcriptions) }
      attr_reader :transcriptions

      sig { returns(OpenAI::Resources::Audio::Translations) }
      attr_reader :translations

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class Speech
        # Generates audio from the input text.
        sig do
          params(
            input: String,
            model: T.any(String, OpenAI::Audio::SpeechModel::OrSymbol),
            voice: T.any(String, OpenAI::Audio::SpeechCreateParams::Voice::OrSymbol),
            instructions: String,
            response_format: OpenAI::Audio::SpeechCreateParams::ResponseFormat::OrSymbol,
            speed: Float,
            stream_format: OpenAI::Audio::SpeechCreateParams::StreamFormat::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(StringIO)
        end
        def create(
          input:, # The text to generate audio for. The maximum length is 4096 characters.
          model:, # One of the available [TTS models](https://platform.openai.com/docs/models#tts):
                  # `tts-1`, `tts-1-hd` or `gpt-4o-mini-tts`.
          voice:, # The voice to use when generating the audio. Supported voices are `alloy`, `ash`,
                  # `ballad`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`, `shimmer`, and
                  # `verse`. Previews of the voices are available in the
                  # [Text to speech guide](https://platform.openai.com/docs/guides/text-to-speech#voice-options).
          instructions: nil, # Control the voice of your generated audio with additional instructions. Does not
                             # work with `tts-1` or `tts-1-hd`.
          response_format: nil, # The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`,
                                # `wav`, and `pcm`.
          speed: nil, # The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is
                      # the default.
          stream_format: nil, # The format to stream the audio in. Supported formats are `sse` and `audio`.
                              # `sse` is not supported for `tts-1` or `tts-1-hd`.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end

      class Transcriptions
        # See {OpenAI::Resources::Audio::Transcriptions#create_streaming} for streaming
        # counterpart.
        #
        # Transcribes audio into the input language.
        sig do
          params(
            file: OpenAI::Internal::FileInput,
            model: T.any(String, OpenAI::AudioModel::OrSymbol),
            chunking_strategy: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig::OrHash
                )
              ),
            include: T::Array[OpenAI::Audio::TranscriptionInclude::OrSymbol],
            language: String,
            prompt: String,
            response_format: OpenAI::AudioResponseFormat::OrSymbol,
            temperature: Float,
            timestamp_granularities: T::Array[
                OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity::OrSymbol
              ],
            stream: T.noreturn,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Models::Audio::TranscriptionCreateResponse::Variants)
        end
        def create(
          file:, # The audio file object (not file name) to transcribe, in one of these formats:
                 # flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
          model:, # ID of the model to use. The options are `gpt-4o-transcribe`,
                  # `gpt-4o-mini-transcribe`, and `whisper-1` (which is powered by our open source
                  # Whisper V2 model).
          chunking_strategy: nil, # Controls how the audio is cut into chunks. When set to `"auto"`, the server
                                  # first normalizes loudness and then uses voice activity detection (VAD) to choose
                                  # boundaries. `server_vad` object can be provided to tweak VAD detection
                                  # parameters manually. If unset, the audio is transcribed as a single block.
          include: nil, # Additional information to include in the transcription response. `logprobs` will
                        # return the log probabilities of the tokens in the response to understand the
                        # model's confidence in the transcription. `logprobs` only works with
                        # response_format set to `json` and only with the models `gpt-4o-transcribe` and
                        # `gpt-4o-mini-transcribe`.
          language: nil, # The language of the input audio. Supplying the input language in
                         # [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)
                         # format will improve accuracy and latency.
          prompt: nil, # An optional text to guide the model's style or continue a previous audio
                       # segment. The
                       # [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)
                       # should match the audio language.
          response_format: nil, # The format of the output, in one of these options: `json`, `text`, `srt`,
                                # `verbose_json`, or `vtt`. For `gpt-4o-transcribe` and `gpt-4o-mini-transcribe`,
                                # the only supported format is `json`.
          temperature: nil, # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the
                            # output more random, while lower values like 0.2 will make it more focused and
                            # deterministic. If set to 0, the model will use
                            # [log probability](https://en.wikipedia.org/wiki/Log_probability) to
                            # automatically increase the temperature until certain thresholds are hit.
          timestamp_granularities: nil, # The timestamp granularities to populate for this transcription.
                                        # `response_format` must be set `verbose_json` to use timestamp granularities.
                                        # Either or both of these options are supported: `word`, or `segment`. Note: There
                                        # is no additional latency for segment timestamps, but generating word timestamps
                                        # incurs additional latency.
          stream: false, # There is no need to provide `stream:`. Instead, use `#create_streaming` or
                         # `#create` for streaming and non-streaming use cases, respectively.
          request_options: {}
); end

        # See {OpenAI::Resources::Audio::Transcriptions#create} for non-streaming
        # counterpart.
        #
        # Transcribes audio into the input language.
        sig do
          params(
            file: OpenAI::Internal::FileInput,
            model: T.any(String, OpenAI::AudioModel::OrSymbol),
            chunking_strategy: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::Audio::TranscriptionCreateParams::ChunkingStrategy::VadConfig::OrHash
                )
              ),
            include: T::Array[OpenAI::Audio::TranscriptionInclude::OrSymbol],
            language: String,
            prompt: String,
            response_format: OpenAI::AudioResponseFormat::OrSymbol,
            temperature: Float,
            timestamp_granularities: T::Array[
                OpenAI::Audio::TranscriptionCreateParams::TimestampGranularity::OrSymbol
              ],
            stream: T.noreturn,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::Stream[
              OpenAI::Audio::TranscriptionStreamEvent::Variants
            ])
        end
        def create_streaming(
          file:, # The audio file object (not file name) to transcribe, in one of these formats:
                 # flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
          model:, # ID of the model to use. The options are `gpt-4o-transcribe`,
                  # `gpt-4o-mini-transcribe`, and `whisper-1` (which is powered by our open source
                  # Whisper V2 model).
          chunking_strategy: nil, # Controls how the audio is cut into chunks. When set to `"auto"`, the server
                                  # first normalizes loudness and then uses voice activity detection (VAD) to choose
                                  # boundaries. `server_vad` object can be provided to tweak VAD detection
                                  # parameters manually. If unset, the audio is transcribed as a single block.
          include: nil, # Additional information to include in the transcription response. `logprobs` will
                        # return the log probabilities of the tokens in the response to understand the
                        # model's confidence in the transcription. `logprobs` only works with
                        # response_format set to `json` and only with the models `gpt-4o-transcribe` and
                        # `gpt-4o-mini-transcribe`.
          language: nil, # The language of the input audio. Supplying the input language in
                         # [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)
                         # format will improve accuracy and latency.
          prompt: nil, # An optional text to guide the model's style or continue a previous audio
                       # segment. The
                       # [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)
                       # should match the audio language.
          response_format: nil, # The format of the output, in one of these options: `json`, `text`, `srt`,
                                # `verbose_json`, or `vtt`. For `gpt-4o-transcribe` and `gpt-4o-mini-transcribe`,
                                # the only supported format is `json`.
          temperature: nil, # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the
                            # output more random, while lower values like 0.2 will make it more focused and
                            # deterministic. If set to 0, the model will use
                            # [log probability](https://en.wikipedia.org/wiki/Log_probability) to
                            # automatically increase the temperature until certain thresholds are hit.
          timestamp_granularities: nil, # The timestamp granularities to populate for this transcription.
                                        # `response_format` must be set `verbose_json` to use timestamp granularities.
                                        # Either or both of these options are supported: `word`, or `segment`. Note: There
                                        # is no additional latency for segment timestamps, but generating word timestamps
                                        # incurs additional latency.
          stream: true, # There is no need to provide `stream:`. Instead, use `#create_streaming` or
                        # `#create` for streaming and non-streaming use cases, respectively.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end

      class Translations
        # Translates audio into English.
        sig do
          params(
            file: OpenAI::Internal::FileInput,
            model: T.any(String, OpenAI::AudioModel::OrSymbol),
            prompt: String,
            response_format: OpenAI::Audio::TranslationCreateParams::ResponseFormat::OrSymbol,
            temperature: Float,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Models::Audio::TranslationCreateResponse::Variants)
        end
        def create(
          file:, # The audio file object (not file name) translate, in one of these formats: flac,
                 # mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
          model:, # ID of the model to use. Only `whisper-1` (which is powered by our open source
                  # Whisper V2 model) is currently available.
          prompt: nil, # An optional text to guide the model's style or continue a previous audio
                       # segment. The
                       # [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)
                       # should be in English.
          response_format: nil, # The format of the output, in one of these options: `json`, `text`, `srt`,
                                # `verbose_json`, or `vtt`.
          temperature: nil, # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the
                            # output more random, while lower values like 0.2 will make it more focused and
                            # deterministic. If set to 0, the model will use
                            # [log probability](https://en.wikipedia.org/wiki/Log_probability) to
                            # automatically increase the temperature until certain thresholds are hit.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end
    end

    class Batches
      # Cancels an in-progress batch. The batch will be in status `cancelling` for up to
      # 10 minutes, before changing to `cancelled`, where it will have partial results
      # (if any) available in the output file.
      sig { params(batch_id: String, request_options: OpenAI::RequestOptions::OrHash).returns(OpenAI::Batch) }
      def cancel(
        batch_id, # The ID of the batch to cancel.
        request_options: {}
); end

      # Creates and executes a batch from an uploaded file of requests
      sig do
        params(
          completion_window: OpenAI::BatchCreateParams::CompletionWindow::OrSymbol,
          endpoint: OpenAI::BatchCreateParams::Endpoint::OrSymbol,
          input_file_id: String,
          metadata: T.nilable(T::Hash[Symbol, String]),
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Batch)
      end
      def create(
        completion_window:, # The time frame within which the batch should be processed. Currently only `24h`
                            # is supported.
        endpoint:, # The endpoint to be used for all requests in the batch. Currently
                   # `/v1/responses`, `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions`
                   # are supported. Note that `/v1/embeddings` batches are also restricted to a
                   # maximum of 50,000 embedding inputs across all requests in the batch.
        input_file_id:, # The ID of an uploaded file that contains requests for the new batch.
                        # See [upload file](https://platform.openai.com/docs/api-reference/files/create)
                        # for how to upload a file.
                        # Your input file must be formatted as a
                        # [JSONL file](https://platform.openai.com/docs/api-reference/batch/request-input),
                        # and must be uploaded with the purpose `batch`. The file can contain up to 50,000
                        # requests, and can be up to 200 MB in size.
        metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
        request_options: {}
); end

      # List your organization's batches.
      sig do
        params(
          after: String,
          limit: Integer,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::CursorPage[OpenAI::Batch])
      end
      def list(
        after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                    # in the list. For instance, if you make a list request and receive 100 objects,
                    # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                    # fetch the next page of the list.
        limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                    # 100, and the default is 20.
        request_options: {}
); end

      # Retrieves a batch.
      sig { params(batch_id: String, request_options: OpenAI::RequestOptions::OrHash).returns(OpenAI::Batch) }
      def retrieve(
        batch_id, # The ID of the batch to retrieve.
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end
    end

    class Beta
      sig { returns(OpenAI::Resources::Beta::Assistants) }
      attr_reader :assistants

      sig { returns(OpenAI::Resources::Beta::Threads) }
      attr_reader :threads

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class Assistants
        # Create an assistant with a model and instructions.
        sig do
          params(
            model: T.any(String, OpenAI::ChatModel::OrSymbol),
            description: T.nilable(String),
            instructions: T.nilable(String),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: T.nilable(String),
            reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
            response_format: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash
                )
              ),
            temperature: T.nilable(Float),
            tool_resources: T.nilable(
                OpenAI::Beta::AssistantCreateParams::ToolResources::OrHash
              ),
            tools: T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool::OrHash,
                  OpenAI::Beta::FileSearchTool::OrHash,
                  OpenAI::Beta::FunctionTool::OrHash
                )
              ],
            top_p: T.nilable(Float),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Beta::Assistant)
        end
        def create(
          model:, # ID of the model to use. You can use the
                  # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                  # see all of your available models, or see our
                  # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                  # them.
          description: nil, # The description of the assistant. The maximum length is 512 characters.
          instructions: nil, # The system instructions that the assistant uses. The maximum length is 256,000
                             # characters.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          name: nil, # The name of the assistant. The maximum length is 256 characters.
          reasoning_effort: nil, # **o-series models only**
                                 # Constrains effort on reasoning for
                                 # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                 # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                 # result in faster responses and fewer tokens used on reasoning in a response.
          response_format: nil, # Specifies the format that the model must output. Compatible with
                                # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                # in the
                                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                # message the model generates is valid JSON.
                                # **Important:** when using JSON mode, you **must** also instruct the model to
                                # produce JSON yourself via a system or user message. Without this, the model may
                                # generate an unending stream of whitespace until the generation reaches the token
                                # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                # the message content may be partially cut off if `finish_reason="length"`, which
                                # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                # max context length.
          temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                            # make the output more random, while lower values like 0.2 will make it more
                            # focused and deterministic.
          tool_resources: nil, # A set of resources that are used by the assistant's tools. The resources are
                               # specific to the type of tool. For example, the `code_interpreter` tool requires
                               # a list of file IDs, while the `file_search` tool requires a list of vector store
                               # IDs.
          tools: nil, # A list of tool enabled on the assistant. There can be a maximum of 128 tools per
                      # assistant. Tools can be of types `code_interpreter`, `file_search`, or
                      # `function`.
          top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                      # model considers the results of the tokens with top_p probability mass. So 0.1
                      # means only the tokens comprising the top 10% probability mass are considered.
                      # We generally recommend altering this or temperature but not both.
          request_options: {}
); end

        # Delete an assistant.
        sig do
          params(
            assistant_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Beta::AssistantDeleted)
        end
        def delete(
          assistant_id, # The ID of the assistant to delete.
          request_options: {}
); end

        # Returns a list of assistants.
        sig do
          params(
            after: String,
            before: String,
            limit: Integer,
            order: OpenAI::Beta::AssistantListParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::CursorPage[OpenAI::Beta::Assistant])
        end
        def list(
          after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                      # in the list. For instance, if you make a list request and receive 100 objects,
                      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                      # fetch the next page of the list.
          before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                       # in the list. For instance, if you make a list request and receive 100 objects,
                       # starting with obj_foo, your subsequent call can include before=obj_foo in order
                       # to fetch the previous page of the list.
          limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                      # 100, and the default is 20.
          order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                      # order and `desc` for descending order.
          request_options: {}
); end

        # Retrieves an assistant.
        sig do
          params(
            assistant_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Beta::Assistant)
        end
        def retrieve(
          assistant_id, # The ID of the assistant to retrieve.
          request_options: {}
); end

        # Modifies an assistant.
        sig do
          params(
            assistant_id: String,
            description: T.nilable(String),
            instructions: T.nilable(String),
            metadata: T.nilable(T::Hash[Symbol, String]),
            model: T.any(
                String,
                OpenAI::Beta::AssistantUpdateParams::Model::OrSymbol
              ),
            name: T.nilable(String),
            reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
            response_format: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash
                )
              ),
            temperature: T.nilable(Float),
            tool_resources: T.nilable(
                OpenAI::Beta::AssistantUpdateParams::ToolResources::OrHash
              ),
            tools: T::Array[
                T.any(
                  OpenAI::Beta::CodeInterpreterTool::OrHash,
                  OpenAI::Beta::FileSearchTool::OrHash,
                  OpenAI::Beta::FunctionTool::OrHash
                )
              ],
            top_p: T.nilable(Float),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Beta::Assistant)
        end
        def update(
          assistant_id, # The ID of the assistant to modify.
          description: nil, # The description of the assistant. The maximum length is 512 characters.
          instructions: nil, # The system instructions that the assistant uses. The maximum length is 256,000
                             # characters.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          model: nil, # ID of the model to use. You can use the
                      # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                      # see all of your available models, or see our
                      # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                      # them.
          name: nil, # The name of the assistant. The maximum length is 256 characters.
          reasoning_effort: nil, # **o-series models only**
                                 # Constrains effort on reasoning for
                                 # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                 # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                 # result in faster responses and fewer tokens used on reasoning in a response.
          response_format: nil, # Specifies the format that the model must output. Compatible with
                                # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                # in the
                                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                # message the model generates is valid JSON.
                                # **Important:** when using JSON mode, you **must** also instruct the model to
                                # produce JSON yourself via a system or user message. Without this, the model may
                                # generate an unending stream of whitespace until the generation reaches the token
                                # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                # the message content may be partially cut off if `finish_reason="length"`, which
                                # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                # max context length.
          temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                            # make the output more random, while lower values like 0.2 will make it more
                            # focused and deterministic.
          tool_resources: nil, # A set of resources that are used by the assistant's tools. The resources are
                               # specific to the type of tool. For example, the `code_interpreter` tool requires
                               # a list of file IDs, while the `file_search` tool requires a list of vector store
                               # IDs.
          tools: nil, # A list of tool enabled on the assistant. There can be a maximum of 128 tools per
                      # assistant. Tools can be of types `code_interpreter`, `file_search`, or
                      # `function`.
          top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                      # model considers the results of the tokens with top_p probability mass. So 0.1
                      # means only the tokens comprising the top 10% probability mass are considered.
                      # We generally recommend altering this or temperature but not both.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end

      class Threads
        sig { returns(OpenAI::Resources::Beta::Threads::Messages) }
        attr_reader :messages

        sig { returns(OpenAI::Resources::Beta::Threads::Runs) }
        attr_reader :runs

        # Create a thread.
        sig do
          params(
            messages: T::Array[OpenAI::Beta::ThreadCreateParams::Message::OrHash],
            metadata: T.nilable(T::Hash[Symbol, String]),
            tool_resources: T.nilable(
                OpenAI::Beta::ThreadCreateParams::ToolResources::OrHash
              ),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Beta::Thread)
        end
        def create(
          messages: nil, # A list of [messages](https://platform.openai.com/docs/api-reference/messages) to
                         # start the thread with.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          tool_resources: nil, # A set of resources that are made available to the assistant's tools in this
                               # thread. The resources are specific to the type of tool. For example, the
                               # `code_interpreter` tool requires a list of file IDs, while the `file_search`
                               # tool requires a list of vector store IDs.
          request_options: {}
); end

        # See {OpenAI::Resources::Beta::Threads#stream_raw} for streaming counterpart.
        #
        # Create a thread and run it in one request.
        sig do
          params(
            assistant_id: String,
            instructions: T.nilable(String),
            max_completion_tokens: T.nilable(Integer),
            max_prompt_tokens: T.nilable(Integer),
            metadata: T.nilable(T::Hash[Symbol, String]),
            model: T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol)),
            parallel_tool_calls: T::Boolean,
            response_format: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash
                )
              ),
            temperature: T.nilable(Float),
            thread: OpenAI::Beta::ThreadCreateAndRunParams::Thread::OrHash,
            tool_choice: T.nilable(
                T.any(
                  OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                  OpenAI::Beta::AssistantToolChoice::OrHash
                )
              ),
            tool_resources: T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::OrHash
              ),
            tools: T.nilable(
                T::Array[
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool::OrHash,
                    OpenAI::Beta::FileSearchTool::OrHash,
                    OpenAI::Beta::FunctionTool::OrHash
                  )
                ]
              ),
            top_p: T.nilable(Float),
            truncation_strategy: T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::OrHash
              ),
            stream: T.noreturn,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Beta::Threads::Run)
        end
        def create_and_run(
          assistant_id:, # The ID of the
                         # [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to
                         # execute this run.
          instructions: nil, # Override the default system message of the assistant. This is useful for
                             # modifying the behavior on a per-run basis.
          max_completion_tokens: nil, # The maximum number of completion tokens that may be used over the course of the
                                      # run. The run will make a best effort to use only the number of completion tokens
                                      # specified, across multiple turns of the run. If the run exceeds the number of
                                      # completion tokens specified, the run will end with status `incomplete`. See
                                      # `incomplete_details` for more info.
          max_prompt_tokens: nil, # The maximum number of prompt tokens that may be used over the course of the run.
                                  # The run will make a best effort to use only the number of prompt tokens
                                  # specified, across multiple turns of the run. If the run exceeds the number of
                                  # prompt tokens specified, the run will end with status `incomplete`. See
                                  # `incomplete_details` for more info.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          model: nil, # The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to
                      # be used to execute this run. If a value is provided here, it will override the
                      # model associated with the assistant. If not, the model associated with the
                      # assistant will be used.
          parallel_tool_calls: nil, # Whether to enable
                                    # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                    # during tool use.
          response_format: nil, # Specifies the format that the model must output. Compatible with
                                # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                # in the
                                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                # message the model generates is valid JSON.
                                # **Important:** when using JSON mode, you **must** also instruct the model to
                                # produce JSON yourself via a system or user message. Without this, the model may
                                # generate an unending stream of whitespace until the generation reaches the token
                                # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                # the message content may be partially cut off if `finish_reason="length"`, which
                                # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                # max context length.
          temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                            # make the output more random, while lower values like 0.2 will make it more
                            # focused and deterministic.
          thread: nil, # Options to create a new thread. If no thread is provided when running a request,
                       # an empty thread will be created.
          tool_choice: nil, # Controls which (if any) tool is called by the model. `none` means the model will
                            # not call any tools and instead generates a message. `auto` is the default value
                            # and means the model can pick between generating a message or calling one or more
                            # tools. `required` means the model must call one or more tools before responding
                            # to the user. Specifying a particular tool like `{"type": "file_search"}` or
                            # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                            # call that tool.
          tool_resources: nil, # A set of resources that are used by the assistant's tools. The resources are
                               # specific to the type of tool. For example, the `code_interpreter` tool requires
                               # a list of file IDs, while the `file_search` tool requires a list of vector store
                               # IDs.
          tools: nil, # Override the tools the assistant can use for this run. This is useful for
                      # modifying the behavior on a per-run basis.
          top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                      # model considers the results of the tokens with top_p probability mass. So 0.1
                      # means only the tokens comprising the top 10% probability mass are considered.
                      # We generally recommend altering this or temperature but not both.
          truncation_strategy: nil, # Controls for how a thread will be truncated prior to the run. Use this to
                                    # control the intial context window of the run.
          stream: false, # There is no need to provide `stream:`. Instead, use `#stream_raw` or
                         # `#create_and_run` for streaming and non-streaming use cases, respectively.
          request_options: {}
); end

        # Delete a thread.
        sig do
          params(
            thread_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Beta::ThreadDeleted)
        end
        def delete(
          thread_id, # The ID of the thread to delete.
          request_options: {}
); end

        # Retrieves a thread.
        sig do
          params(
            thread_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Beta::Thread)
        end
        def retrieve(
          thread_id, # The ID of the thread to retrieve.
          request_options: {}
); end

        # See {OpenAI::Resources::Beta::Threads#create_and_run} for non-streaming
        # counterpart.
        #
        # Create a thread and run it in one request.
        sig do
          params(
            assistant_id: String,
            instructions: T.nilable(String),
            max_completion_tokens: T.nilable(Integer),
            max_prompt_tokens: T.nilable(Integer),
            metadata: T.nilable(T::Hash[Symbol, String]),
            model: T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol)),
            parallel_tool_calls: T::Boolean,
            response_format: T.nilable(
                T.any(
                  Symbol,
                  OpenAI::ResponseFormatText::OrHash,
                  OpenAI::ResponseFormatJSONObject::OrHash,
                  OpenAI::ResponseFormatJSONSchema::OrHash
                )
              ),
            temperature: T.nilable(Float),
            thread: OpenAI::Beta::ThreadCreateAndRunParams::Thread::OrHash,
            tool_choice: T.nilable(
                T.any(
                  OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                  OpenAI::Beta::AssistantToolChoice::OrHash
                )
              ),
            tool_resources: T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::ToolResources::OrHash
              ),
            tools: T.nilable(
                T::Array[
                  T.any(
                    OpenAI::Beta::CodeInterpreterTool::OrHash,
                    OpenAI::Beta::FileSearchTool::OrHash,
                    OpenAI::Beta::FunctionTool::OrHash
                  )
                ]
              ),
            top_p: T.nilable(Float),
            truncation_strategy: T.nilable(
                OpenAI::Beta::ThreadCreateAndRunParams::TruncationStrategy::OrHash
              ),
            stream: T.noreturn,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::Stream[
              OpenAI::Beta::AssistantStreamEvent::Variants
            ])
        end
        def stream_raw(
          assistant_id:, # The ID of the
                         # [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to
                         # execute this run.
          instructions: nil, # Override the default system message of the assistant. This is useful for
                             # modifying the behavior on a per-run basis.
          max_completion_tokens: nil, # The maximum number of completion tokens that may be used over the course of the
                                      # run. The run will make a best effort to use only the number of completion tokens
                                      # specified, across multiple turns of the run. If the run exceeds the number of
                                      # completion tokens specified, the run will end with status `incomplete`. See
                                      # `incomplete_details` for more info.
          max_prompt_tokens: nil, # The maximum number of prompt tokens that may be used over the course of the run.
                                  # The run will make a best effort to use only the number of prompt tokens
                                  # specified, across multiple turns of the run. If the run exceeds the number of
                                  # prompt tokens specified, the run will end with status `incomplete`. See
                                  # `incomplete_details` for more info.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          model: nil, # The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to
                      # be used to execute this run. If a value is provided here, it will override the
                      # model associated with the assistant. If not, the model associated with the
                      # assistant will be used.
          parallel_tool_calls: nil, # Whether to enable
                                    # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                    # during tool use.
          response_format: nil, # Specifies the format that the model must output. Compatible with
                                # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                # in the
                                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                # message the model generates is valid JSON.
                                # **Important:** when using JSON mode, you **must** also instruct the model to
                                # produce JSON yourself via a system or user message. Without this, the model may
                                # generate an unending stream of whitespace until the generation reaches the token
                                # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                # the message content may be partially cut off if `finish_reason="length"`, which
                                # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                # max context length.
          temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                            # make the output more random, while lower values like 0.2 will make it more
                            # focused and deterministic.
          thread: nil, # Options to create a new thread. If no thread is provided when running a request,
                       # an empty thread will be created.
          tool_choice: nil, # Controls which (if any) tool is called by the model. `none` means the model will
                            # not call any tools and instead generates a message. `auto` is the default value
                            # and means the model can pick between generating a message or calling one or more
                            # tools. `required` means the model must call one or more tools before responding
                            # to the user. Specifying a particular tool like `{"type": "file_search"}` or
                            # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                            # call that tool.
          tool_resources: nil, # A set of resources that are used by the assistant's tools. The resources are
                               # specific to the type of tool. For example, the `code_interpreter` tool requires
                               # a list of file IDs, while the `file_search` tool requires a list of vector store
                               # IDs.
          tools: nil, # Override the tools the assistant can use for this run. This is useful for
                      # modifying the behavior on a per-run basis.
          top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                      # model considers the results of the tokens with top_p probability mass. So 0.1
                      # means only the tokens comprising the top 10% probability mass are considered.
                      # We generally recommend altering this or temperature but not both.
          truncation_strategy: nil, # Controls for how a thread will be truncated prior to the run. Use this to
                                    # control the intial context window of the run.
          stream: true, # There is no need to provide `stream:`. Instead, use `#stream_raw` or
                        # `#create_and_run` for streaming and non-streaming use cases, respectively.
          request_options: {}
); end

        # Modifies a thread.
        sig do
          params(
            thread_id: String,
            metadata: T.nilable(T::Hash[Symbol, String]),
            tool_resources: T.nilable(
                OpenAI::Beta::ThreadUpdateParams::ToolResources::OrHash
              ),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Beta::Thread)
        end
        def update(
          thread_id, # The ID of the thread to modify. Only the `metadata` can be modified.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          tool_resources: nil, # A set of resources that are made available to the assistant's tools in this
                               # thread. The resources are specific to the type of tool. For example, the
                               # `code_interpreter` tool requires a list of file IDs, while the `file_search`
                               # tool requires a list of vector store IDs.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end

        class Messages
          # Create a message.
          sig do
            params(
              thread_id: String,
              content: OpenAI::Beta::Threads::MessageCreateParams::Content::Variants,
              role: OpenAI::Beta::Threads::MessageCreateParams::Role::OrSymbol,
              attachments: T.nilable(
                  T::Array[
                    OpenAI::Beta::Threads::MessageCreateParams::Attachment::OrHash
                  ]
                ),
              metadata: T.nilable(T::Hash[Symbol, String]),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Beta::Threads::Message)
          end
          def create(
            thread_id, # The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)
                       # to create a message for.
            content:, # The text contents of the message.
            role:, # The role of the entity that is creating the message. Allowed values include:
                   # - `user`: Indicates the message is sent by an actual user and should be used in
                   #   most cases to represent user-generated messages.
                   # - `assistant`: Indicates the message is generated by the assistant. Use this
                   #   value to insert messages from the assistant into the conversation.
            attachments: nil, # A list of files attached to the message, and the tools they should be added to.
            metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            request_options: {}
); end

          # Deletes a message.
          sig do
            params(
              message_id: String,
              thread_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Beta::Threads::MessageDeleted)
          end
          def delete(
            message_id, # The ID of the message to delete.
            thread_id:, # The ID of the thread to which this message belongs.
            request_options: {}
); end

          # Returns a list of messages for a given thread.
          sig do
            params(
              thread_id: String,
              after: String,
              before: String,
              limit: Integer,
              order: OpenAI::Beta::Threads::MessageListParams::Order::OrSymbol,
              run_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Internal::CursorPage[OpenAI::Beta::Threads::Message])
          end
          def list(
            thread_id, # The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)
                       # the messages belong to.
            after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                        # in the list. For instance, if you make a list request and receive 100 objects,
                        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                        # fetch the next page of the list.
            before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                         # in the list. For instance, if you make a list request and receive 100 objects,
                         # starting with obj_foo, your subsequent call can include before=obj_foo in order
                         # to fetch the previous page of the list.
            limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                        # 100, and the default is 20.
            order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                        # order and `desc` for descending order.
            run_id: nil, # Filter messages by the run ID that generated them.
            request_options: {}
); end

          # Retrieve a message.
          sig do
            params(
              message_id: String,
              thread_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Beta::Threads::Message)
          end
          def retrieve(
            message_id, # The ID of the message to retrieve.
            thread_id:, # The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)
                        # to which this message belongs.
            request_options: {}
); end

          # Modifies a message.
          sig do
            params(
              message_id: String,
              thread_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Beta::Threads::Message)
          end
          def update(
            message_id, # Path param: The ID of the message to modify.
            thread_id:, # Path param: The ID of the thread to which this message belongs.
            metadata: nil, # Body param: Set of 16 key-value pairs that can be attached to an object. This
                           # can be useful for storing additional information about the object in a
                           # structured format, and querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            request_options: {}
); end

          class << self
            # @api private
            sig { params(client: OpenAI::Client).returns(T.attached_class) }
            def new(client:); end
          end
        end

        class Runs
          sig { returns(OpenAI::Resources::Beta::Threads::Runs::Steps) }
          attr_reader :steps

          # Cancels a run that is `in_progress`.
          sig do
            params(
              run_id: String,
              thread_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Beta::Threads::Run)
          end
          def cancel(
            run_id, # The ID of the run to cancel.
            thread_id:, # The ID of the thread to which this run belongs.
            request_options: {}
); end

          # See {OpenAI::Resources::Beta::Threads::Runs#create_stream_raw} for streaming
          # counterpart.
          #
          # Create a run.
          sig do
            params(
              thread_id: String,
              assistant_id: String,
              include: T::Array[OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol],
              additional_instructions: T.nilable(String),
              additional_messages: T.nilable(
                  T::Array[
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::OrHash
                  ]
                ),
              instructions: T.nilable(String),
              max_completion_tokens: T.nilable(Integer),
              max_prompt_tokens: T.nilable(Integer),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol)),
              parallel_tool_calls: T::Boolean,
              reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
              response_format: T.nilable(
                  T.any(
                    Symbol,
                    OpenAI::ResponseFormatText::OrHash,
                    OpenAI::ResponseFormatJSONObject::OrHash,
                    OpenAI::ResponseFormatJSONSchema::OrHash
                  )
                ),
              temperature: T.nilable(Float),
              tool_choice: T.nilable(
                  T.any(
                    OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                    OpenAI::Beta::AssistantToolChoice::OrHash
                  )
                ),
              tools: T.nilable(
                  T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool::OrHash,
                      OpenAI::Beta::FileSearchTool::OrHash,
                      OpenAI::Beta::FunctionTool::OrHash
                    )
                  ]
                ),
              top_p: T.nilable(Float),
              truncation_strategy: T.nilable(
                  OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::OrHash
                ),
              stream: T.noreturn,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Beta::Threads::Run)
          end
          def create(
            thread_id, # Path param: The ID of the thread to run.
            assistant_id:, # Body param: The ID of the
                           # [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to
                           # execute this run.
            include: nil, # Query param: A list of additional fields to include in the response. Currently
                          # the only supported value is
                          # `step_details.tool_calls[*].file_search.results[*].content` to fetch the file
                          # search result content.
                          # See the
                          # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
                          # for more information.
            additional_instructions: nil, # Body param: Appends additional instructions at the end of the instructions for
                                          # the run. This is useful for modifying the behavior on a per-run basis without
                                          # overriding other instructions.
            additional_messages: nil, # Body param: Adds additional messages to the thread before creating the run.
            instructions: nil, # Body param: Overrides the
                               # [instructions](https://platform.openai.com/docs/api-reference/assistants/createAssistant)
                               # of the assistant. This is useful for modifying the behavior on a per-run basis.
            max_completion_tokens: nil, # Body param: The maximum number of completion tokens that may be used over the
                                        # course of the run. The run will make a best effort to use only the number of
                                        # completion tokens specified, across multiple turns of the run. If the run
                                        # exceeds the number of completion tokens specified, the run will end with status
                                        # `incomplete`. See `incomplete_details` for more info.
            max_prompt_tokens: nil, # Body param: The maximum number of prompt tokens that may be used over the course
                                    # of the run. The run will make a best effort to use only the number of prompt
                                    # tokens specified, across multiple turns of the run. If the run exceeds the
                                    # number of prompt tokens specified, the run will end with status `incomplete`.
                                    # See `incomplete_details` for more info.
            metadata: nil, # Body param: Set of 16 key-value pairs that can be attached to an object. This
                           # can be useful for storing additional information about the object in a
                           # structured format, and querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            model: nil, # Body param: The ID of the
                        # [Model](https://platform.openai.com/docs/api-reference/models) to be used to
                        # execute this run. If a value is provided here, it will override the model
                        # associated with the assistant. If not, the model associated with the assistant
                        # will be used.
            parallel_tool_calls: nil, # Body param: Whether to enable
                                      # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                      # during tool use.
            reasoning_effort: nil, # Body param: **o-series models only**
                                   # Constrains effort on reasoning for
                                   # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                   # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                   # result in faster responses and fewer tokens used on reasoning in a response.
            response_format: nil, # Body param: Specifies the format that the model must output. Compatible with
                                  # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                  # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                  # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                  # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                  # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                  # in the
                                  # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                  # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                  # message the model generates is valid JSON.
                                  # **Important:** when using JSON mode, you **must** also instruct the model to
                                  # produce JSON yourself via a system or user message. Without this, the model may
                                  # generate an unending stream of whitespace until the generation reaches the token
                                  # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                  # the message content may be partially cut off if `finish_reason="length"`, which
                                  # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                  # max context length.
            temperature: nil, # Body param: What sampling temperature to use, between 0 and 2. Higher values
                              # like 0.8 will make the output more random, while lower values like 0.2 will make
                              # it more focused and deterministic.
            tool_choice: nil, # Body param: Controls which (if any) tool is called by the model. `none` means
                              # the model will not call any tools and instead generates a message. `auto` is the
                              # default value and means the model can pick between generating a message or
                              # calling one or more tools. `required` means the model must call one or more
                              # tools before responding to the user. Specifying a particular tool like
                              # `{"type": "file_search"}` or
                              # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                              # call that tool.
            tools: nil, # Body param: Override the tools the assistant can use for this run. This is
                        # useful for modifying the behavior on a per-run basis.
            top_p: nil, # Body param: An alternative to sampling with temperature, called nucleus
                        # sampling, where the model considers the results of the tokens with top_p
                        # probability mass. So 0.1 means only the tokens comprising the top 10%
                        # probability mass are considered.
                        # We generally recommend altering this or temperature but not both.
            truncation_strategy: nil, # Body param: Controls for how a thread will be truncated prior to the run. Use
                                      # this to control the intial context window of the run.
            stream: false, # There is no need to provide `stream:`. Instead, use `#create_stream_raw` or
                           # `#create` for streaming and non-streaming use cases, respectively.
            request_options: {}
); end

          # See {OpenAI::Resources::Beta::Threads::Runs#create} for non-streaming
          # counterpart.
          #
          # Create a run.
          sig do
            params(
              thread_id: String,
              assistant_id: String,
              include: T::Array[OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol],
              additional_instructions: T.nilable(String),
              additional_messages: T.nilable(
                  T::Array[
                    OpenAI::Beta::Threads::RunCreateParams::AdditionalMessage::OrHash
                  ]
                ),
              instructions: T.nilable(String),
              max_completion_tokens: T.nilable(Integer),
              max_prompt_tokens: T.nilable(Integer),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: T.nilable(T.any(String, OpenAI::ChatModel::OrSymbol)),
              parallel_tool_calls: T::Boolean,
              reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
              response_format: T.nilable(
                  T.any(
                    Symbol,
                    OpenAI::ResponseFormatText::OrHash,
                    OpenAI::ResponseFormatJSONObject::OrHash,
                    OpenAI::ResponseFormatJSONSchema::OrHash
                  )
                ),
              temperature: T.nilable(Float),
              tool_choice: T.nilable(
                  T.any(
                    OpenAI::Beta::AssistantToolChoiceOption::Auto::OrSymbol,
                    OpenAI::Beta::AssistantToolChoice::OrHash
                  )
                ),
              tools: T.nilable(
                  T::Array[
                    T.any(
                      OpenAI::Beta::CodeInterpreterTool::OrHash,
                      OpenAI::Beta::FileSearchTool::OrHash,
                      OpenAI::Beta::FunctionTool::OrHash
                    )
                  ]
                ),
              top_p: T.nilable(Float),
              truncation_strategy: T.nilable(
                  OpenAI::Beta::Threads::RunCreateParams::TruncationStrategy::OrHash
                ),
              stream: T.noreturn,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Internal::Stream[
                OpenAI::Beta::AssistantStreamEvent::Variants
              ])
          end
          def create_stream_raw(
            thread_id, # Path param: The ID of the thread to run.
            assistant_id:, # Body param: The ID of the
                           # [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to
                           # execute this run.
            include: nil, # Query param: A list of additional fields to include in the response. Currently
                          # the only supported value is
                          # `step_details.tool_calls[*].file_search.results[*].content` to fetch the file
                          # search result content.
                          # See the
                          # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
                          # for more information.
            additional_instructions: nil, # Body param: Appends additional instructions at the end of the instructions for
                                          # the run. This is useful for modifying the behavior on a per-run basis without
                                          # overriding other instructions.
            additional_messages: nil, # Body param: Adds additional messages to the thread before creating the run.
            instructions: nil, # Body param: Overrides the
                               # [instructions](https://platform.openai.com/docs/api-reference/assistants/createAssistant)
                               # of the assistant. This is useful for modifying the behavior on a per-run basis.
            max_completion_tokens: nil, # Body param: The maximum number of completion tokens that may be used over the
                                        # course of the run. The run will make a best effort to use only the number of
                                        # completion tokens specified, across multiple turns of the run. If the run
                                        # exceeds the number of completion tokens specified, the run will end with status
                                        # `incomplete`. See `incomplete_details` for more info.
            max_prompt_tokens: nil, # Body param: The maximum number of prompt tokens that may be used over the course
                                    # of the run. The run will make a best effort to use only the number of prompt
                                    # tokens specified, across multiple turns of the run. If the run exceeds the
                                    # number of prompt tokens specified, the run will end with status `incomplete`.
                                    # See `incomplete_details` for more info.
            metadata: nil, # Body param: Set of 16 key-value pairs that can be attached to an object. This
                           # can be useful for storing additional information about the object in a
                           # structured format, and querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            model: nil, # Body param: The ID of the
                        # [Model](https://platform.openai.com/docs/api-reference/models) to be used to
                        # execute this run. If a value is provided here, it will override the model
                        # associated with the assistant. If not, the model associated with the assistant
                        # will be used.
            parallel_tool_calls: nil, # Body param: Whether to enable
                                      # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                      # during tool use.
            reasoning_effort: nil, # Body param: **o-series models only**
                                   # Constrains effort on reasoning for
                                   # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                   # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                   # result in faster responses and fewer tokens used on reasoning in a response.
            response_format: nil, # Body param: Specifies the format that the model must output. Compatible with
                                  # [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
                                  # [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
                                  # and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
                                  # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                  # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                  # in the
                                  # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                  # Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
                                  # message the model generates is valid JSON.
                                  # **Important:** when using JSON mode, you **must** also instruct the model to
                                  # produce JSON yourself via a system or user message. Without this, the model may
                                  # generate an unending stream of whitespace until the generation reaches the token
                                  # limit, resulting in a long-running and seemingly "stuck" request. Also note that
                                  # the message content may be partially cut off if `finish_reason="length"`, which
                                  # indicates the generation exceeded `max_tokens` or the conversation exceeded the
                                  # max context length.
            temperature: nil, # Body param: What sampling temperature to use, between 0 and 2. Higher values
                              # like 0.8 will make the output more random, while lower values like 0.2 will make
                              # it more focused and deterministic.
            tool_choice: nil, # Body param: Controls which (if any) tool is called by the model. `none` means
                              # the model will not call any tools and instead generates a message. `auto` is the
                              # default value and means the model can pick between generating a message or
                              # calling one or more tools. `required` means the model must call one or more
                              # tools before responding to the user. Specifying a particular tool like
                              # `{"type": "file_search"}` or
                              # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                              # call that tool.
            tools: nil, # Body param: Override the tools the assistant can use for this run. This is
                        # useful for modifying the behavior on a per-run basis.
            top_p: nil, # Body param: An alternative to sampling with temperature, called nucleus
                        # sampling, where the model considers the results of the tokens with top_p
                        # probability mass. So 0.1 means only the tokens comprising the top 10%
                        # probability mass are considered.
                        # We generally recommend altering this or temperature but not both.
            truncation_strategy: nil, # Body param: Controls for how a thread will be truncated prior to the run. Use
                                      # this to control the intial context window of the run.
            stream: true, # There is no need to provide `stream:`. Instead, use `#create_stream_raw` or
                          # `#create` for streaming and non-streaming use cases, respectively.
            request_options: {}
); end

          # Returns a list of runs belonging to a thread.
          sig do
            params(
              thread_id: String,
              after: String,
              before: String,
              limit: Integer,
              order: OpenAI::Beta::Threads::RunListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Internal::CursorPage[OpenAI::Beta::Threads::Run])
          end
          def list(
            thread_id, # The ID of the thread the run belongs to.
            after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                        # in the list. For instance, if you make a list request and receive 100 objects,
                        # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                        # fetch the next page of the list.
            before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                         # in the list. For instance, if you make a list request and receive 100 objects,
                         # starting with obj_foo, your subsequent call can include before=obj_foo in order
                         # to fetch the previous page of the list.
            limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                        # 100, and the default is 20.
            order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                        # order and `desc` for descending order.
            request_options: {}
); end

          # Retrieves a run.
          sig do
            params(
              run_id: String,
              thread_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Beta::Threads::Run)
          end
          def retrieve(
            run_id, # The ID of the run to retrieve.
            thread_id:, # The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)
                        # that was run.
            request_options: {}
); end

          # See {OpenAI::Resources::Beta::Threads::Runs#submit_tool_outputs_stream_raw} for
          # streaming counterpart.
          #
          # When a run has the `status: "requires_action"` and `required_action.type` is
          # `submit_tool_outputs`, this endpoint can be used to submit the outputs from the
          # tool calls once they're all completed. All outputs must be submitted in a single
          # request.
          sig do
            params(
              run_id: String,
              thread_id: String,
              tool_outputs: T::Array[
                  OpenAI::Beta::Threads::RunSubmitToolOutputsParams::ToolOutput::OrHash
                ],
              stream: T.noreturn,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Beta::Threads::Run)
          end
          def submit_tool_outputs(
            run_id, # Path param: The ID of the run that requires the tool output submission.
            thread_id:, # Path param: The ID of the
                        # [thread](https://platform.openai.com/docs/api-reference/threads) to which this
                        # run belongs.
            tool_outputs:, # Body param: A list of tools for which the outputs are being submitted.
            stream: false, # There is no need to provide `stream:`. Instead, use
                           # `#submit_tool_outputs_stream_raw` or `#submit_tool_outputs` for streaming and
                           # non-streaming use cases, respectively.
            request_options: {}
); end

          # See {OpenAI::Resources::Beta::Threads::Runs#submit_tool_outputs} for
          # non-streaming counterpart.
          #
          # When a run has the `status: "requires_action"` and `required_action.type` is
          # `submit_tool_outputs`, this endpoint can be used to submit the outputs from the
          # tool calls once they're all completed. All outputs must be submitted in a single
          # request.
          sig do
            params(
              run_id: String,
              thread_id: String,
              tool_outputs: T::Array[
                  OpenAI::Beta::Threads::RunSubmitToolOutputsParams::ToolOutput::OrHash
                ],
              stream: T.noreturn,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Internal::Stream[
                OpenAI::Beta::AssistantStreamEvent::Variants
              ])
          end
          def submit_tool_outputs_stream_raw(
            run_id, # Path param: The ID of the run that requires the tool output submission.
            thread_id:, # Path param: The ID of the
                        # [thread](https://platform.openai.com/docs/api-reference/threads) to which this
                        # run belongs.
            tool_outputs:, # Body param: A list of tools for which the outputs are being submitted.
            stream: true, # There is no need to provide `stream:`. Instead, use
                          # `#submit_tool_outputs_stream_raw` or `#submit_tool_outputs` for streaming and
                          # non-streaming use cases, respectively.
            request_options: {}
); end

          # Modifies a run.
          sig do
            params(
              run_id: String,
              thread_id: String,
              metadata: T.nilable(T::Hash[Symbol, String]),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Beta::Threads::Run)
          end
          def update(
            run_id, # Path param: The ID of the run to modify.
            thread_id:, # Path param: The ID of the
                        # [thread](https://platform.openai.com/docs/api-reference/threads) that was run.
            metadata: nil, # Body param: Set of 16 key-value pairs that can be attached to an object. This
                           # can be useful for storing additional information about the object in a
                           # structured format, and querying for objects via API or the dashboard.
                           # Keys are strings with a maximum length of 64 characters. Values are strings with
                           # a maximum length of 512 characters.
            request_options: {}
); end

          class << self
            # @api private
            sig { params(client: OpenAI::Client).returns(T.attached_class) }
            def new(client:); end
          end

          class Steps
            # Returns a list of run steps belonging to a run.
            sig do
              params(
                run_id: String,
                thread_id: String,
                after: String,
                before: String,
                include: T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                  ],
                limit: Integer,
                order: OpenAI::Beta::Threads::Runs::StepListParams::Order::OrSymbol,
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(OpenAI::Internal::CursorPage[
                  OpenAI::Beta::Threads::Runs::RunStep
                ])
            end
            def list(
              run_id, # Path param: The ID of the run the run steps belong to.
              thread_id:, # Path param: The ID of the thread the run and run steps belong to.
              after: nil, # Query param: A cursor for use in pagination. `after` is an object ID that
                          # defines your place in the list. For instance, if you make a list request and
                          # receive 100 objects, ending with obj_foo, your subsequent call can include
                          # after=obj_foo in order to fetch the next page of the list.
              before: nil, # Query param: A cursor for use in pagination. `before` is an object ID that
                           # defines your place in the list. For instance, if you make a list request and
                           # receive 100 objects, starting with obj_foo, your subsequent call can include
                           # before=obj_foo in order to fetch the previous page of the list.
              include: nil, # Query param: A list of additional fields to include in the response. Currently
                            # the only supported value is
                            # `step_details.tool_calls[*].file_search.results[*].content` to fetch the file
                            # search result content.
                            # See the
                            # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
                            # for more information.
              limit: nil, # Query param: A limit on the number of objects to be returned. Limit can range
                          # between 1 and 100, and the default is 20.
              order: nil, # Query param: Sort order by the `created_at` timestamp of the objects. `asc` for
                          # ascending order and `desc` for descending order.
              request_options: {}
); end

            # Retrieves a run step.
            sig do
              params(
                step_id: String,
                thread_id: String,
                run_id: String,
                include: T::Array[
                    OpenAI::Beta::Threads::Runs::RunStepInclude::OrSymbol
                  ],
                request_options: OpenAI::RequestOptions::OrHash
              ).returns(OpenAI::Beta::Threads::Runs::RunStep)
            end
            def retrieve(
              step_id, # Path param: The ID of the run step to retrieve.
              thread_id:, # Path param: The ID of the thread to which the run and run step belongs.
              run_id:, # Path param: The ID of the run to which the run step belongs.
              include: nil, # Query param: A list of additional fields to include in the response. Currently
                            # the only supported value is
                            # `step_details.tool_calls[*].file_search.results[*].content` to fetch the file
                            # search result content.
                            # See the
                            # [file search tool documentation](https://platform.openai.com/docs/assistants/tools/file-search#customizing-file-search-settings)
                            # for more information.
              request_options: {}
); end

            class << self
              # @api private
              sig { params(client: OpenAI::Client).returns(T.attached_class) }
              def new(client:); end
            end
          end
        end
      end
    end

    class Chat
      sig { returns(OpenAI::Resources::Chat::Completions) }
      attr_reader :completions

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class Completions
        sig { returns(OpenAI::Resources::Chat::Completions::Messages) }
        attr_reader :messages

        # See {OpenAI::Resources::Chat::Completions#stream_raw} for streaming counterpart.
        #
        # **Starting a new project?** We recommend trying
        # [Responses](https://platform.openai.com/docs/api-reference/responses) to take
        # advantage of the latest OpenAI platform features. Compare
        # [Chat Completions with Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions?api-mode=responses).
        #
        # ---
        #
        # Creates a model response for the given chat conversation. Learn more in the
        # [text generation](https://platform.openai.com/docs/guides/text-generation),
        # [vision](https://platform.openai.com/docs/guides/vision), and
        # [audio](https://platform.openai.com/docs/guides/audio) guides.
        #
        # Parameter support can differ depending on the model used to generate the
        # response, particularly for newer reasoning models. Parameters that are only
        # supported for reasoning models are noted below. For the current state of
        # unsupported parameters in reasoning models,
        # [refer to the reasoning guide](https://platform.openai.com/docs/guides/reasoning).
        sig do
          params(
            messages: T::Array[
                T.any(
                  OpenAI::Chat::ChatCompletionDeveloperMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionSystemMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionUserMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionToolMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionFunctionMessageParam::OrHash
                )
              ],
            model: T.any(String, OpenAI::ChatModel::OrSymbol),
            audio: T.nilable(OpenAI::Chat::ChatCompletionAudioParam::OrHash),
            frequency_penalty: T.nilable(Float),
            function_call: T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption::OrHash
              ),
            functions: T::Array[OpenAI::Chat::CompletionCreateParams::Function::OrHash],
            logit_bias: T.nilable(T::Hash[Symbol, Integer]),
            logprobs: T.nilable(T::Boolean),
            max_completion_tokens: T.nilable(Integer),
            max_tokens: T.nilable(Integer),
            metadata: T.nilable(T::Hash[Symbol, String]),
            modalities: T.nilable(
                T::Array[
                  OpenAI::Chat::CompletionCreateParams::Modality::OrSymbol
                ]
              ),
            n: T.nilable(Integer),
            parallel_tool_calls: T::Boolean,
            prediction: T.nilable(OpenAI::Chat::ChatCompletionPredictionContent::OrHash),
            presence_penalty: T.nilable(Float),
            prompt_cache_key: String,
            reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
            response_format: T.any(
                OpenAI::ResponseFormatText::OrHash,
                OpenAI::ResponseFormatJSONSchema::OrHash,
                OpenAI::StructuredOutput::JsonSchemaConverter,
                OpenAI::ResponseFormatJSONObject::OrHash
              ),
            safety_identifier: String,
            seed: T.nilable(Integer),
            service_tier: T.nilable(
                OpenAI::Chat::CompletionCreateParams::ServiceTier::OrSymbol
              ),
            stop: T.nilable(OpenAI::Chat::CompletionCreateParams::Stop::Variants),
            store: T.nilable(T::Boolean),
            stream_options: T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash),
            temperature: T.nilable(Float),
            tool_choice: T.any(
                OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                OpenAI::Chat::ChatCompletionNamedToolChoice::OrHash
              ),
            tools: T::Array[
                T.any(
                  OpenAI::Chat::ChatCompletionTool::OrHash,
                  OpenAI::StructuredOutput::JsonSchemaConverter
                )
              ],
            top_logprobs: T.nilable(Integer),
            top_p: T.nilable(Float),
            user: String,
            web_search_options: OpenAI::Chat::CompletionCreateParams::WebSearchOptions::OrHash,
            stream: T.noreturn,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Chat::ChatCompletion)
        end
        def create(
          messages:, # A list of messages comprising the conversation so far. Depending on the
                     # [model](https://platform.openai.com/docs/models) you use, different message
                     # types (modalities) are supported, like
                     # [text](https://platform.openai.com/docs/guides/text-generation),
                     # [images](https://platform.openai.com/docs/guides/vision), and
                     # [audio](https://platform.openai.com/docs/guides/audio).
          model:, # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
                  # wide range of models with different capabilities, performance characteristics,
                  # and price points. Refer to the
                  # [model guide](https://platform.openai.com/docs/models) to browse and compare
                  # available models.
          audio: nil, # Parameters for audio output. Required when audio output is requested with
                      # `modalities: ["audio"]`.
                      # [Learn more](https://platform.openai.com/docs/guides/audio).
          frequency_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
                                  # existing frequency in the text so far, decreasing the model's likelihood to
                                  # repeat the same line verbatim.
          function_call: nil, # Deprecated in favor of `tool_choice`.
                              # Controls which (if any) function is called by the model.
                              # `none` means the model will not call a function and instead generates a message.
                              # `auto` means the model can pick between generating a message or calling a
                              # function.
                              # Specifying a particular function via `{"name": "my_function"}` forces the model
                              # to call that function.
                              # `none` is the default when no functions are present. `auto` is the default if
                              # functions are present.
          functions: nil, # Deprecated in favor of `tools`.
                          # A list of functions the model may generate JSON inputs for.
          logit_bias: nil, # Modify the likelihood of specified tokens appearing in the completion.
                           # Accepts a JSON object that maps tokens (specified by their token ID in the
                           # tokenizer) to an associated bias value from -100 to 100. Mathematically, the
                           # bias is added to the logits generated by the model prior to sampling. The exact
                           # effect will vary per model, but values between -1 and 1 should decrease or
                           # increase likelihood of selection; values like -100 or 100 should result in a ban
                           # or exclusive selection of the relevant token.
          logprobs: nil, # Whether to return log probabilities of the output tokens or not. If true,
                         # returns the log probabilities of each output token returned in the `content` of
                         # `message`.
          max_completion_tokens: nil, # An upper bound for the number of tokens that can be generated for a completion,
                                      # including visible output tokens and
                                      # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
          max_tokens: nil, # The maximum number of [tokens](/tokenizer) that can be generated in the chat
                           # completion. This value can be used to control
                           # [costs](https://openai.com/api/pricing/) for text generated via API.
                           # This value is now deprecated in favor of `max_completion_tokens`, and is not
                           # compatible with
                           # [o-series models](https://platform.openai.com/docs/guides/reasoning).
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          modalities: nil, # Output types that you would like the model to generate. Most models are capable
                           # of generating text, which is the default:
                           # `["text"]`
                           # The `gpt-4o-audio-preview` model can also be used to
                           # [generate audio](https://platform.openai.com/docs/guides/audio). To request that
                           # this model generate both text and audio responses, you can use:
                           # `["text", "audio"]`
          n: nil, # How many chat completion choices to generate for each input message. Note that
                  # you will be charged based on the number of generated tokens across all of the
                  # choices. Keep `n` as `1` to minimize costs.
          parallel_tool_calls: nil, # Whether to enable
                                    # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                    # during tool use.
          prediction: nil, # Static predicted output content, such as the content of a text file that is
                           # being regenerated.
          presence_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on
                                 # whether they appear in the text so far, increasing the model's likelihood to
                                 # talk about new topics.
          prompt_cache_key: nil, # Used by OpenAI to cache responses for similar requests to optimize your cache
                                 # hit rates. Replaces the `user` field.
                                 # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
          reasoning_effort: nil, # **o-series models only**
                                 # Constrains effort on reasoning for
                                 # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                 # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                 # result in faster responses and fewer tokens used on reasoning in a response.
          response_format: nil, # An object specifying the format that the model must output.
                                # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                # in the
                                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                                # ensures the message the model generates is valid JSON. Using `json_schema` is
                                # preferred for models that support it.
          safety_identifier: nil, # A stable identifier used to help detect users of your application that may be
                                  # violating OpenAI's usage policies. The IDs should be a string that uniquely
                                  # identifies each user. We recommend hashing their username or email address, in
                                  # order to avoid sending us any identifying information.
                                  # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
          seed: nil, # This feature is in Beta. If specified, our system will make a best effort to
                     # sample deterministically, such that repeated requests with the same `seed` and
                     # parameters should return the same result. Determinism is not guaranteed, and you
                     # should refer to the `system_fingerprint` response parameter to monitor changes
                     # in the backend.
          service_tier: nil, # Specifies the processing type used for serving the request.
                             # - If set to 'auto', then the request will be processed with the service tier
                             #   configured in the Project settings. Unless otherwise configured, the Project
                             #   will use 'default'.
                             # - If set to 'default', then the request will be processed with the standard
                             #   pricing and performance for the selected model.
                             # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
                             #   'priority', then the request will be processed with the corresponding service
                             #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
                             #   Priority processing.
                             # - When not set, the default behavior is 'auto'.
                             # When the `service_tier` parameter is set, the response body will include the
                             # `service_tier` value based on the processing mode actually used to serve the
                             # request. This response value may be different from the value set in the
                             # parameter.
          stop: nil, # Not supported with latest reasoning models `o3` and `o4-mini`.
                     # Up to 4 sequences where the API will stop generating further tokens. The
                     # returned text will not contain the stop sequence.
          store: nil, # Whether or not to store the output of this chat completion request for use in
                      # our [model distillation](https://platform.openai.com/docs/guides/distillation)
                      # or [evals](https://platform.openai.com/docs/guides/evals) products.
                      # Supports text and image inputs. Note: image inputs over 10MB will be dropped.
          stream_options: nil, # Options for streaming response. Only set this when you set `stream: true`.
          temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                            # make the output more random, while lower values like 0.2 will make it more
                            # focused and deterministic. We generally recommend altering this or `top_p` but
                            # not both.
          tool_choice: nil, # Controls which (if any) tool is called by the model. `none` means the model will
                            # not call any tool and instead generates a message. `auto` means the model can
                            # pick between generating a message or calling one or more tools. `required` means
                            # the model must call one or more tools. Specifying a particular tool via
                            # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                            # call that tool.
                            # `none` is the default when no tools are present. `auto` is the default if tools
                            # are present.
          tools: nil, # A list of tools the model may call. Currently, only functions are supported as a
                      # tool. Use this to provide a list of functions the model may generate JSON inputs
                      # for. A max of 128 functions are supported.
          top_logprobs: nil, # An integer between 0 and 20 specifying the number of most likely tokens to
                             # return at each token position, each with an associated log probability.
                             # `logprobs` must be set to `true` if this parameter is used.
          top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                      # model considers the results of the tokens with top_p probability mass. So 0.1
                      # means only the tokens comprising the top 10% probability mass are considered.
                      # We generally recommend altering this or `temperature` but not both.
          user: nil, # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
                     # `prompt_cache_key` instead to maintain caching optimizations. A stable
                     # identifier for your end-users. Used to boost cache hit rates by better bucketing
                     # similar requests and to help OpenAI detect and prevent abuse.
                     # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
          web_search_options: nil, # This tool searches the web for relevant results to use in a response. Learn more
                                   # about the
                                   # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
          stream: false, # There is no need to provide `stream:`. Instead, use `#stream_raw` or `#create`
                         # for streaming and non-streaming use cases, respectively.
          request_options: {}
); end

        # Delete a stored chat completion. Only Chat Completions that have been created
        # with the `store` parameter set to `true` can be deleted.
        sig do
          params(
            completion_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Chat::ChatCompletionDeleted)
        end
        def delete(
          completion_id, # The ID of the chat completion to delete.
          request_options: {}
); end

        # List stored Chat Completions. Only Chat Completions that have been stored with
        # the `store` parameter set to `true` will be returned.
        sig do
          params(
            after: String,
            limit: Integer,
            metadata: T.nilable(T::Hash[Symbol, String]),
            model: String,
            order: OpenAI::Chat::CompletionListParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::CursorPage[OpenAI::Chat::ChatCompletion])
        end
        def list(
          after: nil, # Identifier for the last chat completion from the previous pagination request.
          limit: nil, # Number of Chat Completions to retrieve.
          metadata: nil, # A list of metadata keys to filter the Chat Completions by. Example:
                         # `metadata[key1]=value1&metadata[key2]=value2`
          model: nil, # The model used to generate the Chat Completions.
          order: nil, # Sort order for Chat Completions by timestamp. Use `asc` for ascending order or
                      # `desc` for descending order. Defaults to `asc`.
          request_options: {}
); end

        # Get a stored chat completion. Only Chat Completions that have been created with
        # the `store` parameter set to `true` will be returned.
        sig do
          params(
            completion_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Chat::ChatCompletion)
        end
        def retrieve(
          completion_id, # The ID of the chat completion to retrieve.
          request_options: {}
); end

        # See {OpenAI::Resources::Chat::Completions#create} for non-streaming counterpart.
        #
        # **Starting a new project?** We recommend trying
        # [Responses](https://platform.openai.com/docs/api-reference/responses) to take
        # advantage of the latest OpenAI platform features. Compare
        # [Chat Completions with Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions?api-mode=responses).
        #
        # ---
        #
        # Creates a model response for the given chat conversation. Learn more in the
        # [text generation](https://platform.openai.com/docs/guides/text-generation),
        # [vision](https://platform.openai.com/docs/guides/vision), and
        # [audio](https://platform.openai.com/docs/guides/audio) guides.
        #
        # Parameter support can differ depending on the model used to generate the
        # response, particularly for newer reasoning models. Parameters that are only
        # supported for reasoning models are noted below. For the current state of
        # unsupported parameters in reasoning models,
        # [refer to the reasoning guide](https://platform.openai.com/docs/guides/reasoning).
        sig do
          params(
            messages: T::Array[
                T.any(
                  OpenAI::Chat::ChatCompletionDeveloperMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionSystemMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionUserMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionToolMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionFunctionMessageParam::OrHash
                )
              ],
            model: T.any(String, OpenAI::ChatModel::OrSymbol),
            audio: T.nilable(OpenAI::Chat::ChatCompletionAudioParam::OrHash),
            frequency_penalty: T.nilable(Float),
            function_call: T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption::OrHash
              ),
            functions: T::Array[OpenAI::Chat::CompletionCreateParams::Function::OrHash],
            logit_bias: T.nilable(T::Hash[Symbol, Integer]),
            logprobs: T.nilable(T::Boolean),
            max_completion_tokens: T.nilable(Integer),
            max_tokens: T.nilable(Integer),
            metadata: T.nilable(T::Hash[Symbol, String]),
            modalities: T.nilable(
                T::Array[
                  OpenAI::Chat::CompletionCreateParams::Modality::OrSymbol
                ]
              ),
            n: T.nilable(Integer),
            parallel_tool_calls: T::Boolean,
            prediction: T.nilable(OpenAI::Chat::ChatCompletionPredictionContent::OrHash),
            presence_penalty: T.nilable(Float),
            prompt_cache_key: String,
            reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
            response_format: T.any(
                OpenAI::ResponseFormatText::OrHash,
                OpenAI::ResponseFormatJSONSchema::OrHash,
                OpenAI::ResponseFormatJSONObject::OrHash
              ),
            safety_identifier: String,
            seed: T.nilable(Integer),
            service_tier: T.nilable(
                OpenAI::Chat::CompletionCreateParams::ServiceTier::OrSymbol
              ),
            stop: T.nilable(OpenAI::Chat::CompletionCreateParams::Stop::Variants),
            store: T.nilable(T::Boolean),
            stream_options: T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash),
            temperature: T.nilable(Float),
            tool_choice: T.any(
                OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                OpenAI::Chat::ChatCompletionNamedToolChoice::OrHash
              ),
            tools: T::Array[OpenAI::Chat::ChatCompletionTool::OrHash],
            top_logprobs: T.nilable(Integer),
            top_p: T.nilable(Float),
            user: String,
            web_search_options: OpenAI::Chat::CompletionCreateParams::WebSearchOptions::OrHash,
            stream: T.noreturn,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::Stream[OpenAI::Chat::ChatCompletionChunk])
        end
        def stream_raw(
          messages:, # A list of messages comprising the conversation so far. Depending on the
                     # [model](https://platform.openai.com/docs/models) you use, different message
                     # types (modalities) are supported, like
                     # [text](https://platform.openai.com/docs/guides/text-generation),
                     # [images](https://platform.openai.com/docs/guides/vision), and
                     # [audio](https://platform.openai.com/docs/guides/audio).
          model:, # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
                  # wide range of models with different capabilities, performance characteristics,
                  # and price points. Refer to the
                  # [model guide](https://platform.openai.com/docs/models) to browse and compare
                  # available models.
          audio: nil, # Parameters for audio output. Required when audio output is requested with
                      # `modalities: ["audio"]`.
                      # [Learn more](https://platform.openai.com/docs/guides/audio).
          frequency_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
                                  # existing frequency in the text so far, decreasing the model's likelihood to
                                  # repeat the same line verbatim.
          function_call: nil, # Deprecated in favor of `tool_choice`.
                              # Controls which (if any) function is called by the model.
                              # `none` means the model will not call a function and instead generates a message.
                              # `auto` means the model can pick between generating a message or calling a
                              # function.
                              # Specifying a particular function via `{"name": "my_function"}` forces the model
                              # to call that function.
                              # `none` is the default when no functions are present. `auto` is the default if
                              # functions are present.
          functions: nil, # Deprecated in favor of `tools`.
                          # A list of functions the model may generate JSON inputs for.
          logit_bias: nil, # Modify the likelihood of specified tokens appearing in the completion.
                           # Accepts a JSON object that maps tokens (specified by their token ID in the
                           # tokenizer) to an associated bias value from -100 to 100. Mathematically, the
                           # bias is added to the logits generated by the model prior to sampling. The exact
                           # effect will vary per model, but values between -1 and 1 should decrease or
                           # increase likelihood of selection; values like -100 or 100 should result in a ban
                           # or exclusive selection of the relevant token.
          logprobs: nil, # Whether to return log probabilities of the output tokens or not. If true,
                         # returns the log probabilities of each output token returned in the `content` of
                         # `message`.
          max_completion_tokens: nil, # An upper bound for the number of tokens that can be generated for a completion,
                                      # including visible output tokens and
                                      # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
          max_tokens: nil, # The maximum number of [tokens](/tokenizer) that can be generated in the chat
                           # completion. This value can be used to control
                           # [costs](https://openai.com/api/pricing/) for text generated via API.
                           # This value is now deprecated in favor of `max_completion_tokens`, and is not
                           # compatible with
                           # [o-series models](https://platform.openai.com/docs/guides/reasoning).
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          modalities: nil, # Output types that you would like the model to generate. Most models are capable
                           # of generating text, which is the default:
                           # `["text"]`
                           # The `gpt-4o-audio-preview` model can also be used to
                           # [generate audio](https://platform.openai.com/docs/guides/audio). To request that
                           # this model generate both text and audio responses, you can use:
                           # `["text", "audio"]`
          n: nil, # How many chat completion choices to generate for each input message. Note that
                  # you will be charged based on the number of generated tokens across all of the
                  # choices. Keep `n` as `1` to minimize costs.
          parallel_tool_calls: nil, # Whether to enable
                                    # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
                                    # during tool use.
          prediction: nil, # Static predicted output content, such as the content of a text file that is
                           # being regenerated.
          presence_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on
                                 # whether they appear in the text so far, increasing the model's likelihood to
                                 # talk about new topics.
          prompt_cache_key: nil, # Used by OpenAI to cache responses for similar requests to optimize your cache
                                 # hit rates. Replaces the `user` field.
                                 # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
          reasoning_effort: nil, # **o-series models only**
                                 # Constrains effort on reasoning for
                                 # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
                                 # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
                                 # result in faster responses and fewer tokens used on reasoning in a response.
          response_format: nil, # An object specifying the format that the model must output.
                                # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
                                # Outputs which ensures the model will match your supplied JSON schema. Learn more
                                # in the
                                # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
                                # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                                # ensures the message the model generates is valid JSON. Using `json_schema` is
                                # preferred for models that support it.
          safety_identifier: nil, # A stable identifier used to help detect users of your application that may be
                                  # violating OpenAI's usage policies. The IDs should be a string that uniquely
                                  # identifies each user. We recommend hashing their username or email address, in
                                  # order to avoid sending us any identifying information.
                                  # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
          seed: nil, # This feature is in Beta. If specified, our system will make a best effort to
                     # sample deterministically, such that repeated requests with the same `seed` and
                     # parameters should return the same result. Determinism is not guaranteed, and you
                     # should refer to the `system_fingerprint` response parameter to monitor changes
                     # in the backend.
          service_tier: nil, # Specifies the processing type used for serving the request.
                             # - If set to 'auto', then the request will be processed with the service tier
                             #   configured in the Project settings. Unless otherwise configured, the Project
                             #   will use 'default'.
                             # - If set to 'default', then the request will be processed with the standard
                             #   pricing and performance for the selected model.
                             # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
                             #   'priority', then the request will be processed with the corresponding service
                             #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
                             #   Priority processing.
                             # - When not set, the default behavior is 'auto'.
                             # When the `service_tier` parameter is set, the response body will include the
                             # `service_tier` value based on the processing mode actually used to serve the
                             # request. This response value may be different from the value set in the
                             # parameter.
          stop: nil, # Not supported with latest reasoning models `o3` and `o4-mini`.
                     # Up to 4 sequences where the API will stop generating further tokens. The
                     # returned text will not contain the stop sequence.
          store: nil, # Whether or not to store the output of this chat completion request for use in
                      # our [model distillation](https://platform.openai.com/docs/guides/distillation)
                      # or [evals](https://platform.openai.com/docs/guides/evals) products.
                      # Supports text and image inputs. Note: image inputs over 10MB will be dropped.
          stream_options: nil, # Options for streaming response. Only set this when you set `stream: true`.
          temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                            # make the output more random, while lower values like 0.2 will make it more
                            # focused and deterministic. We generally recommend altering this or `top_p` but
                            # not both.
          tool_choice: nil, # Controls which (if any) tool is called by the model. `none` means the model will
                            # not call any tool and instead generates a message. `auto` means the model can
                            # pick between generating a message or calling one or more tools. `required` means
                            # the model must call one or more tools. Specifying a particular tool via
                            # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
                            # call that tool.
                            # `none` is the default when no tools are present. `auto` is the default if tools
                            # are present.
          tools: nil, # A list of tools the model may call. Currently, only functions are supported as a
                      # tool. Use this to provide a list of functions the model may generate JSON inputs
                      # for. A max of 128 functions are supported.
          top_logprobs: nil, # An integer between 0 and 20 specifying the number of most likely tokens to
                             # return at each token position, each with an associated log probability.
                             # `logprobs` must be set to `true` if this parameter is used.
          top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                      # model considers the results of the tokens with top_p probability mass. So 0.1
                      # means only the tokens comprising the top 10% probability mass are considered.
                      # We generally recommend altering this or `temperature` but not both.
          user: nil, # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
                     # `prompt_cache_key` instead to maintain caching optimizations. A stable
                     # identifier for your end-users. Used to boost cache hit rates by better bucketing
                     # similar requests and to help OpenAI detect and prevent abuse.
                     # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
          web_search_options: nil, # This tool searches the web for relevant results to use in a response. Learn more
                                   # about the
                                   # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
          stream: true, # There is no need to provide `stream:`. Instead, use `#stream_raw` or `#create`
                        # for streaming and non-streaming use cases, respectively.
          request_options: {}
); end

        # Modify a stored chat completion. Only Chat Completions that have been created
        # with the `store` parameter set to `true` can be modified. Currently, the only
        # supported modification is to update the `metadata` field.
        sig do
          params(
            completion_id: String,
            metadata: T.nilable(T::Hash[Symbol, String]),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Chat::ChatCompletion)
        end
        def update(
          completion_id, # The ID of the chat completion to update.
          metadata:, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                     # for storing additional information about the object in a structured format, and
                     # querying for objects via API or the dashboard.
                     # Keys are strings with a maximum length of 64 characters. Values are strings with
                     # a maximum length of 512 characters.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end

        class Messages
          # Get the messages in a stored chat completion. Only Chat Completions that have
          # been created with the `store` parameter set to `true` will be returned.
          sig do
            params(
              completion_id: String,
              after: String,
              limit: Integer,
              order: OpenAI::Chat::Completions::MessageListParams::Order::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Internal::CursorPage[
                OpenAI::Chat::ChatCompletionStoreMessage
              ])
          end
          def list(
            completion_id, # The ID of the chat completion to retrieve messages from.
            after: nil, # Identifier for the last message from the previous pagination request.
            limit: nil, # Number of messages to retrieve.
            order: nil, # Sort order for messages by timestamp. Use `asc` for ascending order or `desc`
                        # for descending order. Defaults to `asc`.
            request_options: {}
); end

          class << self
            # @api private
            sig { params(client: OpenAI::Client).returns(T.attached_class) }
            def new(client:); end
          end
        end
      end
    end

    class Completions
      # See {OpenAI::Resources::Completions#create_streaming} for streaming counterpart.
      #
      # Creates a completion for the provided prompt and parameters.
      sig do
        params(
          model: T.any(String, OpenAI::CompletionCreateParams::Model::OrSymbol),
          prompt: T.nilable(OpenAI::CompletionCreateParams::Prompt::Variants),
          best_of: T.nilable(Integer),
          echo: T.nilable(T::Boolean),
          frequency_penalty: T.nilable(Float),
          logit_bias: T.nilable(T::Hash[Symbol, Integer]),
          logprobs: T.nilable(Integer),
          max_tokens: T.nilable(Integer),
          n: T.nilable(Integer),
          presence_penalty: T.nilable(Float),
          seed: T.nilable(Integer),
          stop: T.nilable(OpenAI::CompletionCreateParams::Stop::Variants),
          stream_options: T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash),
          suffix: T.nilable(String),
          temperature: T.nilable(Float),
          top_p: T.nilable(Float),
          user: String,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Completion)
      end
      def create(
        model:, # ID of the model to use. You can use the
                # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                # see all of your available models, or see our
                # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                # them.
        prompt:, # The prompt(s) to generate completions for, encoded as a string, array of
                 # strings, array of tokens, or array of token arrays.
                 # Note that <|endoftext|> is the document separator that the model sees during
                 # training, so if a prompt is not specified the model will generate as if from the
                 # beginning of a new document.
        best_of: nil, # Generates `best_of` completions server-side and returns the "best" (the one with
                      # the highest log probability per token). Results cannot be streamed.
                      # When used with `n`, `best_of` controls the number of candidate completions and
                      # `n` specifies how many to return – `best_of` must be greater than `n`.
                      # **Note:** Because this parameter generates many completions, it can quickly
                      # consume your token quota. Use carefully and ensure that you have reasonable
                      # settings for `max_tokens` and `stop`.
        echo: nil, # Echo back the prompt in addition to the completion
        frequency_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
                                # existing frequency in the text so far, decreasing the model's likelihood to
                                # repeat the same line verbatim.
                                # [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
        logit_bias: nil, # Modify the likelihood of specified tokens appearing in the completion.
                         # Accepts a JSON object that maps tokens (specified by their token ID in the GPT
                         # tokenizer) to an associated bias value from -100 to 100. You can use this
                         # [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.
                         # Mathematically, the bias is added to the logits generated by the model prior to
                         # sampling. The exact effect will vary per model, but values between -1 and 1
                         # should decrease or increase likelihood of selection; values like -100 or 100
                         # should result in a ban or exclusive selection of the relevant token.
                         # As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token
                         # from being generated.
        logprobs: nil, # Include the log probabilities on the `logprobs` most likely output tokens, as
                       # well the chosen tokens. For example, if `logprobs` is 5, the API will return a
                       # list of the 5 most likely tokens. The API will always return the `logprob` of
                       # the sampled token, so there may be up to `logprobs+1` elements in the response.
                       # The maximum value for `logprobs` is 5.
        max_tokens: nil, # The maximum number of [tokens](/tokenizer) that can be generated in the
                         # completion.
                         # The token count of your prompt plus `max_tokens` cannot exceed the model's
                         # context length.
                         # [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
                         # for counting tokens.
        n: nil, # How many completions to generate for each prompt.
                # **Note:** Because this parameter generates many completions, it can quickly
                # consume your token quota. Use carefully and ensure that you have reasonable
                # settings for `max_tokens` and `stop`.
        presence_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on
                               # whether they appear in the text so far, increasing the model's likelihood to
                               # talk about new topics.
                               # [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
        seed: nil, # If specified, our system will make a best effort to sample deterministically,
                   # such that repeated requests with the same `seed` and parameters should return
                   # the same result.
                   # Determinism is not guaranteed, and you should refer to the `system_fingerprint`
                   # response parameter to monitor changes in the backend.
        stop: nil, # Not supported with latest reasoning models `o3` and `o4-mini`.
                   # Up to 4 sequences where the API will stop generating further tokens. The
                   # returned text will not contain the stop sequence.
        stream_options: nil, # Options for streaming response. Only set this when you set `stream: true`.
        suffix: nil, # The suffix that comes after a completion of inserted text.
                     # This parameter is only supported for `gpt-3.5-turbo-instruct`.
        temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                          # make the output more random, while lower values like 0.2 will make it more
                          # focused and deterministic.
                          # We generally recommend altering this or `top_p` but not both.
        top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                    # model considers the results of the tokens with top_p probability mass. So 0.1
                    # means only the tokens comprising the top 10% probability mass are considered.
                    # We generally recommend altering this or `temperature` but not both.
        user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                   # and detect abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        stream: false, # There is no need to provide `stream:`. Instead, use `#create_streaming` or
                       # `#create` for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      # See {OpenAI::Resources::Completions#create} for non-streaming counterpart.
      #
      # Creates a completion for the provided prompt and parameters.
      sig do
        params(
          model: T.any(String, OpenAI::CompletionCreateParams::Model::OrSymbol),
          prompt: T.nilable(OpenAI::CompletionCreateParams::Prompt::Variants),
          best_of: T.nilable(Integer),
          echo: T.nilable(T::Boolean),
          frequency_penalty: T.nilable(Float),
          logit_bias: T.nilable(T::Hash[Symbol, Integer]),
          logprobs: T.nilable(Integer),
          max_tokens: T.nilable(Integer),
          n: T.nilable(Integer),
          presence_penalty: T.nilable(Float),
          seed: T.nilable(Integer),
          stop: T.nilable(OpenAI::CompletionCreateParams::Stop::Variants),
          stream_options: T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash),
          suffix: T.nilable(String),
          temperature: T.nilable(Float),
          top_p: T.nilable(Float),
          user: String,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::Stream[OpenAI::Completion])
      end
      def create_streaming(
        model:, # ID of the model to use. You can use the
                # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                # see all of your available models, or see our
                # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                # them.
        prompt:, # The prompt(s) to generate completions for, encoded as a string, array of
                 # strings, array of tokens, or array of token arrays.
                 # Note that <|endoftext|> is the document separator that the model sees during
                 # training, so if a prompt is not specified the model will generate as if from the
                 # beginning of a new document.
        best_of: nil, # Generates `best_of` completions server-side and returns the "best" (the one with
                      # the highest log probability per token). Results cannot be streamed.
                      # When used with `n`, `best_of` controls the number of candidate completions and
                      # `n` specifies how many to return – `best_of` must be greater than `n`.
                      # **Note:** Because this parameter generates many completions, it can quickly
                      # consume your token quota. Use carefully and ensure that you have reasonable
                      # settings for `max_tokens` and `stop`.
        echo: nil, # Echo back the prompt in addition to the completion
        frequency_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
                                # existing frequency in the text so far, decreasing the model's likelihood to
                                # repeat the same line verbatim.
                                # [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
        logit_bias: nil, # Modify the likelihood of specified tokens appearing in the completion.
                         # Accepts a JSON object that maps tokens (specified by their token ID in the GPT
                         # tokenizer) to an associated bias value from -100 to 100. You can use this
                         # [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.
                         # Mathematically, the bias is added to the logits generated by the model prior to
                         # sampling. The exact effect will vary per model, but values between -1 and 1
                         # should decrease or increase likelihood of selection; values like -100 or 100
                         # should result in a ban or exclusive selection of the relevant token.
                         # As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token
                         # from being generated.
        logprobs: nil, # Include the log probabilities on the `logprobs` most likely output tokens, as
                       # well the chosen tokens. For example, if `logprobs` is 5, the API will return a
                       # list of the 5 most likely tokens. The API will always return the `logprob` of
                       # the sampled token, so there may be up to `logprobs+1` elements in the response.
                       # The maximum value for `logprobs` is 5.
        max_tokens: nil, # The maximum number of [tokens](/tokenizer) that can be generated in the
                         # completion.
                         # The token count of your prompt plus `max_tokens` cannot exceed the model's
                         # context length.
                         # [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
                         # for counting tokens.
        n: nil, # How many completions to generate for each prompt.
                # **Note:** Because this parameter generates many completions, it can quickly
                # consume your token quota. Use carefully and ensure that you have reasonable
                # settings for `max_tokens` and `stop`.
        presence_penalty: nil, # Number between -2.0 and 2.0. Positive values penalize new tokens based on
                               # whether they appear in the text so far, increasing the model's likelihood to
                               # talk about new topics.
                               # [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
        seed: nil, # If specified, our system will make a best effort to sample deterministically,
                   # such that repeated requests with the same `seed` and parameters should return
                   # the same result.
                   # Determinism is not guaranteed, and you should refer to the `system_fingerprint`
                   # response parameter to monitor changes in the backend.
        stop: nil, # Not supported with latest reasoning models `o3` and `o4-mini`.
                   # Up to 4 sequences where the API will stop generating further tokens. The
                   # returned text will not contain the stop sequence.
        stream_options: nil, # Options for streaming response. Only set this when you set `stream: true`.
        suffix: nil, # The suffix that comes after a completion of inserted text.
                     # This parameter is only supported for `gpt-3.5-turbo-instruct`.
        temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                          # make the output more random, while lower values like 0.2 will make it more
                          # focused and deterministic.
                          # We generally recommend altering this or `top_p` but not both.
        top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                    # model considers the results of the tokens with top_p probability mass. So 0.1
                    # means only the tokens comprising the top 10% probability mass are considered.
                    # We generally recommend altering this or `temperature` but not both.
        user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                   # and detect abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        stream: true, # There is no need to provide `stream:`. Instead, use `#create_streaming` or
                      # `#create` for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end
    end

    class Containers
      sig { returns(OpenAI::Resources::Containers::Files) }
      attr_reader :files

      # Create Container
      sig do
        params(
          name: String,
          expires_after: OpenAI::ContainerCreateParams::ExpiresAfter::OrHash,
          file_ids: T::Array[String],
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Models::ContainerCreateResponse)
      end
      def create(
        name:, # Name of the container to create.
        expires_after: nil, # Container expiration time in seconds relative to the 'anchor' time.
        file_ids: nil, # IDs of files to copy to the container.
        request_options: {}
); end

      # Delete Container
      sig { params(container_id: String, request_options: OpenAI::RequestOptions::OrHash).void }
      def delete(
        container_id, # The ID of the container to delete.
        request_options: {}
); end

      # List Containers
      sig do
        params(
          after: String,
          limit: Integer,
          order: OpenAI::ContainerListParams::Order::OrSymbol,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::CursorPage[OpenAI::Models::ContainerListResponse])
      end
      def list(
        after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                    # in the list. For instance, if you make a list request and receive 100 objects,
                    # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                    # fetch the next page of the list.
        limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                    # 100, and the default is 20.
        order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                    # order and `desc` for descending order.
        request_options: {}
); end

      # Retrieve Container
      sig do
        params(
          container_id: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Models::ContainerRetrieveResponse)
      end
      def retrieve(container_id, request_options: {}); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class Files
        sig { returns(OpenAI::Resources::Containers::Files::Content) }
        attr_reader :content

        # Create a Container File
        #
        # You can send either a multipart/form-data request with the raw file content, or
        # a JSON request with a file ID.
        sig do
          params(
            container_id: String,
            file: OpenAI::Internal::FileInput,
            file_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Models::Containers::FileCreateResponse)
        end
        def create(
          container_id,
          file: nil, # The File object (not file name) to be uploaded.
          file_id: nil, # Name of the file to create.
          request_options: {}
); end

        # Delete Container File
        sig { params(file_id: String, container_id: String, request_options: OpenAI::RequestOptions::OrHash).void }
        def delete(file_id, container_id:, request_options: {}); end

        # List Container files
        sig do
          params(
            container_id: String,
            after: String,
            limit: Integer,
            order: OpenAI::Containers::FileListParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::CursorPage[
              OpenAI::Models::Containers::FileListResponse
            ])
        end
        def list(
          container_id,
          after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                      # in the list. For instance, if you make a list request and receive 100 objects,
                      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                      # fetch the next page of the list.
          limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                      # 100, and the default is 20.
          order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                      # order and `desc` for descending order.
          request_options: {}
); end

        # Retrieve Container File
        sig do
          params(
            file_id: String,
            container_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Models::Containers::FileRetrieveResponse)
        end
        def retrieve(file_id, container_id:, request_options: {}); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end

        class Content
          # Retrieve Container File Content
          sig do
            params(
              file_id: String,
              container_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(StringIO)
          end
          def retrieve(file_id, container_id:, request_options: {}); end

          class << self
            # @api private
            sig { params(client: OpenAI::Client).returns(T.attached_class) }
            def new(client:); end
          end
        end
      end
    end

    class Embeddings
      # Creates an embedding vector representing the input text.
      sig do
        params(
          input: OpenAI::EmbeddingCreateParams::Input::Variants,
          model: T.any(String, OpenAI::EmbeddingModel::OrSymbol),
          dimensions: Integer,
          encoding_format: OpenAI::EmbeddingCreateParams::EncodingFormat::OrSymbol,
          user: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::CreateEmbeddingResponse)
      end
      def create(
        input:, # Input text to embed, encoded as a string or array of tokens. To embed multiple
                # inputs in a single request, pass an array of strings or array of token arrays.
                # The input must not exceed the max input tokens for the model (8192 tokens for
                # all embedding models), cannot be an empty string, and any array must be 2048
                # dimensions or less.
                # [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
                # for counting tokens. In addition to the per-input token limit, all embedding
                # models enforce a maximum of 300,000 tokens summed across all inputs in a single
                # request.
        model:, # ID of the model to use. You can use the
                # [List models](https://platform.openai.com/docs/api-reference/models/list) API to
                # see all of your available models, or see our
                # [Model overview](https://platform.openai.com/docs/models) for descriptions of
                # them.
        dimensions: nil, # The number of dimensions the resulting output embeddings should have. Only
                         # supported in `text-embedding-3` and later models.
        encoding_format: nil, # The format to return the embeddings in. Can be either `float` or
                              # [`base64`](https://pypi.org/project/pybase64/).
        user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                   # and detect abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end
    end

    class Evals
      sig { returns(OpenAI::Resources::Evals::Runs) }
      attr_reader :runs

      # Create the structure of an evaluation that can be used to test a model's
      # performance. An evaluation is a set of testing criteria and the config for a
      # data source, which dictates the schema of the data used in the evaluation. After
      # creating an evaluation, you can run it on different models and model parameters.
      # We support several types of graders and datasources. For more information, see
      # the [Evals guide](https://platform.openai.com/docs/guides/evals).
      sig do
        params(
          data_source_config: T.any(
              OpenAI::EvalCreateParams::DataSourceConfig::Custom::OrHash,
              OpenAI::EvalCreateParams::DataSourceConfig::Logs::OrHash,
              OpenAI::EvalCreateParams::DataSourceConfig::StoredCompletions::OrHash
            ),
          testing_criteria: T::Array[
              T.any(
                OpenAI::EvalCreateParams::TestingCriterion::LabelModel::OrHash,
                OpenAI::Graders::StringCheckGrader::OrHash,
                OpenAI::EvalCreateParams::TestingCriterion::TextSimilarity::OrHash,
                OpenAI::EvalCreateParams::TestingCriterion::Python::OrHash,
                OpenAI::EvalCreateParams::TestingCriterion::ScoreModel::OrHash
              )
            ],
          metadata: T.nilable(T::Hash[Symbol, String]),
          name: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Models::EvalCreateResponse)
      end
      def create(
        data_source_config:, # The configuration for the data source used for the evaluation runs. Dictates the
                             # schema of the data used in the evaluation.
        testing_criteria:, # A list of graders for all eval runs in this group. Graders can reference
                           # variables in the data source using double curly braces notation, like
                           # `{{item.variable_name}}`. To reference the model's output, use the `sample`
                           # namespace (ie, `{{sample.output_text}}`).
        metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
        name: nil, # The name of the evaluation.
        request_options: {}
); end

      # Delete an evaluation.
      sig do
        params(
          eval_id: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Models::EvalDeleteResponse)
      end
      def delete(
        eval_id, # The ID of the evaluation to delete.
        request_options: {}
); end

      # List evaluations for a project.
      sig do
        params(
          after: String,
          limit: Integer,
          order: OpenAI::EvalListParams::Order::OrSymbol,
          order_by: OpenAI::EvalListParams::OrderBy::OrSymbol,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::CursorPage[OpenAI::Models::EvalListResponse])
      end
      def list(
        after: nil, # Identifier for the last eval from the previous pagination request.
        limit: nil, # Number of evals to retrieve.
        order: nil, # Sort order for evals by timestamp. Use `asc` for ascending order or `desc` for
                    # descending order.
        order_by: nil, # Evals can be ordered by creation time or last updated time. Use `created_at` for
                       # creation time or `updated_at` for last updated time.
        request_options: {}
); end

      # Get an evaluation by ID.
      sig do
        params(
          eval_id: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Models::EvalRetrieveResponse)
      end
      def retrieve(
        eval_id, # The ID of the evaluation to retrieve.
        request_options: {}
); end

      # Update certain properties of an evaluation.
      sig do
        params(
          eval_id: String,
          metadata: T.nilable(T::Hash[Symbol, String]),
          name: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Models::EvalUpdateResponse)
      end
      def update(
        eval_id, # The ID of the evaluation to update.
        metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
        name: nil, # Rename the evaluation.
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class Runs
        sig { returns(OpenAI::Resources::Evals::Runs::OutputItems) }
        attr_reader :output_items

        # Cancel an ongoing evaluation run.
        sig do
          params(
            run_id: String,
            eval_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Models::Evals::RunCancelResponse)
        end
        def cancel(
          run_id, # The ID of the run to cancel.
          eval_id:, # The ID of the evaluation whose run you want to cancel.
          request_options: {}
); end

        # Kicks off a new run for a given evaluation, specifying the data source, and what
        # model configuration to use to test. The datasource will be validated against the
        # schema specified in the config of the evaluation.
        sig do
          params(
            eval_id: String,
            data_source: T.any(
                OpenAI::Evals::CreateEvalJSONLRunDataSource::OrHash,
                OpenAI::Evals::CreateEvalCompletionsRunDataSource::OrHash,
                OpenAI::Evals::RunCreateParams::DataSource::CreateEvalResponsesRunDataSource::OrHash
              ),
            metadata: T.nilable(T::Hash[Symbol, String]),
            name: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Models::Evals::RunCreateResponse)
        end
        def create(
          eval_id, # The ID of the evaluation to create a run for.
          data_source:, # Details about the run's data source.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          name: nil, # The name of the run.
          request_options: {}
); end

        # Delete an eval run.
        sig do
          params(
            run_id: String,
            eval_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Models::Evals::RunDeleteResponse)
        end
        def delete(
          run_id, # The ID of the run to delete.
          eval_id:, # The ID of the evaluation to delete the run from.
          request_options: {}
); end

        # Get a list of runs for an evaluation.
        sig do
          params(
            eval_id: String,
            after: String,
            limit: Integer,
            order: OpenAI::Evals::RunListParams::Order::OrSymbol,
            status: OpenAI::Evals::RunListParams::Status::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::CursorPage[OpenAI::Models::Evals::RunListResponse])
        end
        def list(
          eval_id, # The ID of the evaluation to retrieve runs for.
          after: nil, # Identifier for the last run from the previous pagination request.
          limit: nil, # Number of runs to retrieve.
          order: nil, # Sort order for runs by timestamp. Use `asc` for ascending order or `desc` for
                      # descending order. Defaults to `asc`.
          status: nil, # Filter runs by status. One of `queued` | `in_progress` | `failed` | `completed`
                       # | `canceled`.
          request_options: {}
); end

        # Get an evaluation run by ID.
        sig do
          params(
            run_id: String,
            eval_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Models::Evals::RunRetrieveResponse)
        end
        def retrieve(
          run_id, # The ID of the run to retrieve.
          eval_id:, # The ID of the evaluation to retrieve runs for.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end

        class OutputItems
          # Get a list of output items for an evaluation run.
          sig do
            params(
              run_id: String,
              eval_id: String,
              after: String,
              limit: Integer,
              order: OpenAI::Evals::Runs::OutputItemListParams::Order::OrSymbol,
              status: OpenAI::Evals::Runs::OutputItemListParams::Status::OrSymbol,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Internal::CursorPage[
                OpenAI::Models::Evals::Runs::OutputItemListResponse
              ])
          end
          def list(
            run_id, # Path param: The ID of the run to retrieve output items for.
            eval_id:, # Path param: The ID of the evaluation to retrieve runs for.
            after: nil, # Query param: Identifier for the last output item from the previous pagination
                        # request.
            limit: nil, # Query param: Number of output items to retrieve.
            order: nil, # Query param: Sort order for output items by timestamp. Use `asc` for ascending
                        # order or `desc` for descending order. Defaults to `asc`.
            status: nil, # Query param: Filter output items by status. Use `failed` to filter by failed
                         # output items or `pass` to filter by passed output items.
            request_options: {}
); end

          # Get an evaluation run output item by ID.
          sig do
            params(
              output_item_id: String,
              eval_id: String,
              run_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Models::Evals::Runs::OutputItemRetrieveResponse)
          end
          def retrieve(
            output_item_id, # The ID of the output item to retrieve.
            eval_id:, # The ID of the evaluation to retrieve runs for.
            run_id:, # The ID of the run to retrieve.
            request_options: {}
); end

          class << self
            # @api private
            sig { params(client: OpenAI::Client).returns(T.attached_class) }
            def new(client:); end
          end
        end
      end
    end

    class Files
      # Returns the contents of the specified file.
      sig { params(file_id: String, request_options: OpenAI::RequestOptions::OrHash).returns(StringIO) }
      def content(
        file_id, # The ID of the file to use for this request.
        request_options: {}
); end

      # Upload a file that can be used across various endpoints. Individual files can be
      # up to 512 MB, and the size of all files uploaded by one organization can be up
      # to 100 GB.
      #
      # The Assistants API supports files up to 2 million tokens and of specific file
      # types. See the
      # [Assistants Tools guide](https://platform.openai.com/docs/assistants/tools) for
      # details.
      #
      # The Fine-tuning API only supports `.jsonl` files. The input also has certain
      # required formats for fine-tuning
      # [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input) or
      # [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)
      # models.
      #
      # The Batch API only supports `.jsonl` files up to 200 MB in size. The input also
      # has a specific required
      # [format](https://platform.openai.com/docs/api-reference/batch/request-input).
      #
      # Please [contact us](https://help.openai.com/) if you need to increase these
      # storage limits.
      sig do
        params(
          file: OpenAI::Internal::FileInput,
          purpose: OpenAI::FilePurpose::OrSymbol,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::FileObject)
      end
      def create(
        file:, # The File object (not file name) to be uploaded.
        purpose:, # The intended purpose of the uploaded file. One of: - `assistants`: Used in the
                  # Assistants API - `batch`: Used in the Batch API - `fine-tune`: Used for
                  # fine-tuning - `vision`: Images used for vision fine-tuning - `user_data`:
                  # Flexible file type for any purpose - `evals`: Used for eval data sets
        request_options: {}
); end

      # Delete a file.
      sig { params(file_id: String, request_options: OpenAI::RequestOptions::OrHash).returns(OpenAI::FileDeleted) }
      def delete(
        file_id, # The ID of the file to use for this request.
        request_options: {}
); end

      # Returns a list of files.
      sig do
        params(
          after: String,
          limit: Integer,
          order: OpenAI::FileListParams::Order::OrSymbol,
          purpose: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::CursorPage[OpenAI::FileObject])
      end
      def list(
        after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                    # in the list. For instance, if you make a list request and receive 100 objects,
                    # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                    # fetch the next page of the list.
        limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                    # 10,000, and the default is 10,000.
        order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                    # order and `desc` for descending order.
        purpose: nil, # Only return files with the given purpose.
        request_options: {}
); end

      # Returns information about a specific file.
      sig { params(file_id: String, request_options: OpenAI::RequestOptions::OrHash).returns(OpenAI::FileObject) }
      def retrieve(
        file_id, # The ID of the file to use for this request.
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end
    end

    class FineTuning
      sig { returns(OpenAI::Resources::FineTuning::Alpha) }
      attr_reader :alpha

      sig { returns(OpenAI::Resources::FineTuning::Checkpoints) }
      attr_reader :checkpoints

      sig { returns(OpenAI::Resources::FineTuning::Jobs) }
      attr_reader :jobs

      sig { returns(OpenAI::Resources::FineTuning::Methods) }
      attr_reader :methods_

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class Alpha
        sig { returns(OpenAI::Resources::FineTuning::Alpha::Graders) }
        attr_reader :graders

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end

        class Graders
          # Run a grader.
          sig do
            params(
              grader: T.any(
                  OpenAI::Graders::StringCheckGrader::OrHash,
                  OpenAI::Graders::TextSimilarityGrader::OrHash,
                  OpenAI::Graders::PythonGrader::OrHash,
                  OpenAI::Graders::ScoreModelGrader::OrHash,
                  OpenAI::Graders::MultiGrader::OrHash
                ),
              model_sample: String,
              item: T.anything,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Models::FineTuning::Alpha::GraderRunResponse)
          end
          def run(
            grader:, # The grader used for the fine-tuning job.
            model_sample:, # The model sample to be evaluated. This value will be used to populate the
                           # `sample` namespace. See
                           # [the guide](https://platform.openai.com/docs/guides/graders) for more details.
                           # The `output_json` variable will be populated if the model sample is a valid JSON
                           # string.
            item: nil, # The dataset item provided to the grader. This will be used to populate the
                       # `item` namespace. See
                       # [the guide](https://platform.openai.com/docs/guides/graders) for more details.
            request_options: {}
); end

          # Validate a grader.
          sig do
            params(
              grader: T.any(
                  OpenAI::Graders::StringCheckGrader::OrHash,
                  OpenAI::Graders::TextSimilarityGrader::OrHash,
                  OpenAI::Graders::PythonGrader::OrHash,
                  OpenAI::Graders::ScoreModelGrader::OrHash,
                  OpenAI::Graders::MultiGrader::OrHash
                ),
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Models::FineTuning::Alpha::GraderValidateResponse)
          end
          def validate(
            grader:, # The grader used for the fine-tuning job.
            request_options: {}
); end

          class << self
            # @api private
            sig { params(client: OpenAI::Client).returns(T.attached_class) }
            def new(client:); end
          end
        end
      end

      class Checkpoints
        sig { returns(OpenAI::Resources::FineTuning::Checkpoints::Permissions) }
        attr_reader :permissions

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end

        class Permissions
          # **NOTE:** Calling this endpoint requires an [admin API key](../admin-api-keys).
          #
          # This enables organization owners to share fine-tuned models with other projects
          # in their organization.
          sig do
            params(
              fine_tuned_model_checkpoint: String,
              project_ids: T::Array[String],
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Internal::Page[
                OpenAI::Models::FineTuning::Checkpoints::PermissionCreateResponse
              ])
          end
          def create(
            fine_tuned_model_checkpoint, # The ID of the fine-tuned model checkpoint to create a permission for.
            project_ids:, # The project identifiers to grant access to.
            request_options: {}
); end

          # **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).
          #
          # Organization owners can use this endpoint to delete a permission for a
          # fine-tuned model checkpoint.
          sig do
            params(
              permission_id: String,
              fine_tuned_model_checkpoint: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Models::FineTuning::Checkpoints::PermissionDeleteResponse)
          end
          def delete(
            permission_id, # The ID of the fine-tuned model checkpoint permission to delete.
            fine_tuned_model_checkpoint:, # The ID of the fine-tuned model checkpoint to delete a permission for.
            request_options: {}
); end

          # **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).
          #
          # Organization owners can use this endpoint to view all permissions for a
          # fine-tuned model checkpoint.
          sig do
            params(
              fine_tuned_model_checkpoint: String,
              after: String,
              limit: Integer,
              order: OpenAI::FineTuning::Checkpoints::PermissionRetrieveParams::Order::OrSymbol,
              project_id: String,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Models::FineTuning::Checkpoints::PermissionRetrieveResponse)
          end
          def retrieve(
            fine_tuned_model_checkpoint, # The ID of the fine-tuned model checkpoint to get permissions for.
            after: nil, # Identifier for the last permission ID from the previous pagination request.
            limit: nil, # Number of permissions to retrieve.
            order: nil, # The order in which to retrieve permissions.
            project_id: nil, # The ID of the project to get permissions for.
            request_options: {}
); end

          class << self
            # @api private
            sig { params(client: OpenAI::Client).returns(T.attached_class) }
            def new(client:); end
          end
        end
      end

      class Jobs
        sig { returns(OpenAI::Resources::FineTuning::Jobs::Checkpoints) }
        attr_reader :checkpoints

        # Immediately cancel a fine-tune job.
        sig do
          params(
            fine_tuning_job_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::FineTuning::FineTuningJob)
        end
        def cancel(
          fine_tuning_job_id, # The ID of the fine-tuning job to cancel.
          request_options: {}
); end

        # Creates a fine-tuning job which begins the process of creating a new model from
        # a given dataset.
        #
        # Response includes details of the enqueued job including job status and the name
        # of the fine-tuned models once complete.
        #
        # [Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)
        sig do
          params(
            model: T.any(
                String,
                OpenAI::FineTuning::JobCreateParams::Model::OrSymbol
              ),
            training_file: String,
            hyperparameters: OpenAI::FineTuning::JobCreateParams::Hyperparameters::OrHash,
            integrations: T.nilable(
                T::Array[
                  OpenAI::FineTuning::JobCreateParams::Integration::OrHash
                ]
              ),
            metadata: T.nilable(T::Hash[Symbol, String]),
            method_: OpenAI::FineTuning::JobCreateParams::Method::OrHash,
            seed: T.nilable(Integer),
            suffix: T.nilable(String),
            validation_file: T.nilable(String),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::FineTuning::FineTuningJob)
        end
        def create(
          model:, # The name of the model to fine-tune. You can select one of the
                  # [supported models](https://platform.openai.com/docs/guides/fine-tuning#which-models-can-be-fine-tuned).
          training_file:, # The ID of an uploaded file that contains training data.
                          # See [upload file](https://platform.openai.com/docs/api-reference/files/create)
                          # for how to upload a file.
                          # Your dataset must be formatted as a JSONL file. Additionally, you must upload
                          # your file with the purpose `fine-tune`.
                          # The contents of the file should differ depending on if the model uses the
                          # [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input),
                          # [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)
                          # format, or if the fine-tuning method uses the
                          # [preference](https://platform.openai.com/docs/api-reference/fine-tuning/preference-input)
                          # format.
                          # See the
                          # [fine-tuning guide](https://platform.openai.com/docs/guides/model-optimization)
                          # for more details.
          hyperparameters: nil, # The hyperparameters used for the fine-tuning job. This value is now deprecated
                                # in favor of `method`, and should be passed in under the `method` parameter.
          integrations: nil, # A list of integrations to enable for your fine-tuning job.
          metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                         # for storing additional information about the object in a structured format, and
                         # querying for objects via API or the dashboard.
                         # Keys are strings with a maximum length of 64 characters. Values are strings with
                         # a maximum length of 512 characters.
          method_: nil, # The method used for fine-tuning.
          seed: nil, # The seed controls the reproducibility of the job. Passing in the same seed and
                     # job parameters should produce the same results, but may differ in rare cases. If
                     # a seed is not specified, one will be generated for you.
          suffix: nil, # A string of up to 64 characters that will be added to your fine-tuned model
                       # name.
                       # For example, a `suffix` of "custom-model-name" would produce a model name like
                       # `ft:gpt-4o-mini:openai:custom-model-name:7p4lURel`.
          validation_file: nil, # The ID of an uploaded file that contains validation data.
                                # If you provide this file, the data is used to generate validation metrics
                                # periodically during fine-tuning. These metrics can be viewed in the fine-tuning
                                # results file. The same data should not be present in both train and validation
                                # files.
                                # Your dataset must be formatted as a JSONL file. You must upload your file with
                                # the purpose `fine-tune`.
                                # See the
                                # [fine-tuning guide](https://platform.openai.com/docs/guides/model-optimization)
                                # for more details.
          request_options: {}
); end

        # List your organization's fine-tuning jobs
        sig do
          params(
            after: String,
            limit: Integer,
            metadata: T.nilable(T::Hash[Symbol, String]),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::CursorPage[OpenAI::FineTuning::FineTuningJob])
        end
        def list(
          after: nil, # Identifier for the last job from the previous pagination request.
          limit: nil, # Number of fine-tuning jobs to retrieve.
          metadata: nil, # Optional metadata filter. To filter, use the syntax `metadata[k]=v`.
                         # Alternatively, set `metadata=null` to indicate no metadata.
          request_options: {}
); end

        # Get status updates for a fine-tuning job.
        sig do
          params(
            fine_tuning_job_id: String,
            after: String,
            limit: Integer,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::CursorPage[OpenAI::FineTuning::FineTuningJobEvent])
        end
        def list_events(
          fine_tuning_job_id, # The ID of the fine-tuning job to get events for.
          after: nil, # Identifier for the last event from the previous pagination request.
          limit: nil, # Number of events to retrieve.
          request_options: {}
); end

        # Pause a fine-tune job.
        sig do
          params(
            fine_tuning_job_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::FineTuning::FineTuningJob)
        end
        def pause(
          fine_tuning_job_id, # The ID of the fine-tuning job to pause.
          request_options: {}
); end

        # Resume a fine-tune job.
        sig do
          params(
            fine_tuning_job_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::FineTuning::FineTuningJob)
        end
        def resume(
          fine_tuning_job_id, # The ID of the fine-tuning job to resume.
          request_options: {}
); end

        # Get info about a fine-tuning job.
        #
        # [Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)
        sig do
          params(
            fine_tuning_job_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::FineTuning::FineTuningJob)
        end
        def retrieve(
          fine_tuning_job_id, # The ID of the fine-tuning job.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end

        class Checkpoints
          # List checkpoints for a fine-tuning job.
          sig do
            params(
              fine_tuning_job_id: String,
              after: String,
              limit: Integer,
              request_options: OpenAI::RequestOptions::OrHash
            ).returns(OpenAI::Internal::CursorPage[
                OpenAI::FineTuning::Jobs::FineTuningJobCheckpoint
              ])
          end
          def list(
            fine_tuning_job_id, # The ID of the fine-tuning job to get checkpoints for.
            after: nil, # Identifier for the last checkpoint ID from the previous pagination request.
            limit: nil, # Number of checkpoints to retrieve.
            request_options: {}
); end

          class << self
            # @api private
            sig { params(client: OpenAI::Client).returns(T.attached_class) }
            def new(client:); end
          end
        end
      end

      class Methods
        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end
    end

    class Graders
      sig { returns(OpenAI::Resources::Graders::GraderModels) }
      attr_reader :grader_models

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class GraderModels
        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end
    end

    class Images
      # Creates a variation of a given image. This endpoint only supports `dall-e-2`.
      sig do
        params(
          image: OpenAI::Internal::FileInput,
          model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
          n: T.nilable(Integer),
          response_format: T.nilable(
              OpenAI::ImageCreateVariationParams::ResponseFormat::OrSymbol
            ),
          size: T.nilable(OpenAI::ImageCreateVariationParams::Size::OrSymbol),
          user: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::ImagesResponse)
      end
      def create_variation(
        image:, # The image to use as the basis for the variation(s). Must be a valid PNG file,
                # less than 4MB, and square.
        model: nil, # The model to use for image generation. Only `dall-e-2` is supported at this
                    # time.
        n: nil, # The number of images to generate. Must be between 1 and 10.
        response_format: nil, # The format in which the generated images are returned. Must be one of `url` or
                              # `b64_json`. URLs are only valid for 60 minutes after the image has been
                              # generated.
        size: nil, # The size of the generated images. Must be one of `256x256`, `512x512`, or
                   # `1024x1024`.
        user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                   # and detect abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        request_options: {}
); end

      # See {OpenAI::Resources::Images#edit_stream_raw} for streaming counterpart.
      #
      # Creates an edited or extended image given one or more source images and a
      # prompt. This endpoint only supports `gpt-image-1` and `dall-e-2`.
      sig do
        params(
          image: OpenAI::ImageEditParams::Image::Variants,
          prompt: String,
          background: T.nilable(OpenAI::ImageEditParams::Background::OrSymbol),
          input_fidelity: T.nilable(OpenAI::ImageEditParams::InputFidelity::OrSymbol),
          mask: OpenAI::Internal::FileInput,
          model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
          n: T.nilable(Integer),
          output_compression: T.nilable(Integer),
          output_format: T.nilable(OpenAI::ImageEditParams::OutputFormat::OrSymbol),
          partial_images: T.nilable(Integer),
          quality: T.nilable(OpenAI::ImageEditParams::Quality::OrSymbol),
          response_format: T.nilable(OpenAI::ImageEditParams::ResponseFormat::OrSymbol),
          size: T.nilable(OpenAI::ImageEditParams::Size::OrSymbol),
          user: String,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::ImagesResponse)
      end
      def edit(
        image:, # The image(s) to edit. Must be a supported image file or an array of images.
                # For `gpt-image-1`, each image should be a `png`, `webp`, or `jpg` file less than
                # 50MB. You can provide up to 16 images.
                # For `dall-e-2`, you can only provide one image, and it should be a square `png`
                # file less than 4MB.
        prompt:, # A text description of the desired image(s). The maximum length is 1000
                 # characters for `dall-e-2`, and 32000 characters for `gpt-image-1`.
        background: nil, # Allows to set transparency for the background of the generated image(s). This
                         # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
                         # `opaque` or `auto` (default value). When `auto` is used, the model will
                         # automatically determine the best background for the image.
                         # If `transparent`, the output format needs to support transparency, so it should
                         # be set to either `png` (default value) or `webp`.
        input_fidelity: nil, # Control how much effort the model will exert to match the style and features,
                             # especially facial features, of input images. This parameter is only supported
                             # for `gpt-image-1`. Supports `high` and `low`. Defaults to `low`.
        mask: nil, # An additional image whose fully transparent areas (e.g. where alpha is zero)
                   # indicate where `image` should be edited. If there are multiple images provided,
                   # the mask will be applied on the first image. Must be a valid PNG file, less than
                   # 4MB, and have the same dimensions as `image`.
        model: nil, # The model to use for image generation. Only `dall-e-2` and `gpt-image-1` are
                    # supported. Defaults to `dall-e-2` unless a parameter specific to `gpt-image-1`
                    # is used.
        n: nil, # The number of images to generate. Must be between 1 and 10.
        output_compression: nil, # The compression level (0-100%) for the generated images. This parameter is only
                                 # supported for `gpt-image-1` with the `webp` or `jpeg` output formats, and
                                 # defaults to 100.
        output_format: nil, # The format in which the generated images are returned. This parameter is only
                            # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`. The
                            # default value is `png`.
        partial_images: nil, # The number of partial images to generate. This parameter is used for streaming
                             # responses that return partial images. Value must be between 0 and 3. When set to
                             # 0, the response will be a single image sent in one streaming event.
                             # Note that the final image may be sent before the full number of partial images
                             # are generated if the full image is generated more quickly.
        quality: nil, # The quality of the image that will be generated. `high`, `medium` and `low` are
                      # only supported for `gpt-image-1`. `dall-e-2` only supports `standard` quality.
                      # Defaults to `auto`.
        response_format: nil, # The format in which the generated images are returned. Must be one of `url` or
                              # `b64_json`. URLs are only valid for 60 minutes after the image has been
                              # generated. This parameter is only supported for `dall-e-2`, as `gpt-image-1`
                              # will always return base64-encoded images.
        size: nil, # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
                   # (landscape), `1024x1536` (portrait), or `auto` (default value) for
                   # `gpt-image-1`, and one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`.
        user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                   # and detect abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        stream: false, # There is no need to provide `stream:`. Instead, use `#edit_stream_raw` or
                       # `#edit` for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      # See {OpenAI::Resources::Images#edit} for non-streaming counterpart.
      #
      # Creates an edited or extended image given one or more source images and a
      # prompt. This endpoint only supports `gpt-image-1` and `dall-e-2`.
      sig do
        params(
          image: OpenAI::ImageEditParams::Image::Variants,
          prompt: String,
          background: T.nilable(OpenAI::ImageEditParams::Background::OrSymbol),
          input_fidelity: T.nilable(OpenAI::ImageEditParams::InputFidelity::OrSymbol),
          mask: OpenAI::Internal::FileInput,
          model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
          n: T.nilable(Integer),
          output_compression: T.nilable(Integer),
          output_format: T.nilable(OpenAI::ImageEditParams::OutputFormat::OrSymbol),
          partial_images: T.nilable(Integer),
          quality: T.nilable(OpenAI::ImageEditParams::Quality::OrSymbol),
          response_format: T.nilable(OpenAI::ImageEditParams::ResponseFormat::OrSymbol),
          size: T.nilable(OpenAI::ImageEditParams::Size::OrSymbol),
          user: String,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::Stream[OpenAI::ImageEditStreamEvent::Variants])
      end
      def edit_stream_raw(
        image:, # The image(s) to edit. Must be a supported image file or an array of images.
                # For `gpt-image-1`, each image should be a `png`, `webp`, or `jpg` file less than
                # 50MB. You can provide up to 16 images.
                # For `dall-e-2`, you can only provide one image, and it should be a square `png`
                # file less than 4MB.
        prompt:, # A text description of the desired image(s). The maximum length is 1000
                 # characters for `dall-e-2`, and 32000 characters for `gpt-image-1`.
        background: nil, # Allows to set transparency for the background of the generated image(s). This
                         # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
                         # `opaque` or `auto` (default value). When `auto` is used, the model will
                         # automatically determine the best background for the image.
                         # If `transparent`, the output format needs to support transparency, so it should
                         # be set to either `png` (default value) or `webp`.
        input_fidelity: nil, # Control how much effort the model will exert to match the style and features,
                             # especially facial features, of input images. This parameter is only supported
                             # for `gpt-image-1`. Supports `high` and `low`. Defaults to `low`.
        mask: nil, # An additional image whose fully transparent areas (e.g. where alpha is zero)
                   # indicate where `image` should be edited. If there are multiple images provided,
                   # the mask will be applied on the first image. Must be a valid PNG file, less than
                   # 4MB, and have the same dimensions as `image`.
        model: nil, # The model to use for image generation. Only `dall-e-2` and `gpt-image-1` are
                    # supported. Defaults to `dall-e-2` unless a parameter specific to `gpt-image-1`
                    # is used.
        n: nil, # The number of images to generate. Must be between 1 and 10.
        output_compression: nil, # The compression level (0-100%) for the generated images. This parameter is only
                                 # supported for `gpt-image-1` with the `webp` or `jpeg` output formats, and
                                 # defaults to 100.
        output_format: nil, # The format in which the generated images are returned. This parameter is only
                            # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`. The
                            # default value is `png`.
        partial_images: nil, # The number of partial images to generate. This parameter is used for streaming
                             # responses that return partial images. Value must be between 0 and 3. When set to
                             # 0, the response will be a single image sent in one streaming event.
                             # Note that the final image may be sent before the full number of partial images
                             # are generated if the full image is generated more quickly.
        quality: nil, # The quality of the image that will be generated. `high`, `medium` and `low` are
                      # only supported for `gpt-image-1`. `dall-e-2` only supports `standard` quality.
                      # Defaults to `auto`.
        response_format: nil, # The format in which the generated images are returned. Must be one of `url` or
                              # `b64_json`. URLs are only valid for 60 minutes after the image has been
                              # generated. This parameter is only supported for `dall-e-2`, as `gpt-image-1`
                              # will always return base64-encoded images.
        size: nil, # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
                   # (landscape), `1024x1536` (portrait), or `auto` (default value) for
                   # `gpt-image-1`, and one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`.
        user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                   # and detect abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        stream: true, # There is no need to provide `stream:`. Instead, use `#edit_stream_raw` or
                      # `#edit` for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      # See {OpenAI::Resources::Images#generate_stream_raw} for streaming counterpart.
      #
      # Creates an image given a prompt.
      # [Learn more](https://platform.openai.com/docs/guides/images).
      sig do
        params(
          prompt: String,
          background: T.nilable(OpenAI::ImageGenerateParams::Background::OrSymbol),
          model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
          moderation: T.nilable(OpenAI::ImageGenerateParams::Moderation::OrSymbol),
          n: T.nilable(Integer),
          output_compression: T.nilable(Integer),
          output_format: T.nilable(OpenAI::ImageGenerateParams::OutputFormat::OrSymbol),
          partial_images: T.nilable(Integer),
          quality: T.nilable(OpenAI::ImageGenerateParams::Quality::OrSymbol),
          response_format: T.nilable(OpenAI::ImageGenerateParams::ResponseFormat::OrSymbol),
          size: T.nilable(OpenAI::ImageGenerateParams::Size::OrSymbol),
          style: T.nilable(OpenAI::ImageGenerateParams::Style::OrSymbol),
          user: String,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::ImagesResponse)
      end
      def generate(
        prompt:, # A text description of the desired image(s). The maximum length is 32000
                 # characters for `gpt-image-1`, 1000 characters for `dall-e-2` and 4000 characters
                 # for `dall-e-3`.
        background: nil, # Allows to set transparency for the background of the generated image(s). This
                         # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
                         # `opaque` or `auto` (default value). When `auto` is used, the model will
                         # automatically determine the best background for the image.
                         # If `transparent`, the output format needs to support transparency, so it should
                         # be set to either `png` (default value) or `webp`.
        model: nil, # The model to use for image generation. One of `dall-e-2`, `dall-e-3`, or
                    # `gpt-image-1`. Defaults to `dall-e-2` unless a parameter specific to
                    # `gpt-image-1` is used.
        moderation: nil, # Control the content-moderation level for images generated by `gpt-image-1`. Must
                         # be either `low` for less restrictive filtering or `auto` (default value).
        n: nil, # The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only
                # `n=1` is supported.
        output_compression: nil, # The compression level (0-100%) for the generated images. This parameter is only
                                 # supported for `gpt-image-1` with the `webp` or `jpeg` output formats, and
                                 # defaults to 100.
        output_format: nil, # The format in which the generated images are returned. This parameter is only
                            # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`.
        partial_images: nil, # The number of partial images to generate. This parameter is used for streaming
                             # responses that return partial images. Value must be between 0 and 3. When set to
                             # 0, the response will be a single image sent in one streaming event.
                             # Note that the final image may be sent before the full number of partial images
                             # are generated if the full image is generated more quickly.
        quality: nil, # The quality of the image that will be generated.
                      # - `auto` (default value) will automatically select the best quality for the
                      #   given model.
                      # - `high`, `medium` and `low` are supported for `gpt-image-1`.
                      # - `hd` and `standard` are supported for `dall-e-3`.
                      # - `standard` is the only option for `dall-e-2`.
        response_format: nil, # The format in which generated images with `dall-e-2` and `dall-e-3` are
                              # returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes
                              # after the image has been generated. This parameter isn't supported for
                              # `gpt-image-1` which will always return base64-encoded images.
        size: nil, # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
                   # (landscape), `1024x1536` (portrait), or `auto` (default value) for
                   # `gpt-image-1`, one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`, and
                   # one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3`.
        style: nil, # The style of the generated images. This parameter is only supported for
                    # `dall-e-3`. Must be one of `vivid` or `natural`. Vivid causes the model to lean
                    # towards generating hyper-real and dramatic images. Natural causes the model to
                    # produce more natural, less hyper-real looking images.
        user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                   # and detect abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        stream: false, # There is no need to provide `stream:`. Instead, use `#generate_stream_raw` or
                       # `#generate` for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      # See {OpenAI::Resources::Images#generate} for non-streaming counterpart.
      #
      # Creates an image given a prompt.
      # [Learn more](https://platform.openai.com/docs/guides/images).
      sig do
        params(
          prompt: String,
          background: T.nilable(OpenAI::ImageGenerateParams::Background::OrSymbol),
          model: T.nilable(T.any(String, OpenAI::ImageModel::OrSymbol)),
          moderation: T.nilable(OpenAI::ImageGenerateParams::Moderation::OrSymbol),
          n: T.nilable(Integer),
          output_compression: T.nilable(Integer),
          output_format: T.nilable(OpenAI::ImageGenerateParams::OutputFormat::OrSymbol),
          partial_images: T.nilable(Integer),
          quality: T.nilable(OpenAI::ImageGenerateParams::Quality::OrSymbol),
          response_format: T.nilable(OpenAI::ImageGenerateParams::ResponseFormat::OrSymbol),
          size: T.nilable(OpenAI::ImageGenerateParams::Size::OrSymbol),
          style: T.nilable(OpenAI::ImageGenerateParams::Style::OrSymbol),
          user: String,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::Stream[OpenAI::ImageGenStreamEvent::Variants])
      end
      def generate_stream_raw(
        prompt:, # A text description of the desired image(s). The maximum length is 32000
                 # characters for `gpt-image-1`, 1000 characters for `dall-e-2` and 4000 characters
                 # for `dall-e-3`.
        background: nil, # Allows to set transparency for the background of the generated image(s). This
                         # parameter is only supported for `gpt-image-1`. Must be one of `transparent`,
                         # `opaque` or `auto` (default value). When `auto` is used, the model will
                         # automatically determine the best background for the image.
                         # If `transparent`, the output format needs to support transparency, so it should
                         # be set to either `png` (default value) or `webp`.
        model: nil, # The model to use for image generation. One of `dall-e-2`, `dall-e-3`, or
                    # `gpt-image-1`. Defaults to `dall-e-2` unless a parameter specific to
                    # `gpt-image-1` is used.
        moderation: nil, # Control the content-moderation level for images generated by `gpt-image-1`. Must
                         # be either `low` for less restrictive filtering or `auto` (default value).
        n: nil, # The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only
                # `n=1` is supported.
        output_compression: nil, # The compression level (0-100%) for the generated images. This parameter is only
                                 # supported for `gpt-image-1` with the `webp` or `jpeg` output formats, and
                                 # defaults to 100.
        output_format: nil, # The format in which the generated images are returned. This parameter is only
                            # supported for `gpt-image-1`. Must be one of `png`, `jpeg`, or `webp`.
        partial_images: nil, # The number of partial images to generate. This parameter is used for streaming
                             # responses that return partial images. Value must be between 0 and 3. When set to
                             # 0, the response will be a single image sent in one streaming event.
                             # Note that the final image may be sent before the full number of partial images
                             # are generated if the full image is generated more quickly.
        quality: nil, # The quality of the image that will be generated.
                      # - `auto` (default value) will automatically select the best quality for the
                      #   given model.
                      # - `high`, `medium` and `low` are supported for `gpt-image-1`.
                      # - `hd` and `standard` are supported for `dall-e-3`.
                      # - `standard` is the only option for `dall-e-2`.
        response_format: nil, # The format in which generated images with `dall-e-2` and `dall-e-3` are
                              # returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes
                              # after the image has been generated. This parameter isn't supported for
                              # `gpt-image-1` which will always return base64-encoded images.
        size: nil, # The size of the generated images. Must be one of `1024x1024`, `1536x1024`
                   # (landscape), `1024x1536` (portrait), or `auto` (default value) for
                   # `gpt-image-1`, one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`, and
                   # one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3`.
        style: nil, # The style of the generated images. This parameter is only supported for
                    # `dall-e-3`. Must be one of `vivid` or `natural`. Vivid causes the model to lean
                    # towards generating hyper-real and dramatic images. Natural causes the model to
                    # produce more natural, less hyper-real looking images.
        user: nil, # A unique identifier representing your end-user, which can help OpenAI to monitor
                   # and detect abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        stream: true, # There is no need to provide `stream:`. Instead, use `#generate_stream_raw` or
                      # `#generate` for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end
    end

    class Models
      # Delete a fine-tuned model. You must have the Owner role in your organization to
      # delete a model.
      sig { params(model: String, request_options: OpenAI::RequestOptions::OrHash).returns(OpenAI::ModelDeleted) }
      def delete(
        model, # The model to delete
        request_options: {}
); end

      # Lists the currently available models, and provides basic information about each
      # one such as the owner and availability.
      sig { params(request_options: OpenAI::RequestOptions::OrHash).returns(OpenAI::Internal::Page[OpenAI::Model]) }
      def list(request_options: {}); end

      # Retrieves a model instance, providing basic information about the model such as
      # the owner and permissioning.
      sig { params(model: String, request_options: OpenAI::RequestOptions::OrHash).returns(OpenAI::Model) }
      def retrieve(
        model, # The ID of the model to use for this request
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end
    end

    class Moderations
      # Classifies if text and/or image inputs are potentially harmful. Learn more in
      # the [moderation guide](https://platform.openai.com/docs/guides/moderation).
      sig do
        params(
          input: OpenAI::ModerationCreateParams::Input::Variants,
          model: T.any(String, OpenAI::ModerationModel::OrSymbol),
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Models::ModerationCreateResponse)
      end
      def create(
        input:, # Input (or inputs) to classify. Can be a single string, an array of strings, or
                # an array of multi-modal input objects similar to other models.
        model: nil, # The content moderation model you would like to use. Learn more in
                    # [the moderation guide](https://platform.openai.com/docs/guides/moderation), and
                    # learn about available models
                    # [here](https://platform.openai.com/docs/models#moderation).
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end
    end

    class Responses
      sig { returns(OpenAI::Resources::Responses::InputItems) }
      attr_reader :input_items

      # Cancels a model response with the given ID. Only responses created with the
      # `background` parameter set to `true` can be cancelled.
      # [Learn more](https://platform.openai.com/docs/guides/background).
      sig do
        params(
          response_id: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Responses::Response)
      end
      def cancel(
        response_id, # The ID of the response to cancel.
        request_options: {}
); end

      # See {OpenAI::Resources::Responses#stream_raw} for streaming counterpart.
      #
      # Creates a model response. Provide
      # [text](https://platform.openai.com/docs/guides/text) or
      # [image](https://platform.openai.com/docs/guides/images) inputs to generate
      # [text](https://platform.openai.com/docs/guides/text) or
      # [JSON](https://platform.openai.com/docs/guides/structured-outputs) outputs. Have
      # the model call your own
      # [custom code](https://platform.openai.com/docs/guides/function-calling) or use
      # built-in [tools](https://platform.openai.com/docs/guides/tools) like
      # [web search](https://platform.openai.com/docs/guides/tools-web-search) or
      # [file search](https://platform.openai.com/docs/guides/tools-file-search) to use
      # your own data as input for the model's response.
      sig do
        params(
          background: T.nilable(T::Boolean),
          include: T.nilable(
              T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol]
            ),
          input: OpenAI::Responses::ResponseCreateParams::Input::Variants,
          instructions: T.nilable(String),
          max_output_tokens: T.nilable(Integer),
          max_tool_calls: T.nilable(Integer),
          metadata: T.nilable(T::Hash[Symbol, String]),
          model: T.any(
              String,
              OpenAI::ChatModel::OrSymbol,
              OpenAI::ResponsesModel::ResponsesOnlyModel::OrSymbol
            ),
          parallel_tool_calls: T.nilable(T::Boolean),
          previous_response_id: T.nilable(String),
          prompt: T.nilable(OpenAI::Responses::ResponsePrompt::OrHash),
          prompt_cache_key: String,
          reasoning: T.nilable(OpenAI::Reasoning::OrHash),
          safety_identifier: String,
          service_tier: T.nilable(
              OpenAI::Responses::ResponseCreateParams::ServiceTier::OrSymbol
            ),
          store: T.nilable(T::Boolean),
          temperature: T.nilable(Float),
          text: T.any(
              OpenAI::Responses::ResponseTextConfig::OrHash,
              OpenAI::StructuredOutput::JsonSchemaConverter
            ),
          tool_choice: T.any(
              OpenAI::Responses::ToolChoiceOptions::OrSymbol,
              OpenAI::Responses::ToolChoiceTypes::OrHash,
              OpenAI::Responses::ToolChoiceFunction::OrHash,
              OpenAI::Responses::ToolChoiceMcp::OrHash
            ),
          tools: T::Array[
              T.any(
                OpenAI::Responses::FunctionTool::OrHash,
                OpenAI::Responses::FileSearchTool::OrHash,
                OpenAI::Responses::ComputerTool::OrHash,
                OpenAI::Responses::Tool::Mcp::OrHash,
                OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                OpenAI::Responses::Tool::ImageGeneration::OrHash,
                OpenAI::Responses::Tool::LocalShell::OrHash,
                OpenAI::Responses::WebSearchTool::OrHash
              )
            ],
          top_logprobs: T.nilable(Integer),
          top_p: T.nilable(Float),
          truncation: T.nilable(
              OpenAI::Responses::ResponseCreateParams::Truncation::OrSymbol
            ),
          user: String,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Responses::Response)
      end
      def create(
        background: nil, # Whether to run the model response in the background.
                         # [Learn more](https://platform.openai.com/docs/guides/background).
        include: nil, # Specify additional output data to include in the model response. Currently
                      # supported values are:
                      # - `code_interpreter_call.outputs`: Includes the outputs of python code execution
                      #   in code interpreter tool call items.
                      # - `computer_call_output.output.image_url`: Include image urls from the computer
                      #   call output.
                      # - `file_search_call.results`: Include the search results of the file search tool
                      #   call.
                      # - `message.input_image.image_url`: Include image urls from the input message.
                      # - `message.output_text.logprobs`: Include logprobs with assistant messages.
                      # - `reasoning.encrypted_content`: Includes an encrypted version of reasoning
                      #   tokens in reasoning item outputs. This enables reasoning items to be used in
                      #   multi-turn conversations when using the Responses API statelessly (like when
                      #   the `store` parameter is set to `false`, or when an organization is enrolled
                      #   in the zero data retention program).
        input: nil, # Text, image, or file inputs to the model, used to generate a response.
                    # Learn more:
                    # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                    # - [Image inputs](https://platform.openai.com/docs/guides/images)
                    # - [File inputs](https://platform.openai.com/docs/guides/pdf-files)
                    # - [Conversation state](https://platform.openai.com/docs/guides/conversation-state)
                    # - [Function calling](https://platform.openai.com/docs/guides/function-calling)
        instructions: nil, # A system (or developer) message inserted into the model's context.
                           # When using along with `previous_response_id`, the instructions from a previous
                           # response will not be carried over to the next response. This makes it simple to
                           # swap out system (or developer) messages in new responses.
        max_output_tokens: nil, # An upper bound for the number of tokens that can be generated for a response,
                                # including visible output tokens and
                                # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
        max_tool_calls: nil, # The maximum number of total calls to built-in tools that can be processed in a
                             # response. This maximum number applies across all built-in tool calls, not per
                             # individual tool. Any further attempts to call a tool by the model will be
                             # ignored.
        metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
        model: nil, # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
                    # wide range of models with different capabilities, performance characteristics,
                    # and price points. Refer to the
                    # [model guide](https://platform.openai.com/docs/models) to browse and compare
                    # available models.
        parallel_tool_calls: nil, # Whether to allow the model to run tool calls in parallel.
        previous_response_id: nil, # The unique ID of the previous response to the model. Use this to create
                                   # multi-turn conversations. Learn more about
                                   # [conversation state](https://platform.openai.com/docs/guides/conversation-state).
        prompt: nil, # Reference to a prompt template and its variables.
                     # [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).
        prompt_cache_key: nil, # Used by OpenAI to cache responses for similar requests to optimize your cache
                               # hit rates. Replaces the `user` field.
                               # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
        reasoning: nil, # **o-series models only**
                        # Configuration options for
                        # [reasoning models](https://platform.openai.com/docs/guides/reasoning).
        safety_identifier: nil, # A stable identifier used to help detect users of your application that may be
                                # violating OpenAI's usage policies. The IDs should be a string that uniquely
                                # identifies each user. We recommend hashing their username or email address, in
                                # order to avoid sending us any identifying information.
                                # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        service_tier: nil, # Specifies the processing type used for serving the request.
                           # - If set to 'auto', then the request will be processed with the service tier
                           #   configured in the Project settings. Unless otherwise configured, the Project
                           #   will use 'default'.
                           # - If set to 'default', then the request will be processed with the standard
                           #   pricing and performance for the selected model.
                           # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
                           #   'priority', then the request will be processed with the corresponding service
                           #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
                           #   Priority processing.
                           # - When not set, the default behavior is 'auto'.
                           # When the `service_tier` parameter is set, the response body will include the
                           # `service_tier` value based on the processing mode actually used to serve the
                           # request. This response value may be different from the value set in the
                           # parameter.
        store: nil, # Whether to store the generated model response for later retrieval via API.
        temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                          # make the output more random, while lower values like 0.2 will make it more
                          # focused and deterministic. We generally recommend altering this or `top_p` but
                          # not both.
        text: nil, # Configuration options for a text response from the model. Can be plain text or
                   # structured JSON data. Learn more:
                   # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                   # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
        tool_choice: nil, # How the model should select which tool (or tools) to use when generating a
                          # response. See the `tools` parameter to see how to specify which tools the model
                          # can call.
        tools: nil, # An array of tools the model may call while generating a response. You can
                    # specify which tool to use by setting the `tool_choice` parameter.
                    # The two categories of tools you can provide the model are:
                    # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
                    #   capabilities, like
                    #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
                    #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
                    #   Learn more about
                    #   [built-in tools](https://platform.openai.com/docs/guides/tools).
                    # - **Function calls (custom tools)**: Functions that are defined by you, enabling
                    #   the model to call your own code. Learn more about
                    #   [function calling](https://platform.openai.com/docs/guides/function-calling).
        top_logprobs: nil, # An integer between 0 and 20 specifying the number of most likely tokens to
                           # return at each token position, each with an associated log probability.
        top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                    # model considers the results of the tokens with top_p probability mass. So 0.1
                    # means only the tokens comprising the top 10% probability mass are considered.
                    # We generally recommend altering this or `temperature` but not both.
        truncation: nil, # The truncation strategy to use for the model response.
                         # - `auto`: If the context of this response and previous ones exceeds the model's
                         #   context window size, the model will truncate the response to fit the context
                         #   window by dropping input items in the middle of the conversation.
                         # - `disabled` (default): If a model response will exceed the context window size
                         #   for a model, the request will fail with a 400 error.
        user: nil, # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
                   # `prompt_cache_key` instead to maintain caching optimizations. A stable
                   # identifier for your end-users. Used to boost cache hit rates by better bucketing
                   # similar requests and to help OpenAI detect and prevent abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        stream: false, # There is no need to provide `stream:`. Instead, use `#stream_raw` or `#create`
                       # for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      # Deletes a model response with the given ID.
      sig { params(response_id: String, request_options: OpenAI::RequestOptions::OrHash).void }
      def delete(
        response_id, # The ID of the response to delete.
        request_options: {}
); end

      # See {OpenAI::Resources::Responses#retrieve_streaming} for streaming counterpart.
      #
      # Retrieves a model response with the given ID.
      sig do
        params(
          response_id: String,
          include: T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol],
          starting_after: Integer,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Responses::Response)
      end
      def retrieve(
        response_id, # The ID of the response to retrieve.
        include: nil, # Additional fields to include in the response. See the `include` parameter for
                      # Response creation above for more information.
        starting_after: nil, # The sequence number of the event after which to start streaming.
        stream: false, # There is no need to provide `stream:`. Instead, use `#retrieve_streaming` or
                       # `#retrieve` for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      # See {OpenAI::Resources::Responses#retrieve} for non-streaming counterpart.
      #
      # Retrieves a model response with the given ID.
      sig do
        params(
          response_id: String,
          include: T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol],
          starting_after: Integer,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::Stream[
            OpenAI::Responses::ResponseStreamEvent::Variants
          ])
      end
      def retrieve_streaming(
        response_id, # The ID of the response to retrieve.
        include: nil, # Additional fields to include in the response. See the `include` parameter for
                      # Response creation above for more information.
        starting_after: nil, # The sequence number of the event after which to start streaming.
        stream: true, # There is no need to provide `stream:`. Instead, use `#retrieve_streaming` or
                      # `#retrieve` for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      # See {OpenAI::Resources::Responses#create} for non-streaming counterpart.
      #
      # Creates a model response with a higher-level streaming interface that provides
      # helper methods for processing events and aggregating stream outputs.
      sig do
        params(
          input: T.nilable(OpenAI::Responses::ResponseCreateParams::Input::Variants),
          model: T.nilable(
              T.any(
                String,
                OpenAI::ChatModel::OrSymbol,
                OpenAI::ResponsesModel::ResponsesOnlyModel::OrSymbol
              )
            ),
          background: T.nilable(T::Boolean),
          include: T.nilable(
              T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol]
            ),
          instructions: T.nilable(String),
          max_output_tokens: T.nilable(Integer),
          metadata: T.nilable(T::Hash[Symbol, String]),
          parallel_tool_calls: T.nilable(T::Boolean),
          previous_response_id: T.nilable(String),
          prompt: T.nilable(OpenAI::Responses::ResponsePrompt::OrHash),
          reasoning: T.nilable(OpenAI::Reasoning::OrHash),
          service_tier: T.nilable(
              OpenAI::Responses::ResponseCreateParams::ServiceTier::OrSymbol
            ),
          store: T.nilable(T::Boolean),
          temperature: T.nilable(Float),
          text: T.any(
              OpenAI::Responses::ResponseTextConfig::OrHash,
              OpenAI::StructuredOutput::JsonSchemaConverter
            ),
          tool_choice: T.any(
              OpenAI::Responses::ToolChoiceOptions::OrSymbol,
              OpenAI::Responses::ToolChoiceTypes::OrHash,
              OpenAI::Responses::ToolChoiceFunction::OrHash
            ),
          tools: T.nilable(
              T::Array[
                T.any(
                  OpenAI::Responses::FunctionTool::OrHash,
                  OpenAI::Responses::FileSearchTool::OrHash,
                  OpenAI::Responses::ComputerTool::OrHash,
                  OpenAI::Responses::Tool::Mcp::OrHash,
                  OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                  OpenAI::Responses::Tool::ImageGeneration::OrHash,
                  OpenAI::Responses::Tool::LocalShell::OrHash,
                  OpenAI::Responses::WebSearchTool::OrHash,
                  OpenAI::StructuredOutput::JsonSchemaConverter
                )
              ]
            ),
          top_p: T.nilable(Float),
          truncation: T.nilable(
              OpenAI::Responses::ResponseCreateParams::Truncation::OrSymbol
            ),
          user: T.nilable(String),
          starting_after: T.nilable(Integer),
          request_options: T.nilable(OpenAI::RequestOptions::OrHash)
        ).returns(OpenAI::Streaming::ResponseStream)
      end
      def stream(
        input: nil, # Text, image, or file inputs to the model, used to generate a response.
        model: nil, # Model ID used to generate the response, like `gpt-4o` or `o3`.
        background: nil, # Whether to run the model response in the background.
        include: nil, # Specify additional output data to include in the model response.
        instructions: nil, # A system (or developer) message inserted into the model's context.
        max_output_tokens: nil, # An upper bound for the number of tokens that can be generated for a response.
        metadata: nil, # Set of 16 key-value pairs that can be attached to an object.
        parallel_tool_calls: nil, # Whether to allow the model to run tool calls in parallel.
        previous_response_id: nil, # The unique ID of the previous response to the model. Use this to create
                                   # multi-turn conversations.
        prompt: nil, # Reference to a prompt template and its variables.
        reasoning: nil, # Configuration options for reasoning models.
        service_tier: nil, # Specifies the latency tier to use for processing the request.
        store: nil, # Whether to store the generated model response for later retrieval via API.
        temperature: nil, # What sampling temperature to use, between 0 and 2.
        text: nil, # Configuration options for a text response from the model.
        tool_choice: nil, # How the model should select which tool (or tools) to use when generating a response.
        tools: nil, # An array of tools the model may call while generating a response.
        top_p: nil, # An alternative to sampling with temperature, called nucleus sampling.
        truncation: nil, # The truncation strategy to use for the model response.
        user: nil, # A stable identifier for your end-users.
        starting_after: nil, # The sequence number of the event after which to start streaming (for resuming streams).
        request_options: {}
); end

      # See {OpenAI::Resources::Responses#create} for non-streaming counterpart.
      #
      # Creates a model response. Provide
      # [text](https://platform.openai.com/docs/guides/text) or
      # [image](https://platform.openai.com/docs/guides/images) inputs to generate
      # [text](https://platform.openai.com/docs/guides/text) or
      # [JSON](https://platform.openai.com/docs/guides/structured-outputs) outputs. Have
      # the model call your own
      # [custom code](https://platform.openai.com/docs/guides/function-calling) or use
      # built-in [tools](https://platform.openai.com/docs/guides/tools) like
      # [web search](https://platform.openai.com/docs/guides/tools-web-search) or
      # [file search](https://platform.openai.com/docs/guides/tools-file-search) to use
      # your own data as input for the model's response.
      sig do
        params(
          background: T.nilable(T::Boolean),
          include: T.nilable(
              T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol]
            ),
          input: OpenAI::Responses::ResponseCreateParams::Input::Variants,
          instructions: T.nilable(String),
          max_output_tokens: T.nilable(Integer),
          max_tool_calls: T.nilable(Integer),
          metadata: T.nilable(T::Hash[Symbol, String]),
          model: T.any(
              String,
              OpenAI::ChatModel::OrSymbol,
              OpenAI::ResponsesModel::ResponsesOnlyModel::OrSymbol
            ),
          parallel_tool_calls: T.nilable(T::Boolean),
          previous_response_id: T.nilable(String),
          prompt: T.nilable(OpenAI::Responses::ResponsePrompt::OrHash),
          prompt_cache_key: String,
          reasoning: T.nilable(OpenAI::Reasoning::OrHash),
          safety_identifier: String,
          service_tier: T.nilable(
              OpenAI::Responses::ResponseCreateParams::ServiceTier::OrSymbol
            ),
          store: T.nilable(T::Boolean),
          temperature: T.nilable(Float),
          text: T.nilable(
              T.any(
                OpenAI::Responses::ResponseTextConfig::OrHash,
                OpenAI::StructuredOutput::JsonSchemaConverter
              )
            ),
          tool_choice: T.any(
              OpenAI::Responses::ToolChoiceOptions::OrSymbol,
              OpenAI::Responses::ToolChoiceTypes::OrHash,
              OpenAI::Responses::ToolChoiceFunction::OrHash,
              OpenAI::Responses::ToolChoiceMcp::OrHash
            ),
          tools: T::Array[
              T.any(
                OpenAI::Responses::FunctionTool::OrHash,
                OpenAI::Responses::FileSearchTool::OrHash,
                OpenAI::Responses::ComputerTool::OrHash,
                OpenAI::Responses::Tool::Mcp::OrHash,
                OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                OpenAI::Responses::Tool::ImageGeneration::OrHash,
                OpenAI::Responses::Tool::LocalShell::OrHash,
                OpenAI::Responses::WebSearchTool::OrHash
              )
            ],
          top_logprobs: T.nilable(Integer),
          top_p: T.nilable(Float),
          truncation: T.nilable(
              OpenAI::Responses::ResponseCreateParams::Truncation::OrSymbol
            ),
          user: String,
          stream: T.noreturn,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::Stream[
            OpenAI::Responses::ResponseStreamEvent::Variants
          ])
      end
      def stream_raw(
        background: nil, # Whether to run the model response in the background.
                         # [Learn more](https://platform.openai.com/docs/guides/background).
        include: nil, # Specify additional output data to include in the model response. Currently
                      # supported values are:
                      # - `code_interpreter_call.outputs`: Includes the outputs of python code execution
                      #   in code interpreter tool call items.
                      # - `computer_call_output.output.image_url`: Include image urls from the computer
                      #   call output.
                      # - `file_search_call.results`: Include the search results of the file search tool
                      #   call.
                      # - `message.input_image.image_url`: Include image urls from the input message.
                      # - `message.output_text.logprobs`: Include logprobs with assistant messages.
                      # - `reasoning.encrypted_content`: Includes an encrypted version of reasoning
                      #   tokens in reasoning item outputs. This enables reasoning items to be used in
                      #   multi-turn conversations when using the Responses API statelessly (like when
                      #   the `store` parameter is set to `false`, or when an organization is enrolled
                      #   in the zero data retention program).
        input: nil, # Text, image, or file inputs to the model, used to generate a response.
                    # Learn more:
                    # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                    # - [Image inputs](https://platform.openai.com/docs/guides/images)
                    # - [File inputs](https://platform.openai.com/docs/guides/pdf-files)
                    # - [Conversation state](https://platform.openai.com/docs/guides/conversation-state)
                    # - [Function calling](https://platform.openai.com/docs/guides/function-calling)
        instructions: nil, # A system (or developer) message inserted into the model's context.
                           # When using along with `previous_response_id`, the instructions from a previous
                           # response will not be carried over to the next response. This makes it simple to
                           # swap out system (or developer) messages in new responses.
        max_output_tokens: nil, # An upper bound for the number of tokens that can be generated for a response,
                                # including visible output tokens and
                                # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
        max_tool_calls: nil, # The maximum number of total calls to built-in tools that can be processed in a
                             # response. This maximum number applies across all built-in tool calls, not per
                             # individual tool. Any further attempts to call a tool by the model will be
                             # ignored.
        metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
        model: nil, # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
                    # wide range of models with different capabilities, performance characteristics,
                    # and price points. Refer to the
                    # [model guide](https://platform.openai.com/docs/models) to browse and compare
                    # available models.
        parallel_tool_calls: nil, # Whether to allow the model to run tool calls in parallel.
        previous_response_id: nil, # The unique ID of the previous response to the model. Use this to create
                                   # multi-turn conversations. Learn more about
                                   # [conversation state](https://platform.openai.com/docs/guides/conversation-state).
        prompt: nil, # Reference to a prompt template and its variables.
                     # [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).
        prompt_cache_key: nil, # Used by OpenAI to cache responses for similar requests to optimize your cache
                               # hit rates. Replaces the `user` field.
                               # [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
        reasoning: nil, # **o-series models only**
                        # Configuration options for
                        # [reasoning models](https://platform.openai.com/docs/guides/reasoning).
        safety_identifier: nil, # A stable identifier used to help detect users of your application that may be
                                # violating OpenAI's usage policies. The IDs should be a string that uniquely
                                # identifies each user. We recommend hashing their username or email address, in
                                # order to avoid sending us any identifying information.
                                # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        service_tier: nil, # Specifies the processing type used for serving the request.
                           # - If set to 'auto', then the request will be processed with the service tier
                           #   configured in the Project settings. Unless otherwise configured, the Project
                           #   will use 'default'.
                           # - If set to 'default', then the request will be processed with the standard
                           #   pricing and performance for the selected model.
                           # - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
                           #   'priority', then the request will be processed with the corresponding service
                           #   tier. [Contact sales](https://openai.com/contact-sales) to learn more about
                           #   Priority processing.
                           # - When not set, the default behavior is 'auto'.
                           # When the `service_tier` parameter is set, the response body will include the
                           # `service_tier` value based on the processing mode actually used to serve the
                           # request. This response value may be different from the value set in the
                           # parameter.
        store: nil, # Whether to store the generated model response for later retrieval via API.
        temperature: nil, # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
                          # make the output more random, while lower values like 0.2 will make it more
                          # focused and deterministic. We generally recommend altering this or `top_p` but
                          # not both.
        text: nil, # Configuration options for a text response from the model. Can be plain text or
                   # structured JSON data. Learn more:
                   # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
                   # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
        tool_choice: nil, # How the model should select which tool (or tools) to use when generating a
                          # response. See the `tools` parameter to see how to specify which tools the model
                          # can call.
        tools: nil, # An array of tools the model may call while generating a response. You can
                    # specify which tool to use by setting the `tool_choice` parameter.
                    # The two categories of tools you can provide the model are:
                    # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
                    #   capabilities, like
                    #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
                    #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
                    #   Learn more about
                    #   [built-in tools](https://platform.openai.com/docs/guides/tools).
                    # - **Function calls (custom tools)**: Functions that are defined by you, enabling
                    #   the model to call your own code. Learn more about
                    #   [function calling](https://platform.openai.com/docs/guides/function-calling).
        top_logprobs: nil, # An integer between 0 and 20 specifying the number of most likely tokens to
                           # return at each token position, each with an associated log probability.
        top_p: nil, # An alternative to sampling with temperature, called nucleus sampling, where the
                    # model considers the results of the tokens with top_p probability mass. So 0.1
                    # means only the tokens comprising the top 10% probability mass are considered.
                    # We generally recommend altering this or `temperature` but not both.
        truncation: nil, # The truncation strategy to use for the model response.
                         # - `auto`: If the context of this response and previous ones exceeds the model's
                         #   context window size, the model will truncate the response to fit the context
                         #   window by dropping input items in the middle of the conversation.
                         # - `disabled` (default): If a model response will exceed the context window size
                         #   for a model, the request will fail with a 400 error.
        user: nil, # This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
                   # `prompt_cache_key` instead to maintain caching optimizations. A stable
                   # identifier for your end-users. Used to boost cache hit rates by better bucketing
                   # similar requests and to help OpenAI detect and prevent abuse.
                   # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
        stream: true, # There is no need to provide `stream:`. Instead, use `#stream_raw` or `#create`
                      # for streaming and non-streaming use cases, respectively.
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class InputItems
        # Returns a list of input items for a given response.
        sig do
          params(
            response_id: String,
            after: String,
            before: String,
            include: T::Array[OpenAI::Responses::ResponseIncludable::OrSymbol],
            limit: Integer,
            order: OpenAI::Responses::InputItemListParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::CursorPage[
              OpenAI::Responses::ResponseItem::Variants
            ])
        end
        def list(
          response_id, # The ID of the response to retrieve input items for.
          after: nil, # An item ID to list items after, used in pagination.
          before: nil, # An item ID to list items before, used in pagination.
          include: nil, # Additional fields to include in the response. See the `include` parameter for
                        # Response creation above for more information.
          limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                      # 100, and the default is 20.
          order: nil, # The order to return the input items in. Default is `desc`.
                      # - `asc`: Return the input items in ascending order.
                      # - `desc`: Return the input items in descending order.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end
    end

    class Uploads
      sig { returns(OpenAI::Resources::Uploads::Parts) }
      attr_reader :parts

      # Cancels the Upload. No Parts may be added after an Upload is cancelled.
      sig { params(upload_id: String, request_options: OpenAI::RequestOptions::OrHash).returns(OpenAI::Upload) }
      def cancel(
        upload_id, # The ID of the Upload.
        request_options: {}
); end

      # Completes the
      # [Upload](https://platform.openai.com/docs/api-reference/uploads/object).
      #
      # Within the returned Upload object, there is a nested
      # [File](https://platform.openai.com/docs/api-reference/files/object) object that
      # is ready to use in the rest of the platform.
      #
      # You can specify the order of the Parts by passing in an ordered list of the Part
      # IDs.
      #
      # The number of bytes uploaded upon completion must match the number of bytes
      # initially specified when creating the Upload object. No Parts may be added after
      # an Upload is completed.
      sig do
        params(
          upload_id: String,
          part_ids: T::Array[String],
          md5: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Upload)
      end
      def complete(
        upload_id, # The ID of the Upload.
        part_ids:, # The ordered list of Part IDs.
        md5: nil, # The optional md5 checksum for the file contents to verify if the bytes uploaded
                  # matches what you expect.
        request_options: {}
); end

      # Creates an intermediate
      # [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object
      # that you can add
      # [Parts](https://platform.openai.com/docs/api-reference/uploads/part-object) to.
      # Currently, an Upload can accept at most 8 GB in total and expires after an hour
      # after you create it.
      #
      # Once you complete the Upload, we will create a
      # [File](https://platform.openai.com/docs/api-reference/files/object) object that
      # contains all the parts you uploaded. This File is usable in the rest of our
      # platform as a regular File object.
      #
      # For certain `purpose` values, the correct `mime_type` must be specified. Please
      # refer to documentation for the
      # [supported MIME types for your use case](https://platform.openai.com/docs/assistants/tools/file-search#supported-files).
      #
      # For guidance on the proper filename extensions for each purpose, please follow
      # the documentation on
      # [creating a File](https://platform.openai.com/docs/api-reference/files/create).
      sig do
        params(
          bytes: Integer,
          filename: String,
          mime_type: String,
          purpose: OpenAI::FilePurpose::OrSymbol,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Upload)
      end
      def create(
        bytes:, # The number of bytes in the file you are uploading.
        filename:, # The name of the file to upload.
        mime_type:, # The MIME type of the file.
                    # This must fall within the supported MIME types for your file purpose. See the
                    # supported MIME types for assistants and vision.
        purpose:, # The intended purpose of the uploaded file.
                  # See the
                  # [documentation on File purposes](https://platform.openai.com/docs/api-reference/files/create#files-create-purpose).
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class Parts
        # Adds a
        # [Part](https://platform.openai.com/docs/api-reference/uploads/part-object) to an
        # [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object.
        # A Part represents a chunk of bytes from the file you are trying to upload.
        #
        # Each Part can be at most 64 MB, and you can add Parts until you hit the Upload
        # maximum of 8 GB.
        #
        # It is possible to add multiple Parts in parallel. You can decide the intended
        # order of the Parts when you
        # [complete the Upload](https://platform.openai.com/docs/api-reference/uploads/complete).
        sig do
          params(
            upload_id: String,
            data: OpenAI::Internal::FileInput,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Uploads::UploadPart)
        end
        def create(
          upload_id, # The ID of the Upload.
          data:, # The chunk of bytes for this Part.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end
    end

    class VectorStores
      sig { returns(OpenAI::Resources::VectorStores::FileBatches) }
      attr_reader :file_batches

      sig { returns(OpenAI::Resources::VectorStores::Files) }
      attr_reader :files

      # Create a vector store.
      sig do
        params(
          chunking_strategy: T.any(
              OpenAI::AutoFileChunkingStrategyParam::OrHash,
              OpenAI::StaticFileChunkingStrategyObjectParam::OrHash
            ),
          expires_after: OpenAI::VectorStoreCreateParams::ExpiresAfter::OrHash,
          file_ids: T::Array[String],
          metadata: T.nilable(T::Hash[Symbol, String]),
          name: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::VectorStore)
      end
      def create(
        chunking_strategy: nil, # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                                # strategy. Only applicable if `file_ids` is non-empty.
        expires_after: nil, # The expiration policy for a vector store.
        file_ids: nil, # A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that
                       # the vector store should use. Useful for tools like `file_search` that can access
                       # files.
        metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
        name: nil, # The name of the vector store.
        request_options: {}
); end

      # Delete a vector store.
      sig do
        params(
          vector_store_id: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::VectorStoreDeleted)
      end
      def delete(
        vector_store_id, # The ID of the vector store to delete.
        request_options: {}
); end

      # Returns a list of vector stores.
      sig do
        params(
          after: String,
          before: String,
          limit: Integer,
          order: OpenAI::VectorStoreListParams::Order::OrSymbol,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::CursorPage[OpenAI::VectorStore])
      end
      def list(
        after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                    # in the list. For instance, if you make a list request and receive 100 objects,
                    # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                    # fetch the next page of the list.
        before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                     # in the list. For instance, if you make a list request and receive 100 objects,
                     # starting with obj_foo, your subsequent call can include before=obj_foo in order
                     # to fetch the previous page of the list.
        limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                    # 100, and the default is 20.
        order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                    # order and `desc` for descending order.
        request_options: {}
); end

      # Retrieves a vector store.
      sig do
        params(
          vector_store_id: String,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::VectorStore)
      end
      def retrieve(
        vector_store_id, # The ID of the vector store to retrieve.
        request_options: {}
); end

      # Search a vector store for relevant chunks based on a query and file attributes
      # filter.
      sig do
        params(
          vector_store_id: String,
          query: OpenAI::VectorStoreSearchParams::Query::Variants,
          filters: T.any(
              OpenAI::ComparisonFilter::OrHash,
              OpenAI::CompoundFilter::OrHash
            ),
          max_num_results: Integer,
          ranking_options: OpenAI::VectorStoreSearchParams::RankingOptions::OrHash,
          rewrite_query: T::Boolean,
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::Internal::Page[OpenAI::Models::VectorStoreSearchResponse])
      end
      def search(
        vector_store_id, # The ID of the vector store to search.
        query:, # A query string for a search
        filters: nil, # A filter to apply based on file attributes.
        max_num_results: nil, # The maximum number of results to return. This number should be between 1 and 50
                              # inclusive.
        ranking_options: nil, # Ranking options for search.
        rewrite_query: nil, # Whether to rewrite the natural language query for vector search.
        request_options: {}
); end

      # Modifies a vector store.
      sig do
        params(
          vector_store_id: String,
          expires_after: T.nilable(OpenAI::VectorStoreUpdateParams::ExpiresAfter::OrHash),
          metadata: T.nilable(T::Hash[Symbol, String]),
          name: T.nilable(String),
          request_options: OpenAI::RequestOptions::OrHash
        ).returns(OpenAI::VectorStore)
      end
      def update(
        vector_store_id, # The ID of the vector store to modify.
        expires_after: nil, # The expiration policy for a vector store.
        metadata: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                       # for storing additional information about the object in a structured format, and
                       # querying for objects via API or the dashboard.
                       # Keys are strings with a maximum length of 64 characters. Values are strings with
                       # a maximum length of 512 characters.
        name: nil, # The name of the vector store.
        request_options: {}
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end

      class FileBatches
        # Cancel a vector store file batch. This attempts to cancel the processing of
        # files in this batch as soon as possible.
        sig do
          params(
            batch_id: String,
            vector_store_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::VectorStores::VectorStoreFileBatch)
        end
        def cancel(
          batch_id, # The ID of the file batch to cancel.
          vector_store_id:, # The ID of the vector store that the file batch belongs to.
          request_options: {}
); end

        # Create a vector store file batch.
        sig do
          params(
            vector_store_id: String,
            file_ids: T::Array[String],
            attributes: T.nilable(
                T::Hash[
                  Symbol,
                  OpenAI::VectorStores::FileBatchCreateParams::Attribute::Variants
                ]
              ),
            chunking_strategy: T.any(
                OpenAI::AutoFileChunkingStrategyParam::OrHash,
                OpenAI::StaticFileChunkingStrategyObjectParam::OrHash
              ),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::VectorStores::VectorStoreFileBatch)
        end
        def create(
          vector_store_id, # The ID of the vector store for which to create a File Batch.
          file_ids:, # A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that
                     # the vector store should use. Useful for tools like `file_search` that can access
                     # files.
          attributes: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard. Keys are strings with a maximum
                           # length of 64 characters. Values are strings with a maximum length of 512
                           # characters, booleans, or numbers.
          chunking_strategy: nil, # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                                  # strategy. Only applicable if `file_ids` is non-empty.
          request_options: {}
); end

        # Returns a list of vector store files in a batch.
        sig do
          params(
            batch_id: String,
            vector_store_id: String,
            after: String,
            before: String,
            filter: OpenAI::VectorStores::FileBatchListFilesParams::Filter::OrSymbol,
            limit: Integer,
            order: OpenAI::VectorStores::FileBatchListFilesParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::CursorPage[OpenAI::VectorStores::VectorStoreFile])
        end
        def list_files(
          batch_id, # Path param: The ID of the file batch that the files belong to.
          vector_store_id:, # Path param: The ID of the vector store that the files belong to.
          after: nil, # Query param: A cursor for use in pagination. `after` is an object ID that
                      # defines your place in the list. For instance, if you make a list request and
                      # receive 100 objects, ending with obj_foo, your subsequent call can include
                      # after=obj_foo in order to fetch the next page of the list.
          before: nil, # Query param: A cursor for use in pagination. `before` is an object ID that
                       # defines your place in the list. For instance, if you make a list request and
                       # receive 100 objects, starting with obj_foo, your subsequent call can include
                       # before=obj_foo in order to fetch the previous page of the list.
          filter: nil, # Query param: Filter by file status. One of `in_progress`, `completed`, `failed`,
                       # `cancelled`.
          limit: nil, # Query param: A limit on the number of objects to be returned. Limit can range
                      # between 1 and 100, and the default is 20.
          order: nil, # Query param: Sort order by the `created_at` timestamp of the objects. `asc` for
                      # ascending order and `desc` for descending order.
          request_options: {}
); end

        # Retrieves a vector store file batch.
        sig do
          params(
            batch_id: String,
            vector_store_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::VectorStores::VectorStoreFileBatch)
        end
        def retrieve(
          batch_id, # The ID of the file batch being retrieved.
          vector_store_id:, # The ID of the vector store that the file batch belongs to.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end

      class Files
        # Retrieve the parsed contents of a vector store file.
        sig do
          params(
            file_id: String,
            vector_store_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::Page[
              OpenAI::Models::VectorStores::FileContentResponse
            ])
        end
        def content(
          file_id, # The ID of the file within the vector store.
          vector_store_id:, # The ID of the vector store.
          request_options: {}
); end

        # Create a vector store file by attaching a
        # [File](https://platform.openai.com/docs/api-reference/files) to a
        # [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object).
        sig do
          params(
            vector_store_id: String,
            file_id: String,
            attributes: T.nilable(
                T::Hash[
                  Symbol,
                  OpenAI::VectorStores::FileCreateParams::Attribute::Variants
                ]
              ),
            chunking_strategy: T.any(
                OpenAI::AutoFileChunkingStrategyParam::OrHash,
                OpenAI::StaticFileChunkingStrategyObjectParam::OrHash
              ),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::VectorStores::VectorStoreFile)
        end
        def create(
          vector_store_id, # The ID of the vector store for which to create a File.
          file_id:, # A [File](https://platform.openai.com/docs/api-reference/files) ID that the
                    # vector store should use. Useful for tools like `file_search` that can access
                    # files.
          attributes: nil, # Set of 16 key-value pairs that can be attached to an object. This can be useful
                           # for storing additional information about the object in a structured format, and
                           # querying for objects via API or the dashboard. Keys are strings with a maximum
                           # length of 64 characters. Values are strings with a maximum length of 512
                           # characters, booleans, or numbers.
          chunking_strategy: nil, # The chunking strategy used to chunk the file(s). If not set, will use the `auto`
                                  # strategy. Only applicable if `file_ids` is non-empty.
          request_options: {}
); end

        # Delete a vector store file. This will remove the file from the vector store but
        # the file itself will not be deleted. To delete the file, use the
        # [delete file](https://platform.openai.com/docs/api-reference/files/delete)
        # endpoint.
        sig do
          params(
            file_id: String,
            vector_store_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::VectorStores::VectorStoreFileDeleted)
        end
        def delete(
          file_id, # The ID of the file to delete.
          vector_store_id:, # The ID of the vector store that the file belongs to.
          request_options: {}
); end

        # Returns a list of vector store files.
        sig do
          params(
            vector_store_id: String,
            after: String,
            before: String,
            filter: OpenAI::VectorStores::FileListParams::Filter::OrSymbol,
            limit: Integer,
            order: OpenAI::VectorStores::FileListParams::Order::OrSymbol,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::Internal::CursorPage[OpenAI::VectorStores::VectorStoreFile])
        end
        def list(
          vector_store_id, # The ID of the vector store that the files belong to.
          after: nil, # A cursor for use in pagination. `after` is an object ID that defines your place
                      # in the list. For instance, if you make a list request and receive 100 objects,
                      # ending with obj_foo, your subsequent call can include after=obj_foo in order to
                      # fetch the next page of the list.
          before: nil, # A cursor for use in pagination. `before` is an object ID that defines your place
                       # in the list. For instance, if you make a list request and receive 100 objects,
                       # starting with obj_foo, your subsequent call can include before=obj_foo in order
                       # to fetch the previous page of the list.
          filter: nil, # Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.
          limit: nil, # A limit on the number of objects to be returned. Limit can range between 1 and
                      # 100, and the default is 20.
          order: nil, # Sort order by the `created_at` timestamp of the objects. `asc` for ascending
                      # order and `desc` for descending order.
          request_options: {}
); end

        # Retrieves a vector store file.
        sig do
          params(
            file_id: String,
            vector_store_id: String,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::VectorStores::VectorStoreFile)
        end
        def retrieve(
          file_id, # The ID of the file being retrieved.
          vector_store_id:, # The ID of the vector store that the file belongs to.
          request_options: {}
); end

        # Update attributes on a vector store file.
        sig do
          params(
            file_id: String,
            vector_store_id: String,
            attributes: T.nilable(
                T::Hash[
                  Symbol,
                  OpenAI::VectorStores::FileUpdateParams::Attribute::Variants
                ]
              ),
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(OpenAI::VectorStores::VectorStoreFile)
        end
        def update(
          file_id, # Path param: The ID of the file to update attributes.
          vector_store_id:, # Path param: The ID of the vector store the file belongs to.
          attributes:, # Body param: Set of 16 key-value pairs that can be attached to an object. This
                       # can be useful for storing additional information about the object in a
                       # structured format, and querying for objects via API or the dashboard. Keys are
                       # strings with a maximum length of 64 characters. Values are strings with a
                       # maximum length of 512 characters, booleans, or numbers.
          request_options: {}
); end

        class << self
          # @api private
          sig { params(client: OpenAI::Client).returns(T.attached_class) }
          def new(client:); end
        end
      end
    end

    class Webhooks
      # Validates that the given payload was sent by OpenAI and parses the payload.
      sig do
        params(
          payload: String,
          headers: T.nilable(T::Hash[T.any(String, Symbol), String]),
          webhook_secret: T.nilable(String)
        ).returns(T.any(
            OpenAI::Webhooks::BatchCancelledWebhookEvent,
            OpenAI::Webhooks::BatchCompletedWebhookEvent,
            OpenAI::Webhooks::BatchExpiredWebhookEvent,
            OpenAI::Webhooks::BatchFailedWebhookEvent,
            OpenAI::Webhooks::EvalRunCanceledWebhookEvent,
            OpenAI::Webhooks::EvalRunFailedWebhookEvent,
            OpenAI::Webhooks::EvalRunSucceededWebhookEvent,
            OpenAI::Webhooks::FineTuningJobCancelledWebhookEvent,
            OpenAI::Webhooks::FineTuningJobFailedWebhookEvent,
            OpenAI::Webhooks::FineTuningJobSucceededWebhookEvent,
            OpenAI::Webhooks::ResponseCancelledWebhookEvent,
            OpenAI::Webhooks::ResponseCompletedWebhookEvent,
            OpenAI::Webhooks::ResponseFailedWebhookEvent,
            OpenAI::Webhooks::ResponseIncompleteWebhookEvent
          ))
      end
      def unwrap(
        payload, # The raw webhook payload as a string
        headers = {}, # The webhook headers
        webhook_secret = nil # The webhook secret (optional, will use ENV["OPENAI_WEBHOOK_SECRET"] if not provided)
); end

      # Validates whether or not the webhook payload was sent by OpenAI.
      sig do
        params(
          payload: String,
          headers: T::Hash[T.any(String, Symbol), String],
          webhook_secret: T.nilable(String),
          tolerance: Integer
        ).void
      end
      def verify_signature(
        payload, # The webhook payload as a string
        headers, # The webhook headers
        webhook_secret = nil, # The webhook secret (optional, will use ENV["OPENAI_WEBHOOK_SECRET"] if not provided)
        tolerance = 300 # Maximum age of the webhook in seconds (default: 300 = 5 minutes)
); end

      class << self
        # @api private
        sig { params(client: OpenAI::Client).returns(T.attached_class) }
        def new(client:); end
      end
    end
  end

  ResponseFormatJSONObject = OpenAI::Models::ResponseFormatJSONObject
  ResponseFormatJSONSchema = OpenAI::Models::ResponseFormatJSONSchema
  ResponseFormatText = OpenAI::Models::ResponseFormatText
  Responses = OpenAI::Models::Responses
  ResponsesModel = OpenAI::Models::ResponsesModel
  StaticFileChunkingStrategy = OpenAI::Models::StaticFileChunkingStrategy

  StaticFileChunkingStrategyObject = OpenAI::Models::StaticFileChunkingStrategyObject

  StaticFileChunkingStrategyObjectParam = OpenAI::Models::StaticFileChunkingStrategyObjectParam

  Streaming = OpenAI::Helpers::Streaming
  StructuredOutput = OpenAI::Helpers::StructuredOutput
  UnionOf = OpenAI::Helpers::StructuredOutput::UnionOf
  Upload = OpenAI::Models::Upload
  UploadCancelParams = OpenAI::Models::UploadCancelParams
  UploadCompleteParams = OpenAI::Models::UploadCompleteParams
  UploadCreateParams = OpenAI::Models::UploadCreateParams
  Uploads = OpenAI::Models::Uploads
  VERSION = T.let(T.unsafe(nil), String)
  VectorStore = OpenAI::Models::VectorStore
  VectorStoreCreateParams = OpenAI::Models::VectorStoreCreateParams
  VectorStoreDeleteParams = OpenAI::Models::VectorStoreDeleteParams
  VectorStoreDeleted = OpenAI::Models::VectorStoreDeleted
  VectorStoreListParams = OpenAI::Models::VectorStoreListParams
  VectorStoreRetrieveParams = OpenAI::Models::VectorStoreRetrieveParams
  VectorStoreSearchParams = OpenAI::Models::VectorStoreSearchParams
  VectorStoreUpdateParams = OpenAI::Models::VectorStoreUpdateParams
  VectorStores = OpenAI::Models::VectorStores
  Webhooks = OpenAI::Models::Webhooks
end
